{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "zIMZtN2_SB6c",
        "outputId": "2598e5ef-314b-4ff6-8d77-0ef9ca16c384"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a directed graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Define the entities and relations\n",
        "entities = [\n",
        "    \"electronic health records\",\n",
        "    \"patient care\",\n",
        "    \"healthcare providers\",\n",
        "    \"medical history\",\n",
        "    \"treatment plans\",\n",
        "    \"diagnostic tests\"\n",
        "]\n",
        "\n",
        "relations = [\n",
        "    (\"electronic health records\", \"patient care\", {\"relation\": \"IMPROVE\"}),\n",
        "    (\"healthcare providers\", \"electronic health records\", {\"relation\": \"ACCESS\"}),\n",
        "    (\"electronic health records\", \"medical history\", {\"relation\": \"CONTAIN\"}),\n",
        "    (\"electronic health records\", \"treatment plans\", {\"relation\": \"STORE\"}),\n",
        "    (\"electronic health records\", \"diagnostic tests\", {\"relation\": \"INCLUDE\"}),\n",
        "    (\"healthcare providers\", \"patient care\", {\"relation\": \"provide\"})\n",
        "]\n",
        "\n",
        "# Add nodes and edges to the graph\n",
        "G.add_nodes_from(entities)\n",
        "G.add_edges_from(relations)\n",
        "\n",
        "# Define the layout\n",
        "pos = {\n",
        "    \"patient care\": (1, 2),\n",
        "    \"IMPROVE\": (1, 1),\n",
        "    \"electronic health records\": (1, 0),\n",
        "    \"ACCESS\": (0, -1),\n",
        "    \"CONTAIN\": (1, -1),\n",
        "    \"STORE\": (2, -1),\n",
        "    \"INCLUDE\": (3, -1),\n",
        "    \"healthcare providers\": (0, -2),\n",
        "    \"medical history\": (1, -2),\n",
        "    \"treatment plans\": (2, -2),\n",
        "    \"diagnostic tests\": (3, -2)\n",
        "}\n",
        "\n",
        "# Draw the graph\n",
        "plt.figure(figsize=(10, 7))  # Adjust the figure size\n",
        "nx.draw_networkx(G, pos, with_labels=True, node_size=2000, node_color='lightblue', font_size=10, font_weight='bold')\n",
        "\n",
        "# Add edge labels\n",
        "labels = nx.get_edge_attributes(G, 'relation')\n",
        "nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)\n",
        "\n",
        "# Display the graph\n",
        "plt.title(\"Graph of Entities and Relations\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "#preprocessed data\n",
        "#Improving Patient Care Through Electronic Health Records\n",
        "#electronic health records ; patient care ; healthcare providers ;\n",
        "# medical history ; treatment plans ; diagnostic tests  <material> <task> <material> <material> <material> <material>\n",
        "# electronic health records -- IMPROVE -- patient care ; healthcare providers -- ACCESS -- electronic health records ;\n",
        "# electronic health records -- CONTAIN -- medical history ; electronic health records -- STORE -- treatment plans ;\n",
        "#electronic health records -- INCLUDE -- diagnostic tests   This study explores the impact of electronic health records (EHRs) on patient care in the healthcare industry.\n",
        "#EHRs enable healthcare providers to efficiently access and manage patients' medical history, treatment plans, and diagnostic test results. By improving access to comprehensive patient information, EHRs contribute to better-informed decision-making and more effective healthcare delivery."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "MfLpjOiuSB95",
        "outputId": "1fceb601-3f37-40e9-d9d2-015efc09fcb7"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1400x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a disconnected labeled graph\n",
        "G_labeled = nx.Graph()\n",
        "G_labeled.add_nodes_from([\"EHRs\", \"patient care\", \"healthcare providers\", \"medical history\", \"treatment plans\", \"diagnostic tests\"])\n",
        "G_labeled.add_edges_from([(\"EHRs\", \"patient care\", {\"label\": \"IMPROVE\"}),\n",
        "                          (\"healthcare providers\", \"EHRs\", {\"label\": \"ACCESS\"}),\n",
        "                          (\"EHRs\", \"medical history\", {\"label\": \"CONTAIN\"}),\n",
        "                          (\"EHRs\", \"treatment plans\", {\"label\": \"STORE\"}),\n",
        "                          (\"EHRs\", \"diagnostic tests\", {\"label\": \"INCLUDE\"})])\n",
        "\n",
        "# Create a connected unlabeled graph with edge labels\n",
        "G_unlabeled = nx.Graph()\n",
        "G_unlabeled.add_nodes_from([\"EHRs\", \"patient care\", \"healthcare providers\", \"medical history\", \"treatment plans\", \"diagnostic tests\", \"IMPROVE\", \"ACCESS\", \"CONTAIN\", \"STORE\", \"INCLUDE\", \"global\"])\n",
        "G_unlabeled.add_edges_from([(\"EHRs\", \"IMPROVE\", {\"label\": \"IMPROVE\"}),\n",
        "                            (\"patient care\", \"IMPROVE\"),\n",
        "                            (\"IMPROVE\", \"global\"),\n",
        "                            (\"healthcare providers\", \"ACCESS\", {\"label\": \"ACCESS\"}),\n",
        "                            (\"EHRs\", \"ACCESS\"),\n",
        "                            (\"ACCESS\", \"global\"),\n",
        "                            (\"EHRs\", \"CONTAIN\", {\"label\": \"CONTAIN\"}),\n",
        "                            (\"medical history\", \"CONTAIN\"),\n",
        "                            (\"CONTAIN\", \"global\"),\n",
        "                            (\"EHRs\", \"STORE\", {\"label\": \"STORE\"}),\n",
        "                            (\"treatment plans\", \"STORE\"),\n",
        "                            (\"STORE\", \"global\"),\n",
        "                            (\"EHRs\", \"INCLUDE\", {\"label\": \"INCLUDE\"}),\n",
        "                            (\"diagnostic tests\", \"INCLUDE\"),\n",
        "                            (\"INCLUDE\", \"global\"),\n",
        "                            (\"global\", \"global\")])\n",
        "\n",
        "# Draw the labeled graph\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.subplot(121)\n",
        "pos_labeled = nx.spring_layout(G_labeled)\n",
        "nx.draw(G_labeled, pos_labeled, with_labels=True, node_size=1000, node_color=\"skyblue\",\n",
        "        font_size=12, font_weight=\"bold\", alpha=0.7, width=2, edge_color=\"gray\")\n",
        "nx.draw_networkx_edge_labels(G_labeled, pos_labeled, edge_labels=nx.get_edge_attributes(G_labeled, 'label'))\n",
        "plt.title(\"Disconnected Labeled Graph\")\n",
        "\n",
        "# Draw the unlabeled graph\n",
        "plt.subplot(122)\n",
        "pos_unlabeled = nx.spring_layout(G_unlabeled)\n",
        "nx.draw(G_unlabeled, pos_unlabeled, with_labels=True, node_size=1000, node_color=\"lightgreen\",\n",
        "        font_size=12, font_weight=\"bold\", alpha=0.7, width=2, edge_color=\"gray\")\n",
        "plt.title(\"Connected Unlabeled Graph\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6UBJ9hEEttZ",
        "outputId": "60d77990-4d80-406c-972e-750d1fa2c3f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'GraphWriter'...\n",
            "remote: Enumerating objects: 133, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 133 (delta 26), reused 25 (delta 25), pack-reused 93\u001b[K\n",
            "Receiving objects: 100% (133/133), 102.65 MiB | 29.34 MiB/s, done.\n",
            "Resolving deltas: 100% (54/54), done.\n",
            "Updating files: 100% (55/55), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/rikdz/GraphWriter.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ew-NAbZOMj6g",
        "outputId": "69dd1945-9272-4f3b-95ed-76ff15633ea8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/rithanya/darri_det/GraphWriter\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/rithanya/darri_det/GraphWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3ifPiimG3Hj",
        "outputId": "4fa588c6-0508-497e-cb58-3667ad0db205"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: torchtext 0.17.1\n",
            "Uninstalling torchtext-0.17.1:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/torchtext-0.17.1.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/torchtext/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled torchtext-0.17.1\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall torchtext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33Z2uZChIVSs",
        "outputId": "c718651f-ca44-4840-a5b0-a1768b876e3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch==1.11.0\n",
            "  Downloading torch-1.11.0-cp310-cp310-manylinux1_x86_64.whl (750.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.6/750.6 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchtext==0.6\n",
            "  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.11.0) (4.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6) (4.66.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6) (2.31.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6) (1.25.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6) (1.16.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6) (0.1.99)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6) (2024.2.2)\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.2.1+cu121\n",
            "    Uninstalling torch-2.2.1+cu121:\n",
            "      Successfully uninstalled torch-2.2.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 1.11.0 which is incompatible.\n",
            "torchdata 0.7.1 requires torch>=2, but you have torch 1.11.0 which is incompatible.\n",
            "torchvision 0.17.1+cu121 requires torch==2.2.1, but you have torch 1.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.11.0 torchtext-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.11.0 torchtext==0.6\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdx-lR9iIXZR",
        "outputId": "0273b2c8-9c54-40c7-85fa-7c9d7c56cad1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting dill\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dill\n",
            "Successfully installed dill-0.3.8\n",
            "Collecting allennlp\n",
            "  Downloading allennlp-2.10.1-py3-none-any.whl (730 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m730.2/730.2 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch<1.13.0,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from allennlp) (1.11.0)\n",
            "Collecting torchvision<0.14.0,>=0.8.1 (from allennlp)\n",
            "  Downloading torchvision-0.13.1-cp310-cp310-manylinux1_x86_64.whl (19.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cached-path<1.2.0,>=1.1.3 (from allennlp)\n",
            "  Downloading cached_path-1.1.6-py3-none-any.whl (26 kB)\n",
            "Collecting fairscale==0.4.6 (from allennlp)\n",
            "  Downloading fairscale-0.4.6.tar.gz (248 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.2/248.2 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk>=3.6.5 in /usr/local/lib/python3.10/dist-packages (from allennlp) (3.8.1)\n",
            "Collecting spacy<3.4,>=2.1.0 (from allennlp)\n",
            "  Downloading spacy-3.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.4 in /usr/local/lib/python3.10/dist-packages (from allennlp) (1.25.2)\n",
            "Collecting tensorboardX>=1.2 (from allennlp)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from allennlp) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62 in /usr/local/lib/python3.10/dist-packages (from allennlp) (4.66.2)\n",
            "Requirement already satisfied: h5py>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from allennlp) (3.9.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from allennlp) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.10/dist-packages (from allennlp) (1.11.4)\n",
            "Requirement already satisfied: pytest>=6.2.5 in /usr/local/lib/python3.10/dist-packages (from allennlp) (7.4.4)\n",
            "Collecting transformers<4.21,>=4.1 (from allennlp)\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece>=0.1.96 in /usr/local/lib/python3.10/dist-packages (from allennlp) (0.1.99)\n",
            "Collecting filelock<3.8,>=3.3 (from allennlp)\n",
            "  Downloading filelock-3.7.1-py3-none-any.whl (10 kB)\n",
            "Collecting lmdb>=1.2.1 (from allennlp)\n",
            "  Downloading lmdb-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: more-itertools>=8.12.0 in /usr/local/lib/python3.10/dist-packages (from allennlp) (10.1.0)\n",
            "Collecting termcolor==1.1.0 (from allennlp)\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting wandb<0.13.0,>=0.10.0 (from allennlp)\n",
            "  Downloading wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.0.16 in /usr/local/lib/python3.10/dist-packages (from allennlp) (0.20.3)\n",
            "Requirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.10/dist-packages (from allennlp) (0.3.8)\n",
            "Collecting base58>=2.1.1 (from allennlp)\n",
            "  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
            "Collecting sacremoses (from allennlp)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from allennlp) (0.9.0)\n",
            "Requirement already satisfied: protobuf<4.0.0,>=3.12.0 in /usr/local/lib/python3.10/dist-packages (from allennlp) (3.20.3)\n",
            "Requirement already satisfied: traitlets>5.1.1 in /usr/local/lib/python3.10/dist-packages (from allennlp) (5.7.1)\n",
            "Collecting jsonnet>=0.10.0 (from allennlp)\n",
            "  Downloading jsonnet-0.20.0.tar.gz (594 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.2/594.2 kB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting rich<13.0,>=12.1 (from cached-path<1.2.0,>=1.1.3->allennlp)\n",
            "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.5/237.5 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting boto3<2.0,>=1.0 (from cached-path<1.2.0,>=1.1.3->allennlp)\n",
            "  Downloading boto3-1.34.69-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-cloud-storage<3.0,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from cached-path<1.2.0,>=1.1.3->allennlp) (2.8.0)\n",
            "Collecting huggingface-hub>=0.0.16 (from allennlp)\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.0.16->allennlp) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.0.16->allennlp) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.0.16->allennlp) (24.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.5->allennlp) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.5->allennlp) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.5->allennlp) (2023.12.25)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->allennlp) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->allennlp) (1.4.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->allennlp) (1.2.0)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->allennlp) (2.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->allennlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->allennlp) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->allennlp) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->allennlp) (2024.2.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.1->allennlp) (3.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.0.9)\n",
            "Collecting thinc<8.1.0,>=8.0.14 (from spacy<3.4,>=2.1.0->allennlp)\n",
            "  Downloading thinc-8.0.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (659 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m659.5/659.5 kB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (0.7.11)\n",
            "Collecting wasabi<1.1.0,>=0.9.1 (from spacy<3.4,>=2.1.0->allennlp)\n",
            "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.0.10)\n",
            "Collecting typer>=0.4.1 (from allennlp)\n",
            "  Downloading typer-0.4.2-py3-none-any.whl (27 kB)\n",
            "Collecting pathy>=0.3.5 (from spacy<3.4,>=2.1.0->allennlp)\n",
            "  Downloading pathy-0.11.0-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (6.4.0)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 (from spacy<3.4,>=2.1.0->allennlp)\n",
            "  Downloading pydantic-1.8.2-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (67.7.2)\n",
            "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub>=0.0.16->allennlp)\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.3.0)\n",
            "Collecting torch<1.13.0,>=1.10.0 (from allennlp)\n",
            "  Downloading torch-1.12.1-cp310-cp310-manylinux1_x86_64.whl (776.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision<0.14.0,>=0.8.1->allennlp) (9.4.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers<4.21,>=4.1->allennlp)\n",
            "  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m110.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting GitPython>=1.0.0 (from wandb<0.13.0,>=0.10.0->allennlp)\n",
            "  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (2.3)\n",
            "Collecting shortuuid>=0.5.0 (from wandb<0.13.0,>=0.10.0->allennlp)\n",
            "  Downloading shortuuid-1.0.13-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb<0.13.0,>=0.10.0->allennlp)\n",
            "  Downloading sentry_sdk-1.43.0-py2.py3-none-any.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.16.0)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb<0.13.0,>=0.10.0->allennlp)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools (from wandb<0.13.0,>=0.10.0->allennlp)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb<0.13.0,>=0.10.0->allennlp)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Collecting botocore<1.35.0,>=1.34.69 (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp)\n",
            "  Downloading botocore-1.34.69-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m110.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp)\n",
            "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1 (from GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.27.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.11.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.7.0)\n",
            "Collecting pathlib-abc==0.1.1 (from pathy>=0.3.5->spacy<3.4,>=2.1.0->allennlp)\n",
            "  Downloading pathlib_abc-0.1.1-py3-none-any.whl (23 kB)\n",
            "Collecting commonmark<0.10.0,>=0.9.0 (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp)\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp) (2.16.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.4,>=2.1.0->allennlp) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.69->boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.8.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.63.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.5.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.5.1)\n",
            "Building wheels for collected packages: fairscale, termcolor, jsonnet, pathtools\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.6-py3-none-any.whl size=307222 sha256=5b2baae8007fd84fdff8fb7b472a8bebcee27c9b537106c3848e89b839e7d4c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/58/3d/e114952ab4a8f31eb9dae230658450afff986b211a5b1f2256\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4832 sha256=9a3237b2634f0004ce246760fcb7c08a215600a3b94c6c9352d8de6191bcf959\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/49/46/1b13a65d8da11238af9616b00fdde6d45b0f95d9291bac8452\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.20.0-cp310-cp310-linux_x86_64.whl size=6406879 sha256=a2dea3844817a44ea4e8797ad57a6b3df95a9a0da980ba60be838a1a31155a93\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/0d/6b/5467dd1db9332ba4bd5cf4153e2870c5f89bb4db473d989cc2\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=2f395a24a15326254f4ad0390cbbb4039d8e9a9204f16af52101544ec980e453\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built fairscale termcolor jsonnet pathtools\n",
            "Installing collected packages: wasabi, tokenizers, termcolor, pathtools, lmdb, jsonnet, commonmark, typing-extensions, typer, tensorboardX, smmap, shortuuid, setproctitle, sentry-sdk, sacremoses, rich, pathlib-abc, jmespath, filelock, docker-pycreds, base58, torch, pydantic, pathy, huggingface-hub, gitdb, botocore, transformers, torchvision, thinc, s3transfer, GitPython, fairscale, wandb, spacy, boto3, cached-path, allennlp\n",
            "  Attempting uninstall: wasabi\n",
            "    Found existing installation: wasabi 1.1.2\n",
            "    Uninstalling wasabi-1.1.2:\n",
            "      Successfully uninstalled wasabi-1.1.2\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.2\n",
            "    Uninstalling tokenizers-0.15.2:\n",
            "      Successfully uninstalled tokenizers-0.15.2\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 2.4.0\n",
            "    Uninstalling termcolor-2.4.0:\n",
            "      Successfully uninstalled termcolor-2.4.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.10.0\n",
            "    Uninstalling typing_extensions-4.10.0:\n",
            "      Successfully uninstalled typing_extensions-4.10.0\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.9.0\n",
            "    Uninstalling typer-0.9.0:\n",
            "      Successfully uninstalled typer-0.9.0\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 13.7.1\n",
            "    Uninstalling rich-13.7.1:\n",
            "      Successfully uninstalled rich-13.7.1\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.13.1\n",
            "    Uninstalling filelock-3.13.1:\n",
            "      Successfully uninstalled filelock-3.13.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0\n",
            "    Uninstalling torch-1.11.0:\n",
            "      Successfully uninstalled torch-1.11.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.6.4\n",
            "    Uninstalling pydantic-2.6.4:\n",
            "      Successfully uninstalled pydantic-2.6.4\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.38.2\n",
            "    Uninstalling transformers-4.38.2:\n",
            "      Successfully uninstalled transformers-4.38.2\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.17.1+cu121\n",
            "    Uninstalling torchvision-0.17.1+cu121:\n",
            "      Successfully uninstalled torchvision-0.17.1+cu121\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.2.3\n",
            "    Uninstalling thinc-8.2.3:\n",
            "      Successfully uninstalled thinc-8.2.3\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.7.4\n",
            "    Uninstalling spacy-3.7.4:\n",
            "      Successfully uninstalled spacy-3.7.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sqlalchemy 2.0.28 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "en-core-web-sm 3.7.1 requires spacy<3.8.0,>=3.7.2, but you have spacy 3.3.3 which is incompatible.\n",
            "inflect 7.0.0 requires pydantic>=1.9.1, but you have pydantic 1.8.2 which is incompatible.\n",
            "pydantic-core 2.16.3 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 1.12.1 which is incompatible.\n",
            "torchdata 0.7.1 requires torch>=2, but you have torch 1.12.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GitPython-3.1.42 allennlp-2.10.1 base58-2.1.1 boto3-1.34.69 botocore-1.34.69 cached-path-1.1.6 commonmark-0.9.1 docker-pycreds-0.4.0 fairscale-0.4.6 filelock-3.7.1 gitdb-4.0.11 huggingface-hub-0.10.1 jmespath-1.0.1 jsonnet-0.20.0 lmdb-1.4.1 pathlib-abc-0.1.1 pathtools-0.1.2 pathy-0.11.0 pydantic-1.8.2 rich-12.6.0 s3transfer-0.10.1 sacremoses-0.1.1 sentry-sdk-1.43.0 setproctitle-1.3.3 shortuuid-1.0.13 smmap-5.0.1 spacy-3.3.3 tensorboardX-2.6.2.2 termcolor-1.1.0 thinc-8.0.17 tokenizers-0.12.1 torch-1.12.1 torchvision-0.13.1 transformers-4.20.1 typer-0.4.2 typing-extensions-4.5.0 wandb-0.12.21 wasabi-0.10.1\n"
          ]
        }
      ],
      "source": [
        "!pip install dill\n",
        "!pip install allennlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uc5mKTNiIjCw",
        "outputId": "6573bbf8-d3dc-4298-988a-159e063d019e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-24 06:51:58.201639: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-24 06:51:58.201686: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-24 06:51:58.203047: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-24 06:51:58.210442: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-24 06:51:59.270893: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Save File Exists, OverWrite? <CTL-C> for noyes\n",
            "Loading Data from  data/preprocessed.train.tsv\n",
            "building vocab\n",
            "done\n",
            "Sorting training data by len\n",
            "ds sizes:\t11880\t26156\t684\t1000\tVocab sizes:\n",
            "src 6343\n",
            "ent 53343\n",
            "nerd 8\n",
            "rel 17\n",
            "out 11738\n",
            "graph\n",
            "cuda:0\n",
            "epoch  0 lr 0.1\n",
            "Training\t3\n",
            "2\n",
            "2268 of like 40k -- current avg loss  5.701380867596447\n",
            "3868 of like 40k -- current avg loss  5.433923655236766\n",
            "5468 of like 40k -- current avg loss  5.278036955419735\n",
            "7068 of like 40k -- current avg loss  5.166623440447269\n",
            "8668 of like 40k -- current avg loss  5.078294785274687\n",
            "10268 of like 40k -- current avg loss  5.003925625526538\n",
            "11868 of like 40k -- current avg loss  4.9387697114420535\n",
            "13468 of like 40k -- current avg loss  4.8817130398389175\n",
            "15068 of like 40k -- current avg loss  4.834072020125383\n",
            "16668 of like 40k -- current avg loss  4.789444668904637\n",
            "18268 of like 40k -- current avg loss  4.751029102291193\n",
            "19868 of like 40k -- current avg loss  4.715192026023488\n",
            "21468 of like 40k -- current avg loss  4.681028127270481\n",
            "23068 of like 40k -- current avg loss  4.64930405872253\n",
            "24668 of like 40k -- current avg loss  4.619685762218898\n",
            "26268 of like 40k -- current avg loss  4.592644241423627\n",
            "1\n",
            "30008 of like 40k -- current avg loss  4.511357782110155\n",
            "33208 of like 40k -- current avg loss  4.449670723621256\n",
            "36408 of like 40k -- current avg loss  4.397296783348357\n",
            "AVG TRAIN LOSS:  4.363538062227659\t PPL:  78.53450341947665\n",
            "Evaluating\twe present a novel for on the , on the the the a with C in the and the <unk> . . we approaches for the , a D , the , we , the O , a only . we show this O of the the I in between the and N . and different to , are the in the and . in . we to the O of we and in the to are to . the . D . . is a . and and in the D efficient D is not . . . the N . \n",
            "VAL LOSS:  4.098458280563355\t PPL:  60.24733147770084\n",
            "Saving model\n",
            "epoch  1 lr 0.1\n",
            "Training\t3\n",
            "2\n",
            "2268 of like 40k -- current avg loss  4.189036754493781\n",
            "3868 of like 40k -- current avg loss  4.130608058641467\n",
            "5468 of like 40k -- current avg loss  4.0990425910754364\n",
            "7068 of like 40k -- current avg loss  4.078494314291953\n",
            "8668 of like 40k -- current avg loss  4.064160011636535\n",
            "10268 of like 40k -- current avg loss  4.049571782547907\n",
            "11868 of like 40k -- current avg loss  4.04260982613713\n",
            "13468 of like 40k -- current avg loss  4.034771468329932\n",
            "15068 of like 40k -- current avg loss  4.027808780533457\n",
            "16668 of like 40k -- current avg loss  4.020663910928263\n",
            "18268 of like 40k -- current avg loss  4.014448213921821\n",
            "19868 of like 40k -- current avg loss  4.008701057580003\n",
            "21468 of like 40k -- current avg loss  4.004994123724335\n",
            "23068 of like 40k -- current avg loss  3.9990204140861754\n",
            "24668 of like 40k -- current avg loss  3.993740333698162\n",
            "26268 of like 40k -- current avg loss  3.9883406470496126\n",
            "1\n",
            "30008 of like 40k -- current avg loss  3.952259248715978\n",
            "33208 of like 40k -- current avg loss  3.9214896732168123\n",
            "36408 of like 40k -- current avg loss  3.8952060098099044\n",
            "AVG TRAIN LOSS:  3.8789328396074043\t PPL:  48.37256623067865\n",
            "Evaluating\twe propose a O for on the and to the the the the with a in the and the <unk> are are we approaches for the and a D , the and the , the I , a a . we show the D of the of I of between the and the . and different to . are the in the . . . . we to the D of we and in the to are to . the of I I . is the . and are in the I general I is not . . . a information . \n",
            "VAL LOSS:  3.86834641456604\t PPL:  47.863174758920024\n",
            "Saving model\n",
            "epoch  2 lr 0.1\n",
            "Training\t1\n",
            "3168 of like 40k -- current avg loss  3.537180113069939\n",
            "6368 of like 40k -- current avg loss  3.5439047382105535\n",
            "9568 of like 40k -- current avg loss  3.5431693987702846\n",
            "3\n",
            "2\n",
            "14148 of like 40k -- current avg loss  3.607856405566633\n",
            "15748 of like 40k -- current avg loss  3.6282259511983965\n",
            "17348 of like 40k -- current avg loss  3.6435247910635686\n",
            "18948 of like 40k -- current avg loss  3.65751230678957\n",
            "20548 of like 40k -- current avg loss  3.6675039561592655\n",
            "22148 of like 40k -- current avg loss  3.6762201511323807\n",
            "23748 of like 40k -- current avg loss  3.683820711114661\n",
            "25348 of like 40k -- current avg loss  3.6890995531818063\n",
            "26948 of like 40k -- current avg loss  3.695097472665217\n",
            "28548 of like 40k -- current avg loss  3.69950679662399\n",
            "30148 of like 40k -- current avg loss  3.7025078391940234\n",
            "31748 of like 40k -- current avg loss  3.7061120342123908\n",
            "33348 of like 40k -- current avg loss  3.7088796628583918\n",
            "34948 of like 40k -- current avg loss  3.7105742981794894\n",
            "36548 of like 40k -- current avg loss  3.7122806005226363\n",
            "38148 of like 40k -- current avg loss  3.7137355251208257\n",
            "AVG TRAIN LOSS:  3.714383704874141\t PPL:  41.03329067312613\n",
            "Evaluating\twe propose a new for on the for for the the the and with a in the and the <unk> . . we methods for the and a I is the is however , the I , a a . we propose this proposed of I the I of between the and the . and different problems , are , in the -lrb- . -lrb- . we to the D of we and in the to are to to a and I I . is a . and , in the I than and is not . . . a N . \n",
            "VAL LOSS:  3.7591223678588865\t PPL:  42.91074959513476\n",
            "Saving model\n",
            "epoch  3 lr 0.1\n",
            "Training\t3\n",
            "2\n",
            "2268 of like 40k -- current avg loss  3.777911389736061\n",
            "3868 of like 40k -- current avg loss  3.73625754422461\n",
            "5468 of like 40k -- current avg loss  3.723166213213482\n",
            "7068 of like 40k -- current avg loss  3.7125345784928343\n",
            "8668 of like 40k -- current avg loss  3.7089748857865206\n",
            "10268 of like 40k -- current avg loss  3.7038059342357963\n",
            "11868 of like 40k -- current avg loss  3.699649144793987\n",
            "13468 of like 40k -- current avg loss  3.6958227137765864\n",
            "15068 of like 40k -- current avg loss  3.694726787260302\n",
            "16668 of like 40k -- current avg loss  3.6930631791369533\n",
            "18268 of like 40k -- current avg loss  3.691273197502335\n",
            "19868 of like 40k -- current avg loss  3.688659433534214\n",
            "21468 of like 40k -- current avg loss  3.6870779796576887\n",
            "23068 of like 40k -- current avg loss  3.685123140484906\n",
            "24668 of like 40k -- current avg loss  3.683460357762512\n",
            "26268 of like 40k -- current avg loss  3.6821378520839523\n",
            "1\n",
            "30008 of like 40k -- current avg loss  3.6561443371252835\n",
            "33208 of like 40k -- current avg loss  3.634107074373402\n",
            "36408 of like 40k -- current avg loss  3.6166523484915487\n",
            "AVG TRAIN LOSS:  3.605226629159667\t PPL:  36.79002060398715\n",
            "Evaluating\twe present a O for on the and on the the the and with the in the and the and . . we I of the and a I are the and and , the I , a a . we show this O of the the I of between the and N . and of problems , are , in the N performance . . we to the O of we are in the to are to . the the I D . is a . and are in the I complex D is not . . . a N . \n",
            "VAL LOSS:  3.6981162452697753\t PPL:  40.37118327838796\n",
            "Saving model\n",
            "epoch  4 lr 0.1\n",
            "Training\t2\n",
            "1584 of like 40k -- current avg loss  3.5921016124763874\n",
            "3184 of like 40k -- current avg loss  3.584170747641942\n",
            "4784 of like 40k -- current avg loss  3.588798552452521\n",
            "6384 of like 40k -- current avg loss  3.5897115131368613\n",
            "7984 of like 40k -- current avg loss  3.590608185899998\n",
            "9584 of like 40k -- current avg loss  3.59165392654368\n",
            "11184 of like 40k -- current avg loss  3.5904388209439824\n",
            "12784 of like 40k -- current avg loss  3.5884738595076886\n",
            "14384 of like 40k -- current avg loss  3.5867517241116227\n",
            "15984 of like 40k -- current avg loss  3.5885152833478466\n",
            "17584 of like 40k -- current avg loss  3.59079411487562\n",
            "19184 of like 40k -- current avg loss  3.5904520379194524\n",
            "20784 of like 40k -- current avg loss  3.591501910471751\n",
            "22384 of like 40k -- current avg loss  3.5914168146527436\n",
            "23984 of like 40k -- current avg loss  3.5907133140907517\n",
            "25584 of like 40k -- current avg loss  3.589970619846986\n",
            "3\n",
            "1\n",
            "30008 of like 40k -- current avg loss  3.5752124230215245\n",
            "33208 of like 40k -- current avg loss  3.5543862469539214\n",
            "36408 of like 40k -- current avg loss  3.537709640424557\n",
            "AVG TRAIN LOSS:  3.527342201658517\t PPL:  34.03339340752912\n",
            "Evaluating\twe propose a O for on the and to the the the with with the about the and the and . . we approaches for N and a I are the and and about the I , a a . we show this D to I the I of between N and the . and of to , are N in the N . . . we to our D of we and in the to are to . the the D D . can a . we , in the D general I is not . . . a N . \n",
            "VAL LOSS:  3.6540946102142335\t PPL:  38.63252778019726\n",
            "Saving model\n"
          ]
        }
      ],
      "source": [
        "#@title ON ORIGINAL DATA 5 EPOCHS \n",
        "!python train.py -save '/content/drive/MyDrive/rithanya/darri_det/GraphWriter/output/' -epochs 5 -savevocab '/content/drive/MyDrive/rithanya/darri_det/GraphWriter/output/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9l8lsfGsFWX",
        "outputId": "7f187742-9896-4b56-d321-2b3641bc8d76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-24 09:36:20.134080: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-24 09:36:20.134138: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-24 09:36:20.135582: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-24 09:36:20.143690: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-24 09:36:21.265150: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Loading Data from  data/preprocessed.train.tsv\n",
            "building vocab\n",
            "done\n",
            "Vocab sizes:\n",
            "src 6343\n",
            "ent 53343\n",
            "nerd 8\n",
            "rel 17\n",
            "out 11738\n",
            "graph\n",
            "0 1000\n",
            "/content/drive/MyDrive/rithanya/darri_det/GraphWriter/models/newmodel.py:152: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  cx = torch.tensor(hx)\n",
            "we present a LEARNING ARCHITECTURE for LEXICAL SEMANTIC CLASSIFICATION PROBLEMS that <unk> TASK-SPECIFIC TRAINING DATA with BACKGROUND DATA encoding general '' world knowledge '' . the LEARNING ARCHITECTURE compiles knowledge contained in a <unk> into additional training data , and integrates TASK-SPECIFIC AND BACKGROUND DATA through a novel HIERARCHICAL LEARNING ARCHITECTURE . experiments on a WORD SENSE DISAMBIGUATION TASK provide empirical evidence that this '' HIERARCHICAL LEARNING ARCHITECTURE '' outperforms a state-of-the-art standard '' flat '' one . \n",
            "this paper presents a HIERARCHICAL LEARNING ARCHITECTURE for LEXICAL SEMANTIC CLASSIFICATION PROBLEMS . the HIERARCHICAL LEARNING ARCHITECTURE is based on a HIERARCHICAL LEARNING ARCHITECTURE . the LEARNING ARCHITECTURE is based on a HIERARCHICAL LEARNING ARCHITECTURE . the LEARNING ARCHITECTURE is based on a HIERARCHICAL LEARNING ARCHITECTURE . the LEARNING ARCHITECTURE is applied to the task of LEXICAL SEMANTIC CLASSIFICATION PROBLEMS .\n",
            "\n",
            "1 1000\n",
            "we consider the STOCHASTIC MULTI-ARMED BANDIT PROBLEM with a PRIOR DISTRIBUTION on the REWARD DISTRIBUTIONS . we are interested in studying PRIOR-FREE AND PRIOR-DEPENDENT REGRET BOUNDS , very much in the same spirit than the usual DISTRIBUTION-FREE AND DISTRIBUTION-DEPENDENT BOUNDS for the NON-BAYESIAN STOCHASTIC BANDIT . we first show that THOMPSON SAMPLING attains an optimal <unk> bound in the sense that for any PRIOR DISTRIBUTION its BAYESIAN REGRET is bounded from above by 14 √ <unk> . this result is <unk> in the sense that there exists a PRIOR DISTRIBUTION such that any algorithm has a BAYESIAN REGRET bounded from below by 1 20 √ <unk> . we also study the case of THOMPSON SAMPLING for the setting of <unk> et al. -lsb- 2013 -rsb- -lrb- where the optimal mean is known as well as a lower bound on the smallest gap -rrb- and we show that in this case the regret of THOMPSON SAMPLING is in fact uniformly bounded over time , thus showing that THOMPSON SAMPLING can greatly take advantage of the nice properties of these THOMPSON SAMPLING . \n",
            "this paper addresses the problem of STOCHASTIC MULTI-ARMED BANDIT PROBLEM in NON-BAYESIAN STOCHASTIC BANDIT . we propose a new method for estimating the parameters of the NON-BAYESIAN STOCHASTIC BANDIT . the proposed method is based on the use of THOMPSON SAMPLING with THOMPSON SAMPLING . the proposed method is based on the use of THOMPSON SAMPLING with THOMPSON SAMPLING . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "2 1000\n",
            "the paper presents a new application of AUTOMATIC SPEECH PROCESSING in the AMBIENT ASSISTED LIVING AREA , developed in the course of a three year research project . recording and automatic processing of SPOKEN CONVERSATIONS plays a major role in this solution enabling effective search in a PERSONAL AUDIO ARCHIVE and fast browsing of conversations . processing of ELDERLY CONVERSATIONAL SPEECH recorded by a distant pda microphone poses a great challenge . the SPEECH PROCESSING FLOW includes TRANSCRIPTION , SPEAKER TRACKING and combined indexing and search of spoken terms and participating speakers identity extracted from the AUDIO . we present the entire application and individual SPEECH PROCESSING COMPONENTS as well as evaluation results of the individual components and of the END-TO-END SPOKEN INFORMATION RETRIEVAL SOLUTION . \n",
            "this paper presents a novel approach for SPEAKER TRACKING in ELDERLY CONVERSATIONAL SPEECH . the PERSONAL AUDIO ARCHIVE consists of a PERSONAL AUDIO ARCHIVE and a AMBIENT ASSISTED LIVING AREA into a PERSONAL AUDIO ARCHIVE . the proposed method is based on a PERSONAL AUDIO ARCHIVE , which consists of a PERSONAL AUDIO ARCHIVE and a AMBIENT ASSISTED LIVING AREA . the END-TO-END SPOKEN INFORMATION RETRIEVAL SOLUTION is tested on a PERSONAL AUDIO ARCHIVE and a PERSONAL AUDIO ARCHIVE . the END-TO-END SPOKEN INFORMATION RETRIEVAL SOLUTION is evaluated on a PERSONAL AUDIO ARCHIVE and a PERSONAL AUDIO ARCHIVE . the results show that the proposed method is robust and robust to SPEAKER TRACKING and SPEAKER TRACKING .\n",
            "\n",
            "3 1000\n",
            "we address the problem of estimating the WORD ERROR RATE of an AUTOMATIC SPEECH RECOGNITION SYSTEM without using ACOUSTIC TEST DATA . this is an important problem which is faced by the designers of new applications which use AUTOMATIC SPEECH RECOGNITION SYSTEM . quick estimate of WORD ERROR RATE early in the DESIGN CYCLE can be used to guide the decisions involving DIALOG STRATEGY and GRAMMAR DESIGN . our approach involves estimating the PROBABILITY DISTRIBUTION of the WORD HYPOTHESES produced by the underlying AUTOMATIC SPEECH RECOGNITION SYSTEM given the TEXT TEST CORPUS . a critical component of this system is a PHONEMIC CONFUSION MODEL which seeks to capture the errors made by AUTOMATIC SPEECH RECOGNITION SYSTEM on the ACOUSTIC DATA at a PHONEMIC LEVEL . we use a CONFUSION MODEL composed of PROBABILISTIC PHONEME SEQUENCE CONVERSION RULES which are learned from PHONEMIC TRANSCRIPTION PAIRS obtained by LEAVE-ONE-OUT DECODING of the training set . we show reasonably close estimation of WORD ERROR RATE when applying the system to test sets from different domains . \n",
            "this paper presents a novel DIALOG STRATEGY for LEAVE-ONE-OUT DECODING . the proposed AUTOMATIC SPEECH RECOGNITION SYSTEM is based on the PHONEMIC CONFUSION MODEL and the CONFUSION MODEL . the proposed AUTOMATIC SPEECH RECOGNITION SYSTEM is based on the CONFUSION MODEL and the PHONEMIC CONFUSION MODEL . the proposed AUTOMATIC SPEECH RECOGNITION SYSTEM is based on the PHONEMIC CONFUSION MODEL and the CONFUSION MODEL . the proposed AUTOMATIC SPEECH RECOGNITION SYSTEM is evaluated on a TEXT TEST CORPUS and on the TEXT TEST CORPUS . the proposed AUTOMATIC SPEECH RECOGNITION SYSTEM is shown to outperform the conventional AUTOMATIC SPEECH RECOGNITION SYSTEM in terms of WORD ERROR RATE and WORD ERROR RATE .\n",
            "\n",
            "4 1000\n",
            "mel-frequency cepstral coefficients -lrb- MEL-FREQUENCY CEPSTRAL COEFFICIENTS -rrb- are the most widely used FEATURES for SPEECH RECOGNITION . these are derived from the POWER SPECTRUM of the SPEECH SIGNAL . recently , the CEPSTRAL FEATURES derived from the MODIFIED GROUP DELAY FUNCTION have been studied by <unk> and <unk> -lsb- 6 -rsb- for SPEECH RECOGNITION . in this paper , we propose to use the product of the POWER SPECTRUM and the GROUP DELAY FUNCTION , and derive the MEL-FREQUENCY CEPSTRAL COEFFICIENTS from the product MEL-FREQUENCY CEPSTRAL COEFFICIENTS . this MEL-FREQUENCY CEPSTRAL COEFFICIENTS combines the information from the MAGNITUDE SPECTRUM as well as the PHASE SPECTRUM . the MEL-FREQUENCY CEPSTRAL COEFFICIENTS of the MODIFIED GROUP DELAY FUNCTION are also investigated in this paper . results show that the CEPSTRAL FEATURES derived from the POWER SPECTRUM perform better than that from the MODIFIED GROUP DELAY FUNCTION , and the product MEL-FREQUENCY CEPSTRAL COEFFICIENTS based FEATURES provide the best performance . \n",
            "this paper presents a method for SPEECH RECOGNITION from MEL-FREQUENCY CEPSTRAL COEFFICIENTS . the proposed method is based on a MODIFIED GROUP DELAY FUNCTION , which is based on the MODIFIED GROUP DELAY FUNCTION . the proposed method is based on a MODIFIED GROUP DELAY FUNCTION , which is based on the MODIFIED GROUP DELAY FUNCTION . the proposed method is based on a MODIFIED GROUP DELAY FUNCTION . the proposed method is based on a MODIFIED GROUP DELAY FUNCTION . the proposed method is based on a MODIFIED GROUP DELAY FUNCTION . the proposed method is based on the MODIFIED GROUP DELAY FUNCTION . the proposed method is based on the MODIFIED GROUP DELAY FUNCTION . the proposed method is based on the MODIFIED GROUP DELAY FUNCTION .\n",
            "\n",
            "5 1000\n",
            "three-dimensional -lrb- 3d -rrb- graphic scenes require considerable NETWORK BANDWIDTH to be transmitted and computing power to be rendered on users ' terminals . toward high-quality display in real time , we propose a SENDER-DRIVEN MECHANISM for STREAMING 3D SCENES in a RESOURCE-CONSTRAINED ENVIRONMENT . by pre-processing the database , objects in the scene are properly weighted upon their rendering importance , and their resolutions are selected accordingly to reduce the bit rate . partially ordered delivery is then performed using DECODING INDEPENDENCIES between the objects . simulation results show the efficacy of the proposed SENDER-DRIVEN MECHANISM . for a test benchmark , for example , the proposed algorithm outperforms the comparing HEURISTIC by 4 db under a 100-KB BIT RATE . \n",
            "this paper presents a SENDER-DRIVEN MECHANISM for STREAMING 3D SCENES . the SENDER-DRIVEN MECHANISM is based on the SENDER-DRIVEN MECHANISM . the SENDER-DRIVEN MECHANISM is based on the SENDER-DRIVEN MECHANISM . the SENDER-DRIVEN MECHANISM is based on the SENDER-DRIVEN MECHANISM and the SENDER-DRIVEN MECHANISM is applied to the RESOURCE-CONSTRAINED ENVIRONMENT .\n",
            "\n",
            "6 1000\n",
            "we present in this paper a system which automatically builds , from REAL IMAGES , a SCENE MODEL containing both 3D GEOMETRIC INFORMATION OF THE SCENE STRUCTURE and its PHOTOMETRIC INFORMATION under various ILLUMINATION CONDITIONS . the GEOMETRIC STRUCTURE is recovered from IMAGES taken from distinct viewpoints . STRUCTURE-FROM-MOTION AND CORRELATION-BASED STEREO TECHNIQUES are used to match pix-els between IMAGES of different viewpoints and to reconstruct the scene in 3D SPACE . the GEOMETRIC STRUCTURE is extracted from IMAGES taken under different ILLUMINATION CONDITIONS -lrb- orientation , POSITION and intensity of the light sources -rrb- . this is achieved by computing a LOW-DIMENSIONAL LINEAR SPACE of the <unk> volume , and is represented by a set of BASIS IMAGES . the model that has been built can be used to create realistic <unk> from different viewpoints and ILLUMINATION CONDITIONS . applications include OBJECT RECOGNITION , VIRTUAL REALITY and PRODUCT ADVERTISEMENT . \n",
            "this paper presents a new method for OBJECT RECOGNITION from BASIS IMAGES . the proposed method is based on the 3D GEOMETRIC INFORMATION OF THE SCENE STRUCTURE and the GEOMETRIC STRUCTURE . the proposed method is based on the PHOTOMETRIC INFORMATION and the PHOTOMETRIC INFORMATION . the proposed method is based on the PHOTOMETRIC INFORMATION and the GEOMETRIC STRUCTURE . the proposed method is based on the PHOTOMETRIC INFORMATION and the PHOTOMETRIC INFORMATION . the proposed method is evaluated on REAL IMAGES and REAL IMAGES . the results show that the proposed method is robust to ILLUMINATION CONDITIONS and ILLUMINATION CONDITIONS .\n",
            "\n",
            "7 1000\n",
            "we seek to both detect and segment objects in images . to exploit both LOCAL IMAGE DATA as well as CONTEXTUAL INFORMATION , we introduce BOOSTED RANDOM FIELDS , which uses BOOSTING to learn the GRAPH STRUCTURE and local evidence of a CONDITIONAL RANDOM FIELD . the GRAPH STRUCTURE is learned by assembling GRAPH FRAGMENTS in an ADDITIVE MODEL . the connections between individual pixels are not very informative , but by using DENSE GRAPHS , we can pool information from large regions of the image ; DENSE MODELS also support efficient inference . we show how CONTEXTUAL INFORMATION from other objects can improve DETECTION performance , both in terms of ACCURACY and SPEED , by using a COMPUTATIONAL CASCADE . we apply our system to detect <unk> and things in office and street scenes . 1 . \n",
            "this paper addresses the problem of DETECTION from LOCAL IMAGE DATA . we propose a method for DETECTION based on BOOSTED RANDOM FIELDS . the proposed method is based on a CONDITIONAL RANDOM FIELD , which is based on the CONTEXTUAL INFORMATION of the CONDITIONAL RANDOM FIELD . the proposed method is based on the CONTEXTUAL INFORMATION and the CONTEXTUAL INFORMATION . the DETECTION is formulated as a CONDITIONAL RANDOM FIELD . the proposed method is based on a COMPUTATIONAL CASCADE . the proposed method is based on a COMPUTATIONAL CASCADE and is shown to be robust to SPEED and SPEED .\n",
            "\n",
            "8 1000\n",
            "we investigate the COMPUTATIONAL COMPLEXITY of testing DOMINANCE and CONSISTENCY in CP-NETS . up until now , the complexity of DOMINANCE has been determined only for RESTRICTED CLASSES in which the DEPENDENCY GRAPH of the CP-NETS is acyclic . however , there are preferences of interest that define CYCLIC DEPENDENCY GRAPHS ; CYCLIC DEPENDENCY GRAPHS are modeled with GENERAL CP-NETS . we show here that both DOMINANCE and CONSISTENCY testing for GENERAL CP-NETS are PSPACE-COMPLETE . the reductions used in the proofs are from STRIPS PLANNING , and thus establish strong connections between both areas . \n",
            "this paper addresses the problem of GENERAL CP-NETS in CYCLIC DEPENDENCY GRAPHS . we propose a method for estimating the CONSISTENCY of the DEPENDENCY GRAPH in the DEPENDENCY GRAPH . the proposed method is based on the use of CYCLIC DEPENDENCY GRAPHS in the DEPENDENCY GRAPH . the proposed method is based on the use of CYCLIC DEPENDENCY GRAPHS in the DEPENDENCY GRAPH of the DEPENDENCY GRAPH . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "9 1000\n",
            "<unk> was once regarded as an indication of good sleep . but recently it has been known to be one of the symptoms which indicate SLEEP DISORDERED BREATHING such as SLEEP APNEA SYNDROME . moreover , HEAVY SNORING caused by ORAL BREATHING sometimes leads BENIGN SNORERS to be <unk> . thus , it is important to detect ORAL SNORING for MEDICAL TREATMENT in the earlier stage , but we can not know our own <unk> . this paper describes a method to detect ORAL SNORING by extracting the ACOUSTIC PROPERTIES OF SNORING SOUNDS and using the K-NEAREST NEIGHBOR CLASSIFIER . as a result , over 92 % of SNORING SOUNDS are successfully classified under the various CROSS VALIDATION EVALUATIONS . \n",
            "this paper presents a new method for ACOUSTIC PROPERTIES OF SNORING SOUNDS , which is based on a K-NEAREST NEIGHBOR CLASSIFIER . the proposed method is based on a K-NEAREST NEIGHBOR CLASSIFIER , which is based on the ACOUSTIC PROPERTIES OF SNORING SOUNDS . the proposed method is based on the ACOUSTIC PROPERTIES OF SNORING SOUNDS . the proposed method is based on the ACOUSTIC PROPERTIES OF SNORING SOUNDS . the proposed method is based on the ACOUSTIC PROPERTIES OF SNORING SOUNDS . the method is based on the ACOUSTIC PROPERTIES OF SNORING SOUNDS . the proposed method is based on the ACOUSTIC PROPERTIES OF SNORING SOUNDS . the proposed method is based on the ACOUSTIC PROPERTIES OF SNORING SOUNDS . the proposed method is based on the ACOUSTIC PROPERTIES OF SNORING SOUNDS .\n",
            "\n",
            "10 1000\n",
            "existing research on SENTIMENT ANALYSIS mainly uses SENTIMENT WORDS and phrases to determine SENTIMENTS expressed in DOCUMENTS and sentences . techniques have also been developed to find such words and phrases using dictionaries and domain corpora . however , there are still other types of words and phrases that do not bear SENTIMENTS on their own , but when they appear in some particular contexts , they imply POSITIVE OR NEGATIVE OPINIONS . one class of such words or phrases is those that express resources such as WATER , ELECTRICITY , GAS , etc. . for example , '' this <unk> uses a lot of ELECTRICITY '' is negative but '' this <unk> uses little WATER '' is positive . extracting such RESOURCE WORDS and phrases are important for SENTIMENT ANALYSIS . this paper formulates the problem based on a BIPARTITE GRAPH and proposes a novel ITERATIVE ALGORITHM to solve the problem . experimental results using diverse REAL-LIFE SENTIMENT CORPORA show good results . \n",
            "this paper presents a new method for SENTIMENT ANALYSIS from REAL-LIFE SENTIMENT CORPORA . the method is based on the ITERATIVE ALGORITHM and the ITERATIVE ALGORITHM . the proposed method is based on the ITERATIVE ALGORITHM and the ITERATIVE ALGORITHM . the proposed method is based on the ITERATIVE ALGORITHM and the ITERATIVE ALGORITHM . the proposed method is based on the ITERATIVE ALGORITHM and the ITERATIVE ALGORITHM . the proposed method is based on the ITERATIVE ALGORITHM and the ITERATIVE ALGORITHM . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "11 1000\n",
            "recently , <unk> and <unk> demonstrated that popular DIFFUSION-BASED PROCEDURES to compute a quick approximation to the first NONTRIVIAL EIGENVECTOR of a DATA GRAPH LAPLACIAN exactly solve certain REGULARIZED SEMI-DEFINITE PROGRAMS . in this paper , we extend that result by providing a statistical interpretation of their APPROXIMATION PROCEDURE . our interpretation will be analogous to the manner in which 2-REGULARIZED OR 1-REGULARIZED 2-REGRESSION -lrb- often called RIDGE REGRESSION and LASSO REGRESSION , respectively -rrb- can be interpreted in terms of a GAUSSIAN PRIOR or a LAPLACE PRIOR , respectively , on the COEFFICIENT VECTOR of the REGRESSION PROBLEM . our framework will imply that the solutions to the MAHONEY-ORECCHIA REGULARIZED SDP can be interpreted as REGULARIZED ESTIMATES of the <unk> of the DATA GRAPH LAPLACIAN . conversely , it will imply that the solution to this REGULARIZED ESTIMATION PROBLEM can be computed very quickly by running , e.g. , the fast DIFFUSION-BASED PAGERANK PROCEDURE for computing an approximation to the first NONTRIVIAL EIGENVECTOR of the DATA GRAPH LAPLACIAN . empirical results are also provided to illustrate the manner in which APPROXIMATE EIGENVECTOR COMPUTATION implicitly performs STATISTICAL REGULARIZATION , relative to running the corresponding exact algorithm . \n",
            "this paper proposes a new method for LASSO REGRESSION based on the DATA GRAPH LAPLACIAN . the proposed method is based on the use of a GAUSSIAN PRIOR and a GAUSSIAN PRIOR . the proposed method is based on the DATA GRAPH LAPLACIAN and the APPROXIMATION PROCEDURE . the proposed method is based on the DATA GRAPH LAPLACIAN and the APPROXIMATION PROCEDURE . the proposed method is based on the use of the DATA GRAPH LAPLACIAN and the COEFFICIENT VECTOR . the proposed method is based on the DATA GRAPH LAPLACIAN and the APPROXIMATION PROCEDURE . the proposed method is compared with the conventional DIFFUSION-BASED PROCEDURES and the DIFFUSION-BASED PROCEDURES .\n",
            "\n",
            "12 1000\n",
            "network ALARM TRIAGE refers to GROUPING and <unk> a stream of LOW-LEVEL DEVICE HEALTH INFORMATION to help operators find and fix problems . today , this process tends to be largely manual because existing RULE-BASED TOOLS can not easily evolve with the network . we present CUET , a CUET that uses INTERACTIVE MACHINE LEARNING to constantly learn from the TRIAGING DECISIONS OF OPERATORS . CUET then uses that learning in novel visualizations to help them quickly and accurately <unk> alarms . unlike prior INTERACTIVE MACHINE LEARNING SYSTEMS , CUET handles a highly dynamic environment where the groups of interest are not known a priori and evolve constantly . our evaluations with real operators and data from a large network show that CUET significantly improves the speed and ACCURACY of ALARM TRIAGE . \n",
            "this paper addresses the problem of INTERACTIVE MACHINE LEARNING for INTERACTIVE MACHINE LEARNING . the TRIAGING DECISIONS OF OPERATORS is based on the use of LOW-LEVEL DEVICE HEALTH INFORMATION and the TRIAGING DECISIONS OF OPERATORS . the proposed method is based on the TRIAGING DECISIONS OF OPERATORS , which is based on the TRIAGING DECISIONS OF OPERATORS . the proposed method is based on the TRIAGING DECISIONS OF OPERATORS . the proposed method is based on the TRIAGING DECISIONS OF OPERATORS . the proposed method is based on the TRIAGING DECISIONS OF OPERATORS and is shown to be robust to ACCURACY .\n",
            "\n",
            "13 1000\n",
            "we consider the problem of CLUSTERING DATA lying on multiple SUBSPACES of unknown and possibly different dimensions . we show that one can represent the SUBSPACES with a set of POLYNOMIALS whose derivatives at a data point give normal vectors to the SUBSPACE associated with the data point . since the POLYNOMIALS can be estimated linearly from data , SUBSPACE CLUSTERING is reduced to classifying one point per SUBSPACE . we do so by choosing points in the data set that minimize a DISTANCE FUNCTION . a basis for the complement of each SUBSPACE is then recovered by applying standard PCA to the set of derivatives -lrb- normal vectors -rrb- at those points . the final result is a new GPCA ALGORITHM for SUBSPACE CLUSTERING based on simple LINEAR AND POLYNOMIAL ALGEBRA . our experiments show that our GPCA ALGORITHM outperforms existing ALGEBRAIC ALGORITHMS based on POLYNOMIAL FACTORIZATION and provides a good INITIALIZATION to ITERATIVE TECHNIQUES such as K-SUBSPACE and EM . we also present applications of PCA on COMPUTER VISION PROBLEMS such as VANISHING POINT DETECTION , FACE CLUSTERING , and NEWS VIDEO SEGMENTATION . \n",
            "this paper proposes a new GPCA ALGORITHM for NEWS VIDEO SEGMENTATION . the proposed GPCA ALGORITHM is based on the use of a DISTANCE FUNCTION and a DISTANCE FUNCTION . the proposed GPCA ALGORITHM is based on the use of a DISTANCE FUNCTION and a DISTANCE FUNCTION . the proposed GPCA ALGORITHM is based on the use of a DISTANCE FUNCTION and a DISTANCE FUNCTION . the proposed GPCA ALGORITHM is compared with other ITERATIVE TECHNIQUES such as PCA and POLYNOMIAL FACTORIZATION . the proposed GPCA ALGORITHM is compared with other ALGEBRAIC ALGORITHMS such as PCA and PCA .\n",
            "\n",
            "14 1000\n",
            "in an attempt to estimate the SEMANTIC CASES for NOUN-PARTICLE-VERB TRIPLES in the ATR DIALOGUE CORPUS , the authors propose a MEASURE OF DISTANCE based on STATISTICS OF DEPENDENT NOUN-PARTICLE-VERB TRIPLES . a CLUSTERING ANALYSIS of all the triples in the corpus was conducted using the MEASURE OF DISTANCE . competence of the proposed MEASURE OF DISTANCE is verified by examination of the distribution of the SINGLE-CASE CLUSTERS . by use of the score derived from the MEASURE OF DISTANCE of the training corpus , the authors conducted the estimation of the correct semantic case for a given NOUN-PARTICLE-VERB TRIPLES in the test corpus . the result remarkably differentiates the particles with respect to the ESTIMATION ACCURACIES . for instance , particle ` <unk> ' has accuracies over 80 % , while ` de ' has accuracies less than 40 % . the CORRELATION ANALYSIS between the ACCURACY and the CONSISTENCY RATES indicates that the particles of higher consistency have also tendencies to higher accuracies . \n",
            "this paper addresses the problem of CLUSTERING ANALYSIS for SEMANTIC CASES in SEMANTIC CASES . we propose a MEASURE OF DISTANCE for the STATISTICS OF DEPENDENT NOUN-PARTICLE-VERB TRIPLES , which is based on the MEASURE OF DISTANCE of the ATR DIALOGUE CORPUS . the ACCURACY of the proposed CLUSTERING ANALYSIS is compared with the conventional CORRELATION ANALYSIS . the experimental results show that the proposed method achieves a significant improvement in ACCURACY compared to the conventional CLUSTERING ANALYSIS .\n",
            "\n",
            "15 1000\n",
            "this paper provides a new perspective on HUMAN MOTION ANALYSIS , namely regarding human motions in video as GENERAL DISCRETE TIME SIGNALS . while this seems an intuitive idea , research on HUMAN MOTION ANALYSIS has attracted little attention from the SIGNAL PROCESSING COMMUNITY . SOPHISTICATED SIGNAL PROCESSING TECHNIQUES create important opportunities for new solutions to the problem of HUMAN MOTION ANALYSIS . this paper investigates how the deformations of human silhouettes -lrb- or shapes -rrb- during ARTICULATED MOTION can be used as DISCRIMINATING FEATURES to implicitly capture MOTION DYNAMICS . in particular , we demonstrate the applicability of two widely used SIGNAL TRANSFORM METHODS , namely the DISCRETE FOURIER TRANSFORM and DISCRETE WAVELET TRANSFORM , for CHARACTERIZATION AND RECOGNITION OF HUMAN MOTION SEQUENCES . experimental results show the effectiveness of the proposed method on two state-of-the-art DATA SETS . \n",
            "this paper presents a new method for CHARACTERIZATION AND RECOGNITION OF HUMAN MOTION SEQUENCES based on DISCRETE WAVELET TRANSFORM . the proposed method is based on the DISCRETE WAVELET TRANSFORM , which is based on the DISCRETE FOURIER TRANSFORM . the proposed method is based on the DISCRETE WAVELET TRANSFORM and the DISCRETE FOURIER TRANSFORM . the proposed method is based on the DISCRETE FOURIER TRANSFORM and the DISCRETE WAVELET TRANSFORM . the proposed method is based on the DISCRETE WAVELET TRANSFORM and the DISCRETE WAVELET TRANSFORM . the proposed method is compared with conventional SOPHISTICATED SIGNAL PROCESSING TECHNIQUES such as HUMAN MOTION ANALYSIS and HUMAN MOTION ANALYSIS .\n",
            "\n",
            "16 1000\n",
            "spectral maxima sound coding algorithms , for example N-OF-M STRATEGIES , used in COMMERCIAL COCHLEAR IMPLANT DEVICES rely on selecting channels with the highest energy in each frequency band . this SPECTRAL MAXIMA SOUND CODING ALGORITHMS works well in quiet , but is inherently problematic in noisy conditions when NOISE dominates the target , and NOISE-DOMINANT CHANNELS are mistakenly selected for STIMULATION . a new CHANNEL SELECTION CRITERION is proposed to addresses this shortcoming which adaptively assigns weights to each TIME-FREQUENCY UNIT based on the FORMANT LOCATION OF SPEECH and INSTANTANEOUS SIGNAL to NOISE ratio . the performance of the proposed SPECTRAL MAXIMA SOUND CODING ALGORITHMS is evaluated <unk> with three COCHLEAR IMPLANT USERS in different NOISE SCENARIOS . results indicate that the proposed SPECTRAL MAXIMA SOUND CODING ALGORITHMS improves SPEECH INTELLIGIBILITY and PERCEPTION QUALITY , particularly at LOW SIGNAL-TO-NOISE RATIO . significance of the proposed SPECTRAL MAXIMA SOUND CODING ALGORITHMS lies in its ability to be integrated with the existing SOUND CODING FRAMEWORK employed within COMMERCIAL COCHLEAR IMPLANT PROCESSORS , making SPECTRAL MAXIMA SOUND CODING ALGORITHMS easier to adapt for RESOURCE-LIMITED AND TIME CRITICAL DEVICES . \n",
            "this paper addresses the problem of FORMANT LOCATION OF SPEECH in COMMERCIAL COCHLEAR IMPLANT DEVICES . in this paper , we propose a SOUND CODING FRAMEWORK based on the CHANNEL SELECTION CRITERION . the proposed SPECTRAL MAXIMA SOUND CODING ALGORITHMS is based on the CHANNEL SELECTION CRITERION . the proposed SPECTRAL MAXIMA SOUND CODING ALGORITHMS is based on the CHANNEL SELECTION CRITERION of the TIME-FREQUENCY UNIT and the INSTANTANEOUS SIGNAL . the proposed SPECTRAL MAXIMA SOUND CODING ALGORITHMS is evaluated on COMMERCIAL COCHLEAR IMPLANT DEVICES and compared to the conventional SPECTRAL MAXIMA SOUND CODING ALGORITHMS . the proposed SPECTRAL MAXIMA SOUND CODING ALGORITHMS is compared with other SPECTRAL MAXIMA SOUND CODING ALGORITHMS in terms of PERCEPTION QUALITY and PERCEPTION QUALITY .\n",
            "\n",
            "17 1000\n",
            "the SCENE FLOW in BINOCULAR STEREO SETUP is estimated using a SEED GROWING ALGORITHM . a pair of calibrated and synchronized cameras observe a scene and output a sequence of IMAGES . the algorithm jointly computes a DISPARITY MAP between the STEREO IMAGES and OPTICAL FLOW MAPS between consecutive frames . having the CALIBRATION , this is a representation of the SCENE FLOW , i.e. a 3D VELOCITY VECTOR is associated with each RECONSTRUCTED 3D POINT . the proposed algorithm starts from correspondence seeds and propagates the correspondences to the neighborhood . it is accurate for complex scenes with large motion and produces TEMPORALLY COHERENT STEREO DISPARITY and OPTICAL FLOW results . the algorithm is fast due to INHERENT SEARCH SPACE REDUCTION . \n",
            "this paper presents a novel SEED GROWING ALGORITHM for STEREO IMAGES . the proposed SEED GROWING ALGORITHM is based on the SEED GROWING ALGORITHM and the SEED GROWING ALGORITHM . the proposed SEED GROWING ALGORITHM is based on the SEED GROWING ALGORITHM and the SEED GROWING ALGORITHM . the proposed SEED GROWING ALGORITHM is based on the SEED GROWING ALGORITHM and the SEED GROWING ALGORITHM . the proposed SEED GROWING ALGORITHM is based on the SEED GROWING ALGORITHM and the SEED GROWING ALGORITHM . the proposed SEED GROWING ALGORITHM is based on the SEED GROWING ALGORITHM and the SEED GROWING ALGORITHM . experimental results show that the proposed method is robust and robust to CALIBRATION and CALIBRATION .\n",
            "\n",
            "18 1000\n",
            "in this paper we report on advances regarding our approach to porting an AUTOMATIC SPEECH RECOGNITION SYSTEM to a new target task . in case there is not enough ACOUSTIC DATA available to allow for thorough estimation of HMM PARAMETERS it is impossible to train an appropriate model . the basic idea to overcome this problem is to create a TASK INDEPENDENT SEED MODEL that can cope with all tasks equally well . however , the performance of such <unk> model is of course lower than the performance of TASK DEPENDENT MODELS -lrb- if these were available -rrb- . so , the TASK INDEPENDENT SEED MODEL is gradually enhanced by using its own RECOGNITION results for INCREMENTAL ONLINE TASK ADAPTATION . here , we use a MULTILINGUAL ROMANIC/GERMANIC SEED MODEL for a SLAVIC TARGET TASK . in tests on SLOVENE DIGITS MULTILINGUAL MODELING yields the best RECOGNITION ACCURACY compared to other LANGUAGE DEPENDENT MODELS . applying UNSUPERVISED ONLINE TASK ADAPTATION we observe a remarkable boost of RECOGNITION performance . \n",
            "this paper presents a novel MULTILINGUAL ROMANIC/GERMANIC SEED MODEL for UNSUPERVISED ONLINE TASK ADAPTATION . the proposed MULTILINGUAL ROMANIC/GERMANIC SEED MODEL is based on the TASK INDEPENDENT SEED MODEL . the proposed MULTILINGUAL ROMANIC/GERMANIC SEED MODEL is based on the TASK INDEPENDENT SEED MODEL . the proposed MULTILINGUAL ROMANIC/GERMANIC SEED MODEL is based on the TASK INDEPENDENT SEED MODEL . the proposed MULTILINGUAL ROMANIC/GERMANIC SEED MODEL is evaluated on the SLAVIC TARGET TASK . experimental results show that the proposed MULTILINGUAL ROMANIC/GERMANIC SEED MODEL improves the RECOGNITION ACCURACY of the AUTOMATIC SPEECH RECOGNITION SYSTEM .\n",
            "\n",
            "19 1000\n",
            "this paper presents a new UNSUPERVISED ALGORITHM -lrb- WORDENDS -rrb- for INFERRING WORD BOUNDARIES from TRANSCRIBED ADULT CONVERSATIONS . PHONE NGRAMS before and after observed pauses are used to bootstrap a simple DIS-CRIMINATIVE MODEL OF BOUNDARY MARKING . this fast algorithm delivers high performance even on MORPHOLOGICALLY COMPLEX WORDS in ENGLISH and ARABIC , and promising results on accurate PHONETIC TRANSCRIPTIONS with extensive pronunciation variation . expanding training data beyond the traditional MINIATURE DATASETS pushes performance numbers well above those previously reported . this suggests that WORDENDS is a viable model of CHILD LANGUAGE ACQUISITION and might be useful in SPEECH UNDERSTANDING . \n",
            "this paper presents a method for DIS-CRIMINATIVE MODEL OF BOUNDARY MARKING based on DIS-CRIMINATIVE MODEL OF BOUNDARY MARKING . the proposed UNSUPERVISED ALGORITHM is based on the DIS-CRIMINATIVE MODEL OF BOUNDARY MARKING and the UNSUPERVISED ALGORITHM . the proposed UNSUPERVISED ALGORITHM is based on the DIS-CRIMINATIVE MODEL OF BOUNDARY MARKING and the UNSUPERVISED ALGORITHM . the proposed method is based on the DIS-CRIMINATIVE MODEL OF BOUNDARY MARKING and the UNSUPERVISED ALGORITHM . the proposed UNSUPERVISED ALGORITHM is based on the DIS-CRIMINATIVE MODEL OF BOUNDARY MARKING and the UNSUPERVISED ALGORITHM . experimental results show the effectiveness of the proposed UNSUPERVISED ALGORITHM .\n",
            "\n",
            "20 1000\n",
            "we report an investigation of the perception of AMERICAN ENGLISH PHONEMES by DUTCH LISTENERS proficient in ENGLISH . listeners identified either the CONSONANT or the VOWEL in most possible ENGLISH cv and vc syllables . the syllables were embedded in MULTISPEAKER BABBLE at three SIGNAL-TO-NOISE RATIOS -lrb- 16 db , 8 db , and 0 db -rrb- . effects of SIGNAL-TO-NOISE RATIO on VOWEL AND CONSONANT IDENTIFICATION are discussed as a function of SYLLABLE POSITION and of relationship to the NATIVE PHONEME INVENTORY . comparison of the results with previously reported data from NATIVE LISTENERS reveals that noise affected the responding of native and non-native listeners similarly . \n",
            "this paper presents a method for DUTCH LISTENERS , which is based on the NATIVE PHONEME INVENTORY and the VOWEL . the proposed method is based on the NATIVE PHONEME INVENTORY and the VOWEL . the proposed method is based on the NATIVE PHONEME INVENTORY and the VOWEL . the proposed method is based on the NATIVE PHONEME INVENTORY and the SIGNAL-TO-NOISE RATIO of the CONSONANT .\n",
            "\n",
            "21 1000\n",
            "the STATUS QUO APPROACH to training OBJECT DETECTORS requires expensive BOUNDING BOX ANNOTATIONS . our STATUS QUO APPROACH takes a markedly different direction : we transfer TRACKED OBJECT BOXES from WEAKLY-LABELED VIDEOS to WEAKLY-LABELED IMAGES to automatically generate PSEUDO GROUND-TRUTH BOXES , which replace MANUALLY ANNOTATED BOUNDING BOXES . we first mine DISCRIMINATIVE REGIONS in the WEAKLY-LABELED IMAGE COLLECTION that <unk> appear in the POSI-TIVE/NEGATIVE IMAGES . we then match those regions to videos and retrieve the corresponding TRACKED OBJECT BOXES . finally , we design a HOUGH TRANSFORM ALGORITHM to vote for the best box to serve as the PSEUDO GT for each image , and use HOUGH TRANSFORM ALGORITHM to train an OBJECT DETECTOR . together , DISCRIMINATIVE REGIONS lead to state-of-the-art <unk> detection results on the PASCAL 2007 AND 2010 DATASETS . \n",
            "this paper presents a new method for WEAKLY-LABELED IMAGE COLLECTION in WEAKLY-LABELED VIDEOS . the HOUGH TRANSFORM ALGORITHM is based on the HOUGH TRANSFORM ALGORITHM . the HOUGH TRANSFORM ALGORITHM is based on the HOUGH TRANSFORM ALGORITHM . the proposed HOUGH TRANSFORM ALGORITHM is based on the HOUGH TRANSFORM ALGORITHM . the proposed HOUGH TRANSFORM ALGORITHM is based on the HOUGH TRANSFORM ALGORITHM . the proposed HOUGH TRANSFORM ALGORITHM is based on the HOUGH TRANSFORM ALGORITHM . the proposed HOUGH TRANSFORM ALGORITHM is based on the HOUGH TRANSFORM ALGORITHM . the proposed HOUGH TRANSFORM ALGORITHM is based on the HOUGH TRANSFORM ALGORITHM . the proposed HOUGH TRANSFORM ALGORITHM is based on the HOUGH TRANSFORM ALGORITHM . the proposed HOUGH TRANSFORM ALGORITHM is evaluated on PASCAL 2007 AND 2010 DATASETS and on PASCAL 2007 AND 2010 DATASETS .\n",
            "\n",
            "22 1000\n",
            "in this paper we study how children of different age groups -lrb- 8 and 12 years old -rrb- and with different cultural backgrounds -lrb- dutch and PAKISTANI -rrb- signal positive and negative emotions in AUDIOVISUAL SPEECH . data was collected in an ethical way using a simple but surprisingly effective game in which pairs of participants have to guess whether an upcoming card will contain a higher or lower number than a reference card . the data thus collected was used in a series of CROSS-CULTURAL PERCEPTION STUDIES , in which dutch and PAKISTANI observers classified emotional expressions of dutch and PAKISTANI speakers . results show that CLASSIFICATION ACCURACY is uniformly high for PAKISTANI CHILDREN , but drops for older and for winning dutch children 1 . \n",
            "this paper presents a new method for AUDIOVISUAL SPEECH . the proposed method is based on the use of PAKISTANI CHILDREN . the proposed method is based on the use of PAKISTANI CHILDREN , and is shown to improve the performance of the proposed CROSS-CULTURAL PERCEPTION STUDIES .\n",
            "\n",
            "23 1000\n",
            "the EXPECTATION-MAXIMIZATION is the dominant algorithm for estimating the parameters of a GAUSS MIXTURE . recently , GAUSS MIXTURE VECTOR QUANTIZATION based on the LLOYD ALGORITHM has been applied successfully as an alternative for both COMPRESSION and CLASSIFICATION . we investigate the performance of the two algorithms for <unk> 's in IMAGE RETRIEVAL . the ASYMPTOTIC LIKELIHOOD APPROXIMATION is used as a SIMILARITY CRITERION to compare <unk> 's directly . the two algorithms result in very close retrieval performance . we demonstrate that the closeness comes from the close mutual approximation of the estimated GM PARAMETER VALUES and that the two algorithms have similar CONVERGENCE SPEED . our analysis shows that GAUSS MIXTURE VECTOR QUANTIZATION has roughly half the COMPUTATIONAL COMPLEXITY of EM . \n",
            "this paper presents a new method for IMAGE RETRIEVAL based on GAUSS MIXTURE VECTOR QUANTIZATION . the proposed method is based on the ASYMPTOTIC LIKELIHOOD APPROXIMATION of the GAUSS MIXTURE . the proposed method is based on the ASYMPTOTIC LIKELIHOOD APPROXIMATION of the GAUSS MIXTURE . the proposed method is based on the ASYMPTOTIC LIKELIHOOD APPROXIMATION of the GAUSS MIXTURE . the proposed method is based on the ASYMPTOTIC LIKELIHOOD APPROXIMATION of the GAUSS MIXTURE . the proposed method is shown to outperform the conventional LLOYD ALGORITHM in terms of CONVERGENCE SPEED and CONVERGENCE SPEED .\n",
            "\n",
            "24 1000\n",
            "over the last few years , two of the main research directions in MACHINE LEARNING of NATURAL LANGUAGE PROCESSING have been the study of SEMI-SUPERVISED LEARNING ALGORITHMS as a way to train CLASSIFIERS when the LABELED DATA is scarce , and the study of ways to exploit knowledge and global information in STRUCTURED LEARNING TASKS . in this paper , we suggest a method for incorporating DOMAIN KNOWLEDGE in SEMI-SUPERVISED LEARNING ALGORITHMS . our novel framework unifies and can exploit several kinds of task specific constraints . the experimental results presented in the INFORMATION EXTRACTION DOMAIN demonstrate that applying constraints helps the model to generate better feedback during learning , and hence the framework allows for high performance learning with significantly less training data than was possible before on these tasks . \n",
            "this paper addresses the problem of MACHINE LEARNING in NATURAL LANGUAGE PROCESSING . in this paper , we propose a SEMI-SUPERVISED LEARNING ALGORITHMS for MACHINE LEARNING . the proposed SEMI-SUPERVISED LEARNING ALGORITHMS is based on the use of CLASSIFIERS to improve the performance of NATURAL LANGUAGE PROCESSING . experimental results show that the proposed SEMI-SUPERVISED LEARNING ALGORITHMS improves the performance of the proposed SEMI-SUPERVISED LEARNING ALGORITHMS .\n",
            "\n",
            "25 1000\n",
            "many approaches to REINFORCEMENT LEARNING combine NEURAL NETWORKS or other PARAMETRIC FUNCTION APPROXIMATORS with a form of TEMPORAL-DIIERENCE LEARNING to estimate the value function of a MARKOV DECISION PROCESS . a signiicant disadvantage of those procedures is that the resulting LEARNING ALGORITHMS are frequently unstable . in this work , we present a new , KERNEL-BASED APPROACH to REINFORCEMENT LEARNING which overcomes this diiculty and provably converges to a unique solution . by contrast to existing LEARNING ALGORITHMS , our KERNEL-BASED APPROACH can also be shown to be consistent in the sense that its costs converge to the optimal costs asymptotically . our focus is on learning in an AVERAGE-COST FRAMEWORK and on a practical application to the optimal PORTFOLIO CHOICE PROBLEM . \n",
            "this paper presents a new KERNEL-BASED APPROACH for REINFORCEMENT LEARNING . the proposed KERNEL-BASED APPROACH is based on a MARKOV DECISION PROCESS and a KERNEL-BASED APPROACH . the proposed KERNEL-BASED APPROACH is based on the KERNEL-BASED APPROACH and the KERNEL-BASED APPROACH . the proposed KERNEL-BASED APPROACH is compared with the conventional KERNEL-BASED APPROACH and the KERNEL-BASED APPROACH .\n",
            "\n",
            "26 1000\n",
            "we propose a BLOCK-ITERATIVE PARALLEL DECOMPOSITION METHOD to solve QUADRATIC SIGNAL RECOVERY PROBLEMS under CONVEX CONSTRAINTS . the idea of the BLOCK-ITERATIVE PARALLEL DECOMPOSITION METHOD is to <unk> the original MULTI-CONSTRAINT PROBLEM into a sequence of simple QUADRATIC MINIMIZATIONS over the intersection of two <unk> constructed by linearizing blocks of constraints . the implementation of the BLOCK-ITERATIVE PARALLEL DECOMPOSITION METHOD is quite exible thanks to its BLOCK-PARALLEL STRUCTURE . in addition a wide range of complex constraints can be incorporated since the BLOCK-ITERATIVE PARALLEL DECOMPOSITION METHOD does not require exact CONSTRAINT ENFORCEMENT at each step but merely APPROXIMATE ENFORCEMENT via LINEARIZATION . an application to DECONVOLUTION is demonstrated . \n",
            "this paper presents a new BLOCK-ITERATIVE PARALLEL DECOMPOSITION METHOD for QUADRATIC SIGNAL RECOVERY PROBLEMS . the BLOCK-ITERATIVE PARALLEL DECOMPOSITION METHOD is based on the BLOCK-ITERATIVE PARALLEL DECOMPOSITION METHOD . the BLOCK-ITERATIVE PARALLEL DECOMPOSITION METHOD is based on a BLOCK-ITERATIVE PARALLEL DECOMPOSITION METHOD . the proposed BLOCK-ITERATIVE PARALLEL DECOMPOSITION METHOD is based on the BLOCK-ITERATIVE PARALLEL DECOMPOSITION METHOD . the proposed BLOCK-ITERATIVE PARALLEL DECOMPOSITION METHOD is based on the BLOCK-ITERATIVE PARALLEL DECOMPOSITION METHOD . the proposed BLOCK-ITERATIVE PARALLEL DECOMPOSITION METHOD is based on the BLOCK-ITERATIVE PARALLEL DECOMPOSITION METHOD . the proposed BLOCK-ITERATIVE PARALLEL DECOMPOSITION METHOD is based on a BLOCK-ITERATIVE PARALLEL DECOMPOSITION METHOD . the proposed BLOCK-ITERATIVE PARALLEL DECOMPOSITION METHOD is based on a BLOCK-ITERATIVE PARALLEL DECOMPOSITION METHOD .\n",
            "\n",
            "27 1000\n",
            "this paper proposes a practical OPTIMIZATION METHOD for LAYERED NEURAL NETWORKS , by which the optimal model and parameter can be found simultaneously . ` i \\ te modify the conventional INFORMATION CRITERION into a differentiable function of parameters , and then , minimize INFORMATION CRITERION , while controlling INFORMATION CRITERION back to the ordinary form . effectiveness of this OPTIMIZATION METHOD is discussed theoretically and experimentally . \n",
            "this paper presents a new OPTIMIZATION METHOD for LAYERED NEURAL NETWORKS . the proposed OPTIMIZATION METHOD is based on a OPTIMIZATION METHOD . the proposed OPTIMIZATION METHOD is based on the INFORMATION CRITERION .\n",
            "\n",
            "28 1000\n",
            "this paper describes ongoing research on a JAPANESE-TO-ENGLISH SPEECH-TO-SPEECH TRANSLATION SYSTEM for '' controlled <unk> '' , such as TV NEWS and COMMENTARY PROGRAMS in which the SPEAKING STYLES are controlled as a <unk> . we have adopted the DATA-DRIVEN APPROACH since the tv programs in question cover a wide range of topics , and because it seems much too labor intensive to <unk> TRANSLATION RULES . the DATA-DRIVEN APPROACH therefore requires a <unk> PARALLEL CORPUS for the target domain , although the ABSOLUTE SIZE needed is not so easy to obtain . there are also difficulties inherent in <unk> such as the need to handle long sentences , averaging over 25 words , and the REALIZATION OF SIMULTANEITY . these problems are presented in the light of our available corpora and we go on to present the kinds of problems we have to solve . finally , we present our prospective system architecture and introduce the present status of the work . \n",
            "this paper presents a method for REALIZATION OF SIMULTANEITY in TV NEWS . the proposed method is based on the REALIZATION OF SIMULTANEITY and the REALIZATION OF SIMULTANEITY . the proposed method is based on the DATA-DRIVEN APPROACH and the DATA-DRIVEN APPROACH . it is shown that the proposed method is robust to SPEAKING STYLES and ABSOLUTE SIZE .\n",
            "\n",
            "29 1000\n",
            "efficient two-step algorithms are described for optimizing the STOPBAND RESPONSE of the PROTOTYPE FILTER for COSINE-MODULATED AND MODIFIED DFT FILTER BANKS either in the MINIMAX or in the least-mean-square sense subject to the maximum allowable ALIASING AND AMPLITUDE ERRORS . the first step involves finding a good START-UP SOLUTION using a simple technique . this START-UP SOLUTION is improved in the second step by using NONLINEAR OPTIMIZATION . several examples are included illustrating the flexibility of the proposed START-UP SOLUTION for making compromises between the required FILTER LENGTHS and the ALIASING AND AMPLITUDE ERRORS . these examples show that by allowing very small amplitude and ALIASING ERRORS , the stopband performance of the resulting PROTOTYPE FILTER is significantly improved compared to the corresponding PERFECT-RECONSTRUCTION FILTER BANK . alternatively , the FILTER ORDERS and , consequently , the overall delay can be significantly reduced to achieve practically the same performance . \n",
            "this paper proposes a new PROTOTYPE FILTER for NONLINEAR OPTIMIZATION . the proposed PROTOTYPE FILTER is based on the PERFECT-RECONSTRUCTION FILTER BANK . the proposed PROTOTYPE FILTER is based on the PERFECT-RECONSTRUCTION FILTER BANK . the proposed PROTOTYPE FILTER is based on the PERFECT-RECONSTRUCTION FILTER BANK . the proposed PROTOTYPE FILTER is compared with the conventional PROTOTYPE FILTER and the PROTOTYPE FILTER . the proposed START-UP SOLUTION is compared with the conventional EFFICIENT TWO-STEP ALGORITHMS and the PROTOTYPE FILTER .\n",
            "\n",
            "30 1000\n",
            "in this work we describe two distinct novel improvements to our SPEAKER DIARIZATION SYSTEM , previously proposed for ANALYSIS OF MEETING SPEECH . the first approach focuses on recurrent selection of REPRESENTATIVE SPEECH SEGMENTS for SPEAKER CLUSTERING while the other is based on PARTICIPANT INTERACTION PATTERN MODELING . the former selects SPEECH SEGMENTS with high relevance to SPEAKER CLUSTERING , especially from a ROBUST CLUSTER MODELING PERSPECTIVE , and keeps updating them throughout CLUSTERING PROCEDURES . the latter statistically models CONVERSATION PATTERNS between meeting participants and applies it as a PRIORI INFORMATION when refining diarization results . experimental results reveal that the two proposed approaches provide performance enhancement by <unk> % -lrb- relative -rrb- in terms of DI-ARIZATION ERROR RATE in tests on 13 meeting excerpts from various MEETING SPEECH CORPORA . \n",
            "this paper presents a novel SPEAKER DIARIZATION SYSTEM for ANALYSIS OF MEETING SPEECH . the SPEAKER DIARIZATION SYSTEM is based on a ROBUST CLUSTER MODELING PERSPECTIVE and a PRIORI INFORMATION . the SPEAKER DIARIZATION SYSTEM is based on a ROBUST CLUSTER MODELING PERSPECTIVE and a PRIORI INFORMATION . the proposed SPEAKER DIARIZATION SYSTEM is based on a ROBUST CLUSTER MODELING PERSPECTIVE , which is based on the PRIORI INFORMATION . the proposed method is based on a ROBUST CLUSTER MODELING PERSPECTIVE , which is based on the PRIORI INFORMATION . the proposed SPEAKER DIARIZATION SYSTEM is applied to the ANALYSIS OF MEETING SPEECH .\n",
            "\n",
            "31 1000\n",
            "we model the responses of cells in VISUAL AREA VI during NATURAL VISION . our model consists of a CLASSICAL ENERGY MECHANISM whose output is divided by NONCLASSICAL GAIN CONTROL and texture contrast mechanisms . we apply this model to review movies , a STIMULUS SEQUENCE that <unk> the stimulation a cell receives during free viewing of NATURAL IMAGES . data were collected from three cells using five different review movies , and the model was fit separately to the data from each movie . for the ENERGY MECHANISM alone we find modest but significant correlations -lrb- re = <unk> , <unk> , <unk> , 0.35 -rrb- between model and data . these correlations are improved somewhat when we allow for SUPPRESSIVE SURROUND EFFECTS -lrb- re + g = <unk> , <unk> , <unk> , <unk> -rrb- . in one case the inclusion of a DELAYED SUPPRESSIVE SURROUND dramatically improves the fit to the data by modifying the time course of the model 's response . \n",
            "this paper addresses the problem of NONCLASSICAL GAIN CONTROL in NATURAL IMAGES . we propose a method for NONCLASSICAL GAIN CONTROL based on the CLASSICAL ENERGY MECHANISM . the proposed method is based on the CLASSICAL ENERGY MECHANISM . the proposed method is based on the CLASSICAL ENERGY MECHANISM . the proposed method is based on the CLASSICAL ENERGY MECHANISM . the proposed method is based on the CLASSICAL ENERGY MECHANISM .\n",
            "\n",
            "32 1000\n",
            "the most popular TIME-FREQUENCY ANALYSIS TOOL , the SHORT-TIME FOURIER TRANSFORM , suffers from BLURRY HARMONIC REPRESENTATION when VOICED SPEECH undergoes changes in pitch . these relatively fast variations lead to INCONSISTENT BINS in frequency domain and can not be accurately described by the FOURIER ANALYSIS with high resolution both in time and frequency . in this paper a new ANALYSIS TOOL , called SHORT-TIME FOURIER TRANSFORM is presented , offering more precise time-frequency representation of speech signals . the base of this SHORT-TIME FOURIER TRANSFORM is composed of QUADRATIC CHIRPS that follow the PITCH TENDENCY SEGMENT-BY-SEGMENT . comparative results between the proposed SHORT-TIME FOURIER TRANSFORM and popular TIME-FREQUENCY TECHNIQUES reveal an improvement in TIME-FREQUENCY LOCALIZATION and finer SPECTRAL REPRESENTATION . since the signal can be resynthesized from its SHORT-TIME FOURIER TRANSFORM , the proposed ANALYSIS TOOL is also suitable for FILTERING PURPOSES . \n",
            "this paper presents a new ANALYSIS TOOL called BLURRY HARMONIC REPRESENTATION , which is a TIME-FREQUENCY ANALYSIS TOOL for FILTERING PURPOSES . the SHORT-TIME FOURIER TRANSFORM is a TIME-FREQUENCY ANALYSIS TOOL , called PITCH TENDENCY SEGMENT-BY-SEGMENT , for FILTERING PURPOSES . the SHORT-TIME FOURIER TRANSFORM is a SPECTRAL REPRESENTATION , which is a ANALYSIS TOOL . the ANALYSIS TOOL is a ANALYSIS TOOL , which is a ANALYSIS TOOL . the ANALYSIS TOOL is a ANALYSIS TOOL , which is a TIME-FREQUENCY ANALYSIS TOOL . the ANALYSIS TOOL is a ANALYSIS TOOL , which is a ANALYSIS TOOL . the ANALYSIS TOOL is a ANALYSIS TOOL and a ANALYSIS TOOL .\n",
            "\n",
            "33 1000\n",
            "we present an algorithm for REVERBERANT SPEECH ENHANCEMENT using one microphone . we first propose a novel PITCH-BASED REVERBERATION MEASURE for ESTIMATING REVERBERATION TIME based on the distribution of RELATIVE TIME LAGS . this measure of PITCH STRENGTH correlates with REVERBERATION and decreases systematically as detrimental effects of REVERBERATION on HARMONIC STRUCTURE increase . then a REVERBERANT SPEECH ENHANCEMENT METHOD is developed to estimate and <unk> later ECHO COMPONENTS . the results show that our approach appreciably reduces REVERBERATION EFFECTS . \n",
            "this paper presents a new method for REVERBERANT SPEECH ENHANCEMENT based on PITCH STRENGTH . the proposed method is based on the HARMONIC STRUCTURE of the ECHO COMPONENTS . the proposed method is based on the HARMONIC STRUCTURE of the ECHO COMPONENTS . the proposed method is based on the HARMONIC STRUCTURE and the PITCH STRENGTH of the ECHO COMPONENTS . the proposed method is compared with the conventional REVERBERANT SPEECH ENHANCEMENT METHOD .\n",
            "\n",
            "34 1000\n",
            "this paper presents several fundamental FREQUENCY-DOMAIN BOUNDS for a NON-NEGATIVE IMPULSE RESPONSE FILTER . UPPER-BOUNDS on POWER SPECTRAL ATTENUATION and POWER SPECTRAL GAIN in GEOMETRICALLY SPACED FREQUENCY REGIONS are derived , when POWER SPECTRAL ATTENUATION near frequency zero is limited . by analyzing the <unk> of these bounds , the relationship between the MAXIMALLY ALLOWABLE POWER ATTENUA-TION and gain is also treated . all results hold for both CONTINUOUS AND DISCRETE-TIME DOMAINS . \n",
            "this paper presents a new method for GEOMETRICALLY SPACED FREQUENCY REGIONS in CONTINUOUS AND DISCRETE-TIME DOMAINS . the proposed method is based on the NON-NEGATIVE IMPULSE RESPONSE FILTER of the NON-NEGATIVE IMPULSE RESPONSE FILTER . the proposed method is based on the NON-NEGATIVE IMPULSE RESPONSE FILTER of the NON-NEGATIVE IMPULSE RESPONSE FILTER . the proposed method is based on the NON-NEGATIVE IMPULSE RESPONSE FILTER . the proposed method is shown to outperform the conventional FREQUENCY-DOMAIN BOUNDS in terms of POWER SPECTRAL ATTENUATION .\n",
            "\n",
            "35 1000\n",
            "this paper presents a framework for <unk> belief change in PROPOSITIONAL HORN LOGIC . we firstly establish a PARALLEL INTERPOLATION THEOREM for HORN LOGIC and show that <unk> 's finest splitting theorem holds with HORN FORMULAE . by reformulating PARIKH 'S RELEVANCE CRITERION in the setting of horn belief change , we construct a RELEVANCE-BASED PARTIAL MEET HORN CONTRACTION OPERATOR and provide a REPRESENTATION THEOREM for the operator . interestingly , we find that this RELEVANCE-BASED PARTIAL MEET HORN CONTRACTION OPERATOR can be fully characterised by <unk> and <unk> 's postulates for PARTIAL MEET HORN CONTRACTION as well as PARIKH 'S RELEVANCE POSTULATE without requiring any change on the postulates , which is qualitatively different from the case in CLASSICAL PROPOSITIONAL LOGIC . \n",
            "this paper addresses the problem of PARIKH 'S RELEVANCE POSTULATE in CLASSICAL PROPOSITIONAL LOGIC . we propose a REPRESENTATION THEOREM based on the PARIKH 'S RELEVANCE CRITERION . the proposed REPRESENTATION THEOREM is based on the REPRESENTATION THEOREM . the proposed REPRESENTATION THEOREM is based on the REPRESENTATION THEOREM . the proposed REPRESENTATION THEOREM is based on the REPRESENTATION THEOREM . the proposed REPRESENTATION THEOREM is based on the REPRESENTATION THEOREM .\n",
            "\n",
            "36 1000\n",
            "in this paper , based on LARGE SPEECH CORPUS with prosodic structure label -lrb- <unk> -rrb- , we present some statistic result on ACOUSTIC PARAMETER at PROSODIC BOUNDARY . we study the SYLLABLE DURATION , INTENSITY and pitch at the boundary and select a SERIAL ACOUSTIC PARAMETER to train a CART . then the CART was employed to classify the PROSODIC BOUNDARY TYPE . the result shows that the PARAMETER characterize ACOUSTIC FEATURE of the PROSODIC BOUNDARY and the trained CART can classify different BOUNDARY EFFICIENCY . so it is possible to train STATISTICAL MODEL for PROSODIC BOUNDARY LOCATION in MANDARIN , this is very important both for SPEECH RECOGNITION and synthesis . \n",
            "this paper presents a method for SPEECH RECOGNITION from a LARGE SPEECH CORPUS . the STATISTICAL MODEL is based on a STATISTICAL MODEL and a STATISTICAL MODEL . the proposed STATISTICAL MODEL is based on a STATISTICAL MODEL and a STATISTICAL MODEL . the proposed method is based on a STATISTICAL MODEL and a STATISTICAL MODEL . the proposed method is based on a STATISTICAL MODEL and a STATISTICAL MODEL . the proposed method is based on a STATISTICAL MODEL . the proposed method is based on a STATISTICAL MODEL and is shown to be robust to BOUNDARY EFFICIENCY and BOUNDARY EFFICIENCY .\n",
            "\n",
            "37 1000\n",
            "our goal is to obtain a NOISE-FREE , HIGH RESOLUTION IMAGE , from an observed , noisy , low resolution -lrb- lr -rrb- IMAGE . the conventional approach of preprocessing the IMAGE with a DENOISING ALGORITHM , followed by applying a SUPER-RESOLUTION ALGORITHM , has an important limitation : along with NOISE , some high frequency content of the IMAGE -lrb- particularly textural detail -rrb- is invariably lost during the DENOISING STEP . this ` denoising loss ' restricts the performance of the subsequent SR STEP , wherein the challenge is to synthesize such TEXTURAL DETAILS . in this paper , we show that high frequency content in the NOISY IMAGE -lrb- which is <unk> removed by DENOISING ALGORITHMS -rrb- can be effectively used to obtain the MISSING TEXTURAL DETAILS in the HR DOMAIN . to do so , we first obtain HR VERSIONS of both the NOISY AND THE DENOISED IMAGES , using a PATCH-SIMILARITY BASED SR ALGORITHM . we then show that by taking a CONVEX COMBINATION of orientation and frequency selective bands of the noisy and the denoised HR IMAGE , we can obtain a desired HR IMAGE where -lrb- i -rrb- some of the TEXTURAL SIGNAL lost in the DENOISING STEP is effectively recovered in the HR DOMAIN , and -lrb- ii -rrb- additional textures can be easily synthesized by appropriately constraining the parameters of the CONVEX COMBINATION . we show that this PART-RECOVERY AND PART-SYNTHESIS OF TEXTURES through our algorithm yields HR IMAGE that are visually more pleasing than those obtained using the conventional PROCESSING PIPELINE . furthermore , our results show a consistent improvement in NUMERICAL METRICS , further <unk> the ability of our algorithm to recover LOST SIGNAL . \n",
            "this paper presents a new method for PART-RECOVERY AND PART-SYNTHESIS OF TEXTURES in the HR DOMAIN . the proposed method is based on a CONVEX COMBINATION , which is based on the DENOISING ALGORITHM . the proposed method is based on a CONVEX COMBINATION and a CONVEX COMBINATION . the proposed method is based on a CONVEX COMBINATION , which is based on the DENOISING ALGORITHM . the proposed method is based on a CONVEX COMBINATION and a CONVEX COMBINATION . the proposed method is based on a CONVEX COMBINATION and a CONVEX COMBINATION . the proposed method is based on the PATCH-SIMILARITY BASED SR ALGORITHM . the proposed method is based on a CONVEX COMBINATION and a CONVEX COMBINATION . the proposed method is based on a CONVEX COMBINATION and a CONVEX COMBINATION .\n",
            "\n",
            "38 1000\n",
            "we examined the sequence of DECISION PROBLEMS that are encountered in the game of TETRIS and found that most of the problems are easy in the following sense : one can choose well among the available actions without knowing an EVALUATION FUNCTION that scores well in the game . this is a consequence of three conditions that are prevalent in the game : simple DOMINANCE , CUMULATIVE DOMINANCE , and NONCOMPENSATION . these conditions can be exploited to develop faster and more effective LEARNING ALGORITHMS . in addition , they allow certain types of DOMAIN KNOWLEDGE to be incorporated with ease into a LEARNING ALGORITHMS . among the SEQUENTIAL DECISION PROBLEMS we encounter , it is unlikely that TETRIS is unique or rare in having these properties . \n",
            "this paper addresses the problem of SEQUENTIAL DECISION PROBLEMS in DECISION PROBLEMS . we propose a method for SEQUENTIAL DECISION PROBLEMS , which is based on the EVALUATION FUNCTION and the CUMULATIVE DOMINANCE . the proposed method is based on the use of CUMULATIVE DOMINANCE and DOMINANCE . the proposed method is evaluated on the EVALUATION FUNCTION and the DECISION PROBLEMS .\n",
            "\n",
            "39 1000\n",
            "camera images saved in RAW FORMAT are being adopted in COMPUTER VISION TASKS since RAW VALUES represent MINIMALLY PROCESSED SENSOR RESPONSES . CAMERA MANUFACTURERS , however , have yet to adopt a standard for RAW IMAGES and current RAW-RGB VALUES are device specific due to different SENSORS SPECTRAL SENSITIVITIES . this results in significantly different RAW IMAGES for the same scene captured with different cameras . this paper focuses on estimating a MAPPING that can convert a RAW IMAGE of an arbitrary scene and illumination from one camera 's raw space to another . to this end , we examine various MAPPING STRATEGIES including LINEAR AND NON-LINEAR TRANSFORMATIONS applied both in a GLOBAL AND ILLUMINATION-SPECIFIC MANNER . we show that ILLUMINATION-SPECIFIC MAPPINGS give the best result , however , at the expense of requiring a large number of transformations . to address this issue , we introduce an ILLUMINATION-INDEPENDENT MAPPING APPROACH that uses WHITE-BALANCING to assist in reducing the number of required transformations . we show that this ILLUMINATION-INDEPENDENT MAPPING APPROACH achieves state-of-the-art results on a range of CONSUMER CAMERAS and IMAGES OF ARBITRARY SCENES and ILLUMINATIONS . \n",
            "this paper presents a novel ILLUMINATION-INDEPENDENT MAPPING APPROACH for IMAGES OF ARBITRARY SCENES . the proposed ILLUMINATION-INDEPENDENT MAPPING APPROACH is based on the ILLUMINATION-INDEPENDENT MAPPING APPROACH and the ILLUMINATION-INDEPENDENT MAPPING APPROACH . the proposed ILLUMINATION-INDEPENDENT MAPPING APPROACH is based on the ILLUMINATION-INDEPENDENT MAPPING APPROACH and the ILLUMINATION-INDEPENDENT MAPPING APPROACH . the proposed ILLUMINATION-INDEPENDENT MAPPING APPROACH is based on the ILLUMINATION-INDEPENDENT MAPPING APPROACH and the ILLUMINATION-INDEPENDENT MAPPING APPROACH . the proposed ILLUMINATION-INDEPENDENT MAPPING APPROACH is based on the ILLUMINATION-INDEPENDENT MAPPING APPROACH and the ILLUMINATION-INDEPENDENT MAPPING APPROACH . experimental results on IMAGES OF ARBITRARY SCENES demonstrate the effectiveness of the proposed ILLUMINATION-INDEPENDENT MAPPING APPROACH .\n",
            "\n",
            "40 1000\n",
            "<unk> knowledge representation formalisms can be used to represent objective , <unk> facts about an APPLICATION DOMAIN . NOTIONS like BELIEF , INTENTIONS , and time which are essential for the REPRESENTATION OF MULTI-AGENT ENVIRONMENTS can only be expressed in a very limited way . for such notions , MODAL LOGICS with possible worlds semantics provides a formally well-founded and <unk> basis . this paper presents a framework for integrating MODAL OPERATORS into TERMINOLOGICAL KNOWLEDGE REPRESENTATION LANGUAGES . these MODAL OPERATORS can be used both inside of CONCEPT EXPRESSIONS and in front of <unk> and <unk> axioms . we introduce SYNTAX and semantics of the extended language , and show that satisfiability of finite sets of formulas is decidable , provided that all MODAL OPERATORS are interpreted in the basic logic k , and that the increasing DOMAIN ASSUMPTION is used . \n",
            "this paper addresses the problem of TERMINOLOGICAL KNOWLEDGE REPRESENTATION LANGUAGES in APPLICATION DOMAIN . we propose a method for TERMINOLOGICAL KNOWLEDGE REPRESENTATION LANGUAGES , based on MODAL OPERATORS . the method is based on the use of MODAL OPERATORS and MODAL OPERATORS . the proposed method is based on the use of MODAL OPERATORS and MODAL OPERATORS . experimental results show the effectiveness of the proposed method in terms of INTENTIONS and INTENTIONS .\n",
            "\n",
            "41 1000\n",
            "various methods have been proposed for AUTOMATIC SYNONYM ACQUISITION , as SYNONYMS are one of the most fundamental LEXICAL KNOWLEDGE . whereas many methods are based on CONTEXTUAL CLUES OF WORDS , little attention has been paid to what kind of categories of CONTEX-TUAL INFORMATION are useful for the purpose . this study has experimentally investigated the impact of CONTEXTUAL INFORMATION SELECTION , by extracting three kinds of WORD RELATIONSHIPS from corpora : dependency , SENTENCE CO-OCCURRENCE , and PROXIMITY . the evaluation result shows that while dependency and PROXIMITY perform relatively well by themselves , combination of two or more kinds of CONTEXTUAL INFORMATION gives more stable performance . we <unk> further investigated useful selection of DEPENDENCY RELATIONS and MODIFICATION CATEGORIES , and it is found that modification has the greatest contribution , even greater than the widely adopted SUBJECT-OBJECT COMBINATION . \n",
            "this paper addresses the problem of AUTOMATIC SYNONYM ACQUISITION in AUTOMATIC SYNONYM ACQUISITION such as PROXIMITY , PROXIMITY , and SYNONYMS . we propose a method for AUTOMATIC SYNONYM ACQUISITION based on CONTEXTUAL INFORMATION SELECTION . the proposed method is based on CONTEXTUAL INFORMATION SELECTION and CONTEXTUAL INFORMATION SELECTION . the proposed method is based on CONTEXTUAL INFORMATION SELECTION and CONTEXTUAL INFORMATION SELECTION . the proposed method is based on CONTEXTUAL INFORMATION SELECTION and CONTEXTUAL INFORMATION SELECTION . the experimental results show that the proposed method outperforms the conventional method in terms of PROXIMITY and PROXIMITY .\n",
            "\n",
            "42 1000\n",
            "we present a PLANNER NAMED TRANSITION CONSTRAINTS for PARALLEL PLANNING . TCPP constructs a new CONSTRAINT MODEL from DOMAIN TRANSITION GRAPHS of a given planning problem . TCPP encodes the CONSTRAINT MODEL by using TABLE CONSTRAINTS that allow do n't <unk> or WILD CARDS as CELL VALUES . TCPP uses MINION THE CONSTRAINT SOLVER to solve the CONSTRAINT MODEL and returns the PARALLEL PLAN . empirical results exhibit the efficiency of our CONSTRAINT MODEL over state-of-the-art CONSTRAINT-BASED PLANNERS . \n",
            "this paper proposes a new CONSTRAINT MODEL for PARALLEL PLANNING . the CONSTRAINT MODEL is based on the CONSTRAINT MODEL and uses a CONSTRAINT MODEL to estimate the CELL VALUES from the PARALLEL PLAN . the proposed CONSTRAINT MODEL is based on the CONSTRAINT MODEL . the proposed CONSTRAINT MODEL is compared with conventional CONSTRAINT-BASED PLANNERS . the proposed CONSTRAINT MODEL is shown to outperform the conventional CONSTRAINT-BASED PLANNERS in terms of TCPP .\n",
            "\n",
            "43 1000\n",
            "in this paper we consider the problem of describing the action being performed by HUMAN FIGURES in still IMAGES . we will attack this problem using an UNSUPERVISED LEARNING APPROACH , attempting to discover the set of ACTION CLASSES present in a large collection of TRAINING IMAGES . these ACTION CLASSES will then be used to label test IMAGES . our UNSUPERVISED LEARNING APPROACH uses the COARSE SHAPE of the HUMAN FIGURES to match pairs of IMAGES . the distance between a pair of IMAGES is computed using a LINEAR PROGRAMMING RELAXATION TECHNIQUE . this is a computationally expensive process , and we employ a fast PRUNING METHOD to enable its use on a large collection of IMAGES . SPECTRAL CLUSTERING is then performed using the resulting distances . we present CLUSTERING AND IMAGE LABELING results on a variety of datasets . \n",
            "this paper presents a new method for CLUSTERING AND IMAGE LABELING based on SPECTRAL CLUSTERING . the proposed method is based on a LINEAR PROGRAMMING RELAXATION TECHNIQUE of the TRAINING IMAGES . the proposed method is based on the LINEAR PROGRAMMING RELAXATION TECHNIQUE . the proposed method is based on the LINEAR PROGRAMMING RELAXATION TECHNIQUE . the proposed method is based on the LINEAR PROGRAMMING RELAXATION TECHNIQUE . the proposed method is based on the LINEAR PROGRAMMING RELAXATION TECHNIQUE . the proposed method is based on a LINEAR PROGRAMMING RELAXATION TECHNIQUE .\n",
            "\n",
            "44 1000\n",
            "creating large amounts of ANNOTATED DATA to train STATISTICAL PCFG PARSERS is expensive , and the performance of such PARSERS declines when training and test data are taken from different domains . in this paper we use SELF-TRAINING in order to improve the quality of a PARSER and to adapt PARSER to a different domain , using only small amounts of MANUALLY ANNOTATED SEED DATA . we report significant improvement both when the SEED AND TEST DATA are in the same domain and in the OUT-OF-DOMAIN ADAPTATION SCENARIO . in particular , we achieve 50 % reduction in ANNOTATION COST for the IN-DOMAIN CASE , yielding an improvement of 66 % over previous work , and a <unk> % reduction for the DOMAIN ADAPTATION CASE . this is the first time that SELF-TRAINING with small labeled datasets is applied successfully to these tasks . we were also able to formulate a characterization of when SELF-TRAINING is valuable . \n",
            "this paper addresses the problem of SELF-TRAINING in DOMAIN ADAPTATION CASE . we propose a PARSER based on a PARSER and a PARSER for SELF-TRAINING . the ANNOTATION COST of the proposed PARSER is evaluated using the SEED AND TEST DATA and the DOMAIN ADAPTATION CASE .\n",
            "\n",
            "45 1000\n",
            "arai has developed several PHYSICAL MODELS of the HUMAN VOCAL TRACT for education and has reported that PHYSICAL MODELS are intuitive and helpful for students of acoustics and speech science . we first reviewed DYNAMIC MODELS , including the SLIDING THREE-TUBE MODEL and the FLEXIBLE-TONGUE MODEL . we then developed a HEAD-SHAPED MODEL with a SLIDING TONGUE , which has the advantages of both the S3T AND FLEXIBLE-TONGUE MODELS . we also developed a COMPUTER-CONTROLLED VERSION of the UMEDA & TERANISHI MODEL , as the original HEAD-SHAPED MODEL was hard to manipulate precisely by hand . these HEAD-SHAPED MODEL are useful when teaching the dynamic aspects of speech . \n",
            "this paper addresses the problem of SLIDING TONGUE in HUMAN VOCAL TRACT such as the HUMAN VOCAL TRACT . we propose a new method for the HUMAN VOCAL TRACT . the proposed method is based on the UMEDA & TERANISHI MODEL and the HEAD-SHAPED MODEL . the proposed method is based on the UMEDA & TERANISHI MODEL and the UMEDA & TERANISHI MODEL . experimental results show that the proposed method is robust and robust to SLIDING TONGUE such as SLIDING TONGUE , and the UMEDA & TERANISHI MODEL .\n",
            "\n",
            "46 1000\n",
            "we provide a new analysis of an efficient MARGIN-BASED ALGORITHM for SELECTIVE SAMPLING in CLASSIFICATION PROBLEMS . using the so-called <unk> LOW NOISE CONDITION to parametrize the INSTANCE DISTRIBUTION , we show bounds on the CONVERGENCE RATE to the BAYES RISK of both the fully supervised and the SELECTIVE SAMPLING VERSIONS of the basic algorithm . our analysis reveals that , excluding LOGARITHMIC FACTORS , the average risk of the SELECTIVE SAMPLER converges to the BAYES RISK at rate n − -lrb- 1 + α -rrb- -lrb- 2 + α -rrb- / 2 -lrb- 3 + α -rrb- where n denotes the number of queried labels , and α > 0 is the exponent in the LOW NOISE CONDITION . for all α > √ 3 − 1 ≈ 0.73 this CONVERGENCE RATE is asymptotically faster than the rate n − -lrb- 1 + α -rrb- / -lrb- 2 + α -rrb- achieved by the fully supervised version of the same CLASSIFIER , which queries all labels , and for α → ∞ the two rates exhibit an exponential gap . experiments on TEXTUAL DATA reveal that simple variants of the proposed SELECTIVE SAMPLER perform much better than popular and similarly efficient competitors . \n",
            "this paper proposes a new SELECTIVE SAMPLER for SELECTIVE SAMPLING . the proposed MARGIN-BASED ALGORITHM is based on the INSTANCE DISTRIBUTION . the proposed MARGIN-BASED ALGORITHM is based on the MARGIN-BASED ALGORITHM . the proposed MARGIN-BASED ALGORITHM is based on the INSTANCE DISTRIBUTION . the proposed MARGIN-BASED ALGORITHM is compared with the conventional MARGIN-BASED ALGORITHM and the MARGIN-BASED ALGORITHM . the CONVERGENCE RATE of the proposed MARGIN-BASED ALGORITHM is compared with the conventional SELECTIVE SAMPLER .\n",
            "\n",
            "47 1000\n",
            "bilingual speakers are known for their ability to <unk> or mix their languages during communication . this phenomenon occurs when BILINGUALS substitute a word or phrase from one language with a phrase or word from another language . for CODE-SWITCHING SPEECH RECOGNITION , it is essential to collect a LARGE-SCALE CODE-SWITCHING SPEECH DATABASE for MODEL TRAINING . in order to ease the negative effect caused by the DATA SPARSENESS PROBLEM in training code-switching speech recognizers , this study proposes a DATA-DRIVEN APPROACH to PHONE SET CONSTRUCTION by integrating ACOUSTIC FEATURES and CROSS-LINGUAL CONTEXT-SENSITIVE ARTICULATORY FEATURES into DISTANCE MEASURE between phone units . KL-DIVERGENCE and a HIERARCHICAL PHONE UNIT CLUSTERING ALGORITHM are used in this study to cluster similar phone units to reduce the need of the training data for MODEL CONSTRUCTION . the experimental results show that the proposed DATA-DRIVEN APPROACH outperforms other traditional PHONE SET CONSTRUCTION METHODS . \n",
            "this paper proposes a new DATA-DRIVEN APPROACH for CODE-SWITCHING SPEECH RECOGNITION . the proposed HIERARCHICAL PHONE UNIT CLUSTERING ALGORITHM is based on a HIERARCHICAL PHONE UNIT CLUSTERING ALGORITHM and a DISTANCE MEASURE based on the DISTANCE MEASURE . the proposed DATA-DRIVEN APPROACH is based on the HIERARCHICAL PHONE UNIT CLUSTERING ALGORITHM and the HIERARCHICAL PHONE UNIT CLUSTERING ALGORITHM . the proposed DATA-DRIVEN APPROACH is based on the HIERARCHICAL PHONE UNIT CLUSTERING ALGORITHM and the HIERARCHICAL PHONE UNIT CLUSTERING ALGORITHM . the proposed DATA-DRIVEN APPROACH is compared with other PHONE SET CONSTRUCTION METHODS and PHONE SET CONSTRUCTION METHODS .\n",
            "\n",
            "48 1000\n",
            "we describe a generic framework for integrating various STOCHASTIC MODELS OF DISCOURSE COHERENCE in a manner that takes advantage of their individual strengths . an integral part of this framework are algorithms for searching and training these STOCHASTIC MODELS OF DISCOURSE COHERENCE . we evaluate the performance of our models and algorithms and show empirically that UTILITY-TRAINED LOG-LINEAR COHERENCE MODELS out-perform each of the individual COHERENCE MODELS considered . \n",
            "this paper addresses the problem of STOCHASTIC MODELS OF DISCOURSE COHERENCE for STOCHASTIC MODELS OF DISCOURSE COHERENCE . in this paper , we show that the UTILITY-TRAINED LOG-LINEAR COHERENCE MODELS are more robust than the conventional COHERENCE MODELS .\n",
            "\n",
            "49 1000\n",
            "fast <unk> methods for MAXIMUM MARGIN MATRIX FACTORIZATION were recently shown to have great promise -lrb- <unk> & <unk> , 2005 -rrb- , including significantly outperforming the previous state-of-the-art methods on some standard COLLABORATIVE PREDICTION BENCHMARKS -lrb- including <unk> -rrb- . in this paper , we investigate ways to further improve the performance of MAXIMUM MARGIN MATRIX FACTORIZATION , by casting MAXIMUM MARGIN MATRIX FACTORIZATION within an ENSEMBLE APPROACH . we explore and evaluate a variety of alternative ways to define such ensembles . we show that our resulting ensembles can perform significantly better than a single MMMF MODEL , along multiple EVALUATION METRICS . in fact , we find that ensembles of partially trained MMMF MODEL can sometimes even give better predictions in TOTAL TRAINING TIME comparable to a single MMMF MODEL . \n",
            "this paper addresses the problem of MAXIMUM MARGIN MATRIX FACTORIZATION for COLLABORATIVE PREDICTION BENCHMARKS . we propose a new ENSEMBLE APPROACH based on the MAXIMUM MARGIN MATRIX FACTORIZATION . the proposed ENSEMBLE APPROACH is based on the use of MAXIMUM MARGIN MATRIX FACTORIZATION . the proposed ENSEMBLE APPROACH is evaluated on the COLLABORATIVE PREDICTION BENCHMARKS . experimental results show that the proposed ENSEMBLE APPROACH outperforms the state-of-the-art methods .\n",
            "\n",
            "50 1000\n",
            "this paper investigates JAW MOVEMENTS in the production of <unk> syllables with and without VOWELS . we test the hypothesis that / l , r / in the SYLLABLE NUCLEUS POSITION show a degree of JAW OPENING comparable to VOWELS , therefore providing a RISING-FALLING SONORITY PROFILE even in syllables lacking VOWELS . we also investigate whether the PHONEMIC LENGTH DISTINCTION occurring for both VOWELS and syllabic consonants is implemented in a similar fashion for the different nucleus types . our articulatory data show that the JAW ACTIVITY during SYLLABIC LIQUIDS is indeed comparable to that of VOWELS , and that the JAW is recruited to help maintain the MAIN LINGUAL ARTICULATION . this became evident in particular in an interaction between NUCLEUS TYPE and PHONEMIC LENGTH EFFECTS . \n",
            "this paper presents a new method for MAIN LINGUAL ARTICULATION in MAIN LINGUAL ARTICULATION . the proposed method is based on the RISING-FALLING SONORITY PROFILE and the PHONEMIC LENGTH DISTINCTION . the proposed method is based on the PHONEMIC LENGTH DISTINCTION and the PHONEMIC LENGTH DISTINCTION . the proposed method is based on the RISING-FALLING SONORITY PROFILE and the RISING-FALLING SONORITY PROFILE . the proposed method is based on the RISING-FALLING SONORITY PROFILE and the PHONEMIC LENGTH DISTINCTION . the proposed method is evaluated using the RISING-FALLING SONORITY PROFILE and the PHONEMIC LENGTH DISTINCTION .\n",
            "\n",
            "51 1000\n",
            "the COIFMAN WAVELETS created by <unk> have more zero moments than imposed by specifications . this results in systems with approximately equal numbers of ZERO SCALING FUNCTION and WAVELET MOMENTS and gives a partitioning of the systems into three well defined classes . the NONUNIQUE SOLUTIONS are more complex than for DAUBECHIES WAVELETS . \n",
            "this paper addresses the problem of COIFMAN WAVELETS in the presence of DAUBECHIES WAVELETS . we propose a new method for estimating the parameters of the ZERO SCALING FUNCTION and the WAVELET MOMENTS . the proposed method is compared with other NONUNIQUE SOLUTIONS and is shown to outperform other NONUNIQUE SOLUTIONS .\n",
            "\n",
            "52 1000\n",
            "techniques such as PROBABILISTIC TOPIC MODELS and LATENT-SEMANTIC INDEXING have been shown to be broadly useful at automatically extracting the TOPICAL OR SEMANTIC CONTENT OF DOCUMENTS , or more generally for DIMENSION-REDUCTION OF SPARSE COUNT DATA . these types of models and algorithms can be viewed as generating an ABSTRACTION from the words in a document to a LOWER-DIMENSIONAL LATENT VARIABLE REPRESENTATION that captures what the document is generally about beyond the specific words it contains . in this paper we propose a new PROBABILISTIC MODEL that <unk> this PROBABILISTIC MODEL by representing each document as a combination of -lrb- a -rrb- a BACKGROUND DISTRIBUTION over COMMON WORDS , -lrb- b -rrb- a MIXTURE DISTRIBUTION over general topics , and -lrb- c -rrb- a DISTRIBUTION over words that are treated as being specific to that document . we illustrate how this PROBABILISTIC MODEL can be used for INFORMATION RETRIEVAL by matching documents both at a general topic level and at a specific WORD LEVEL , providing an advantage over techniques that only match documents at a general level -lrb- such as TOPIC MODELS or LATENT-SEMATIC INDEXING -rrb- or that only match documents at the specific WORD LEVEL -lrb- such as <unk> -rrb- . \n",
            "this paper presents a PROBABILISTIC MODEL for TOPICAL OR SEMANTIC CONTENT OF DOCUMENTS . the PROBABILISTIC MODEL is based on a PROBABILISTIC MODEL and a PROBABILISTIC MODEL . the PROBABILISTIC MODEL is based on a PROBABILISTIC MODEL and a PROBABILISTIC MODEL . the proposed PROBABILISTIC MODEL is based on a PROBABILISTIC MODEL and a PROBABILISTIC MODEL . the PROBABILISTIC MODEL is based on a PROBABILISTIC MODEL and a PROBABILISTIC MODEL . the proposed PROBABILISTIC MODEL is based on a PROBABILISTIC MODEL and a PROBABILISTIC MODEL .\n",
            "\n",
            "53 1000\n",
            "planning in DYNAMIC CONTINUOUS ENVIRONMENTS requires reasoning about NONLINEAR CONTINUOUS EFFECTS , which previous HIERARCHICAL TASK NETWORK PLANNERS do not support . in this paper , we extend an existing HTN PLANNER with a new STATE PROJECTION ALGORITHM . to our knowledge , this is the first HTN PLANNER that can reason about NONLINEAR CONTINUOUS EFFECTS . we use a WAIT ACTION to <unk> this HTN PLANNER to consider CONTINUOUS EFFECTS in a given state . we also introduce a new PLANNING DOMAIN to demonstrate the benefits of planning with NONLINEAR CONTINUOUS EFFECTS . we compare our approach with a LINEAR CONTINUOUS EFFECTS PLANNER and a DISCRETE EFFECTS HTN PLANNER on a BENCHMARK DOMAIN , which reveals that its additional costs are largely mitigated by DOMAIN KNOWLEDGE . finally , we present an initial application of this algorithm in a practical domain , a NAVY TRAINING SIMULATION , illustrating the utility of this approach for planning in DYNAMIC CONTINUOUS ENVIRONMENTS . \n",
            "this paper addresses the problem of PLANNING in a PLANNING DOMAIN . we propose a method for PLANNING based on the STATE PROJECTION ALGORITHM . the proposed method is based on the STATE PROJECTION ALGORITHM and the STATE PROJECTION ALGORITHM . the proposed method is based on the STATE PROJECTION ALGORITHM and the STATE PROJECTION ALGORITHM . the proposed method is based on the STATE PROJECTION ALGORITHM and the STATE PROJECTION ALGORITHM . the proposed method is compared with the conventional HIERARCHICAL TASK NETWORK PLANNERS and the HIERARCHICAL TASK NETWORK PLANNERS .\n",
            "\n",
            "54 1000\n",
            "this paper considers an AUTOMATIC VOICE RESPONSE APPLICATION in which a WORD UTTERANCE is inserted into a fixed carrier sentence . an important task here is to adjust the F 0 CONTOUR of the INSERTED WORD according to the F 0 CONTEXT of the carrier sentence . instead of generating the F 0 CONTOUR on syllable basis , we employ an approach to adjust the F 0 CONTOUR of the whole word . in this approach , two questions arise : -lrb- a -rrb- how to evaluate the F 0 CONTEXT and -lrb- b -rrb- how to adjust the F 0 CONTOUR suitably for the context . we have found that the F 0 CONTOUR of a word can be appropriately regulated in a <unk> word-level f 0 range -lrb- <unk> 0 r -rrb- . after estimating the WF 0 RS of the preceding and succeeding words , the <unk> 0 r of the INSERTED WORD is set at the mean of these WF 0 RS . the F 0 CONTOUR of the INSERTED WORD is then mapped to the <unk> 0 r taking into account the TONE COMBINATION of the word . a perceptual evaluation experiment showed that the adjusted f 0 was coordinated well with the context . \n",
            "this paper presents a new method for the AUTOMATIC VOICE RESPONSE APPLICATION . the proposed method is based on the F 0 CONTOUR in the WORD UTTERANCE . the proposed method is based on the TONE COMBINATION and the F 0 CONTOUR . the proposed method is tested on the AUTOMATIC VOICE RESPONSE APPLICATION in the AUTOMATIC VOICE RESPONSE APPLICATION .\n",
            "\n",
            "55 1000\n",
            "state-of-the-art COMPUTER-ASSISTED TRANSLATION ENGINES are based on a STATISTICAL PREDICTION ENGINE , which interactively provides completions to what a HUMAN TRANSLATOR TYPES . the integration of HUMAN SPEECH into a COMPUTER-ASSISTED TRANSLATION ENGINES is also a challenging area and is the aim of this paper . so far , only a few methods for integrating STATISTICAL MACHINE TRANSLATION MODELS with AUTOMATIC SPEECH RECOGNITION MODELS have been studied . they were mainly based on N-BEST RESCORING APPROACH . N-BEST RESCOR-ING is not an appropriate SEARCH METHOD for building a COMPUTER-ASSISTED TRANSLATION ENGINES . in this paper , we study the incorporation of AUTOMATIC SPEECH RECOGNITION MODELS and ASR MODELS using FINITE-STATE AUTOMATA . we also propose some transducers based on AUTOMATIC SPEECH RECOGNITION MODELS for rescoring the ASR WORD GRAPHS . \n",
            "this paper addresses the problem of ASR WORD GRAPHS in HUMAN SPEECH . in this paper , we propose a new SEARCH METHOD for ASR WORD GRAPHS . the proposed SEARCH METHOD is based on the SEARCH METHOD . the proposed SEARCH METHOD is based on the SEARCH METHOD . the proposed SEARCH METHOD is based on the SEARCH METHOD and the SEARCH METHOD . experimental results show that the proposed method is robust and robust to HUMAN SPEECH .\n",
            "\n",
            "56 1000\n",
            "one DUMMY SYMBOL has to be included for the sequence in this paper , a new GENERATION and an INFORMATION CONSTRUCTION METHODS for a MODULATED ORTHOGONAL SEQUENCE are suggested the sequence is generated by only INTEGER SUMS and MODULAR TECHNIQUES . the AUTOCORRELATION AND CRAM CORRELATION CHARACTERISTICS of the sequence are investigated via a new procedure . a modified sequence also having the orthogonality and satisfying the mathematical lower bound of the CROSS-CORRELATION is proposed , and the SYMBOL ERROR PROBABILITY of the sequence is investigated . \n",
            "this paper addresses the problem of GENERATION in MODULATED ORTHOGONAL SEQUENCE . we propose a method for GENERATION based on MODULAR TECHNIQUES . the proposed method is based on the use of MODULAR TECHNIQUES and MODULAR TECHNIQUES . the proposed method is based on the use of MODULAR TECHNIQUES and MODULAR TECHNIQUES . experimental results show the effectiveness of the proposed method in terms of both AUTOCORRELATION AND CRAM CORRELATION CHARACTERISTICS and GENERATION .\n",
            "\n",
            "57 1000\n",
            "this work presents our research results on the capability of PASSIVE SOURCE LOCALIZATION using a TOWED MULTI-MODULE ARRAY in UNDERWATER ACOUSTIC ENVIRONMENTS . we developed new CRAMER-RAO LOWER BOUND results for assessing the ARRAY 'S SOURCE LOCALIZATION CAPABILITY under ENVIRONMENTAL UNCERTAINTIES , such as ARRAY MODULES ' positions and ACOUSTIC-FIELD 'S SPATIAL COHERENCE . we also developed a TWO-STAGE APPROACH and provide details for PASSIVE RANGING by utilizing data either <unk> or coherently , depending on the availability of SPATIAL COHERENCE in the received data from multiple modules of TOWED ARRAY . \n",
            "this paper presents a TWO-STAGE APPROACH for PASSIVE SOURCE LOCALIZATION . the proposed TWO-STAGE APPROACH is based on the CRAMER-RAO LOWER BOUND and the CRAMER-RAO LOWER BOUND . the CRAMER-RAO LOWER BOUND is based on the CRAMER-RAO LOWER BOUND and the CRAMER-RAO LOWER BOUND . the CRAMER-RAO LOWER BOUND is based on the CRAMER-RAO LOWER BOUND and the CRAMER-RAO LOWER BOUND . the proposed TWO-STAGE APPROACH is based on a TWO-STAGE APPROACH and is shown to be robust to ENVIRONMENTAL UNCERTAINTIES such as ENVIRONMENTAL UNCERTAINTIES , ENVIRONMENTAL UNCERTAINTIES , and SPATIAL COHERENCE .\n",
            "\n",
            "58 1000\n",
            "this paper proposes an approach for AUTOMATIC ROAD EXTRACTION in AERIAL IMAGERY which exploits the SCALE-SPACE BEHAVIOR OF ROADS in combination with GEOMETRIC CONSTRAINED SNAKE-BASED EDGE EXTRACTION . the approach not only has few parameters to be adjusted , but for the first time allows for a BRIDGING OF SHADOWS and partially occluded areas using the heavily disturbed evidence in the image . the road network is constructed after extracting <unk> of various shape and topology . reasonable results are obtained which are evaluated based on ground truth . \n",
            "this paper presents a new method for AUTOMATIC ROAD EXTRACTION from AERIAL IMAGERY . the method is based on the use of GEOMETRIC CONSTRAINED SNAKE-BASED EDGE EXTRACTION in AERIAL IMAGERY . the proposed method is based on the SCALE-SPACE BEHAVIOR OF ROADS and the BRIDGING OF SHADOWS . the experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "59 1000\n",
            "<unk> residual vector quantizers -lrb- MULTISTAGE RESIDUAL VECTOR QUANTIZERS -rrb- with optimal DIRECT SUM DECODER CODEBOOKS have been successfully designed and implemented for DATA COMPRESSION . due to its MULTISTAGE STRUCTURE , MULTISTAGE RESIDUAL VECTOR QUANTIZERS has the ability to densely populate the INPUT SPACE with VORONOI CELL PARTITIONS . the same design concept has yielded good results in the application of IMAGE-CONTENT CLASSIFICATION -lsb- 1 -rsb- . furthermore , the MULTISTAGE RVQ , with STAGE-WISE CODEBOOKS , provides an opportunity to perform FINE-GRAINED FEATURE ATTRIBUTION for IMAGE UNDERSTANDING , in general , and feature foundation data generation for NATURAL AND MAN-MADE STRUCTURE RECOGNITION , in specific . in -lsb- 1 -rsb- , the information at the stages of MULTISTAGE RESIDUAL VECTOR QUANTIZERS is heuristically integrated to perform CLASS CONDITIONAL PATTERN RECOGNITION ; hence the process is not robust . MARKOV RANDOM FIELD provides a suitable BAYESIAN FRAMEWORK to integrate the information available at the various stages of MULTISTAGE RESIDUAL VECTOR QUANTIZERS to achieve OPTIMIZED CLASSIFICATION in the MAXIMUM A-POSTERIORI SENSE . \n",
            "this paper presents a BAYESIAN FRAMEWORK based on MULTISTAGE RESIDUAL VECTOR QUANTIZERS . the BAYESIAN FRAMEWORK is based on the MULTISTAGE RESIDUAL VECTOR QUANTIZERS . the DIRECT SUM DECODER CODEBOOKS is based on the MULTISTAGE RESIDUAL VECTOR QUANTIZERS . the DIRECT SUM DECODER CODEBOOKS is based on the MULTISTAGE RESIDUAL VECTOR QUANTIZERS . the DIRECT SUM DECODER CODEBOOKS is based on the MULTISTAGE RESIDUAL VECTOR QUANTIZERS . the DIRECT SUM DECODER CODEBOOKS is based on the MULTISTAGE RESIDUAL VECTOR QUANTIZERS . the DIRECT SUM DECODER CODEBOOKS is based on the MULTISTAGE RESIDUAL VECTOR QUANTIZERS . the proposed BAYESIAN FRAMEWORK is based on the MULTISTAGE RESIDUAL VECTOR QUANTIZERS . the DIRECT SUM DECODER CODEBOOKS is applied to the MAXIMUM A-POSTERIORI SENSE for IMAGE UNDERSTANDING . experimental results show the effectiveness of the proposed BAYESIAN FRAMEWORK .\n",
            "\n",
            "60 1000\n",
            "in this work we make use of recent advances in DATA DRIVEN CLASSIFICATION to improve standard approaches for BINOCULAR STEREO MATCHING and SINGLE VIEW DEPTH ESTIMATION . SURFACE NORMAL DIRECTION ESTIMATION has become feasible and shown to work reliably on STATE OF THE ART BENCHMARK DATASETS . information about the SURFACE ORIENTATION contributes crucial information about the SCENE GEOMETRY in cases where standard approaches struggle . we describe , how the responses of such a CLASSIFIER can be included in GLOBAL STEREO MATCHING APPROACHES . one of the strengths of our CLASSIFIER is , that we can use the CLASSIFIER RESPONSES for a whole set of directions and let the final optimization decide about the SURFACE ORIENTATION . this is important in cases where based on the CLASSIFIER , multiple different SURFACE ORIENTATIONS seem likely . we evaluate our CLASSIFIER on two challenging REAL-WORLD DATASETS for the two proposed applications . for the BINOCULAR STEREO MATCHING we use ROAD SCENE IMAGERY taken from a car and for the SINGLE VIEW DEPTH ESTIMATION we use IMAGES taken in INDOOR ENVIRONMENTS . \n",
            "this paper presents a new CLASSIFIER for SINGLE VIEW DEPTH ESTIMATION . the CLASSIFIER is based on the use of SURFACE NORMAL DIRECTION ESTIMATION and BINOCULAR STEREO MATCHING . the CLASSIFIER is based on a CLASSIFIER and a CLASSIFIER . the proposed CLASSIFIER is based on a CLASSIFIER , which is based on SURFACE NORMAL DIRECTION ESTIMATION and DATA DRIVEN CLASSIFICATION . the proposed CLASSIFIER is evaluated on REAL-WORLD DATASETS and REAL-WORLD DATASETS . the proposed CLASSIFIER is evaluated on REAL-WORLD DATASETS and REAL-WORLD DATASETS .\n",
            "\n",
            "61 1000\n",
            "one goal of NATURAL LANGUAGE GENERATION is to produce COHERENT TEXT that presents information in a LOGICAL ORDER . in this paper , we show that TOPOLOGICAL FIELDS , which model HIGH-LEVEL CLAUSAL STRUCTURE , are an important component of LOCAL COHERENCE in GERMAN . first , we show in a sentence ordering experiment that TOPOLOGI-CAL FIELD INFORMATION improves the ENTITY GRID MODEL of <unk> and <unk> -lrb- 2008 -rrb- more than GRAMMATICAL ROLE and simple CLAUSAL ORDER INFORMATION do , particularly when MANUAL ANNOTATIONS of this information are not available . then , we incorporate the model enhanced with TOPOLOG-ICAL FIELDS into a NATURAL LANGUAGE GENERATION SYSTEM that generates CONSTITUENT ORDERS for GERMAN TEXT , and show that the added COHERENCE COMPONENT improves performance slightly , though not statistically significantly . \n",
            "this paper presents a NATURAL LANGUAGE GENERATION SYSTEM for NATURAL LANGUAGE GENERATION . the ENTITY GRID MODEL is based on a ENTITY GRID MODEL and a ENTITY GRID MODEL . the ENTITY GRID MODEL is based on a ENTITY GRID MODEL and a ENTITY GRID MODEL . the ENTITY GRID MODEL is based on a ENTITY GRID MODEL . the ENTITY GRID MODEL is based on a ENTITY GRID MODEL and a ENTITY GRID MODEL . the ENTITY GRID MODEL is applied to the GERMAN TEXT . the ENTITY GRID MODEL is evaluated on a GERMAN TEXT . the results show that the proposed ENTITY GRID MODEL is effective for NATURAL LANGUAGE GENERATION , especially for GERMAN TEXT .\n",
            "\n",
            "62 1000\n",
            "this paper explores PACKET LOSS RECOVERY in CLIENT-SERVER AUTOMATIC SPEECH RECOGNITION SYSTEMS . a FORWARD ERROR CORRECTION SYSTEM is designed and tested over several CHANNEL LOSS MODELS , at variable amounts of DATA ACQUISITION DELAY . in experiments with SIMULATED PACKET LOSS , the CLIENT-SERVER AUTOMATIC SPEECH RECOGNITION SYSTEMS provides robust asr performance which degrades gracefully as PACKET LOSS RATES increase . comparing this CLIENT-SERVER AUTOMATIC SPEECH RECOGNITION SYSTEMS to several alternatives under LOW AND MEDIUM LOSS CHANNEL CONDITIONS , we found one approach -lrb- multiple transmission plus interpolation -rrb- that yielded similar performance , but the CLIENT-SERVER AUTOMATIC SPEECH RECOGNITION SYSTEMS should scale better to lower bit rate conditions . \n",
            "this paper presents a FORWARD ERROR CORRECTION SYSTEM for CLIENT-SERVER AUTOMATIC SPEECH RECOGNITION SYSTEMS . the proposed CLIENT-SERVER AUTOMATIC SPEECH RECOGNITION SYSTEMS is based on the use of CHANNEL LOSS MODELS for PACKET LOSS RECOVERY . the proposed CLIENT-SERVER AUTOMATIC SPEECH RECOGNITION SYSTEMS is based on the FORWARD ERROR CORRECTION SYSTEM . the proposed CLIENT-SERVER AUTOMATIC SPEECH RECOGNITION SYSTEMS is evaluated on the SIMULATED PACKET LOSS . the results show that the proposed CLIENT-SERVER AUTOMATIC SPEECH RECOGNITION SYSTEMS can improve the performance of CLIENT-SERVER AUTOMATIC SPEECH RECOGNITION SYSTEMS .\n",
            "\n",
            "63 1000\n",
            "this paper considers application of DEEP BELIEF NETS to NATURAL LANGUAGE CALL ROUTING . DEEP BELIEF NETS have been successfully applied to a number of tasks , including IMAGE , AUDIO AND SPEECH CLASSIFICATION , thanks to the recent discovery of an efficient LEARNING TECHNIQUE . DEEP BELIEF NETS learn a MULTI-LAYER GENERATIVE MODEL from UNLABELED DATA and the FEATURES discovered by this MULTI-LAYER GENERATIVE MODEL are then used to initialize a FEED-FORWARD NEURAL NETWORK which is fine-tuned with BACKPROPAGATION . we compare a DBN-INITIALIZED NEURAL NETWORK to three widely used TEXT CLASSIFICATION ALGORITHMS ; SUPPORT VECTOR MACHINES , BOOSTING and MAXIMUM ENTROPY . the MULTI-LAYER GENERATIVE MODEL gives a call -- routing classification accuracy that is equal to the best of the other models even though MULTI-LAYER GENERATIVE MODEL currently uses an impoverished representation of the input . \n",
            "this paper addresses the problem of IMAGE , AUDIO AND SPEECH CLASSIFICATION in NATURAL LANGUAGE CALL ROUTING . in this paper , we propose a new LEARNING TECHNIQUE , called BOOSTING , which is based on a MULTI-LAYER GENERATIVE MODEL . the proposed method is based on the use of DEEP BELIEF NETS and BOOSTING . the proposed method is based on the use of DEEP BELIEF NETS and BOOSTING . the proposed method is based on the use of DEEP BELIEF NETS and BOOSTING . the experimental results show that the proposed method is effective for IMAGE , AUDIO AND SPEECH CLASSIFICATION .\n",
            "\n",
            "64 1000\n",
            "approaches to BINAURAL AND STEREO SPEECH SEGREGATION have often assumed that LOCALIZATION INFORMATION can be used as a primary cue to achieve segregation of a target signal . results produced by these systems degrade significantly in the presence of ROOM REVERBERATION . in this work , we present an alternative framework to achieve LOCALIZATION OF GROUPS OF TIME-FREQUENCY UNITS . we show that grouping across time and frequency allows the use of LOCALIZATION INFORMATION as an important cue for SEQUENTIAL GROUPING OF TIME-FREQUENCY OBJECTS . we analyze the level of TIME-FREQUENCY GROUPING needed to achieve accurate OBJECT LOCALIZATION and show preliminary binaural segregation results using the proposed framework . results indicate that both LOCALIZATION INFORMATION and segregation performance can be improved by grouping across time and frequency . \n",
            "this paper presents a method for SEQUENTIAL GROUPING OF TIME-FREQUENCY OBJECTS from ROOM REVERBERATION . the proposed method is based on the use of TIME-FREQUENCY GROUPING in the LOCALIZATION OF GROUPS OF TIME-FREQUENCY UNITS . the proposed method is based on the LOCALIZATION OF GROUPS OF TIME-FREQUENCY UNITS . the proposed method is based on the LOCALIZATION OF GROUPS OF TIME-FREQUENCY UNITS . the proposed method is based on the LOCALIZATION OF GROUPS OF TIME-FREQUENCY UNITS .\n",
            "\n",
            "65 1000\n",
            "this paper shows that a FIRST-ORDER UNIFICATION-BASED SEMANTIC INTERPRETATION for various coordinate constructs is possible without an explicit use of LAMBDA EXPRESSIONS if we slightly modify the standard MONTAGOVIAN SEMANTICS OF COORDINATION . this modification , along with PARTIAL EXECUTION , completely eliminates the LAMBDA REDUCTION STEPS during SEMANTIC INTERPRETATION . \n",
            "this paper presents a new method for MONTAGOVIAN SEMANTICS OF COORDINATION based on MONTAGOVIAN SEMANTICS OF COORDINATION . the proposed method is based on the use of LAMBDA EXPRESSIONS and the MONTAGOVIAN SEMANTICS OF COORDINATION . the proposed method is based on the use of LAMBDA EXPRESSIONS , and is shown to be more robust to LAMBDA EXPRESSIONS than the conventional method .\n",
            "\n",
            "66 1000\n",
            "<unk> optimization via COST ACCUMULATION has become very popular for STEREO ESTIMATION in COMPUTER VISION APPLICATIONS and is often combined with a SEMI-GLOBAL COST INTEGRATION STRATEGY , known as SGM . this paper introduces this combination as a general and effective OPTIMIZATION TECHNIQUE . it is the first time that this concept is applied to 3D MEDICAL IMAGE REGISTRATION . the presented algorithm , SGM-3D , employs a COARSE-TO-FINE STRATEGY and reduces the SEARCH SPACE DIMENSION for CONSECUTIVE PYRAMID LEVELS by a fixed linear rate . this allows it to handle large displacements to an extent that is required for CLINICAL APPLICATIONS in HIGH DIMENSIONAL DATA . SGM-3D is evaluated in context of PULMONARY MOTION ANALYSIS on the recently extended <unk> benchmark that provides ten 4d computed tomography -lrb- ct -rrb- image data sets , as well as ten challenging 3D CT SCAN PAIRS from the COPDGENE STUDY ARCHIVE . results show that both REGISTRATION ERRORS as well as run-time performance are very competitive with current state-of-the-art methods . \n",
            "this paper addresses the problem of 3D MEDICAL IMAGE REGISTRATION from HIGH DIMENSIONAL DATA . we propose a COARSE-TO-FINE STRATEGY , called SGM-3D , to learn a COARSE-TO-FINE STRATEGY from HIGH DIMENSIONAL DATA . we use a COARSE-TO-FINE STRATEGY to learn a COARSE-TO-FINE STRATEGY from a COPDGENE STUDY ARCHIVE . we demonstrate the effectiveness of our OPTIMIZATION TECHNIQUE on a COPDGENE STUDY ARCHIVE and on a COPDGENE STUDY ARCHIVE . we demonstrate the effectiveness of our OPTIMIZATION TECHNIQUE on a COPDGENE STUDY ARCHIVE .\n",
            "\n",
            "67 1000\n",
            "in this paper , we propose a new MULTI-VIEW DOMAIN GENERALIZATION APPROACH for VISUAL RECOGNITION , in which we aim to use the source domain samples with multiple types of FEATURES -lrb- i.e. , MULTI-VIEW FEATURES -rrb- to learn ROBUST CLASSIFIERS that can generalize well to any UNSEEN TARGET DOMAIN . considering the recent works show the DOMAIN GENERALIZATION CAPABILITY can be enhanced by fusing multiple SVM CLASSIFIERS , we build upon EXEMPLAR SVMS to learn a set of SVM CLASSIFIERS by using one positive sample and all negative samples in the source domain each time . when the source domain samples come from multiple LATENT DOMAINS , we expect the WEIGHT VECTORS of exemplar svm clas-sifiers can be organized into multiple hidden clusters . to exploit such CLUSTER STRUCTURE , we organize the WEIGHT VECTORS learnt on each view as a WEIGHT MATRIX and seek the LOW-RANK REPRESENTATION by reconstructing this WEIGHT MATRIX using itself as the dictionary . to enforce the consistency of INHERENT CLUSTER STRUCTURES discovered from the WEIGHT MATRICES learnt on different views , we introduce a new MULTI-VIEW DOMAIN GENERALIZATION APPROACH to minimize the mismatch between any two representation matrices on different views . we also develop an efficient ALTERNATING OPTIMIZATION ALGORITHM and further extend our MULTI-VIEW DOMAIN GENERALIZATION APPROACH for VISUAL RECOGNITION by exploiting the manifold structure of UNLABELED TARGET DOMAIN SAMPLES . comprehensive experiments for VISUAL RECOGNITION clearly demonstrate the effectiveness of our MULTI-VIEW DOMAIN GENERALIZATION APPROACH for DOMAIN GENERALIZATION and VISUAL RECOGNITION . \n",
            "this paper presents a MULTI-VIEW DOMAIN GENERALIZATION APPROACH for VISUAL RECOGNITION . the MULTI-VIEW DOMAIN GENERALIZATION APPROACH is based on a MULTI-VIEW DOMAIN GENERALIZATION APPROACH of the WEIGHT MATRIX and the WEIGHT MATRIX . the proposed MULTI-VIEW DOMAIN GENERALIZATION APPROACH is based on the ALTERNATING OPTIMIZATION ALGORITHM and the ALTERNATING OPTIMIZATION ALGORITHM . the proposed MULTI-VIEW DOMAIN GENERALIZATION APPROACH is based on the ALTERNATING OPTIMIZATION ALGORITHM and the ALTERNATING OPTIMIZATION ALGORITHM . the proposed MULTI-VIEW DOMAIN GENERALIZATION APPROACH is based on the ALTERNATING OPTIMIZATION ALGORITHM and the ALTERNATING OPTIMIZATION ALGORITHM . the proposed MULTI-VIEW DOMAIN GENERALIZATION APPROACH is based on a MULTI-VIEW DOMAIN GENERALIZATION APPROACH . the proposed MULTI-VIEW DOMAIN GENERALIZATION APPROACH is based on a MULTI-VIEW DOMAIN GENERALIZATION APPROACH . the proposed MULTI-VIEW DOMAIN GENERALIZATION APPROACH is based on a MULTI-VIEW DOMAIN GENERALIZATION APPROACH and is shown to be useful for VISUAL RECOGNITION and VISUAL RECOGNITION .\n",
            "\n",
            "68 1000\n",
            "audio TAGS correspond to KEYWORDS that people use to describe different aspects of a MUSIC CLIP , such as the genre , MOOD , and <unk> . since SOCIAL TAGS are usually assigned by people with different levels of MUSICAL KNOWLEDGE , they inevitably contain NOISY INFORMATION . by treating the TAG counts as costs , we can model the AUDIO TAGGING PROBLEM as a COST-SENSITIVE CLASSIFICATION PROBLEM . in addition , TAG CORRELATION is another useful information for AUTOMATIC AUDIO TAGGING since some TAGS often co-occur . by considering the co-occurrences of TAGS , we can model the AUDIO TAGGING PROBLEM as a COST-SENSITIVE CLASSIFICATION PROBLEM . to exploit the TAG COUNT and correlation information jointly , we formulate the AUDIO TAGGING PROBLEM as a novel COST-SENSITIVE MULTI-LABEL LEARNING PROBLEM . the results of AUDIO TAG ANNOTATION and retrieval experiments demonstrate that the new approach outperforms our MIREX 2009 WINNING METHOD . \n",
            "this paper presents a method for AUTOMATIC AUDIO TAGGING from AUDIO TAGS . the MIREX 2009 WINNING METHOD is based on a COST-SENSITIVE MULTI-LABEL LEARNING PROBLEM , which is based on the TAG CORRELATION of the AUDIO TAGS . the proposed MIREX 2009 WINNING METHOD is based on a COST-SENSITIVE MULTI-LABEL LEARNING PROBLEM , which is based on the TAG CORRELATION . the proposed MIREX 2009 WINNING METHOD is based on the TAG CORRELATION and the TAG CORRELATION . the proposed MIREX 2009 WINNING METHOD is based on a COST-SENSITIVE MULTI-LABEL LEARNING PROBLEM , which is based on the MIREX 2009 WINNING METHOD . the proposed MIREX 2009 WINNING METHOD is applied to AUTOMATIC AUDIO TAGGING , and is shown to be robust to AUDIO TAG ANNOTATION .\n",
            "\n",
            "69 1000\n",
            "we formulate the problem of BIPARTITE GRAPH INFERENCE as a SUPERVISED LEARNING PROBLEM , and propose a new method to solve it from the viewpoint of DISTANCE METRIC LEARNING . the method involves the learning of two mappings of the heterogeneous objects to a UNIFIED EUCLIDEAN SPACE representing the NETWORK TOPOLOGY OF THE BIPARTITE GRAPH , where the GRAPH is easy to infer . the algorithm can be formulated as an OPTIMIZATION PROBLEM in a REPRODUCING KERNEL HILBERT SPACE . we report encouraging results on the problem of COMPOUND-PROTEIN INTERACTION NETWORK RECONSTRUCTION from CHEMICAL STRUCTURE DATA and GENOMIC SEQUENCE DATA . \n",
            "this paper addresses the problem of COMPOUND-PROTEIN INTERACTION NETWORK RECONSTRUCTION in REPRODUCING KERNEL HILBERT SPACE . we propose a NETWORK TOPOLOGY OF THE BIPARTITE GRAPH based on a NETWORK TOPOLOGY OF THE BIPARTITE GRAPH and a NETWORK TOPOLOGY OF THE BIPARTITE GRAPH . the OPTIMIZATION PROBLEM is formulated as a SUPERVISED LEARNING PROBLEM . the OPTIMIZATION PROBLEM is formulated as a SUPERVISED LEARNING PROBLEM . the OPTIMIZATION PROBLEM is formulated as a SUPERVISED LEARNING PROBLEM . the OPTIMIZATION PROBLEM is formulated as a SUPERVISED LEARNING PROBLEM .\n",
            "\n",
            "70 1000\n",
            "the classical approach to SAMPLING TIME-INVARIANT SPATIAL FIELDS uses STATIC SENSORS distributed over space . we study a new approach involving MOBILE SENSORS that move through space measuring the FIELD VALUES along their paths . a single MOVING SENSOR can take measurements over a WIDE SPATIAL AREA thus acting as a substitute for a potentially large number of STATIC SENSORS . a MOVING SENSOR encounters the SPATIAL FIELD in its path in the form of a TIME-DOMAIN SIGNAL . hence a TIME-DOMAIN ANTI-ALIASING FILTER can be employed at the MOBILE SENSORS to limit the amount of OUT-OF-BAND NOISE prior to SAMPLING . we analytically quantify the advantage of MOBILE SENSORS over STATIC SENSING in rejecting OUT-OF-BAND NOISE . we also demonstrate via simulations the improvement in RECONSTRUCTION ACCURACY that can be obtained using MOBILE SENSORS and FILTERING in a TEMPERATURE MEASUREMENT PROBLEM . \n",
            "this paper presents a method for SAMPLING TIME-INVARIANT SPATIAL FIELDS in MOBILE SENSORS . the proposed method is based on a TIME-DOMAIN ANTI-ALIASING FILTER and a TIME-DOMAIN ANTI-ALIASING FILTER . the proposed method is based on the TIME-DOMAIN ANTI-ALIASING FILTER and the TIME-DOMAIN ANTI-ALIASING FILTER . the proposed method is based on a TIME-DOMAIN ANTI-ALIASING FILTER , which is based on the TIME-DOMAIN ANTI-ALIASING FILTER and the TIME-DOMAIN ANTI-ALIASING FILTER . the proposed method is based on the TIME-DOMAIN ANTI-ALIASING FILTER and the TIME-DOMAIN ANTI-ALIASING FILTER . the RECONSTRUCTION ACCURACY of the proposed method is compared with the conventional SAMPLING .\n",
            "\n",
            "71 1000\n",
            "when modeling structured outputs such as IMAGE SEG-MENTATIONS , PREDICTION can be improved by accurately MOD-ELING STRUCTURE present in the labels . a key challenge is developing TRACTABLE MODELS that are able to capture COMPLEX HIGH LEVEL STRUCTURE like shape . in this work , we study the learning of a general class of PATTERN-LIKE HIGH ORDER POTENTIAL , which we call COMPOSITIONAL HIGH ORDER PATTERN POTENTIALS . we show that COMPOSITIONAL HIGH ORDER PATTERN POTENTIALS include the LINEAR DEVIATION PATTERN POTENTIALS of <unk> et al. -lsb- 26 -rsb- and also RESTRICTED BOLTZMANN MACHINES ; we also establish the near equivalence of these two models . experimentally , we show that performance is affected significantly by the degree of variability present in the datasets , and we define a QUANTITATIVE VARIABILITY MEASURE to aid in studying this . we then improve COMPOSITIONAL HIGH ORDER PATTERN POTENTIALS performance in HIGH VARIABILITY DATASETS with two primary contributions : -lrb- a -rrb- developing a LOSS-SENSITIVE JOINT LEARNING PROCEDURE , so that INTERNAL PATTERN PARAMETERS can be learned in conjunction with other model potentials to minimize expected loss ; and -lrb- b -rrb- learning an IMAGE-DEPENDENT MAPPING that encourages or <unk> patterns depending on IMAGE FEATURES . we also explore varying how multiple patterns are composed , and learning CONVOLUTIONAL PATTERNS . quantitative results on challenging highly variable datasets show that the JOINT LEARNING and IMAGE-DEPENDENT HIGH ORDER POTENTIALS can improve performance . \n",
            "this paper presents a new method for JOINT LEARNING based on RESTRICTED BOLTZMANN MACHINES . the proposed LOSS-SENSITIVE JOINT LEARNING PROCEDURE is based on the LOSS-SENSITIVE JOINT LEARNING PROCEDURE . the proposed LOSS-SENSITIVE JOINT LEARNING PROCEDURE is based on the LOSS-SENSITIVE JOINT LEARNING PROCEDURE . the proposed method is based on a LOSS-SENSITIVE JOINT LEARNING PROCEDURE . the proposed method is based on the LOSS-SENSITIVE JOINT LEARNING PROCEDURE . the proposed method is based on the LOSS-SENSITIVE JOINT LEARNING PROCEDURE . the proposed method is based on the LOSS-SENSITIVE JOINT LEARNING PROCEDURE . the proposed method is based on the LOSS-SENSITIVE JOINT LEARNING PROCEDURE . the proposed method is based on the LOSS-SENSITIVE JOINT LEARNING PROCEDURE . the proposed method is based on the LOSS-SENSITIVE JOINT LEARNING PROCEDURE . the proposed method is based on the LOSS-SENSITIVE JOINT LEARNING PROCEDURE . the proposed method is based on the LOSS-SENSITIVE JOINT LEARNING PROCEDURE . the proposed method is based on the LOSS-SENSITIVE JOINT LEARNING PROCEDURE . the proposed method is based on the LOSS-SENSITIVE JOINT LEARNING PROCEDURE . the proposed method is based on the LOSS-SENSITIVE JOINT LEARNING PROCEDURE . the proposed method is based on the LOSS-SENSITIVE JOINT LEARNING PROCEDURE . the proposed method is based on the LOSS-SENSITIVE JOINT LEARNING PROCEDURE . the proposed method is based on a LOSS-SENSITIVE JOINT LEARNING PROCEDURE . the proposed method is compared with conventional TRACTABLE MODELS .\n",
            "\n",
            "72 1000\n",
            "we present a GENERATIVE MODEL for the UNSUPERVISED LEARNING OF DEPENDENCY STRUCTURES . we also describe the multiplicative combination of this GENERATIVE MODEL with a MODEL OF LINEAR CONSTITUENCY . the GENERATIVE MODEL outperforms both components on their respective EVALUATION METRICS , giving the best published figures for UN-SUPERVISED DEPENDENCY PARSING and UNSUPERVISED CONSTITUENCY PARSING . we also demonstrate that the combined GENERATIVE MODEL works and is robust <unk> , being able to exploit either attachment or DISTRIBUTIONAL REGULARITIES that are salient in the data . \n",
            "this paper presents a new GENERATIVE MODEL for UNSUPERVISED LEARNING OF DEPENDENCY STRUCTURES . the GENERATIVE MODEL is based on the MODEL OF LINEAR CONSTITUENCY and the GENERATIVE MODEL . the proposed GENERATIVE MODEL is based on the MODEL OF LINEAR CONSTITUENCY and the GENERATIVE MODEL . experimental results show that the proposed GENERATIVE MODEL can improve the performance of UNSUPERVISED CONSTITUENCY PARSING and UN-SUPERVISED DEPENDENCY PARSING .\n",
            "\n",
            "73 1000\n",
            "compressed sensing is a novel technique where one can recover SPARSE SIGNALS from the UNDERSAMPLED MEASUREMENTS . in this paper , a K × N MEASUREMENT MATRIX for COMPRESSED SENSING is deterministically constructed via ADDITIVE CHARACTER SEQUENCES . the WEIL BOUND is then used to show that the DETERMINISTIC SENSING MATRIX has ASYMPTOTICALLY OPTIMAL COHERENCE for n = k 2 , and that it is a tight frame . a sparse recovery guarantee for the INCOHERENT TIGHT FRAME is also discussed . numerical results show that the DETERMINISTIC SENSING MATRIX guarantees empirically reliable recovery performance via an L 1-MINIMIZATION METHOD for NOISELESS MEASUREMENTS . \n",
            "this paper proposes a new L 1-MINIMIZATION METHOD for COMPRESSED SENSING . the L 1-MINIMIZATION METHOD is based on the K × N MEASUREMENT MATRIX . the L 1-MINIMIZATION METHOD is based on the K × N MEASUREMENT MATRIX . the proposed L 1-MINIMIZATION METHOD is based on the L 1-MINIMIZATION METHOD . the proposed L 1-MINIMIZATION METHOD is based on the K × N MEASUREMENT MATRIX . the proposed L 1-MINIMIZATION METHOD is based on the L 1-MINIMIZATION METHOD . the proposed L 1-MINIMIZATION METHOD is based on the K × N MEASUREMENT MATRIX . the proposed L 1-MINIMIZATION METHOD is based on the L 1-MINIMIZATION METHOD . the proposed L 1-MINIMIZATION METHOD is based on the L 1-MINIMIZATION METHOD . the proposed L 1-MINIMIZATION METHOD is based on the L 1-MINIMIZATION METHOD .\n",
            "\n",
            "74 1000\n",
            "the substantial overhead of performing GLOBAL INTERNET MONITORING motivates techniques for inferring SPATIALLY LOCALIZED INFORMATION about performance using only HOST-BASED , END-TO-END MEASUREMENTS . in this paper , we present a novel methodology for INFERRING QUEUING DELAY DISTRIBUTIONS across internal links in the network based solely on <unk> , end-to-end measurements . a key feature of our new approach is that it is nonparametric , meaning that no a PRIORI LIMIT is placed on the number of UNKNOWN PARAMETERS used to model the DELAY DISTRIBUTIONS . the nonparametric approach is required in order to accurately estimate the wide variety of INTERNAL DELAY DISTRIBUTIONS . the methodology is formulated according to a recently proposed NONPARAMETRIC , WAVELET-BASED DENSITY ESTIMATION METHOD in combination with an EXPECTATION-MAXIMIZATION OPTIMIZATION ALGORITHM that employs a novel FAST FOURIER TRANSFORM IMPLEMENTATION . we perform NETWORK LEVEL NS SIMULATIONS to verify the ACCURACY of the ESTIMATION PROCEDURE . \n",
            "this paper proposes a new NONPARAMETRIC , WAVELET-BASED DENSITY ESTIMATION METHOD for INFERRING QUEUING DELAY DISTRIBUTIONS . the proposed NONPARAMETRIC , WAVELET-BASED DENSITY ESTIMATION METHOD is based on the FAST FOURIER TRANSFORM IMPLEMENTATION . the proposed NONPARAMETRIC , WAVELET-BASED DENSITY ESTIMATION METHOD is based on the SPATIALLY LOCALIZED INFORMATION and the SPATIALLY LOCALIZED INFORMATION . the proposed NONPARAMETRIC , WAVELET-BASED DENSITY ESTIMATION METHOD is based on a PRIORI LIMIT . the proposed EXPECTATION-MAXIMIZATION OPTIMIZATION ALGORITHM is evaluated on a PRIORI LIMIT . the experimental results show that the proposed NONPARAMETRIC , WAVELET-BASED DENSITY ESTIMATION METHOD achieves a significant improvement in ACCURACY compared to the conventional NONPARAMETRIC , WAVELET-BASED DENSITY ESTIMATION METHOD .\n",
            "\n",
            "75 1000\n",
            "distributed constraint optimization -lrb- DISTRIBUTED CONSTRAINT OPTIMIZATION -rrb- is an important framework for COORDINATED MULTIAGENT DECISION MAKING . we address a practically useful variant of DISTRIBUTED CONSTRAINT OPTIMIZATION , called DISTRIBUTED CONSTRAINT OPTIMIZATION , which takes into account agents ' consumption of shared limited resources . we present a promising new class of algorithm for DISTRIBUTED CONSTRAINT OPTIMIZATION by translating the underlying COORDINATION PROBLEM to PROBABILISTIC INFERENCE . using INFERENCE TECHNIQUES such as EXPECTATION-MAXIMIZATION AND CONVEX OPTIMIZATION MACHINERY , we develop a novel CONVERGENT MESSAGE-PASSING ALGORITHM for DISTRIBUTED CONSTRAINT OPTIMIZATION . experiments on standard benchmarks show that our approach provides better quality than previous best DCOP ALGORITHMS and has much lower FAILURE RATE . comparisons against an efficient CENTRALIZED SOLVER show that our approach provides NEAR-OPTIMAL SOLUTIONS , and is significantly faster on larger instances . \n",
            "this paper addresses the problem of COORDINATED MULTIAGENT DECISION MAKING in COORDINATED MULTIAGENT DECISION MAKING . in this paper , we propose a CONVERGENT MESSAGE-PASSING ALGORITHM based on the CONVERGENT MESSAGE-PASSING ALGORITHM . the proposed CONVERGENT MESSAGE-PASSING ALGORITHM is based on the use of DISTRIBUTED CONSTRAINT OPTIMIZATION , which is based on the CONVERGENT MESSAGE-PASSING ALGORITHM . the proposed CONVERGENT MESSAGE-PASSING ALGORITHM is based on the use of the CONVERGENT MESSAGE-PASSING ALGORITHM and the CONVERGENT MESSAGE-PASSING ALGORITHM . experimental results show that the proposed CONVERGENT MESSAGE-PASSING ALGORITHM can improve the performance of COORDINATED MULTIAGENT DECISION MAKING .\n",
            "\n",
            "76 1000\n",
            "this paper proposes a framework for training CONDITIONAL RANDOM FIELDS to optimize MULTIVARIATE EVALUATION MEASURES , including NON-LINEAR MEASURES such as F-SCORE . our proposed framework is derived from an ERROR MINIMIZATION APPROACH that provides a simple solution for directly optimizing any EVALUATION MEASURE . specifically focusing on SEQUENTIAL SEGMENTATION TASKS , i.e. TEXT CHUNKING and NAMED ENTITY RECOGNITION , we introduce a LOSS FUNCTION that closely reflects the TARGET EVALUATION MEASURE for these SEQUENTIAL SEGMENTATION TASKS , namely , SEGMENTATION F-SCORE . our experiments show that our method performs better than standard CRF TRAINING . \n",
            "this paper addresses the problem of NAMED ENTITY RECOGNITION in TEXT CHUNKING such as TEXT CHUNKING , TEXT CHUNKING , and NAMED ENTITY RECOGNITION . we propose a new EVALUATION MEASURE based on CONDITIONAL RANDOM FIELDS , which is based on the ERROR MINIMIZATION APPROACH . the proposed method is based on the ERROR MINIMIZATION APPROACH and the ERROR MINIMIZATION APPROACH . the proposed method is compared with state-of-the-art MULTIVARIATE EVALUATION MEASURES on the SEQUENTIAL SEGMENTATION TASKS .\n",
            "\n",
            "77 1000\n",
            "various representations and INFERENCE METHODS have been proposed for LIFTED PROBABILISTIC INFERENCE in RELA-TIONAL MODELS . many of these INFERENCE METHODS choose an order to eliminate -lrb- or branch on -rrb- the PARAMETERIZED RANDOM VARIABLES . similar to such INFERENCE METHODS for NON-RELATIONAL PROBABILISTIC INFERENCE , the order of elimination has a significant role in the performance of the INFERENCE METHODS . since finding the best order is NP-COMPLETE even for RELATIONAL MODELS , heuristics have been proposed to find good orderings in the RELATIONAL MODELS . in this paper , we show that these heuristics are inefficient for RELATIONAL MODELS , because they fail to consider the POPULATION SIZES associated with LOGICAL VARIABLES . we extend existing heuristics for RELATIONAL MODELS and propose new heuristics for RELATIONAL MODELS . we evaluate the existing and new heuristics on a range of GENERATED RELATIONAL GRAPHS . \n",
            "this paper addresses the problem of NON-RELATIONAL PROBABILISTIC INFERENCE in GENERATED RELATIONAL GRAPHS . we propose a method for NON-RELATIONAL PROBABILISTIC INFERENCE based on LIFTED PROBABILISTIC INFERENCE . the proposed method is based on the use of PARAMETERIZED RANDOM VARIABLES in the form of RELA-TIONAL MODELS . the proposed method is based on the use of PARAMETERIZED RANDOM VARIABLES and allows for NON-RELATIONAL PROBABILISTIC INFERENCE . experimental results on GENERATED RELATIONAL GRAPHS show that the proposed method is effective for NON-RELATIONAL PROBABILISTIC INFERENCE .\n",
            "\n",
            "78 1000\n",
            "partially observable markov decision processes -lrb- pomdps -rrb- are often used to model planning problems under uncertainty . the goal in RISK-SENSITIVE POMDPS is to find a POLICY that maximizes the probability that the CUMULATIVE COST is within some USER-DEFINED COST THRESHOLD . in this paper , unlike existing RISK-SENSITIVE POMDPS , we distinguish between the two cases of whether costs can or can not be observed and show the empirical impact of COST OBSERVATIONS . we also introduce a new SEARCH-BASED ALGORITHM to solve RISK-SENSITIVE POMDPS and show that SEARCH-BASED ALGORITHM is faster and more scalable than existing approaches in two SYNTHETIC DOMAINS and a TAXI DOMAIN generated with REAL-WORLD DATA . \n",
            "this paper presents a new SEARCH-BASED ALGORITHM for PARTIALLY OBSERVABLE MARKOV DECISION PROCESSES . the proposed SEARCH-BASED ALGORITHM is based on the SEARCH-BASED ALGORITHM and the SEARCH-BASED ALGORITHM . the proposed SEARCH-BASED ALGORITHM is based on the SEARCH-BASED ALGORITHM and the SEARCH-BASED ALGORITHM . the proposed SEARCH-BASED ALGORITHM is compared with the conventional SEARCH-BASED ALGORITHM and the SEARCH-BASED ALGORITHM . the proposed SEARCH-BASED ALGORITHM is compared with the conventional SEARCH-BASED ALGORITHM and the SEARCH-BASED ALGORITHM .\n",
            "\n",
            "79 1000\n",
            "we study a PHASE RETRIEVAL PROBLEM in the POISSON NOISE MODEL . motivated by the PHASELIFT APPROACH , we approximate the MAXIMUM-LIKELIHOOD ESTIMATOR by solving a CONVEX PROGRAM with a NUCLEAR NORM CONSTRAINT . while the FRANK-WOLFE ALGORITHM , together with the LANCZOS METHOD , can efficiently deal with NUCLEAR NORM CONSTRAINTS , our OBJECTIVE FUNCTION does not have a LIPSCHITZ CONTINUOUS GRADIENT , and hence existing CONVERGENCE GUARANTEES for the FRANK-WOLFE ALGORITHM do not apply . in this paper , we show that the FRANK-WOLFE ALGORITHM works for the POISSON PHASE RETRIEVAL PROBLEM , and has a GLOBAL CONVERGENCE RATE of o -lrb- 1/t -rrb- , where t is the ITERATION COUNTER . we provide rigorous theoretical guarantee and illustrating numerical results . \n",
            "this paper proposes a new LANCZOS METHOD for POISSON PHASE RETRIEVAL PROBLEM . the proposed LANCZOS METHOD is based on a CONVEX PROGRAM with a POISSON NOISE MODEL . the proposed FRANK-WOLFE ALGORITHM is based on a CONVEX PROGRAM . the proposed LANCZOS METHOD is based on a CONVEX PROGRAM with a POISSON NOISE MODEL . the proposed LANCZOS METHOD is compared with the conventional LANCZOS METHOD and the PHASELIFT APPROACH . the proposed PHASELIFT APPROACH is compared with the conventional LANCZOS METHOD and the LANCZOS METHOD .\n",
            "\n",
            "80 1000\n",
            "to ensure high enough RECOGNITION performance from the <unk> of usage of the SPEECH RECOGNITION SYSTEM , prior development of highly precise ACOUSTIC MODEL LIBRARY is necessary . the analysis of HMM ACOUSTIC MODELS expressed with GAUSSIAN DISTRIBUTIONS OF MULTIDIMENSIONAL VECTORS is typically a difficult task . the <unk> -lrb- acoustic space map of sound -rrb- method featuring the visualization of distributions of the HMM ACOUSTIC MODELS in a two dimensional space by utilizing MULTIDIMENSIONAL SCALING TECHNIQUE is proposed in order to support the analysis through capability of HUMAN VISUAL PERCEPTION . the effectiveness of the proposed technique is reviewed based on an analysis on SPEAKING STYLES . the MARGINAL REGION within the TWO-DIMENSIONAL VISUAL MAP -lrb- called COSMOS MAP -rrb- obtained by the proposed method the contains HMM ACOUSTIC MODELS with lower RECOGNITION performance . it is possible to improve RECOGNITION performance by dividing the MARGINAL REGION into several smaller zones in which separate HMM ACOUSTIC MODELS is trained and provided to the speakers belonging to the same zone . \n",
            "this paper presents a new method for HUMAN VISUAL PERCEPTION based on GAUSSIAN DISTRIBUTIONS OF MULTIDIMENSIONAL VECTORS . the proposed method is based on a MULTIDIMENSIONAL SCALING TECHNIQUE , which is based on a MULTIDIMENSIONAL SCALING TECHNIQUE . the proposed method is based on a MULTIDIMENSIONAL SCALING TECHNIQUE , which is based on a MULTIDIMENSIONAL SCALING TECHNIQUE . the proposed method is based on a MULTIDIMENSIONAL SCALING TECHNIQUE . the proposed method is based on a MULTIDIMENSIONAL SCALING TECHNIQUE . the proposed method is based on a MULTIDIMENSIONAL SCALING TECHNIQUE . the proposed method is based on a MULTIDIMENSIONAL SCALING TECHNIQUE and is applied to the SPEECH RECOGNITION SYSTEM .\n",
            "\n",
            "81 1000\n",
            "this paper discusses the application of the EXPECTATION-MAXIMIZATION CLUSTERING ALGORITHM to the task of CHINESE VERB SENSE DISCRIMINATION . the EXPECTATION-MAXIMIZATION CLUSTERING ALGORITHM utilized RICH LINGUISTIC FEATURES that capture PREDICATE-ARGUMENT STRUCTURE INFORMATION of the target verbs . a SEMANTIC TAXONOMY for CHINESE NOUNS , which was built semi-automatically based on two ELECTRONIC CHINESE SEMANTIC DICTIONARIES , was used to provide SEMANTIC FEATURES for the EXPECTATION-MAXIMIZATION CLUSTERING ALGORITHM . purity and NORMALIZED MUTUAL INFORMATION were used to evaluate the clustering performance on 12 CHINESE VERBS . the experimental results show that the EXPECTATION-MAXIMIZATION CLUSTERING ALGORITHM can learn sense or sense group distinctions for most of the verbs successfully . we further enhanced the EXPECTATION-MAXIMIZATION CLUSTERING ALGORITHM with certain FINE-GRAINED SEMANTIC CATEGORIES called LEXICAL SETS . our results indicate that these LEXICAL SETS improve the EXPECTATION-MAXIMIZATION CLUSTERING ALGORITHM 's performance for the three most challenging verbs chosen from the first set of experiments . \n",
            "this paper presents a novel EXPECTATION-MAXIMIZATION CLUSTERING ALGORITHM for CHINESE VERB SENSE DISCRIMINATION . the EXPECTATION-MAXIMIZATION CLUSTERING ALGORITHM is based on the NORMALIZED MUTUAL INFORMATION of the SEMANTIC TAXONOMY . the EXPECTATION-MAXIMIZATION CLUSTERING ALGORITHM is based on the NORMALIZED MUTUAL INFORMATION of the SEMANTIC TAXONOMY . the proposed EXPECTATION-MAXIMIZATION CLUSTERING ALGORITHM is based on the NORMALIZED MUTUAL INFORMATION , which is based on the NORMALIZED MUTUAL INFORMATION . the proposed EXPECTATION-MAXIMIZATION CLUSTERING ALGORITHM is based on the NORMALIZED MUTUAL INFORMATION . the proposed EXPECTATION-MAXIMIZATION CLUSTERING ALGORITHM is applied to CHINESE VERB SENSE DISCRIMINATION .\n",
            "\n",
            "82 1000\n",
            "<unk> speech synthesis is necessary in some applications such as AUTOMATIC FILM DUBBING or SPOKEN TRANSLATION . this paper presents a model for the generation of SYNTHETIC DISFLUENT SPEECH based on inserting each element of a disfluency in a context where they can be considered fluent . PROSODY obtained by the application of standard techniques on these new sentences is used for the synthesis of the <unk> sentence . in addition , LOCAL MODIFICATIONS are applied to SEGMENTAL UNITS adjacent to DISFLUENCY ELEMENTS . experiments evidence that duration follows this behavior , what supports the feasibility of the model . \n",
            "this paper addresses the problem of DISFLUENT SPEECH SYNTHESIS in SPOKEN TRANSLATION . we propose a method for DISFLUENT SPEECH SYNTHESIS based on LOCAL MODIFICATIONS . the proposed method is based on the use of LOCAL MODIFICATIONS and LOCAL MODIFICATIONS . the proposed method is based on the use of SEGMENTAL UNITS and LOCAL MODIFICATIONS . the experimental results show that the proposed method is robust and robust to SPOKEN TRANSLATION and SPOKEN TRANSLATION .\n",
            "\n",
            "83 1000\n",
            "an original concept for computing instantaneous 3D POSE and 3d velocity of fast moving objects using a single view is proposed , implemented and validated . it takes advantage of the IMAGE DEFORMATIONS induced by ROLLING SHUTTER in CMOS IMAGE SENSORS . first of all , after analysing the ROLLING SHUTTER PHENOMENON , we introduce an original model of the IMAGE FORMATION when using such a camera , based on a general model of moving rigid sets of 3d points . using 2D-3D POINT CORRESPONDENCES , we derive two complementary methods , compensating for the ROLLING SHUTTER DEFORMATIONS to deliver an accurate 3D POSE and exploiting them to also estimate the FULL 3D VELOCITY . the first solution is a general one based on NON-LINEAR OPTIMIZATION and BUNDLE ADJUSTMENT , usable for any object , while the second one is a CLOSED-FORM LINEAR SOLUTION valid for PLANAR OBJECTS . the resulting CLOSED-FORM LINEAR SOLUTION enable us to transform a cmos low cost and low power camera into an innovative and powerful VELOCITY SENSOR . finally , experimental results with real data confirm the relevance and ACCURACY of the CLOSED-FORM LINEAR SOLUTION . \n",
            "this paper presents a new method for IMAGE FORMATION in CMOS IMAGE SENSORS . the proposed method is based on a CLOSED-FORM LINEAR SOLUTION and a CLOSED-FORM LINEAR SOLUTION based on NON-LINEAR OPTIMIZATION . the proposed method is based on a CLOSED-FORM LINEAR SOLUTION and a CLOSED-FORM LINEAR SOLUTION . the proposed method is based on a CLOSED-FORM LINEAR SOLUTION and a CLOSED-FORM LINEAR SOLUTION . the proposed method is based on a CLOSED-FORM LINEAR SOLUTION and a CLOSED-FORM LINEAR SOLUTION . the ACCURACY of the proposed method is compared with the conventional BUNDLE ADJUSTMENT .\n",
            "\n",
            "84 1000\n",
            "maximum margin criterion is a well-known method for FEATURE EXTRACTION and DIMENSIONALITY REDUCTION . in this paper , we propose a novel FEATURE EXTRACTION method , namely TWO DIMENSIONAL MAXIMUM MARGIN CRITERION , specifically for MATRIX REPRESENTATION DATA , e.g. IMAGES . 2DMMC aims to find two ORTHOGONAL PROJECTION MATRICES to project the original matrices to a LOW DIMENSIONAL MATRIX SUBSPACE , in which a sample is close to those in the same class but far from those in different classes . both THEORETICAL ANALYSIS and experiments on BENCHMARK FACE RECOGNITION DATA SETS illustrate that the proposed method is very effective and efficient . \n",
            "this paper addresses the problem of DIMENSIONALITY REDUCTION in IMAGES . we propose a method for FEATURE EXTRACTION based on the MAXIMUM MARGIN CRITERION . the proposed method is based on the MAXIMUM MARGIN CRITERION . the proposed method is based on the MAXIMUM MARGIN CRITERION . the proposed method is based on the MAXIMUM MARGIN CRITERION . the proposed method is based on the MAXIMUM MARGIN CRITERION . the proposed method is based on the MAXIMUM MARGIN CRITERION . the proposed method is based on the MAXIMUM MARGIN CRITERION . the proposed method is based on the MAXIMUM MARGIN CRITERION .\n",
            "\n",
            "85 1000\n",
            "this study investigates the ACOUSTIC CHARACTERISTICS of / t h / <unk> in CONVERSATIONAL SPEECH OF BRUNEI MANDARIN , a variety of MANDARIN CHINESE . based on data from 20 CHINESE BRUNEIANS , / t h / <unk> was found in the <unk> pronoun <unk> / t h a / , which is frequently pronounced as <unk> -lsb- ha -rsb- . PERCEPTUAL JUDGMENTS , SPECTROGRAPHIC ANALYSIS and ACOUSTIC CHARACTERISTICS were conducted to examine the FEATURES of this SOUND CHANGE . in comparison with the PERCEPTUAL JUDGMENTS , it was found that the SPECTROGRAPHIC INSPECTION yielded <unk> % CORRECT CLASSIFICATION of -lsb- t h -rsb- and -lsb- h -rsb- for FEMALE SPEAKERS and <unk> % for male speakers , indicating there is reasonably high reliability in IDENTIFICATION in terms of SPECTRAL PROPERTIES . results of the ACOUSTIC CHARACTERISTICS showed that there is an increase in high frequency intensity after the release of the CLOSURE for -lsb- t h -rsb- while there is little change in intensity during the FRICATION for -lsb- h -rsb- . the results showed that the lack of BURST and little increase in intensity are reasonably reliable cues for stop <unk> . \n",
            "this paper addresses the problem of CONVERSATIONAL SPEECH OF BRUNEI MANDARIN in MANDARIN CHINESE . this paper presents a method for IDENTIFICATION in MANDARIN CHINESE . the proposed method is based on the use of FEATURES and FEATURES . the proposed method is based on a SPECTROGRAPHIC ANALYSIS , which is based on SPECTROGRAPHIC ANALYSIS and IDENTIFICATION . the proposed method is based on the use of FEATURES and SPECTRAL PROPERTIES . the proposed method is evaluated on MANDARIN CHINESE and MANDARIN CHINESE .\n",
            "\n",
            "86 1000\n",
            "comparative news summarization aims to highlight the <unk> and differences between two comparable NEWS TOPICS . in this study , we propose a novel approach to generating COMPARATIVE NEWS SUMMARIES . we formulate the task as an OPTIMIZATION PROBLEM of selecting proper sentences to maximize the <unk> within the summary and the representativeness to both NEWS TOPICS . we consider SEMANTIC-RELATED CROSS-TOPIC CONCEPT PAIRS as comparative evidences , and consider TOPIC-RELATED CONCEPTS as representative evidences . the OPTIMIZATION PROBLEM is addressed by using a LINEAR PROGRAMMING MODEL . the experimental results demonstrate the effectiveness of our proposed model . \n",
            "this paper presents a new method for COMPARATIVE NEWS SUMMARIES . the proposed method is based on a LINEAR PROGRAMMING MODEL , which is based on the LINEAR PROGRAMMING MODEL . the proposed method is based on the LINEAR PROGRAMMING MODEL . the proposed method is based on the LINEAR PROGRAMMING MODEL . the experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "87 1000\n",
            "stochastic optimization arising from PRECODING in a MULTI-ANTENNA FADING CHANNEL with channel mean feedback to maximize data rates is important but challenging . the use of RELAYING further complicates the situation , as it may induce a NONCONVEX STRUCTURE in the OBJECTIVE FUNCTION , thereby excluding the use of existing approaches which require convexity or concavity . to deal with challenges as such , this paper presents a new framework for solving a class of STOCHASTIC OPTIMIZATION PROBLEMS . the analysis here involves the comparison of two NONNEGATIVE RANDOM VARIABLES in the LAPLACE TRANSFORM ORDER . our framework is <unk> to optimal PRECODING for MAXIMUM ERGODIC OR EFFECTIVE CAPACITY in MULTI-ANTENNA CHANNELS with or without RELAYING assuming channel mean feedback , where the objectives may or may not have convexity or concavity . the application to STOCHASTIC POWER ALLOCATION is also discussed . \n",
            "this paper addresses the problem of STOCHASTIC POWER ALLOCATION in MULTI-ANTENNA CHANNELS . we propose a method for PRECODING based on the LAPLACE TRANSFORM ORDER . the proposed method is based on the LAPLACE TRANSFORM ORDER of the LAPLACE TRANSFORM ORDER . the proposed method is based on the LAPLACE TRANSFORM ORDER and the LAPLACE TRANSFORM ORDER . the proposed method is based on the LAPLACE TRANSFORM ORDER . the proposed method is shown to be robust to MAXIMUM ERGODIC OR EFFECTIVE CAPACITY and is robust to MAXIMUM ERGODIC OR EFFECTIVE CAPACITY .\n",
            "\n",
            "88 1000\n",
            "<unk> <unk> multiple-access -lrb- <unk> -rrb- has been adopted in the UPLINK of the LTE STANDARD due to its lower PEAK-TO-AVERAGE-POWER RATIO compared to ORTHOGONAL FREQUENCY-DIVISION MULTIPLE ACCESS . recent activities in the LTE-ADVANCED STANDARDIZATION have focused on effective UPLINK TRANSMIT DIVERSITY SCHEMES with low <unk> -lsb- 1 -rsb- but without considering the PERFORMANCE DEGRADATION due to DOPPLER despite the fact that ORTHOGONAL FREQUENCY-DIVISION MULTIPLE ACCESS is required to support high mobility . in this paper , we present an OPEN-LOOP SPACE-FREQUENCY BLOCK CODING SCHEME for the SC-FDMA UPLINK and demonstrate its ROBUSTNESS to high DOPPLER and large multipath delay spread while enjoying FULL SPATIAL DIVERSITY , low <unk> and PRACTICAL DECODING COMPLEXITY by a suitable design of the frequency span of each SFBC CODEWORD . in this paper , we study the DESIGN TRADEOFFS involved in SINGLE-CARRIER FREQUENCY-DIVISION MULTIPLE-ACCESS for MOBILE SC-FDMA . \n",
            "this paper presents a OPEN-LOOP SPACE-FREQUENCY BLOCK CODING SCHEME for ORTHOGONAL FREQUENCY-DIVISION MULTIPLE ACCESS . the proposed OPEN-LOOP SPACE-FREQUENCY BLOCK CODING SCHEME is based on a OPEN-LOOP SPACE-FREQUENCY BLOCK CODING SCHEME of the LTE STANDARD . the proposed OPEN-LOOP SPACE-FREQUENCY BLOCK CODING SCHEME is based on the PEAK-TO-AVERAGE-POWER RATIO of the LTE STANDARD . the proposed OPEN-LOOP SPACE-FREQUENCY BLOCK CODING SCHEME is based on the PEAK-TO-AVERAGE-POWER RATIO of the LTE STANDARD . the ROBUSTNESS of the proposed OPEN-LOOP SPACE-FREQUENCY BLOCK CODING SCHEME is compared with the conventional OPEN-LOOP SPACE-FREQUENCY BLOCK CODING SCHEME . the ROBUSTNESS of the proposed method is compared with the conventional OPEN-LOOP SPACE-FREQUENCY BLOCK CODING SCHEME .\n",
            "\n",
            "89 1000\n",
            "this article deals with FACIAL SEGMENTATION and LIPTRACKING with FEEDBACK CONTROL by FACE MODEL SYNTHESIS . on this topic , the search community is divided into two parts : analysis and synthesis . we want to use all the knowledge to create a GLOBAL ANALYSIS/SYNTHESIS CHAIN where the IMAGE ANALYSIS needs the 3D SYNTHESIS and conversely . as it happens , applications like FACE TRACKING or AUGMENTED REALITY need a rapid , robust and <unk> solution . our solution is based on a TWO STEP APPROACH : the first step is a REAL-TIME FACIAL SEGMENTATION with ACTIVE CONTOUR MODELS and the second step recovers a 3D-FACE MODEL in order to extract more precise parameters to adjust the first step . the contribution of this paper is to couple two research fields for creating a REAL TIME APPLICATION . the results obtained show rapid and robust performances which could be exploited in a more GLOBAL REAL-TIME FACE TRACKING APPLICATION . \n",
            "this paper addresses the problem of REAL-TIME FACIAL SEGMENTATION for FACE TRACKING . we propose a method for REAL-TIME FACIAL SEGMENTATION , which is based on the TWO STEP APPROACH and the TWO STEP APPROACH . the proposed method is based on the TWO STEP APPROACH and the TWO STEP APPROACH . the proposed method is based on the TWO STEP APPROACH and the TWO STEP APPROACH . the proposed method is evaluated on the GLOBAL REAL-TIME FACE TRACKING APPLICATION and the GLOBAL REAL-TIME FACE TRACKING APPLICATION .\n",
            "\n",
            "90 1000\n",
            "we have previously introduced a method of WORD SENSE DISAMBIGUATION that computes the intended sense of a target word , using WORDNET-BASED MEASURES OF SEMANTIC RELATEDNESS -lrb- <unk> et al. , 2003 -rrb- . SENSERELATE : : TARGETWORD is a PERL PACKAGE that implements this algorithm . the WORD SENSE DISAMBIGUATION is carried out by selecting that sense of the target word which is most related to the CONTEXT WORDS . relatedness between WORD SENSES is measured using the SENSERELATE : : SIMILARITY PERL MODULES . \n",
            "this paper introduces a new PERL PACKAGE called TARGETWORD , which is a PERL PACKAGE for WORD SENSE DISAMBIGUATION . the PERL PACKAGE is a PERL PACKAGE , called TARGETWORD , that is based on a PERL PACKAGE , such as SENSERELATE , and a PERL PACKAGE . the PERL PACKAGE is a PERL PACKAGE , which is a PERL PACKAGE .\n",
            "\n",
            "91 1000\n",
            "recently , we have presented a TRANSFER-FUNCTION GENERALIZED SIDELOBE CANCELER BEAMFORMER in the short time fourier transform domain , which relies on a convolutive transfer function approximation of RELATIVE TRANSFER FUNCTIONS between distinct sensors . in this paper , we combine a DELAY-AND-SUM BEAMFORMER with the TF-GSC STRUCTURE in order to suppress the SPEECH SIGNAL REFLECTIONS captured at the sensors in REVERBERANT ENVIRONMENTS . we demonstrate the performance of the proposed TRANSFER-FUNCTION GENERALIZED SIDELOBE CANCELER BEAMFORMER and compare TRANSFER-FUNCTION GENERALIZED SIDELOBE CANCELER BEAMFORMER with the TF-GSC . we show that the proposed TRANSFER-FUNCTION GENERALIZED SIDELOBE CANCELER BEAMFORMER enables SUPPRESSION OF REVERBERATIONS and further NOISE REDUCTION compared with the TF-GSC BEAMFORMER . \n",
            "this paper presents a TRANSFER-FUNCTION GENERALIZED SIDELOBE CANCELER BEAMFORMER for SPEECH SIGNAL REFLECTIONS . the proposed TRANSFER-FUNCTION GENERALIZED SIDELOBE CANCELER BEAMFORMER is based on the TRANSFER-FUNCTION GENERALIZED SIDELOBE CANCELER BEAMFORMER . the TRANSFER-FUNCTION GENERALIZED SIDELOBE CANCELER BEAMFORMER is based on the TRANSFER-FUNCTION GENERALIZED SIDELOBE CANCELER BEAMFORMER and the TRANSFER-FUNCTION GENERALIZED SIDELOBE CANCELER BEAMFORMER . the proposed TRANSFER-FUNCTION GENERALIZED SIDELOBE CANCELER BEAMFORMER is based on the TRANSFER-FUNCTION GENERALIZED SIDELOBE CANCELER BEAMFORMER and the TRANSFER-FUNCTION GENERALIZED SIDELOBE CANCELER BEAMFORMER . the proposed TRANSFER-FUNCTION GENERALIZED SIDELOBE CANCELER BEAMFORMER is compared with the conventional DELAY-AND-SUM BEAMFORMER and the TRANSFER-FUNCTION GENERALIZED SIDELOBE CANCELER BEAMFORMER .\n",
            "\n",
            "92 1000\n",
            "this paper presents a REPRESENTATION THEORY for PERMUTATION-VALUED FUNCTIONS , which in their general form can also be called LISTWISE RANKING FUNCTIONS . POINT-WISE RANKING FUNCTIONS assign a score to each object independently , without taking into account the other objects under consideration ; whereas LISTWISE LOSS FUNCTIONS evaluate the set of scores assigned to all objects as a whole . in many SUPERVISED LEARNING to RANK TASKS , it might be of interest to use LISTWISE RANKING FUNCTIONS instead ; in particular , the BAYES OPTIMAL RANKING FUNCTIONS might themselves be <unk> , especially if the LOSS FUNCTION is <unk> . a key <unk> to using LIST-WISE RANKING FUNCTIONS has been the lack of an appropriate REPRESENTATION THEORY for such functions . we show that a NATURAL SYMMETRICITY ASSUMPTION that we call exchangeability allows us to explicitly characterize the set of such EXCHANGEABLE LISTWISE RANKING FUNCTIONS . our analysis draws from the theories of TENSOR ANALYSIS , FUNCTIONAL ANALYSIS and DE FINETTI THEOREMS . we also present experiments using a novel RERANKING METHOD motivated by our REPRESENTATION THEORY . \n",
            "this paper addresses the problem of EXCHANGEABLE LISTWISE RANKING FUNCTIONS in RANK TASKS . in this paper , we propose a new RERANKING METHOD based on REPRESENTATION THEORY and BAYES OPTIMAL RANKING FUNCTIONS . the proposed RERANKING METHOD is based on the use of PERMUTATION-VALUED FUNCTIONS and BAYES OPTIMAL RANKING FUNCTIONS . the proposed RERANKING METHOD is based on the use of PERMUTATION-VALUED FUNCTIONS and BAYES OPTIMAL RANKING FUNCTIONS . the proposed RERANKING METHOD is based on the RERANKING METHOD and the RERANKING METHOD . experimental results show the effectiveness of the proposed RERANKING METHOD .\n",
            "\n",
            "93 1000\n",
            "in DIALOGUE SYSTEMS , SPEECH RECOGNITION ERRORS force the user to repeat information resulting in more turns , lower DIALOGUE EFFICIENCY and maybe complete failure . HIGHER LEVEL INFORMATION such as HISTORY , EXPECTATION , DISCOURSE KNOWLEDGE and PRAGMATICS can improve performance but are hard to quantify and effectively include in the RECOGNITION PROCESS . in this paper , DIALOGUE EXPECTATION is used to improve RECOGNITION . by permitting the DIALOGUE MANAGER to guide the interaction it is possible to track the DIALOGUE STATE and thus estimate the expected semantic content of the user 's response . the DIALOGUE MANAGER is allowed to process a large number of sentences provided by the DECODER . EXPECTATION is used as an effective criterion for selecting among competing hypotheses . this approach was tested with a simple FLIGHT RESERVATION TASK and results show improvement in CONCEPT RECOGNITION without adding significant computation . \n",
            "this paper addresses the problem of CONCEPT RECOGNITION in DIALOGUE SYSTEMS . in this paper , we present a method for CONCEPT RECOGNITION based on HIGHER LEVEL INFORMATION . the proposed method is based on the use of HIGHER LEVEL INFORMATION and HIGHER LEVEL INFORMATION . the proposed method is based on the use of HIGHER LEVEL INFORMATION and EXPECTATION . the proposed method is based on a DIALOGUE MANAGER and a DIALOGUE MANAGER . experimental results show the effectiveness of the proposed method in terms of DIALOGUE EFFICIENCY and DIALOGUE EFFICIENCY .\n",
            "\n",
            "94 1000\n",
            "in this paper we present a novel POINT SET REGISTRATION ALGORITHM based on the ROBUST GAUSSIAN MIXTURE MODEL . we take advantage of a robust estimation to weigh the NOISE COMPONENT in ROBUST GAUSSIAN MIXTURE MODEL . moreover , a BIDIRECTIONAL EM PROCESS is introduced to model OUTLIERS in both POINT SETS in contrast to traditional methods . the performance of the POINT SET REGISTRATION ALGORITHM is demonstrated and validated in carefully designed SYNTHETIC DATA and POINT SETS extracted from MEDICAL IMAGES . results show that the proposed POINT SET REGISTRATION ALGORITHM can improve the ROBUSTNESS and ACCURACY as compared to the traditional REGISTRATION TECHNIQUES . \n",
            "this paper proposes a new POINT SET REGISTRATION ALGORITHM for MEDICAL IMAGES . the proposed POINT SET REGISTRATION ALGORITHM is based on the ROBUST GAUSSIAN MIXTURE MODEL . the proposed ROBUST GAUSSIAN MIXTURE MODEL is based on the ROBUST GAUSSIAN MIXTURE MODEL and the ROBUST GAUSSIAN MIXTURE MODEL . the proposed POINT SET REGISTRATION ALGORITHM is evaluated on the SYNTHETIC DATA and compared with the conventional POINT SET REGISTRATION ALGORITHM . the proposed POINT SET REGISTRATION ALGORITHM outperforms the conventional POINT SET REGISTRATION ALGORITHM in terms of ACCURACY and ACCURACY .\n",
            "\n",
            "95 1000\n",
            "in this paper we propose a simple but efficient IMAGE REPRESENTATION for solving the SCENE CLASSIFICATION PROBLEM . our new IMAGE REPRESENTATION combines the benefits of SPATIAL PYRAMID REPRESENTATION using NONLINEAR FEATURE CODING and LATENT SUPPORT VECTOR MACHINE to train a set of LATENT PYRAMIDAL REGIONS . each of our LATENT PYRAMIDAL REGIONS captures a DISCRIMINATIVE CHARACTERISTIC OF THE SCENES and is trained by searching over all possible <unk> of the images in a LATENT SVM TRAINING PROCEDURE . each LATENT PYRAMIDAL REGIONS is represented in a SPATIAL PYRAMID and uses NON-LINEAR LOCALITY CONSTRAINT CODING for learning both shape and texture patterns of the scene . the final response of the LATENT PYRAMIDAL REGIONS form a single feature vector which we call the IMAGE REPRESENTATION and can be used for the SCENE CLASSIFICATION PROBLEM . we tested our proposed SCENE REPRESENTATION MODEL in three datasets which contain a variety of scene categories -lrb- <unk> , UIUC-SPORTS and <unk> -rrb- . our IMAGE REPRESENTATION obtains state-of-the-art results on all these datasets which shows that IMAGE REPRESENTATION can simultaneously model the GLOBAL AND LOCAL SCENE CHARACTERISTICS in a single framework and is general enough to be used for both INDOOR AND OUTDOOR SCENE CLASSIFICATION . \n",
            "this paper presents a new SCENE REPRESENTATION MODEL for INDOOR AND OUTDOOR SCENE CLASSIFICATION . the SCENE REPRESENTATION MODEL is based on the LATENT SUPPORT VECTOR MACHINE and the LATENT SVM TRAINING PROCEDURE . the SCENE REPRESENTATION MODEL is based on the LATENT SUPPORT VECTOR MACHINE and the LATENT SVM TRAINING PROCEDURE . the proposed SCENE REPRESENTATION MODEL is based on the LATENT SUPPORT VECTOR MACHINE and the LATENT SVM TRAINING PROCEDURE . the SCENE REPRESENTATION MODEL is applied to the SCENE CLASSIFICATION PROBLEM . the SCENE REPRESENTATION MODEL is applied to the SCENE CLASSIFICATION PROBLEM . the results show that the proposed SCENE REPRESENTATION MODEL is effective for INDOOR AND OUTDOOR SCENE CLASSIFICATION .\n",
            "\n",
            "96 1000\n",
            "in many MACHINE LEARNING APPLICATIONS , one has access , not only to training data , but also to some high-level a PRIORI KNOWLEDGE about the desired behavior of the system . for example , it is known in advance that the output of a CHARACTER RECOGNIZER should be invariant with respect to small spatial distortions of the input images -lrb- translations , ROTATIONS , SCALE CHANGES , <unk> -rrb- . we have implemented a scheme that allows a network to learn the derivative of its outputs with respect to DISTORTION OPERATORS of our choosing . this not only reduces the LEARNING TIME and the amount of training data , but also provides a powerful language for specifying what generalizations we wish the network to perform . \n",
            "this paper presents a new method for MACHINE LEARNING APPLICATIONS in MACHINE LEARNING APPLICATIONS . the method is based on the use of DISTORTION OPERATORS and DISTORTION OPERATORS . the proposed method is based on the use of DISTORTION OPERATORS and DISTORTION OPERATORS .\n",
            "\n",
            "97 1000\n",
            "blind channel estimation is a promising technique to reduce the PILOT OVERHEAD . unfortunately , most existing algorithms suffer from the SCALAR AMBIGUITY PROBLEM , and hence only achieve SEMI-BLIND IDENTIFICATION . in this paper , we show that with the INFORMATION OF SOURCE CONSTELLATION , the phase of the AMBIGUOUS SCALAR can be divided into a FRACTIONAL PART and an INTEGER PART . then we propose a MULTIPLE-CONSTELLATION SCHEME enabling totally BLIND IDENTIFICATION regardless of CONSTELLATION TYPE for OFDM SYSTEMS . the necessary and sufficient condition for eliminating the SCALAR AMBIGUITY is given . an application example shows that our MULTIPLE-CONSTELLATION SCHEME can help other algorithms circumvent the ANNOYING AMBIGUITY . \n",
            "this paper presents a new method for BLIND IDENTIFICATION . the proposed method is based on the INFORMATION OF SOURCE CONSTELLATION and the INFORMATION OF SOURCE CONSTELLATION . the proposed method is based on the INFORMATION OF SOURCE CONSTELLATION and the INFORMATION OF SOURCE CONSTELLATION . the proposed method is based on the INFORMATION OF SOURCE CONSTELLATION and the INFORMATION OF SOURCE CONSTELLATION . the proposed method is based on the INFORMATION OF SOURCE CONSTELLATION and the INFORMATION OF SOURCE CONSTELLATION . the proposed method is based on the INFORMATION OF SOURCE CONSTELLATION and the INFORMATION OF SOURCE CONSTELLATION .\n",
            "\n",
            "98 1000\n",
            "a general framework for MINIMISATION-BASED BELIEF CHANGE is presented . a problem instance is made up of an UNDIRECTED GRAPH , where a formula is associated with each vertex . for example , VERTICES may represent SPATIAL LOCATIONS , points in time , or some other notion of locality . information is shared between VERTICES via a process of VERTICES over the GRAPH . we give equivalent SEMANTIC AND SYNTACTIC CHARACTERISATIONS of this VERTICES . we also show that this approach is general enough to capture existing MINIMISATION-BASED APPROACHES to BELIEF MERGING , BELIEF REVISION , and -lrb- temporal -rrb- <unk> operators . while we focus on a SET-THEORETIC NOTION OF MINIMISATION , we also consider other approaches , such as CARDINALITY-BASED AND PRIORITY-BASED MINIMISATION . \n",
            "this paper addresses the problem of SET-THEORETIC NOTION OF MINIMISATION in MINIMISATION-BASED BELIEF CHANGE . we propose a method for SET-THEORETIC NOTION OF MINIMISATION based on the SET-THEORETIC NOTION OF MINIMISATION . the proposed method is based on the use of MINIMISATION-BASED BELIEF CHANGE and BELIEF REVISION . the proposed method is based on the SET-THEORETIC NOTION OF MINIMISATION . the proposed method is based on the SET-THEORETIC NOTION OF MINIMISATION . the proposed method is based on the SET-THEORETIC NOTION OF MINIMISATION . the proposed method is based on the SET-THEORETIC NOTION OF MINIMISATION .\n",
            "\n",
            "99 1000\n",
            "we propose a FAST REGRESSION MODEL for practical SINGLE IMAGE SUPER-RESOLUTION based on IN-PLACE EXAMPLES , by leveraging two fundamental SUPER-RESOLUTION APPROACHES -- learning from an EXTERNAL DATABASE and learning from <unk> . our IN-PLACE SELF-SIMILARITY refines the recently proposed LOCAL SELF-SIMILARITY by proving that a PATCH in the UPPER SCALE IMAGE have good matches around its ORIGIN LOCATION in the lower scale image . based on the IN-PLACE EXAMPLES , a FIRST-ORDER APPROXIMATION of the NONLINEAR MAPPING FUNCTION from LOW-TO HIGH-RESOLUTION IMAGE PATCHES is learned . extensive experiments on BENCHMARK AND REAL-WORLD IMAGES demonstrate that our FAST REGRESSION MODEL can produce <unk> results with sharp edges and preserved fine details , while the current state-of-the-art algorithms are prone to VISUAL ARTIFACTS . furthermore , our FAST REGRESSION MODEL can easily extend to deal with NOISE by combining the regression results on multiple IN-PLACE EXAMPLES for ROBUST ESTIMATION . the FAST REGRESSION MODEL runs fast and is particularly useful for practical applications , where the INPUT IMAGES typically contain DIVERSE TEXTURES and they are potentially contaminated by NOISE or compression artifacts . \n",
            "this paper presents a FAST REGRESSION MODEL for ROBUST ESTIMATION . the FAST REGRESSION MODEL is based on the NONLINEAR MAPPING FUNCTION of the NONLINEAR MAPPING FUNCTION . the FAST REGRESSION MODEL is based on the NONLINEAR MAPPING FUNCTION . the FAST REGRESSION MODEL is based on the NONLINEAR MAPPING FUNCTION . the FAST REGRESSION MODEL is based on the NONLINEAR MAPPING FUNCTION . the proposed FAST REGRESSION MODEL is based on a FAST REGRESSION MODEL . the proposed FAST REGRESSION MODEL is based on a FAST REGRESSION MODEL . the proposed FAST REGRESSION MODEL is based on a FAST REGRESSION MODEL . the proposed FAST REGRESSION MODEL is based on a FAST REGRESSION MODEL . the proposed FAST REGRESSION MODEL is based on a FAST REGRESSION MODEL . the proposed FAST REGRESSION MODEL is based on a FAST REGRESSION MODEL . the proposed FAST REGRESSION MODEL is based on a FAST REGRESSION MODEL . the proposed FAST REGRESSION MODEL is based on a FAST REGRESSION MODEL . the proposed FAST REGRESSION MODEL is based on a FAST REGRESSION MODEL and is shown to be robust to VISUAL ARTIFACTS .\n",
            "\n",
            "100 1000\n",
            "essence is a new FORMAL LANGUAGE for specifying COMBINATORIAL PROBLEMS in a manner similar to NATURAL RIGOROUS SPECIFICATIONS that use a mixture of NATURAL LANGUAGE and DISCRETE MATHEMATICS . ESSENCE provides a high level of ABSTRACTION , much of which is the consequence of the provision of DECISION VARIABLES whose values can be COMBINA-TORIAL OBJECTS , such as TUPLES , sets , MULTISETS , RELATIONS , PARTITIONS and functions . ESSENCE also allows these COMBINATORIAL PROBLEMS to be nested to ARBITRARY DEPTH , thus providing , for example , sets of PARTITIONS , sets of sets of PARTITIONS , and so forth . therefore , a problem that requires finding a COMPLEX COMBINATORIAL OBJECT can be directly specified by using a DECISION VARIABLE whose type is precisely that COMBINATORIAL OBJECT . \n",
            "this paper addresses the problem of ARBITRARY DEPTH in NATURAL LANGUAGE such as ARBITRARY DEPTH , MULTISETS , and PARTITIONS . we propose a method for ARBITRARY DEPTH , which is based on a DECISION VARIABLE and a DECISION VARIABLE . the proposed method is based on a DECISION VARIABLE , which is a DECISION VARIABLE and a DECISION VARIABLE . the proposed method is based on a DECISION VARIABLE and a DECISION VARIABLE . the proposed method is based on a DECISION VARIABLE , which is robust to ARBITRARY DEPTH , ARBITRARY DEPTH , and ABSTRACTION .\n",
            "\n",
            "101 1000\n",
            "we introduce an UNSUPERVISED LEARNING ALGORITHM that combines PROBABILISTIC MODELING with SOLVER-BASED TECHNIQUES for PROGRAM SYNTHESIS . we apply our techniques to both a VISUAL LEARNING DOMAIN and a LANGUAGE LEARNING PROBLEM , showing that our UNSUPERVISED LEARNING ALGORITHM can learn many VISUAL CONCEPTS from only a few examples and that UNSUPERVISED LEARNING ALGORITHM can recover some ENGLISH INFLECTIONAL MORPHOLOGY . taken together , these results give both a new approach to UNSUPERVISED LEARNING OF SYMBOLIC COMPOSI-TIONAL STRUCTURES , and a technique for applying PROGRAM SYNTHESIS TOOLS to NOISY DATA . \n",
            "this paper presents a new UNSUPERVISED LEARNING ALGORITHM for UNSUPERVISED LEARNING OF SYMBOLIC COMPOSI-TIONAL STRUCTURES . the UNSUPERVISED LEARNING ALGORITHM is based on the UNSUPERVISED LEARNING ALGORITHM and the UNSUPERVISED LEARNING ALGORITHM . the UNSUPERVISED LEARNING ALGORITHM is based on the UNSUPERVISED LEARNING ALGORITHM and the UNSUPERVISED LEARNING ALGORITHM . the proposed UNSUPERVISED LEARNING ALGORITHM is based on the use of SOLVER-BASED TECHNIQUES and SOLVER-BASED TECHNIQUES . the proposed UNSUPERVISED LEARNING ALGORITHM is based on SOLVER-BASED TECHNIQUES and SOLVER-BASED TECHNIQUES .\n",
            "\n",
            "102 1000\n",
            "we study the problem of estimating the POSITION and orientation of a CALIBRATED CAMERA from an IMAGE OF A KNOWN SCENE . a common problem in CAMERA POSE ESTIMATION is the existence of false correspondences between IMAGE FEATURES and modeled 3d points . existing techniques such as RANSAC to handle OUTLIERS have no guarantee of <unk> . in contrast , we work with a natural extension of the L ∞ NORM to the outlier case . using a simple result from CLASSICAL GEOMETRY , we derive necessary conditions for L ∞ OPTIMALITY and show how to use L ∞ OPTIMALITY in a BRANCH AND BOUND SETTING to find the optimum and to detect OUTLIERS . the algorithm has been evaluated on synthetic as well as REAL DATA showing good empirical performance . in addition , for cases with no OUTLIERS , we demonstrate shorter execution times than existing OPTIMAL ALGORITHMS . \n",
            "this paper presents a method for CAMERA POSE ESTIMATION from REAL DATA . the proposed method is based on the L ∞ NORM of the CLASSICAL GEOMETRY . the proposed method is based on the L ∞ NORM of the CLASSICAL GEOMETRY . the proposed method is based on the L ∞ NORM of the CLASSICAL GEOMETRY . the proposed method is based on the L ∞ NORM . the proposed method is based on the L ∞ NORM . the proposed method is compared with the conventional OPTIMAL ALGORITHMS .\n",
            "\n",
            "103 1000\n",
            "we study CONFIDENTIALITY ENFORCEMENT in ontologies under the CONTROLLED QUERY EVALUATION FRAMEWORK , where a policy specifies the sensitive information and a <unk> ensures that query answers that may compromise the policy are not returned . we focus on CENSORS that ensure <unk> while <unk> information access , and consider both <unk> and the OWL 2 PROFILES as ONTOLOGY LANGUAGES . \n",
            "this paper presents a new CONTROLLED QUERY EVALUATION FRAMEWORK for ONTOLOGY LANGUAGES . the proposed CONTROLLED QUERY EVALUATION FRAMEWORK is based on a CONTROLLED QUERY EVALUATION FRAMEWORK . the proposed CONTROLLED QUERY EVALUATION FRAMEWORK is based on a CONTROLLED QUERY EVALUATION FRAMEWORK . the proposed CONTROLLED QUERY EVALUATION FRAMEWORK is based on a CONTROLLED QUERY EVALUATION FRAMEWORK and is shown to be useful for ONTOLOGY LANGUAGES .\n",
            "\n",
            "104 1000\n",
            "we present a PRIVACY-PRESERVING SYSTEM for estimating the size of INHOMOGENEOUS CROWDS , composed of pedestrians that travel in different directions , without using EXPLICIT OBJECT SEGMENTATION or TRACKING . first , the CROWD is segmented into components of HOMOGENEOUS MOTION , using the mixture of DYNAMIC TEXTURES MOTION MODEL . second , a set of simple HOLISTIC FEATURES is extracted from each SEGMENTED REGION , and the correspondence between FEATURES and the number of people per segment is learned with GAUSSIAN PROCESS REGRESSION . we validate both the CROWD SEGMENTATION ALGORITHM , and the CROWD COUNTING SYSTEM , on a LARGE PEDESTRIAN DATASET -lrb- 2000 frames of video , containing <unk> total pedestrian instances -rrb- . finally , we present results of the PRIVACY-PRESERVING SYSTEM running on a full hour of video . \n",
            "this paper presents a new method for EXPLICIT OBJECT SEGMENTATION based on GAUSSIAN PROCESS REGRESSION . the proposed method is based on the CROWD SEGMENTATION ALGORITHM and the CROWD SEGMENTATION ALGORITHM . the proposed method is based on the DYNAMIC TEXTURES MOTION MODEL and the CROWD SEGMENTATION ALGORITHM . the proposed method is based on the CROWD SEGMENTATION ALGORITHM and the CROWD SEGMENTATION ALGORITHM . the proposed method is based on the CROWD SEGMENTATION ALGORITHM and the CROWD SEGMENTATION ALGORITHM . the proposed method is based on the CROWD SEGMENTATION ALGORITHM and the CROWD SEGMENTATION ALGORITHM . the experimental results show the effectiveness of the proposed method in terms of TRACKING and TRACKING .\n",
            "\n",
            "105 1000\n",
            "we introduce a TWO-LAYER UNDIRECTED GRAPHICAL MODEL , called a '' replicated <unk> '' , that can be used to TWO-LAYER UNDIRECTED GRAPHICAL MODEL and automatically extract LOW-DIMENSIONAL LATENT SEMANTIC REPRESENTATIONS from a large UNSTRUCTURED COLLECTION OF DOCUMENTS . we present efficient LEARNING AND INFERENCE ALGORITHMS for this TWO-LAYER UNDIRECTED GRAPHICAL MODEL , and show how a TWO-LAYER UNDIRECTED GRAPHICAL MODEL , TWO-LAYER UNDIRECTED GRAPHICAL MODEL , can be used to produce an accurate estimate of the <unk> the TWO-LAYER UNDIRECTED GRAPHICAL MODEL assigns to test data . this allows us to demonstrate that the proposed TWO-LAYER UNDIRECTED GRAPHICAL MODEL is able to generalize much better compared to LATENT DIRICHLET ALLOCATION in terms of both the <unk> of HELD-OUT DOCUMENTS and the RETRIEVAL ACCURACY . \n",
            "this paper presents a TWO-LAYER UNDIRECTED GRAPHICAL MODEL for UNSTRUCTURED COLLECTION OF DOCUMENTS . the proposed TWO-LAYER UNDIRECTED GRAPHICAL MODEL is based on a TWO-LAYER UNDIRECTED GRAPHICAL MODEL , called LATENT DIRICHLET ALLOCATION . the proposed TWO-LAYER UNDIRECTED GRAPHICAL MODEL is based on the TWO-LAYER UNDIRECTED GRAPHICAL MODEL . the proposed TWO-LAYER UNDIRECTED GRAPHICAL MODEL is based on a TWO-LAYER UNDIRECTED GRAPHICAL MODEL . the proposed TWO-LAYER UNDIRECTED GRAPHICAL MODEL is shown to outperform the conventional LEARNING AND INFERENCE ALGORITHMS in terms of RETRIEVAL ACCURACY and RETRIEVAL ACCURACY .\n",
            "\n",
            "106 1000\n",
            "several early GAME-PLAYING COMPUTER PROGRAMS used FORWARD PRUNING -lrb- i.e. , the practice of deliberately ignoring nodes that are believed unlikely to affect a GAME TREE 's minimax value -rrb- , but this technique did not seem to result in good decision-making . the poor performance of FORWARD PRUNING presents a major puzzle for ai research on GAME PLAYING , because some version of FORWARD PRUNING seems to be '' what people do , '' and the best CHESS-PLAYING PROGRAMS still do not play as well as the best humans . as a step toward deeper understanding of FORWARD PRUNING , we have set up models of FORWARD PRUNING on two different kinds of GAME TREES , and used these models to investigate how FORWARD PRUNING affects the probability of choosing the correct move . in our studies , FORWARD PRUNING did better than <unk> when there was a high correlation among the MINIMAX VALUES OF SIBLING NODES in a GAME TREE . this result suggests that FORWARD PRUNING may possibly be a useful DECISION-MAKING TECHNIQUE in certain kinds of games . in particular , we believe that bridge may be such a game . \n",
            "this paper addresses the problem of GAME PLAYING in a GAME TREE . in this paper , we propose a new method for GAME PLAYING based on the MINIMAX VALUES OF SIBLING NODES . the proposed method is based on the use of CHESS-PLAYING PROGRAMS to estimate the MINIMAX VALUES OF SIBLING NODES . the proposed method is based on a DECISION-MAKING TECHNIQUE . the proposed method is based on a DECISION-MAKING TECHNIQUE .\n",
            "\n",
            "107 1000\n",
            "we propose MODULE GRAPHICAL LASSO , an aggressive dimensionality reduction and NETWORK ESTIMATION TECHNIQUE for a HIGH-DIMENSIONAL GAUSSIAN GRAPHICAL MODEL . MODULE GRAPHICAL LASSO achieves SCALABILITY , interpretability and RO-BUSTNESS by exploiting the modularity property of many REAL-WORLD NETWORKS . VARIABLES are organized into tightly coupled modules and a GRAPH STRUCTURE is estimated to determine the conditional <unk> among modules . MODULE GRAPHICAL LASSO iteratively learns the MODULE ASSIGNMENT OF VARIABLES , the LATENT VARIABLES , each corresponding to a MODULE , and the parameters of the GGM of the LATENT VARIABLES . in synthetic data experiments , MODULE GRAPHICAL LASSO outperforms the standard GRAPHICAL LASSO and three other methods that incorporate LATENT VARIABLES into GGM . when applied to GENE EXPRESSION DATA from OVARIAN CANCER , MODULE GRAPHICAL LASSO out-performs standard CLUSTERING ALGORITHMS in identifying FUNCTIONALLY COHERENT GENE SETS and PREDICTING SURVIVAL TIME OF PATIENTS . the learned modules and their dependencies provide novel insights into CANCER BIOLOGY as well as identifying possible novel drug targets . \n",
            "this paper presents a new NETWORK ESTIMATION TECHNIQUE for PREDICTING SURVIVAL TIME OF PATIENTS . the NETWORK ESTIMATION TECHNIQUE is based on a HIGH-DIMENSIONAL GAUSSIAN GRAPHICAL MODEL and a HIGH-DIMENSIONAL GAUSSIAN GRAPHICAL MODEL for PREDICTING SURVIVAL TIME OF PATIENTS . the proposed NETWORK ESTIMATION TECHNIQUE is based on a HIGH-DIMENSIONAL GAUSSIAN GRAPHICAL MODEL , which is based on the MODULE GRAPHICAL LASSO . the proposed NETWORK ESTIMATION TECHNIQUE is based on the MODULE ASSIGNMENT OF VARIABLES and the MODULE ASSIGNMENT OF VARIABLES . the proposed NETWORK ESTIMATION TECHNIQUE is compared with conventional CLUSTERING ALGORITHMS and CLUSTERING ALGORITHMS . the proposed NETWORK ESTIMATION TECHNIQUE is compared with other CLUSTERING ALGORITHMS and CLUSTERING ALGORITHMS .\n",
            "\n",
            "108 1000\n",
            "the EARTH MOVER 'S DISTANCE -lsb- 19 -rsb- is an important perceptually meaningful metric for comparing HISTOGRAMS , but EARTH MOVER 'S DISTANCE suffers from high -lrb- o -lrb- n 3 log n -rrb- -rrb- COMPUTATIONAL COMPLEXITY . we present a novel LINEAR TIME ALGORITHM for approximating the EARTH MOVER 'S DISTANCE for LOW DIMENSIONAL HIS-TOGRAMS using the sum of absolute values of the WEIGHTED WAVELET COEFFICIENTS of the DIFFERENCE HISTOGRAM . EMD COMPUTATION is a special case of the KANTOROVICH-RUBINSTEIN TRANSSHIPMENT PROBLEM , and we exploit the HÖLDER CONTINUITY CONSTRAINT in its DUAL FORM to convert EARTH MOVER 'S DISTANCE into a simple OPTIMIZATION PROBLEM with an explicit solution in the WAVELET DOMAIN . we prove that the resulting WAVELET EMD METRIC is equivalent to EARTH MOVER 'S DISTANCE , i.e. the ratio of the two is bounded . we also provide estimates for the bounds . the WEIGHTED WAVELET TRANSFORM can be computed in time linear in the number of HISTOGRAM BINS , while the comparison is about as fast as for NORMAL EUCLIDEAN DISTANCE or <unk> 2 statistic . we experimentally show that WAVELET EMD METRIC is a good approximation to EARTH MOVER 'S DISTANCE , has similar performance , but requires much less computation . \n",
            "this paper proposes a new method for EMD COMPUTATION based on the WEIGHTED WAVELET TRANSFORM . the proposed WAVELET EMD METRIC is based on the WEIGHTED WAVELET TRANSFORM of the DIFFERENCE HISTOGRAM . the proposed WAVELET EMD METRIC is based on the WEIGHTED WAVELET TRANSFORM of the DIFFERENCE HISTOGRAM . the proposed WAVELET EMD METRIC is based on the WEIGHTED WAVELET TRANSFORM of the WEIGHTED WAVELET TRANSFORM . the proposed WAVELET EMD METRIC is applied to the KANTOROVICH-RUBINSTEIN TRANSSHIPMENT PROBLEM . the proposed WAVELET EMD METRIC is compared with the conventional WAVELET EMD METRIC and the WAVELET EMD METRIC .\n",
            "\n",
            "109 1000\n",
            "we present a novel precoding or modulation scheme -lrb- matrix modulation -rrb- that allows PARALLEL TRANSMISSION of several DATA SIGNALS over an unknown multiple-input multiple-output -lrb- mimo -rrb- channel . we first present a theorem on unique SIGNAL DEMODULATION and an efficient ITERATIVE DEMODULATION ALGORITHM for TRANSMISSION over an UNKNOWN INSTANTANEOUS-MIXTURE CHANNEL . we then generalize our results to an UNKNOWN MIMO CHANNEL with memory . \n",
            "this paper presents a new ITERATIVE DEMODULATION ALGORITHM for DATA SIGNALS . the ITERATIVE DEMODULATION ALGORITHM is based on the ITERATIVE DEMODULATION ALGORITHM . the proposed ITERATIVE DEMODULATION ALGORITHM is based on the ITERATIVE DEMODULATION ALGORITHM , which is based on the ITERATIVE DEMODULATION ALGORITHM . the proposed ITERATIVE DEMODULATION ALGORITHM is based on the ITERATIVE DEMODULATION ALGORITHM . the ITERATIVE DEMODULATION ALGORITHM is applied to the DATA SIGNALS .\n",
            "\n",
            "110 1000\n",
            "most SPEECH ENHANCEMENT ALGORITHMS heavily depend on the NOISE POWER SPECTRAL DENSITY . because this quantity is unknown in practice , estimation from the NOISY DATA is necessary . we present a LOW COMPLEXITY METHOD for NOISE PSD ESTIMATION . the LOW COMPLEXITY METHOD is based on a MINIMUM MEAN-SQUARED ERROR ESTIMA-TOR of the NOISE MAGNITUDE-SQUARED DFT COEFFICIENTS . compared to MINIMUM STATISTICS BASED NOISE TRACKING , SEGMENTAL SNR and PESQ are improved for NON-STATIONARY NOISE SOURCES with 1 db and 0.25 mos points , respectively . compared to recently published algorithms , similar good NOISE TRACKING performance is obtained , but at a COMPUTATIONAL COMPLEXITY that is in the order of a factor 40 lower . \n",
            "this paper presents a new LOW COMPLEXITY METHOD for NOISE TRACKING . the proposed SPEECH ENHANCEMENT ALGORITHMS is based on the MINIMUM MEAN-SQUARED ERROR ESTIMA-TOR , which is based on the MINIMUM MEAN-SQUARED ERROR ESTIMA-TOR . the proposed SPEECH ENHANCEMENT ALGORITHMS is based on the MINIMUM MEAN-SQUARED ERROR ESTIMA-TOR . the proposed SPEECH ENHANCEMENT ALGORITHMS is based on the MINIMUM MEAN-SQUARED ERROR ESTIMA-TOR and the MINIMUM MEAN-SQUARED ERROR ESTIMA-TOR . the COMPUTATIONAL COMPLEXITY of the proposed SPEECH ENHANCEMENT ALGORITHMS is compared with the conventional SPEECH ENHANCEMENT ALGORITHMS and the LOW COMPLEXITY METHOD .\n",
            "\n",
            "111 1000\n",
            "a new SPEECH ENHANCEMENT SYSTEM , which is based on a TIME-FREQUENCY ADAPTIVE WAVELET SOFT THRESHOLDING , is presented in this paper . the SPEECH ENHANCEMENT SYSTEM utilises a BARK-SCALED WAVELET PACKET DECOMPOSITION integrated into a MODIFIED WEINER FILTERING TECHNIQUE using a novel THRESHOLD ESTIMATION METHOD based on a MAGNITUDE DECISION-DIRECTED APPROACH . first , a BARK-SCALED WAVELET PACKET transform is used to decompose the SPEECH SIGNAL into critical bands . THRESHOLD ESTIMATION is then performed for each WAVELET BAND according to an ADAPTIVE NOISE LEVEL-TRACKING ALGORITHM . finally , the speech is estimated by incorporating the COMPUTED THRESHOLD into a WIENER FILTERING PROCESS , using the MAGNITUDE DECISION-DIRECTED APPROACH . the proposed SPEECH ENHANCEMENT TECHNIQUE has been tested with various STATIONARY AND NON-STATIONARY NOISE CASES . reported results show that the SPEECH ENHANCEMENT SYSTEM is capable of a high-level of NOISE SUPPRESSION while preserving the intelligibility and naturalness of the speech . \n",
            "this paper proposes a MODIFIED WEINER FILTERING TECHNIQUE for NOISE SUPPRESSION . the THRESHOLD ESTIMATION METHOD is based on a MODIFIED WEINER FILTERING TECHNIQUE for NOISE SUPPRESSION . the ADAPTIVE NOISE LEVEL-TRACKING ALGORITHM is based on a MODIFIED WEINER FILTERING TECHNIQUE , which is based on a MODIFIED WEINER FILTERING TECHNIQUE . the proposed THRESHOLD ESTIMATION METHOD is based on a MODIFIED WEINER FILTERING TECHNIQUE , which is based on a MODIFIED WEINER FILTERING TECHNIQUE . the proposed SPEECH ENHANCEMENT TECHNIQUE is based on a MODIFIED WEINER FILTERING TECHNIQUE for NOISE SUPPRESSION . the proposed SPEECH ENHANCEMENT TECHNIQUE is applied to a SPEECH ENHANCEMENT SYSTEM for STATIONARY AND NON-STATIONARY NOISE CASES . the experimental results show that the proposed SPEECH ENHANCEMENT TECHNIQUE is effective for NOISE SUPPRESSION .\n",
            "\n",
            "112 1000\n",
            "this paper proposes a DECENTRALIZED STATE ESTIMATION SCHEME via NETWORK GOSSIPING with applications in SMART GRID WIDE-AREA MONITORING . the proposed DECENTRALIZED STATE ESTIMATION SCHEME allows DISTRIBUTED CONTROL AREAS to solve for an accurate GLOBAL STATE ESTIMATE collaboratively using the proposed GOSSIP-BASED GAUSS-NEWTON ALGORITHM . furthermore , the proposed DECENTRALIZED STATE ESTIMATION SCHEME mitigates the influence of BAD DATA by adap-tively updating the NOISE VARIANCES and re-weighting the contributions of the most recent measurements for STATE ESTIMATION . compared with other DISTRIBUTED TECHNIQUES , our DECENTRALIZED STATE ESTIMATION SCHEME via GOSSIPING is more flexible and resilient in case of NETWORK RECONFIGURATIONS and failures . we further prove that the POWER FLOW EQUATIONS satisfy the sufficient condition for the GOSSIP-BASED GAUSS-NEWTON ALGORITHM to converge to the desired solution . simulations of the IEEE-118 SYSTEM show that the proposed DECENTRALIZED STATE ESTIMATION SCHEME estimates and tracks the global state robustly , and degrades gracefully when there are RANDOM FAILURES and BAD DATA . \n",
            "this paper presents a DECENTRALIZED STATE ESTIMATION SCHEME for SMART GRID WIDE-AREA MONITORING . the GOSSIP-BASED GAUSS-NEWTON ALGORITHM is based on the POWER FLOW EQUATIONS and the GLOBAL STATE ESTIMATE . the GOSSIP-BASED GAUSS-NEWTON ALGORITHM is based on the POWER FLOW EQUATIONS and the POWER FLOW EQUATIONS . the proposed DECENTRALIZED STATE ESTIMATION SCHEME is based on the POWER FLOW EQUATIONS and the POWER FLOW EQUATIONS . the proposed DECENTRALIZED STATE ESTIMATION SCHEME is compared with the conventional DECENTRALIZED STATE ESTIMATION SCHEME and the GOSSIP-BASED GAUSS-NEWTON ALGORITHM . the proposed DECENTRALIZED STATE ESTIMATION SCHEME is compared with the conventional DECENTRALIZED STATE ESTIMATION SCHEME and the GOSSIP-BASED GAUSS-NEWTON ALGORITHM .\n",
            "\n",
            "113 1000\n",
            "integration is affected by the curse of dimen-sionality and quickly becomes intractable as the dimensionality of the problem grows . we propose a RANDOMIZED ALGORITHM that , with high probability , gives a <unk> approximation of a general discrete integral defined over an exponentially large set . this RANDOMIZED ALGORITHM relies on solving only a small number of instances of a DISCRETE COMBINATORIAL OPTIMIZATION PROBLEM subject to RANDOMLY GENERATED PARITY CONSTRAINTS used as a HASH FUNCTION . as an DISCRETE COMBINATORIAL OPTIMIZATION PROBLEM , we demonstrate that with a small number of MAP QUERIES we can efficiently approximate the PARTITION FUNCTION of DISCRETE GRAPHICAL MODELS , which can in turn be used , for instance , for MARGINAL COMPUTATION or MODEL SELECTION . \n",
            "this paper addresses the problem of MODEL SELECTION in DISCRETE GRAPHICAL MODELS . we propose a new RANDOMIZED ALGORITHM based on the RANDOMIZED ALGORITHM . the proposed method is based on the RANDOMIZED ALGORITHM and the RANDOMIZED ALGORITHM . the proposed method is based on the RANDOMIZED ALGORITHM and the RANDOMIZED ALGORITHM . the proposed method is compared with the conventional RANDOMIZED ALGORITHM and the RANDOMIZED ALGORITHM .\n",
            "\n",
            "114 1000\n",
            "we present a robust approach to modeling VOICED SPEECH using a family of minimum variance distortionless response -lrb- mvdr -rrb- spectral estimates . the method exploits the fact that for a SXED MODEL ORDER , for a SINUSOIDAL SIGNAL in noise , the MVDR ESTIMATE at the SINUSOIDAL FREQUENCY is approximately related to the SINUSOIDAL AND NOISE POWER in a simple LINEAR MANNER with the COEFSCIENTS being dependent on the MODEL ORDER . MODELING VOICED SPEECH as a sum of harmonic signals , we then use the aforementioned relationship along with a LEAST SQUARES APPROACH to combine a family of MVDR ESTIMATES -LRB- MVDR ESTIMATES of different orders -rrb- and develop a robust approach for modeling VOICED SPEECH . experimental results of SPECTRAL ESTIMATION OF SINUSOIDS , SYNTHETIC VOWELS , and actual speech signals at MVDR ESTIMATES -LRB- MVDR ESTIMATES of 0 db and 5 db using this approach indicate an increased resolution in the ESTIMATED MVDR SPECTRA . the MVDR ESTIMATES -LRB- MVDR ESTIMATES computed from the MVDR ESTIMATE using this approach are also used for speaker <unk> experiments on the TIMIT DATABASE at various MVDR ESTIMATES -LRB- MVDR ESTIMATES . the results indicate a reasonable improvement in RECOGNITION performance when compared to the MVDR ESTIMATES -LRB- MVDR ESTIMATES and the SXED ORDER MVDR-MFCC . \n",
            "this paper presents a new method for SPECTRAL ESTIMATION OF SINUSOIDS in VOICED SPEECH . the proposed method is based on a LEAST SQUARES APPROACH and a MVDR ESTIMATE based on the LEAST SQUARES APPROACH . the proposed method is based on the LEAST SQUARES APPROACH and the MVDR ESTIMATES -LRB- MVDR ESTIMATES . the proposed method is based on the LEAST SQUARES APPROACH and the LEAST SQUARES APPROACH . the proposed method is based on the LEAST SQUARES APPROACH and the MVDR ESTIMATES -LRB- MVDR ESTIMATES . the proposed method is compared with the conventional LEAST SQUARES APPROACH and the MVDR ESTIMATES -LRB- MVDR ESTIMATES .\n",
            "\n",
            "115 1000\n",
            "clipping or SATURATION in AUDIO SIGNALS is a very common problem in SIGNAL PROCESSING , for which , in the severe case , there is still no satisfactory solution . in such case , there is a tremendous loss of information , and traditional methods fail to appropriately recover the signal . we propose a novel approach for this SIGNAL RESTORATION PROBLEM based on the framework of ITERATIVE HARD THRESHOLDING . this approach , which enforces the consistency of the RECONSTRUCTED SIGNAL with the CLIPPED OBSERVATIONS , shows superior performance in comparison to the state-of-the-art DECLIPPING ALGORITHMS . this is confirmed on synthetic and on actual HIGH-DIMENSIONAL AUDIO DATA PROCESSING , both on SNR and on SUBJECTIVE USER LISTENING EVALUATIONS . \n",
            "in this paper , we propose a method for HIGH-DIMENSIONAL AUDIO DATA PROCESSING in AUDIO SIGNALS . the proposed method is based on the use of ITERATIVE HARD THRESHOLDING and ITERATIVE HARD THRESHOLDING . the proposed method is based on the use of ITERATIVE HARD THRESHOLDING and ITERATIVE HARD THRESHOLDING . the proposed method is based on the use of ITERATIVE HARD THRESHOLDING and ITERATIVE HARD THRESHOLDING . experimental results show the effectiveness of the proposed method in terms of SNR and SNR .\n",
            "\n",
            "116 1000\n",
            "consider a MIMO HETEROGENEOUS NETWORK with multiple transmitters -lrb- including <unk> , <unk> and <unk> base stations -rrb- and many receivers -lrb- mobile users -rrb- . the users are to be assigned to the base stations which then optimize their LINEAR TRANSMIT BEAMFORMERS accordingly . in this work , we consider the problem of JOINT BASE STATION ASSIGNMENT and LINEAR BEAM-FORMER DESIGN to maximize a SYSTEM WIDE UTILITY . we first establish the np-hardness of the resulting OPTIMIZATION PROBLEM for a large family of Α-FAIRNESS UTILITY FUNCTIONS . then , we propose an efficient algorithm to approximately solve this problem for the special case of SUM RATE MAXIMIZATION . the simulation results show that the algorithm improves the SUM RATE . \n",
            "this paper presents a new method for JOINT BASE STATION ASSIGNMENT . the proposed method is based on a LINEAR BEAM-FORMER DESIGN and a LINEAR BEAM-FORMER DESIGN . the proposed method is based on a JOINT BASE STATION ASSIGNMENT and a LINEAR BEAM-FORMER DESIGN . the proposed method is based on a MIMO HETEROGENEOUS NETWORK and a LINEAR BEAM-FORMER DESIGN . the proposed method is based on a MIMO HETEROGENEOUS NETWORK and a LINEAR BEAM-FORMER DESIGN .\n",
            "\n",
            "117 1000\n",
            "blind channel identification using LINEAR REDUNDANT FILTERBANK PRECODERS has been studied extensively in the literature . most methods are proposed based on the assumption that BLOCK SYNCHRONIZATION is perfect . in practice , a BLIND BLOCK SYNCHRONIZATION ALGORITHM must be used to justify this assumption . this paper studies the BLIND BLOCK SYNCHRONIZATION PROBLEM in systems using a ZERO-PADDING PRECODER . a previously reported method is reviewed and a new approach for the BLIND BLOCK SYNCHRONIZATION PROBLEM is proposed . GENERALIZED VERSIONS of both approaches are then developed using a PARAMETER called REPETITION INDEX . simulation results show that when the REPETITION INDEX is chosen to be greater than unity , the BLOCK SYNCHRONIZATION ERROR RATE performance of the proposed BLIND BLOCK SYNCHRONIZATION ALGORITHM has a significant improvement over the previously reported method . ' \n",
            "this paper proposes a new BLIND BLOCK SYNCHRONIZATION ALGORITHM for BLIND CHANNEL IDENTIFICATION . the proposed BLIND BLOCK SYNCHRONIZATION ALGORITHM is based on the ZERO-PADDING PRECODER . the proposed BLIND BLOCK SYNCHRONIZATION ALGORITHM is based on the ZERO-PADDING PRECODER . the proposed BLIND BLOCK SYNCHRONIZATION ALGORITHM is based on the ZERO-PADDING PRECODER . the proposed BLIND BLOCK SYNCHRONIZATION ALGORITHM is compared with the conventional BLIND BLOCK SYNCHRONIZATION ALGORITHM and the BLIND BLOCK SYNCHRONIZATION ALGORITHM .\n",
            "\n",
            "118 1000\n",
            "<unk> dependency -lrb- pd - -rrb- grammars are proposed as a means of efficient treatment of DISCONTINUOUS CONSTRUCTIONS . <unk> describe two kinds of DEPENDENCIES : LOCAL , explicitly derived by the RULES , and long , implicitly specified by NEGATIVE AND POSITIVE VA-LENCIES OF WORDS . if in a <unk> the number of NON-SATURATED VALENCIES in DERIVED STRUCTURES is bounded by a constant , then it is weakly equivalent to a CF-GRAMMAR and has a cents ¡ $ # ¦ ¥ ¨ § - time parsing algorithm . it happens that such BOUNDED PD-GRAMMARS are strong enough to express such phenomena as UNBOUNDED RAISING , EXTRACTION and EX-TRAPOSITION . \n",
            "this paper addresses the problem of EXTRACTION in POLARIZED DEPENDENCY -LRB- PD - -RRB- GRAMMARS . we propose a method for EXTRACTION based on POLARIZED DEPENDENCY -LRB- PD - -RRB- GRAMMARS and POLARIZED DEPENDENCY -LRB- PD - -RRB- GRAMMARS . the method is based on the use of POLARIZED DEPENDENCY -LRB- PD - -RRB- GRAMMARS and POLARIZED DEPENDENCY -LRB- PD - -RRB- GRAMMARS . the proposed method is based on the use of POLARIZED DEPENDENCY -LRB- PD - -RRB- GRAMMARS and POLARIZED DEPENDENCY -LRB- PD - -RRB- GRAMMARS . the proposed method is based on the use of POLARIZED DEPENDENCY -LRB- PD - -RRB- GRAMMARS and POLARIZED DEPENDENCY -LRB- PD - -RRB- GRAMMARS . the proposed method is compared with other RULES such as EXTRACTION , EXTRACTION , EXTRACTION , and EXTRACTION .\n",
            "\n",
            "119 1000\n",
            "we use well-established results in BIOLOGICAL VISION to construct a novel VISION MODEL for HANDWRITTEN DIGIT RECOGNITION . we show empirically that the FEATURES extracted by our VISION MODEL are linearly separable over a large training set -lrb- mnist -rrb- . using only a LINEAR CLASSIFIER on these FEATURES , our VISION MODEL is relatively simple yet outperforms other models on the same data set . \n",
            "this paper presents a new VISION MODEL for BIOLOGICAL VISION . the VISION MODEL is based on a LINEAR CLASSIFIER . the VISION MODEL is based on a LINEAR CLASSIFIER . the VISION MODEL is based on a LINEAR CLASSIFIER . the proposed VISION MODEL is based on a LINEAR CLASSIFIER and is shown to be useful for HANDWRITTEN DIGIT RECOGNITION .\n",
            "\n",
            "120 1000\n",
            "developing individualized head related transfer functions -lrb- HRTF -rrb- is an essential requirement for accurate VIRTUALIZATION OF SOUND . however it is time consuming and complicated for both the subject and the developer . obtaining the SPECTRAL NOTCHES which are the most prominent features of HRTF is very important to reconstruct the head related impulse response -lrb- HRTF -rrb- accurately . in this paper , a method suitable for fast computation of the frequencies of SPECTRAL NOTCHES is proposed . the LINEAR PREDICTION RESIDUAL CEPSTRUM is used to compute the SPECTRAL NOTCHES with a high degree of ACCURACY in this work . subsequent use of BATTEAUS REFLECTION MODEL to <unk> the SPECTRAL NOTCHES on the PINNA IMAGES indicate that the proposed method is able to provide finer contours . experiments on reconstruction of the HRTF indicates that the method performs better than other methods . \n",
            "this paper presents a new BATTEAUS REFLECTION MODEL for PINNA IMAGES . the BATTEAUS REFLECTION MODEL is based on the LINEAR PREDICTION RESIDUAL CEPSTRUM . the proposed BATTEAUS REFLECTION MODEL is based on the LINEAR PREDICTION RESIDUAL CEPSTRUM . the proposed BATTEAUS REFLECTION MODEL is based on the LINEAR PREDICTION RESIDUAL CEPSTRUM . the ACCURACY of the proposed BATTEAUS REFLECTION MODEL is demonstrated by simulations .\n",
            "\n",
            "121 1000\n",
            "description logics , and in particular the WEB ONTOLOGY LANGUAGE OWL has been proposed as an appropriate basis for computing matches between STRUCTURED OBJECTS for the sake of INFORMATION INTEGRATION and SERVICE DISCOVERY . a drawback of the direct use of SUBSUMPTION as a MATCHING CRITERION is the inability to compute PARTIAL MATCHES and <unk> the degree of MISMATCH . in this paper , we describe a method for overcoming these problems that is based on APPROXIMATE LOGICAL REASONING . in particular , we approximate the SUBSUMPTION RELATION by defining the notion of SUBSUMPTION with respect to a certain subset of the concept and relation names . we present the formal semantics of this relation , describe a SOUND AND COMPLETE ALGORITHM for COMPUTING APPROXIMATE SUBSUMPTION and discuss its application to MATCHING TASKS . \n",
            "this paper addresses the problem of SERVICE DISCOVERY in WEB ONTOLOGY LANGUAGE OWL . we propose a method for COMPUTING APPROXIMATE SUBSUMPTION based on the MATCHING CRITERION . the proposed method is based on the MATCHING CRITERION and the MATCHING CRITERION . the proposed method is based on the MATCHING CRITERION and the MATCHING CRITERION . the proposed method is based on the MATCHING CRITERION and the MATCHING CRITERION . the proposed method is evaluated on MATCHING TASKS and MATCHING TASKS .\n",
            "\n",
            "122 1000\n",
            "in this work <unk> 's law is followed for designing a PERCEPTUALLY-SHAPED SIDE-INFORMED DATA HIDING SCHEME . the resulting PERCEPTUALLY-SHAPED SIDE-INFORMED DATA HIDING SCHEME is a GENERALIZED VERSION of a LOGARITHMIC QUANTIZATION ALGORITHM previously proposed by the authors . closed formulas for analyzing the EMBEDDING POWER and decoding error probability of this new PERCEPTUALLY-SHAPED SIDE-INFORMED DATA HIDING SCHEME are provided , and experimental results showing its good behavior against severe attacks are reported . \n",
            "this paper introduces a new PERCEPTUALLY-SHAPED SIDE-INFORMED DATA HIDING SCHEME , called LOGARITHMIC QUANTIZATION ALGORITHM , which is a generalization of the LOGARITHMIC QUANTIZATION ALGORITHM . the proposed PERCEPTUALLY-SHAPED SIDE-INFORMED DATA HIDING SCHEME is a GENERALIZED VERSION , a GENERALIZED VERSION , and a GENERALIZED VERSION . the performance of the proposed PERCEPTUALLY-SHAPED SIDE-INFORMED DATA HIDING SCHEME is demonstrated by simulations .\n",
            "\n",
            "123 1000\n",
            "many REAL WORLD DATA SETS can be viewed as points in a HIGHER-DIMENSIONAL SPACE that lie concentrated around a LOWER-DIMENSIONAL MANIFOLD STRUCTURE . we propose a new MULTISCALE REPRESENTATION for such POINT CLOUDS based on LIFTING and perfect matching . the result is an adaptive wavelet transform that decomposes a POINT CLOUD into MANIFOLD APPROXIMATIONS and details at multiple scales . we illustrate with several examples that the transform can extract an UNKNOWN SMOOTH MANIFOLD from NOISY POINT CLOUD SAMPLES using simple WAVELET THRESHOLDING IDEAS . \n",
            "this paper addresses the problem of WAVELET THRESHOLDING IDEAS for POINT CLOUDS in POINT CLOUDS . the MULTISCALE REPRESENTATION is based on the MULTISCALE REPRESENTATION of the POINT CLOUD . the MULTISCALE REPRESENTATION is based on the MULTISCALE REPRESENTATION of the POINT CLOUD . the proposed method is based on the MULTISCALE REPRESENTATION . the proposed method is based on the MULTISCALE REPRESENTATION . the proposed method is based on the MULTISCALE REPRESENTATION . the proposed method is based on the MULTISCALE REPRESENTATION and is shown to be robust to POINT CLOUDS .\n",
            "\n",
            "124 1000\n",
            "in this work we propose an ERROR-RESILIENT SCHEME that allows enhancing the ROBUSTNESS of a VIDEO STREAM . based on DISTRIBUTED SOURCE CODING PRINCIPLES , an AUXILIARY STREAM is sent in parallel to the MAIN STREAM as a redundant representation of the sequence that is used to correct errors at the DECODER , thus reducing the impact of drift . in order to perform an optimal bit allocation in the AUXILIARY STREAM , the ENCODER needs to compute a reliable estimate of the expected video distortion observed at the DECODER SIDE due to CHANNEL LOSS . this paper proposes an algorithm to calculate the expected distortion of decoded <unk> -lrb- dubbed <unk> -rrb- and its application to the BIT ALLOCATION PROBLEM in a DSC BASED AUXILIARY STREAM . \n",
            "this paper presents a new method for DISTRIBUTED SOURCE CODING PRINCIPLES based on DISTRIBUTED SOURCE CODING PRINCIPLES . the proposed method is based on a ERROR-RESILIENT SCHEME and a ERROR-RESILIENT SCHEME . the proposed method is based on a ERROR-RESILIENT SCHEME . the proposed method is based on a ERROR-RESILIENT SCHEME . the proposed method is based on a ERROR-RESILIENT SCHEME of the AUXILIARY STREAM . the ROBUSTNESS of the proposed method is compared with the conventional ERROR-RESILIENT SCHEME .\n",
            "\n",
            "125 1000\n",
            "this paper reports on experimental results obtained from a performance comparison of FEATURE COMBINATIONS STRATEGIES in CONTENT BASED IMAGE RETRIEVAL . the use of SUPPORT VECTOR MACHINES is compared to COMBMIN , <unk> , <unk> and BORDAFUSE COMBINATION STRATEGIES , all of which are evaluated on a carefully compiled set of COREL IMAGES and the trecvid 2003 search task collection . \n",
            "this paper addresses the problem of CONTENT BASED IMAGE RETRIEVAL in COREL IMAGES . in this paper , we propose a new method for CONTENT BASED IMAGE RETRIEVAL based on SUPPORT VECTOR MACHINES . the proposed method is based on the use of FEATURE COMBINATIONS STRATEGIES . experimental results show that the proposed method outperforms the state-of-the-art methods .\n",
            "\n",
            "126 1000\n",
            "sound zones are two or more regions within a LISTENING SPACE where listeners are provided with PERSONAL AUDIO . ACOUSTIC CONTRAST CONTROL is a SOUND ZONING METHOD that maximizes the AVERAGE SQUARED SOUND PRESSURE in one zone constrained to constant pressure in other zones . state-of-the-art time domain broadband acoustic contrast control -lrb- <unk> -rrb- SOUND ZONING METHOD are designed for ANECHOIC ENVIRONMENTS . these SOUND ZONING METHOD are not able to realize a FLAT FREQUENCY RESPONSE in a limited frequency range within a REVERBERANT ENVIRONMENT . SOUND FIELD CONTROL in a limited frequency range is a requirement to accommodate the effective working range of the loudspeakers . in this paper , a new BACC METHOD is proposed which results in an implementation realizing a FLAT FREQUENCY RESPONSE in the target zone . this BACC METHOD is applied in a BANDLIMITED LOW-FREQUENCY SCENARIO where the LOUDSPEAKER LAYOUT <unk> two CONTROLLED ZONES . the performance is verified with experimental results in an ACOUSTI-CALLY DAMPED ROOM . \n",
            "this paper presents a BACC METHOD for PERSONAL AUDIO , which is based on a BACC METHOD . the BACC METHOD is based on a BACC METHOD . the BACC METHOD is based on a BACC METHOD and a BACC METHOD . the proposed BACC METHOD is based on the BACC METHOD and the BACC METHOD . the proposed BACC METHOD is a BACC METHOD , which is a BACC METHOD . the proposed BACC METHOD is based on a BACC METHOD , which is a BACC METHOD for PERSONAL AUDIO .\n",
            "\n",
            "127 1000\n",
            "many HARD COMPUTATIONAL SOCIAL CHOICE PROBLEMS are known to become tractable when voters ' preferences belong to a RESTRICTED DOMAIN , such as those of SINGLE-PEAKED OR SINGLE-CROSSING PREFERENCES . however , to date , all algorithmic results of this type have been obtained for the setting where each voter 's preference list is a total order of candidates . the goal of this paper is to extend this line of research to the setting where voters ' preferences are <unk> , i.e. , each voter <unk> a subset of candidates and <unk> the remaining candidates . we propose several analogues of the notions of SINGLE-PEAKED AND SINGLE-CROSSING PREFERENCES for DICHOTOMOUS PROFILES and investigate the relationships among them . we then demonstrate that for some of these notions the respective RESTRICTED DOMAINS admit efficient algorithms for COMPUTATIONALLY HARD APPROVAL-BASED MULTI-WINNER RULES . \n",
            "this paper presents a method for HARD COMPUTATIONAL SOCIAL CHOICE PROBLEMS from RESTRICTED DOMAINS . the proposed method is based on the use of COMPUTATIONALLY HARD APPROVAL-BASED MULTI-WINNER RULES in the RESTRICTED DOMAIN . the proposed method is based on the COMPUTATIONALLY HARD APPROVAL-BASED MULTI-WINNER RULES . the proposed method is based on the COMPUTATIONALLY HARD APPROVAL-BASED MULTI-WINNER RULES . the proposed method is tested on the RESTRICTED DOMAIN .\n",
            "\n",
            "128 1000\n",
            "we present a novel algorithm for SIMULTANEOUS COLOR AND DEPTH INPAINTING . the algorithm takes STEREO IMAGES and estimated disparity maps as input and fills in MISSING COLOR and depth information introduced by OCCLUSIONS or OBJECT REMOVAL . we first complete the disparities for the OCCLUSION REGIONS using a SEGMENTATION-BASED APPROACH . the completed disparities can be used to facilitate the user in labeling objects to be removed . since part of the removed regions in one image is visible in the other , we mutually complete the two images through 3D WARPING . finally , we complete the remaining unknown regions using a DEPTH-ASSISTED TEXTURE SYNTHESIS TECHNIQUE , which simultaneously fills in both color and depth . we demonstrate the effectiveness of the proposed algorithm on several challenging DATA SETS . \n",
            "this paper presents a new method for OBJECT REMOVAL in STEREO IMAGES . the proposed method is based on the DEPTH-ASSISTED TEXTURE SYNTHESIS TECHNIQUE and the SEGMENTATION-BASED APPROACH . the proposed method is based on the SEGMENTATION-BASED APPROACH and the SEGMENTATION-BASED APPROACH . the proposed method is based on the SEGMENTATION-BASED APPROACH and the SEGMENTATION-BASED APPROACH . the proposed method is evaluated on the DATA SETS . the results show that the proposed method is robust to OCCLUSIONS and OCCLUSIONS .\n",
            "\n",
            "129 1000\n",
            "we present an approach to RECONSTRUCTION OF DETAILED SCENE GEOMETRY from range video . RANGE DATA produced by COMMODITY HANDHELD CAMERAS suffers from HIGH-FREQUENCY ERRORS and LOW-FREQUENCY DISTORTION . our approach deals with both sources of error by reconstructing LOCALLY SMOOTH SCENE FRAGMENTS and letting these fragments deform in order to align to each other . we develop a volumetric registration formulation that leverages the smoothness of the deformation to make OPTIMIZATION practical for large scenes . experimental results demonstrate that our approach substantially increases the fidelity of COMPLEX SCENE GEOMETRY reconstructed with COMMODITY HANDHELD CAMERAS . \n",
            "this paper addresses the problem of RECONSTRUCTION OF DETAILED SCENE GEOMETRY in COMMODITY HANDHELD CAMERAS . the RECONSTRUCTION OF DETAILED SCENE GEOMETRY is formulated as a RECONSTRUCTION OF DETAILED SCENE GEOMETRY with LOCALLY SMOOTH SCENE FRAGMENTS . the proposed method is based on the RECONSTRUCTION OF DETAILED SCENE GEOMETRY in COMMODITY HANDHELD CAMERAS . the proposed method is based on the RECONSTRUCTION OF DETAILED SCENE GEOMETRY . the proposed method is based on the RECONSTRUCTION OF DETAILED SCENE GEOMETRY in COMMODITY HANDHELD CAMERAS with HIGH-FREQUENCY ERRORS and HIGH-FREQUENCY ERRORS .\n",
            "\n",
            "130 1000\n",
            "we present a novel VARIATIONAL APPROACH to TOP-DOWN IMAGE SEGMENTATION , which accounts for significant projective transformations between a single prior image and the image to be segmented . the proposed VARIATIONAL APPROACH is coupled with reliable estimation of the TRANSFORMATION PARAMETERS , without using POINT CORRESPONDENCES . the prior shape is represented by a GENERALIZED CONE that is based on the CONTOUR of the REFERENCE OBJECT . its UNLEVEL SECTIONS correspond to possible instances of the VISIBLE CONTOUR under PERSPECTIVE DISTORTION and SCALING . we extend the CHAN-VESE ENERGY FUNCTIONAL by adding a SHAPE TERM . this term measures the distance between the currently estimated section of the GENERALIZED CONE and the region bounded by the ZERO-CROSSING OF THE EVOLVING LEVEL SET FUNCTION . promising segmentation results are obtained for images of rotated , translated , corrupted and partly occluded objects . the RECOVERED TRANSFORMATION PARAMETERS are compatible with the GROUND TRUTH . \n",
            "this paper proposes a VARIATIONAL APPROACH for TOP-DOWN IMAGE SEGMENTATION . the proposed VARIATIONAL APPROACH is based on a VARIATIONAL APPROACH and a VARIATIONAL APPROACH . the proposed VARIATIONAL APPROACH is based on a VARIATIONAL APPROACH , which is based on a VARIATIONAL APPROACH . the proposed VARIATIONAL APPROACH is based on a VARIATIONAL APPROACH . the proposed VARIATIONAL APPROACH is based on the VARIATIONAL APPROACH and the VARIATIONAL APPROACH . the proposed VARIATIONAL APPROACH is based on a VARIATIONAL APPROACH and is shown to be robust to PERSPECTIVE DISTORTION and SCALING .\n",
            "\n",
            "131 1000\n",
            "the task of REVIEW RATING PREDICTION can be well addressed by using REGRESSION ALGORITHMS if there is a reliable training set of reviews with HUMAN RATINGS . in this paper , we aim to investigate a more challenging task of CROSS-LANGUAGE REVIEW RATING PREDICTION , which makes use of only rated reviews in a source language -lrb- e.g. english -rrb- to predict the rating scores of <unk> reviews in a target language -lrb- e.g. GERMAN -rrb- . we propose a new CO-REGRESSION ALGORITHM to address this task by leveraging UNLABELED REVIEWS . evaluation results on several datasets show that our proposed CO-REGRESSION ALGORITHM can consistently improve the prediction results . \n",
            "this paper addresses the problem of CROSS-LANGUAGE REVIEW RATING PREDICTION in UNLABELED REVIEWS . we propose a method for CROSS-LANGUAGE REVIEW RATING PREDICTION based on the CO-REGRESSION ALGORITHM . the proposed method is based on the use of UNLABELED REVIEWS , and the CO-REGRESSION ALGORITHM is applied to the UNLABELED REVIEWS . experimental results show that the proposed method is effective for CROSS-LANGUAGE REVIEW RATING PREDICTION .\n",
            "\n",
            "132 1000\n",
            "in this paper we present a PROBABILISTIC ALGORITHM that extracts a MAPPING between two subspaces by representing each SUBSPACE as a collection of states . an arbitrary increase in number of states results in over-fitting the training data without exploring the underlying structure of the map . this paper suggests a method to impose SPARSITY CONSTRAINTS on the STATE MAP by using ENTROPIC PRIORS . this PROBABILISTIC ALGORITHM is applied to the problem of ARTIFICIAL BANDWIDTH EXPANSION that involves estimating the MISSING FREQUENCY COMPONENTS -lrb- 3.7 -- 8 khz and 0 -- 0.3 khz -rrb- of speech given the NAR-ROWBAND SPEECH SIGNAL -lrb- 0.3 -- 3.7 khz -rrb- . \n",
            "this paper presents a new PROBABILISTIC ALGORITHM for ARTIFICIAL BANDWIDTH EXPANSION . the PROBABILISTIC ALGORITHM is based on a PROBABILISTIC ALGORITHM . the PROBABILISTIC ALGORITHM is based on the PROBABILISTIC ALGORITHM . the proposed PROBABILISTIC ALGORITHM is based on a PROBABILISTIC ALGORITHM . the proposed PROBABILISTIC ALGORITHM is based on the PROBABILISTIC ALGORITHM . the proposed PROBABILISTIC ALGORITHM is based on a PROBABILISTIC ALGORITHM . the proposed PROBABILISTIC ALGORITHM is based on a PROBABILISTIC ALGORITHM and is shown to be robust to SPARSITY CONSTRAINTS .\n",
            "\n",
            "133 1000\n",
            "we optimize over the set of CORRECTED LAPLACIANS associated with a WEIGHTED GRAPH to improve the AVERAGE CASE NORMALIZED CUT of a GRAPH PARTITIONING . unlike EDGE-RELAXATION SDPS , optimizing over the set cl naturally exploits the MATRIX SPARSITY by operating solely on the diagonal . this structure is critical to IMAGE SEGMENTATION APPLICATIONS because the number of vertices is generally proportional to the number of pixels in the image . CL OPTIMIZATION provides a guiding principle for improving the COMBINATORIAL SOLUTION over the SPECTRAL RELAXATION , which is important because small improvements in the CUT COST often result in significant improvements in the PERCEPTUAL RELEVANCE of the segmenta-tion . we develop an OPTIMIZATION PROCEDURE to accommodate PRIOR INFORMATION in the form of STATISTICAL SHAPE MODELS , resulting in a SEGMENTATION METHOD that produces FOREGROUND REGIONS which are consistent with a PARAMETERIZED FAMILY OF SHAPES . we validate our OPTIMIZATION PROCEDURE with GROUND TRUTH on MRI MEDICAL IMAGES , providing a QUANTITATIVE COMPARISON against results produced by current SPECTRAL RELAXATION APPROACHES to GRAPH PARTITIONING . \n",
            "this paper proposes a new SEGMENTATION METHOD for IMAGE SEGMENTATION APPLICATIONS . the SEGMENTATION METHOD is based on the AVERAGE CASE NORMALIZED CUT of the WEIGHTED GRAPH . the proposed SEGMENTATION METHOD is based on the AVERAGE CASE NORMALIZED CUT of the WEIGHTED GRAPH . the proposed SEGMENTATION METHOD is based on the AVERAGE CASE NORMALIZED CUT of the WEIGHTED GRAPH . the AVERAGE CASE NORMALIZED CUT of the proposed SEGMENTATION METHOD is compared with the conventional SPECTRAL RELAXATION APPROACHES . the proposed SEGMENTATION METHOD is compared with conventional SPECTRAL RELAXATION APPROACHES . the proposed SEGMENTATION METHOD is compared with conventional SPECTRAL RELAXATION APPROACHES .\n",
            "\n",
            "134 1000\n",
            "real world systems often have PARAMETERIZED CONTROLLERS which can be tuned to improve performance . BAYESIAN OPTIMIZATION METHODS provide for efficient optimization of these REAL WORLD SYSTEMS , so as to reduce the number of required experiments on the expensive PHYSICAL SYSTEM . in this paper we address BAYESIAN OPTIMIZATION in the setting where performance is only observed through a STOCHASTIC BINARY OUTCOME -- success or failure of the experiment . unlike BANDIT PROBLEMS , the goal is to maximize the REAL WORLD SYSTEMS performance after this OFFLINE TRAINING PHASE rather than minimize regret during training . in this work we define the STOCHASTIC BINARY OPTIMIZATION PROBLEM and propose an approach using an adaptation of GAUSSIAN PROCESSES for STOCHASTIC BINARY OPTIMIZATION PROBLEM that presents a BAYESIAN OPTIMIZATION FRAMEWORK for this STOCHASTIC BINARY OPTIMIZATION PROBLEM . we propose an EXPERIMENT SELECTION METRIC for this setting based on expected improvement . we demonstrate the algorithm 's performance on SYNTHETIC PROBLEMS and on a real snake robot learning to move over an obstacle . \n",
            "this paper presents a EXPERIMENT SELECTION METRIC for BANDIT PROBLEMS . the BAYESIAN OPTIMIZATION FRAMEWORK is based on a EXPERIMENT SELECTION METRIC and a EXPERIMENT SELECTION METRIC . the proposed BAYESIAN OPTIMIZATION FRAMEWORK is based on a EXPERIMENT SELECTION METRIC , which is based on the EXPERIMENT SELECTION METRIC . the proposed BAYESIAN OPTIMIZATION FRAMEWORK is based on a EXPERIMENT SELECTION METRIC , which is based on a EXPERIMENT SELECTION METRIC . the proposed BAYESIAN OPTIMIZATION FRAMEWORK is based on a EXPERIMENT SELECTION METRIC . the proposed BAYESIAN OPTIMIZATION FRAMEWORK is based on a EXPERIMENT SELECTION METRIC and is shown to be robust to BANDIT PROBLEMS .\n",
            "\n",
            "135 1000\n",
            "the MAP -lrb- maximum a POSTERIORI HYPOTHESIS -RRB- PROBLEM in BAYESIAN NETWORKS is to find the most likely states of a set of variables given partial evidence on the complement of that set . standard STRUCTURE-BASED INFERENCE METHODS for finding exact solutions to MAP , such as VARIABLE ELIMINATION and JOIN-TREE ALGORITHMS , have complexities that are exponential in the CONSTRAINED TREEWIDTH OF THE NETWORK . a more recent algorithm , proposed by <unk> and <unk> , is exponential only in the TREEWIDTH and has been shown to handle networks whose CONSTRAINED TREEWIDTH is quite high . in this paper we present a new algorithm for exact MAP that is not necessarily limited in scalability even by the TREEWIDTH . this is achieved by leverag-ing recent advances in COMPILATION OF BAYESIAN NETWORKS into ARITHMETIC CIRCUITS , which can circumvent TREEWIDTH-IMPOSED LIMITS by exploiting the LOCAL STRUCTURE present in the BAYESIAN NETWORKS . specifically , we implement a BRANCH-AND-BOUND SEARCH where the BOUNDS are computed using LINEAR-TIME OPERATIONS on the COMPILED ARITHMETIC CIRCUIT . on networks with LOCAL STRUCTURE , we observe orders-of-magnitude improvements over the algorithm of <unk> and <unk> . in particular , we are able to efficiently solve many problems where the latter algorithm runs out of memory because of high TREEWIDTH . \n",
            "this paper presents a new method for COMPILATION OF BAYESIAN NETWORKS based on VARIABLE ELIMINATION . the proposed method is based on the COMPILATION OF BAYESIAN NETWORKS and the COMPILATION OF BAYESIAN NETWORKS . the proposed method is based on the COMPILATION OF BAYESIAN NETWORKS and the COMPILATION OF BAYESIAN NETWORKS . the proposed method is based on the use of BAYESIAN NETWORKS and BAYESIAN NETWORKS . the proposed method is based on the COMPILATION OF BAYESIAN NETWORKS and the COMPILATION OF BAYESIAN NETWORKS . the proposed method is based on the use of BAYESIAN NETWORKS and VARIABLE ELIMINATION . the proposed method is based on the COMPILATION OF BAYESIAN NETWORKS and the BOUNDS . the proposed method is based on the COMPILATION OF BAYESIAN NETWORKS and the JOIN-TREE ALGORITHMS .\n",
            "\n",
            "136 1000\n",
            "a key factor of high quality WORD SEGMENTA-TION for JAPANESE is a HIGH-COVERAGE DICTIONARY , but it is costly to manually build such a LEXICAL RESOURCE . although EXTERNAL LEXICAL RESOURCES for human readers are potentially good knowledge sources , EXTERNAL LEXICAL RESOURCES have not been utilized due to differences in SEGMENTATION CRITERIA . to supplement a MORPHOLOGICAL DICTIONARY with these resources , we propose a new task of JAPANESE NOUN PHRASE SEGMENTATION . we apply NON-PARAMETRIC BAYESIAN LANGUAGE MODELS to segment each noun phrase in these resources according to the statistical behavior of its supposed constituents in text . for INFERENCE , we propose a novel BLOCK SAMPLING PROCEDURE named HYBRID TYPE-BASED SAMPLING , which has the ability to directly escape a LOCAL OPTIMUM that is not too distant from the GLOBAL OPTIMUM . experiments show that the proposed BLOCK SAMPLING PROCEDURE efficiently corrects the initial segmentation given by a MORPHOLOGICAL ANA-LYZER . \n",
            "this paper addresses the problem of JAPANESE NOUN PHRASE SEGMENTATION in JAPANESE . we propose a BLOCK SAMPLING PROCEDURE based on the BLOCK SAMPLING PROCEDURE . the proposed method is based on a BLOCK SAMPLING PROCEDURE of the HIGH-COVERAGE DICTIONARY . the proposed method is based on a BLOCK SAMPLING PROCEDURE . the proposed method is based on the BLOCK SAMPLING PROCEDURE . the proposed method is based on a BLOCK SAMPLING PROCEDURE . the proposed method is based on a BLOCK SAMPLING PROCEDURE . the proposed method is based on the BLOCK SAMPLING PROCEDURE . the proposed method is based on the BLOCK SAMPLING PROCEDURE . the proposed method is based on the BLOCK SAMPLING PROCEDURE . the proposed method is based on a BLOCK SAMPLING PROCEDURE .\n",
            "\n",
            "137 1000\n",
            "this paper presents a new algorithm for PLAN RECOGNITION called <unk> -lrb- engine for LEXICALIZED INTENT RECOGNITION -RRB- . <unk> represents the plans to be recognized with a GRAMMATICAL FORMALISM called COMBINATORY CATEGORIAL GRAMMAR . we show that representing plans with COMBINATORY CATEGORIAL GRAMMAR can allow us to prevent early commitment to PLAN GOALS and thereby reduce runtime . \n",
            "this paper presents a new method for PLAN RECOGNITION . the method is based on a COMBINATORY CATEGORIAL GRAMMAR , called COMBINATORY CATEGORIAL GRAMMAR , which is based on the COMBINATORY CATEGORIAL GRAMMAR . the proposed method is based on a COMBINATORY CATEGORIAL GRAMMAR , which can be applied to a wide range of PLAN GOALS .\n",
            "\n",
            "138 1000\n",
            "mention pair MENTION PAIR MODELS that predict whether or not two mentions are coreferent have historically been very effective for COREF-ERENCE RESOLUTION , but do not make use of ENTITY-LEVEL INFORMATION . however , we show that the scores produced by such MENTION PAIR MODELS can be aggregated to define powerful ENTITY-LEVEL FEATURES between clusters of mentions . using these FEATURES , we train an ENTITY-CENTRIC COREFERENCE SYSTEM that learns an effective POLICY for building up COREFERENCE CHAINS incrementally . the MENTION PAIR SCORES are also used to prune the SEARCH SPACE the ENTITY-CENTRIC COREFERENCE SYSTEM works in , allowing for efficient training with an exact LOSS FUNCTION . we evaluate our ENTITY-CENTRIC COREFERENCE SYSTEM on the ENGLISH PORTION of the 2012 CONLL SHARED TASK DATASET and show that ENTITY-CENTRIC COREFERENCE SYSTEM improves over the current state of the art . \n",
            "this paper presents a ENTITY-CENTRIC COREFERENCE SYSTEM for COREF-ERENCE RESOLUTION . the ENTITY-CENTRIC COREFERENCE SYSTEM is based on a LOSS FUNCTION and a LOSS FUNCTION . the POLICY is modeled by a LOSS FUNCTION and a LOSS FUNCTION . the POLICY is modeled by a LOSS FUNCTION . the POLICY are estimated by the MENTION PAIR MODELS . the proposed ENTITY-CENTRIC COREFERENCE SYSTEM is applied to the CONLL SHARED TASK DATASET . the experimental results show the effectiveness of the proposed ENTITY-CENTRIC COREFERENCE SYSTEM in terms of COREF-ERENCE RESOLUTION .\n",
            "\n",
            "139 1000\n",
            "this paper describes an experiment comparing the effect of two different approaches to INFORMATION PRESENTATION on ITEM RECALL . the results show that using DISCOURSE CUES facilitates <unk> the presented information . \n",
            "this paper presents a new method for INFORMATION PRESENTATION based on INFORMATION PRESENTATION . the proposed method is based on the use of a INFORMATION PRESENTATION . the proposed method is based on the use of INFORMATION PRESENTATION .\n",
            "\n",
            "140 1000\n",
            "a HOMOGRAPHY MATRIX is used in COMPUTER VISION FIELD to solve the CORRESPONDENCE PROBLEM between a pair of STEREO IMAGES . RANSAC ALGORITHM is often used to calculate the HOMOGRAPHY MATRIX by randomly selecting a set of FEATURES iteratively . RANSAC ALGORITHM in this paper converts RANSAC ALGORITHM into TWO-LAYERS . the first layer is addressing SAMPLING PROBLEM which we can describe our knowledge about DEGENERATE FEATURES by mean of CONSTRAINT SATISFACTION PROBLEMS . by dividing the input IMAGE into a GRID and making FEATURE POINTS into discrete domains , we can model the IMAGE into the CONSTRAINT SATISFACTION PROBLEMS to efficiently filter out DEGENERATE FEATURES . by expressing the knowledge about degenerate feature samples using CONSTRAINT SATISFACTION PROBLEMS in the first layer , so that computer has knowledge about how to skip computing the HOMOGRAPHY MATRIX in the MODEL ESTIMATION STEP for the second layer . the experimental results show that the proposed RANSAC ALGORITHM can outperform the most of variants of RANSAC without sacrificing its EXECUTION TIME . \n",
            "this paper presents a RANSAC ALGORITHM for STEREO IMAGES , which is based on a HOMOGRAPHY MATRIX . the RANSAC ALGORITHM is based on a HOMOGRAPHY MATRIX and a HOMOGRAPHY MATRIX . the RANSAC ALGORITHM is based on a HOMOGRAPHY MATRIX , which is based on the HOMOGRAPHY MATRIX . the proposed RANSAC ALGORITHM is based on a HOMOGRAPHY MATRIX , which is based on the HOMOGRAPHY MATRIX . the proposed RANSAC ALGORITHM is based on a MODEL ESTIMATION STEP and a HOMOGRAPHY MATRIX . the proposed RANSAC ALGORITHM is evaluated on a COMPUTER VISION FIELD . the results show that the proposed RANSAC ALGORITHM outperforms the conventional RANSAC in terms of EXECUTION TIME and EXECUTION TIME .\n",
            "\n",
            "141 1000\n",
            "image sequence processing IMAGE SEQUENCE PROCESSING TECHNIQUES are used to study EXCHANGE , GROWTH , AND TRANSPORT PROCESSES and to tackle key questions in ENVIRONMENTAL PHYSICS and BIOLOGY . these applications require high ACCURACY for the ESTIMATION OF THE MOTION FIELD since the most interesting parameters of the DYNAMICAL PROCESSES studied are contained in FIRST-ORDER DERIVATIVES OF THE MOTION FIELD or in DYNAMICAL CHANGES OF THE MOVING OBJECTS . therefore the performance and OPTIMIZATION OF LOW-LEVEL MOTION ESTIMATORS is discussed . a TENSOR METHOD tuned with carefully optimized DERIVATIVE FILTERS yields reliable and dense displacement vector fields -lrb- <unk> -rrb- with an ACCURACY of up to a few <unk> PIXELS/FRAME for REAL-WORLD IMAGES . the ACCURACY of the TENSOR METHOD is verified with COMPUTER-GENERATED SEQUENCES and a CALIBRATED IMAGE SEQUENCE . with the improvements in ACCURACY the MOTION ESTIMATION is now rather limited by imperfections in the CCD SENSORS , especially the SPATIAL NONUNI-FORMITY in the RESPONSIVITY . with a simple TWO-POINT CALIBRATION , these effects can efficiently be suppressed . the application of the IMAGE SEQUENCE PROCESSING TECHNIQUES to the ANALYSIS OF PLANT GROWTH , to OCEAN SURFACE MICROTURBULENCE IN IR IMAGE SEQUENCES , and to SEDIMENT TRANSPORT is demonstrated . \n",
            "this paper presents a novel TENSOR METHOD for MOTION ESTIMATION . the proposed TENSOR METHOD is based on a ANALYSIS OF PLANT GROWTH and a ANALYSIS OF PLANT GROWTH . the proposed TENSOR METHOD is based on a ANALYSIS OF PLANT GROWTH and a ANALYSIS OF PLANT GROWTH . the proposed TENSOR METHOD is based on the ANALYSIS OF PLANT GROWTH and the ANALYSIS OF PLANT GROWTH . the proposed TENSOR METHOD is evaluated on REAL-WORLD IMAGES and REAL-WORLD IMAGES . the ACCURACY of the proposed TENSOR METHOD is compared with the conventional TENSOR METHOD and the TENSOR METHOD .\n",
            "\n",
            "142 1000\n",
            "| MULTIPLE SOURCE SIGNALS impinging on an ANTENNA ARRAY can be separated by TIME-FREQUENCY SYNTHESIS TECHNIQUES . averaging of the TIME-FREQUENCY DISTRIBUTIONS of the data across the array permits the SPATIAL SIGNATURES OF SOURCES to play a fundamental role in improving the synthesis performance . array a VERAGING introduces a WEIGHING FUNCTION in the TIME-FREQUENCY DOMAIN that decreases the NOISE LEVELS , reduces the interactions of the source signals , and mitigates the CROSSTERMS . this is achieved independent of the temporal characteristics of the source signals and without causing any SMEARING OF THE SIGNAL TERMS . the WEIGHING FUNCTION may take NON-INTEGER VALUES , which are determined by the COMMUNICATION CHANNEL , the SOURCE POSITIONS and their ANGULAR SEPARATIONS . unlike the recently devised BLIND SOURCE SEPARATION METHODS using SPATIAL TIME-FREQUENCY DISTRIBUTIONS , the proposed method does not require WHITENING or retrieval of the source directional matrix . the paper evaluates the proposed method in terms of performance and computations relative t o the existing SOURCE SEPARATION TECHNIQUES based on QUADRATIC T-F DISTRIBUTIONS . \n",
            "this paper addresses the problem of MULTIPLE SOURCE SIGNALS in TIME-FREQUENCY DOMAIN . we propose a method for MULTIPLE SOURCE SIGNALS based on QUADRATIC T-F DISTRIBUTIONS . the proposed method is based on a ANTENNA ARRAY , which is based on the WEIGHING FUNCTION of the TIME-FREQUENCY DOMAIN . the proposed method is based on the use of QUADRATIC T-F DISTRIBUTIONS and SMEARING OF THE SIGNAL TERMS . the proposed method is based on the use of QUADRATIC T-F DISTRIBUTIONS and QUADRATIC T-F DISTRIBUTIONS . the proposed method is based on the use of QUADRATIC T-F DISTRIBUTIONS and QUADRATIC T-F DISTRIBUTIONS . the proposed method is based on the use of QUADRATIC T-F DISTRIBUTIONS and QUADRATIC T-F DISTRIBUTIONS . the proposed method is evaluated on the TIME-FREQUENCY DOMAIN and the results show that the proposed method is robust to NOISE LEVELS and is robust to NOISE LEVELS .\n",
            "\n",
            "143 1000\n",
            "we propose a novel method for inferring whether x causes y or vice versa from joint observations of x and y . the basic idea is to model the observed data using PROBABILISTIC LATENT VARIABLE MODELS , which incorporate the effects of UNOBSERVED NOISE . to this end , we consider the HYPOTHETICAL EFFECT VARIABLE to be a function of the HYPOTHETICAL CAUSE VARIABLE and an independent noise term -lrb- not necessarily additive -rrb- . an important novel aspect of our work is that we do not restrict the MODEL CLASS , but instead put general non-parametric priors on this function and on the distribution of the cause . the CAUSAL DIRECTION can then be inferred by using standard BAYESIAN MODEL SELECTION . we evaluate our approach on SYNTHETIC DATA and REAL-WORLD DATA and report encouraging results . \n",
            "this paper presents a new method for BAYESIAN MODEL SELECTION based on PROBABILISTIC LATENT VARIABLE MODELS . the proposed method is based on the use of PROBABILISTIC LATENT VARIABLE MODELS to estimate the MODEL CLASS . the proposed method is based on the BAYESIAN MODEL SELECTION . the proposed method is based on the BAYESIAN MODEL SELECTION . the proposed method is based on the BAYESIAN MODEL SELECTION . the proposed method is based on the BAYESIAN MODEL SELECTION .\n",
            "\n",
            "144 1000\n",
            "this article introduces AUTOMATIC RECOGNITION OF SPEECH without any AUDIO INFORMATION . movements of the tongue , lips , and jaw are tracked by an ELECTROMAGNETIC ARTICULOGRA-PHY DEVICE and are used as FEATURES to create HIDDEN MARKOV MODELS and conduct AUTOMATIC SPEECH RECOGNITION in a conventional way . the results obtained are promising , which confirm that PHONETIC FEATURES characterizing ARTICULATION are as discriminating as those characterizing ACOUSTICS -lrb- except for voicing -rrb- . the results also show that using TONGUE PARAMETERS result in a higher ACCURACY compared with the LIP PARAMETERS . \n",
            "this paper presents a new method for AUTOMATIC RECOGNITION OF SPEECH based on HIDDEN MARKOV MODELS . the proposed method is based on the use of PHONETIC FEATURES and the TONGUE PARAMETERS . the proposed method is based on the use of PHONETIC FEATURES in the ELECTROMAGNETIC ARTICULOGRA-PHY DEVICE . the ACCURACY of the proposed method is compared with the conventional FEATURES in terms of ACCURACY and ACCURACY .\n",
            "\n",
            "145 1000\n",
            "in this paper , we introduce a new IMAGE DESCRIPTOR for BROAD IMAGE CATEGORIZATION , the PROGRESSIVE RANDOMIZA-TION , that uses PERTURBATIONS on the values of the least significant bits -lrb- <unk> -rrb- of IMAGES . we show that different classes of IMAGES have a distinct behavior under our IMAGE DESCRIPTOR , and that using STATISTICAL DESCRIPTORS OF LSB OCCURRENCES and enough TRAINING EXAMPLES , the IMAGE DESCRIPTOR already performs as well or better than comparable existing techniques in the literature . with few TRAINING EXAMPLES , pr still has good separability , and its ACCURACY increases with the size of the training set . we validate our IMAGE DESCRIPTOR using four IMAGE DATABASES with different categories . \n",
            "this paper presents a novel IMAGE DESCRIPTOR for BROAD IMAGE CATEGORIZATION . the IMAGE DESCRIPTOR is based on the STATISTICAL DESCRIPTORS OF LSB OCCURRENCES . the IMAGE DESCRIPTOR is based on the STATISTICAL DESCRIPTORS OF LSB OCCURRENCES . the proposed IMAGE DESCRIPTOR is based on the STATISTICAL DESCRIPTORS OF LSB OCCURRENCES . the ACCURACY of the proposed IMAGE DESCRIPTOR is demonstrated by BROAD IMAGE CATEGORIZATION .\n",
            "\n",
            "146 1000\n",
            "in this paper , we introduce a TUNED EIGENSPACE TECHNIQUE so as to classify HUMAN MOTION . the TUNED EIGENSPACE TECHNIQUE presented here overcomes those problems related to ARTICULATED MOTION and <unk> texture effects by learning various HUMAN MOTIONS in terms of their SEQUENTIAL POSTURES in an eigenspace . in order to cope with the variability inherent to ARTICULATED MOTION , we propose a TUNED EIGENSPACE TECHNIQUE to tune the set of SEQUENTIAL EIGENSPACES . once the learnt TUNED EIGENSPACES are at hand , the RECOGNITION TASK then becomes a nearest-neighbor search over the <unk> . we show how our TUNED EIGENSPACE TECHNIQUE can be used for purposes of REAL-WORLD AND SYNTHETIC POSE RECOGNITION . we also discuss and overcome the problem related to CLOTHING TEXTURE that occurs in REAL-WORLD DATA , and propose a BACKGROUND SUBTRACTION METHOD to employ the TUNED EIGENSPACE TECHNIQUE in OUTDOOR ENVIRONMENT . we provide results on SYNTHETIC IMAGERY for a number of HUMAN POSES and illustrate the utility of the TUNED EIGENSPACE TECHNIQUE for the purposes of HUMAN MOTION RECOGNITION . \n",
            "this paper presents a BACKGROUND SUBTRACTION METHOD for HUMAN MOTION RECOGNITION . the BACKGROUND SUBTRACTION METHOD is based on a TUNED EIGENSPACE TECHNIQUE for HUMAN MOTION RECOGNITION . the BACKGROUND SUBTRACTION METHOD is based on the TUNED EIGENSPACE TECHNIQUE . the BACKGROUND SUBTRACTION METHOD is based on the TUNED EIGENSPACE TECHNIQUE . the proposed BACKGROUND SUBTRACTION METHOD is based on the TUNED EIGENSPACE TECHNIQUE . the proposed BACKGROUND SUBTRACTION METHOD is based on the TUNED EIGENSPACE TECHNIQUE . the proposed BACKGROUND SUBTRACTION METHOD is based on the TUNED EIGENSPACE TECHNIQUE . the proposed BACKGROUND SUBTRACTION METHOD is based on the TUNED EIGENSPACE TECHNIQUE . the proposed BACKGROUND SUBTRACTION METHOD is based on the TUNED EIGENSPACE TECHNIQUE . the proposed BACKGROUND SUBTRACTION METHOD is based on the TUNED EIGENSPACE TECHNIQUE . the proposed BACKGROUND SUBTRACTION METHOD is applied to the RECOGNITION TASK .\n",
            "\n",
            "147 1000\n",
            "we present a CONTINUOUS OPTIMIZATION FRAMEWORK for INTERACTIVE TRACKING OF 2D GENERIC OBJECTS in a single video stream . the user begins with specifying the locations of a target object in a small set of keyframes ; the CONTINUOUS OPTIMIZATION FRAMEWORK then automatically tracks locations of the objects by combining USER CONSTRAINTS with VISUAL MEASUREMENTS across the entire sequence . we formulate the problem in a SPACETIME OPTIMIZATION FRAMEWORK that optimizes over the whole sequence simultaneously . the resulting CONTINUOUS OPTIMIZATION FRAMEWORK is consistent with VISUAL MEASUREMENTS across the entire sequence while satisfying USER CONSTRAINTS . we also introduce prior terms to reduce TRACKING AMBIGUITY . we demonstrate the power of our CONTINUOUS OPTIMIZATION FRAMEWORK on TRACKING AMBIGUITY with significant occlusions , SCALE and orientation changes , ILLUMINATION CHANGES , SUDDEN MOVEMENT OF OBJECTS , and also SIMULTANEOUS TRACKING OF MULTIPLE OBJECTS . we compare the performance of our CONTINUOUS OPTIMIZATION FRAMEWORK with alternative methods . \n",
            "this paper presents a CONTINUOUS OPTIMIZATION FRAMEWORK for SIMULTANEOUS TRACKING OF MULTIPLE OBJECTS . the SPACETIME OPTIMIZATION FRAMEWORK is based on the SPACETIME OPTIMIZATION FRAMEWORK and the SPACETIME OPTIMIZATION FRAMEWORK . the SPACETIME OPTIMIZATION FRAMEWORK is based on the SPACETIME OPTIMIZATION FRAMEWORK and the SPACETIME OPTIMIZATION FRAMEWORK . the proposed SPACETIME OPTIMIZATION FRAMEWORK is based on the SPACETIME OPTIMIZATION FRAMEWORK and the SPACETIME OPTIMIZATION FRAMEWORK . the proposed SPACETIME OPTIMIZATION FRAMEWORK is based on the SPACETIME OPTIMIZATION FRAMEWORK and the SPACETIME OPTIMIZATION FRAMEWORK .\n",
            "\n",
            "148 1000\n",
            "we consider the problem of PARAMETER ESTIMATION for signals characterized by sums of PARAMETERIZED FUNCTIONS . we present a DYNAMIC DICTIONARY SUBSET SELECTION APPROACH to PARAMETER ESTIMATION where we iteratively select a small number of DICTIONARY ELEMENTS and then alter the parameters of these DICTIONARY ELEMENTS to achieve better signal model fit . the proposed DYNAMIC DICTIONARY SUBSET SELECTION APPROACH avoids the use of highly oversampled -lrb- and highly correlated -rrb- DICTIONARY ELEMENTS , which are needed in FIXED DICTIONARY APPROACHES to reduce PARAMETER BIAS associated with DICTIONARY QUANTIZATION . we demonstrate estimation performance on a sinusoidal signal estimation example . \n",
            "this paper proposes a new DYNAMIC DICTIONARY SUBSET SELECTION APPROACH for PARAMETER ESTIMATION . the DYNAMIC DICTIONARY SUBSET SELECTION APPROACH is based on the DYNAMIC DICTIONARY SUBSET SELECTION APPROACH . the proposed DYNAMIC DICTIONARY SUBSET SELECTION APPROACH is based on the DYNAMIC DICTIONARY SUBSET SELECTION APPROACH . the proposed DYNAMIC DICTIONARY SUBSET SELECTION APPROACH is based on the DYNAMIC DICTIONARY SUBSET SELECTION APPROACH . the proposed DYNAMIC DICTIONARY SUBSET SELECTION APPROACH is based on the DYNAMIC DICTIONARY SUBSET SELECTION APPROACH .\n",
            "\n",
            "149 1000\n",
            "cluster identification is introduced as the process of jointly evaluating clustering and labelling schemes for CLUSTER-LABELLING SCHEME SELECTION . normalized <unk> and BBN METRICS for comparing clustering performances across varied clustering and labelling schemes are presented . the merits of the BBN METRICS are evaluated and applied for SPEAKER-ENVIRONMENT TRACKING in BROADCAST NEWS . \n",
            "this paper addresses the problem of SPEAKER-ENVIRONMENT TRACKING in BROADCAST NEWS . we propose a method for SPEAKER-ENVIRONMENT TRACKING based on CLUSTER-LABELLING SCHEME SELECTION . the proposed method is based on the use of BBN METRICS as well as a CLUSTER-LABELLING SCHEME SELECTION . experimental results show that the proposed method can improve the performance of SPEAKER-ENVIRONMENT TRACKING .\n",
            "\n",
            "150 1000\n",
            "we recently proposed a family of ROBUST LINEAR AND NONLIN-EAR ESTIMATION TECHNIQUES for recognizing the three EMOTION PRIMITIVES -- VALENCE , ACTIVATION , and DOMINANCE -- from SPEECH . these were based on both LOCAL AND GLOBAL SPEECH DURATION , ENERGY , MFCC and pitch FEATURES . this paper aims to study the RELATIVE IMPORTANCE of these four categories of ACOUSTIC FEATURES in this EMOTION ESTIMATION CONTEXT . three measures are considered : the number of FEATURES from each category when all FEATURES are used in selection , the MEAN ABSOLUTE ERROR when each category is used separately , and the MEAN ABSOLUTE ERROR when a category is excluded from FEATURE SELECTION . we find that the RELATIVE IMPORTANCE is in the order of MFCC > energy ≈ pitch > duration . additionally , ESTIMATOR FUSION almost always improves performance , and locally weighted fusion always outperforms AVERAGE FUSION regardless of the number of FEATURES used . \n",
            "this paper addresses the problem of FEATURE SELECTION in SPEECH such as SPEECH , SPEECH , and SPEECH . we propose a method for FEATURE SELECTION , which is based on the MEAN ABSOLUTE ERROR of the ACOUSTIC FEATURES . the proposed method is based on the use of ACOUSTIC FEATURES and RELATIVE IMPORTANCE . the proposed method is based on the use of ACOUSTIC FEATURES and FEATURE SELECTION . the proposed method is compared with other ROBUST LINEAR AND NONLIN-EAR ESTIMATION TECHNIQUES such as MFCC and MFCC . the experimental results show that the proposed method outperforms the conventional ESTIMATOR FUSION and the ESTIMATOR FUSION .\n",
            "\n",
            "151 1000\n",
            "this paper extends the EXPECTATION-MAXIMIZATION ALGORITHM to estimate not only OPTIMAL ACOUSTIC MODEL PARAMETERS , but also optimal center frequencies and bandwidths of the FILTER BANK used in CEPSTRAL FEATURE EXTRACTION for BIRD CALL CLASSIFICATION . the search is done using the GRADIENT ASCENT METHOD . filter bank and model parameters are optimized iteratively . experiments are conducted on a large noisy corpus containing ANTBIRD CALLS from 5 species . it is shown that FEATURES extracted using the optimized FILTER BANK result in a lower CLASSIFICATION ERROR RATE than those extracted using a MEL-SCALED FILTER BANK . \n",
            "this paper proposes a new method for BIRD CALL CLASSIFICATION based on a FILTER BANK . the proposed method is based on a GRADIENT ASCENT METHOD , which is based on the EXPECTATION-MAXIMIZATION ALGORITHM . the proposed EXPECTATION-MAXIMIZATION ALGORITHM is based on the GRADIENT ASCENT METHOD . the proposed EXPECTATION-MAXIMIZATION ALGORITHM is based on the EXPECTATION-MAXIMIZATION ALGORITHM . the proposed EXPECTATION-MAXIMIZATION ALGORITHM is evaluated on the MEL-SCALED FILTER BANK . the experimental results show that the proposed method achieves better performance than the conventional EXPECTATION-MAXIMIZATION ALGORITHM .\n",
            "\n",
            "152 1000\n",
            "we present UTILE SUFFIX MEMORY , a REINFORCEMENT LEARNING ALGORITHM that uses SHORT-TERM MEMORY to overcome the STATE ALIASING that results from HIDDEN STATE . by combining the advantages of previous work in <unk> -lrb- or '' <unk> '' -rrb- learning and previous work with STATISTICAL TESTS for separating noise from task structure , the REINFORCEMENT LEARNING ALGORITHM learns quickly , creates only as much memory as needed for the task at hand , and handles noise well . UTILE SUFFIX MEMORY uses a TREE-STRUCTURED REPRESENTATION , and is related to work on <unk> \n",
            "this paper presents a new REINFORCEMENT LEARNING ALGORITHM called UTILE SUFFIX MEMORY . the REINFORCEMENT LEARNING ALGORITHM is based on a TREE-STRUCTURED REPRESENTATION , called UTILE SUFFIX MEMORY , which is based on the UTILE SUFFIX MEMORY . the REINFORCEMENT LEARNING ALGORITHM is based on a TREE-STRUCTURED REPRESENTATION , which is based on the UTILE SUFFIX MEMORY . the proposed REINFORCEMENT LEARNING ALGORITHM is based on a TREE-STRUCTURED REPRESENTATION and is shown to be more robust to STATE ALIASING .\n",
            "\n",
            "153 1000\n",
            "the DOMINANT ACOUSTIC MODELING METHODOLOGY based on HIDDEN MARKOV MODELS is known to have certain weaknesses . partial solutions to these flaws have been presented , but the fundamental problem remains : compression of the data to a compact HMM discards useful information such as TIME DEPENDENCIES and SPEAKER INFORMATION . in this paper , we look at PURE EXAMPLE BASED RECOGNITION as a solution to this problem . by replacing the HMM with the underlying examples , all information in the training data is retained . we show how information about SPEAKER AND ENVIRONMENT can be used , introducing a new interpretation of ADAPTATION . the basis for the PURE EXAMPLE BASED RECOGNITION is the well-known DTW ALGORITHM , which has often been used for small tasks . however , LARGE VOCABULARY SPEECH RECOGNITION introduces new demands , resulting in an explosion of the SEARCH SPACE . we show how this problem can be tackled using a DATA DRIVEN APPROACH which selects appropriate SPEECH EXAMPLES as candidates for DTW-ALIGNMENT . \n",
            "this paper addresses the problem of LARGE VOCABULARY SPEECH RECOGNITION in LARGE VOCABULARY SPEECH RECOGNITION . in this paper , we propose a DATA DRIVEN APPROACH for LARGE VOCABULARY SPEECH RECOGNITION , which is based on the DATA DRIVEN APPROACH . the proposed DATA DRIVEN APPROACH is based on the DATA DRIVEN APPROACH and the DATA DRIVEN APPROACH . the proposed method is based on the DATA DRIVEN APPROACH and the DTW ALGORITHM . the proposed method is based on the DATA DRIVEN APPROACH and the DATA DRIVEN APPROACH . the proposed method is evaluated on the SPEAKER AND ENVIRONMENT and the SPEAKER AND ENVIRONMENT .\n",
            "\n",
            "154 1000\n",
            "we consider a MULTIPLE-INPUT MULTIPLE-OUTPUT WIRELESS COMMUNICATION SCENARIO in which the channel follows a general SPATIALLY-CORRELATED COMPLEX GAUSSIAN DISTRIBUTION with NON-ZERO MEAN . we derive an explicit characterization of the optimal input covariance from an ERGODIC RATE PERSPECTIVE for systems that operate at LOW SNRS . this characterization is in terms of the <unk> decomposition of a matrix that depends on the mean and the covariance of the channel , and typically results in a BEAMFORMING STRATEGY along the principal eigenvector of that matrix . simulation results show the potential impact of -lrb- jointly -rrb- exploiting the mean and the covariance of the channel on the ERGODIC ACHIEVABLE RATE at both low and <unk> snrs . \n",
            "this paper presents a new method for the MULTIPLE-INPUT MULTIPLE-OUTPUT WIRELESS COMMUNICATION SCENARIO of the MULTIPLE-INPUT MULTIPLE-OUTPUT WIRELESS COMMUNICATION SCENARIO . the proposed method is based on the BEAMFORMING STRATEGY and the BEAMFORMING STRATEGY . the proposed method is based on the BEAMFORMING STRATEGY . the proposed method is compared with the conventional BEAMFORMING STRATEGY .\n",
            "\n",
            "155 1000\n",
            "we discuss a few new MOTION DEBLURRING PROBLEMS that are significant to KERNEL ESTIMATION and NON-BLIND DECONVOLUTION . we found that strong EDGES do not always PROFIT KERNEL ESTIMATION , but instead under certain circumstance degrade it . this finding leads to a new metric to measure the usefulness of IMAGE EDGES in MOTION DEBLURRING and a GRADIENT SELECTION PROCESS to mitigate their possible adverse effect . we also propose an efficient and high-quality KERNEL ESTIMATION method based on using the SPATIAL PRIOR and the ITERATIVE SUPPORT DETECTION KERNEL REFINEMENT , which avoids HARD THRESHOLD of the kernel elements to enforce SPARSITY . we employ the TV-1 DECONVOLUTION MODEL , solved with a new VARIABLE SUBSTITUTION SCHEME to robustly suppress NOISE . \n",
            "this paper presents a new TV-1 DECONVOLUTION MODEL for MOTION DEBLURRING . the TV-1 DECONVOLUTION MODEL is based on a VARIABLE SUBSTITUTION SCHEME and a VARIABLE SUBSTITUTION SCHEME . the proposed TV-1 DECONVOLUTION MODEL is based on a VARIABLE SUBSTITUTION SCHEME and a VARIABLE SUBSTITUTION SCHEME . the proposed TV-1 DECONVOLUTION MODEL is based on a VARIABLE SUBSTITUTION SCHEME and a VARIABLE SUBSTITUTION SCHEME . the proposed TV-1 DECONVOLUTION MODEL is based on a VARIABLE SUBSTITUTION SCHEME and a VARIABLE SUBSTITUTION SCHEME . the proposed TV-1 DECONVOLUTION MODEL is based on the GRADIENT SELECTION PROCESS and the TV-1 DECONVOLUTION MODEL . the proposed VARIABLE SUBSTITUTION SCHEME is based on a VARIABLE SUBSTITUTION SCHEME and a VARIABLE SUBSTITUTION SCHEME .\n",
            "\n",
            "156 1000\n",
            "in an extended IMAGE SEQUENCE of an outdoor scene , one observes changes in color induced by variations in the SPECTRAL COMPOSITION OF DAYLIGHT . this paper proposes a model for these TEMPORAL COLOR CHANGES and explores its use for the ANALYSIS OF OUTDOOR SCENES from TIME-LAPSE VIDEO DATA . we show that the time-varying changes in DIRECT SUNLIGHT and AMBIENT SKYLIGHT can be recovered with this model , and that an IMAGE SEQUENCE can be decomposed into two corresponding components . the decomposition provides access to both RADIOMETRIC AND GEOMETRIC INFORMATION about a scene , and we demonstrate how this can be exploited for a variety of VISUAL TASKS , including <unk> , BACKGROUND SUBTRACTION , SHADOW DETECTION , SCENE RECONSTRUCTION , and CAMERA GEO-LOCATION . \n",
            "this paper addresses the problem of SHADOW DETECTION in VISUAL TASKS , such as SHADOW DETECTION , SHADOW DETECTION , and SHADOW DETECTION . we propose a SPECTRAL COMPOSITION OF DAYLIGHT , which is based on the SPECTRAL COMPOSITION OF DAYLIGHT and the SPECTRAL COMPOSITION OF DAYLIGHT . the proposed method is based on the SPECTRAL COMPOSITION OF DAYLIGHT and the SPECTRAL COMPOSITION OF DAYLIGHT . the proposed method is evaluated on the ANALYSIS OF OUTDOOR SCENES , including SHADOW DETECTION , SHADOW DETECTION , and SHADOW DETECTION .\n",
            "\n",
            "157 1000\n",
            "this paper describes a method for the AUTOMATIC ALIGNMENT OF PARALLEL TEXTS at CLAUSE LEVEL . the method features STATISTICAL TECHNIQUES coupled with SHALLOW LINGUISTIC PROCESSING . it <unk> a PARALLEL BILINGUAL CORPUS and identifies alignments between the clauses of the source and target language sides of the corpus . PARALLEL TEXTS are first statistically aligned at sentence level and then tagged with their PART-OF-SPEECH CATEGORIES . REGULAR GRAMMARS functioning on tags , recognize clauses on both sides of the PARALLEL TEXT . a PROBABILISTIC MODEL is applied next , operating on the basis of WORD OCCURRENCE and CO-OCCURRENCE PROBABILITIES and CHARACTER LENGTHS . depending on SENTENCE SIZE , possible alignments arc fed into a DYNAMIC PROGRANUNING FRAMEWORK or a SIMULATED ANNEALING SYSTEM in order to find or <unk> ~ te the best alignment . 1HE METHOD has been tested on a SMALL ENG ~ LISH-GREEK CORPUS consisting of texts relevant to SOFTWARE SYSTEMS and has produced promising results in terms of correctly identified CLAUSE ALIGNMENTS . \n",
            "this paper addresses the problem of AUTOMATIC ALIGNMENT OF PARALLEL TEXTS in SOFTWARE SYSTEMS . we propose a new 1HE METHOD based on the DYNAMIC PROGRANUNING FRAMEWORK and the 1HE METHOD . the proposed 1HE METHOD is based on the DYNAMIC PROGRANUNING FRAMEWORK and the 1HE METHOD . the proposed 1HE METHOD is based on the 1HE METHOD and the 1HE METHOD . the proposed 1HE METHOD is evaluated on the SMALL ENG ~ LISH-GREEK CORPUS and on the PARALLEL BILINGUAL CORPUS . the proposed 1HE METHOD is compared with other STATISTICAL TECHNIQUES and STATISTICAL TECHNIQUES .\n",
            "\n",
            "158 1000\n",
            "we investigate a general SEMI-MARKOV DECISION PROCESS FRAMEWORK for modeling CONCURRENT DECISION MAKING , where agents learn optimal plans over CONCURRENT TEMPORALLY EXTENDED ACTIONS . we introduce three types of PARALLEL TERMINATION SCHEMES -- all , any and continue -- and theoretically and experimentally compare them . \n",
            "this paper presents a novel SEMI-MARKOV DECISION PROCESS FRAMEWORK for CONCURRENT DECISION MAKING . the proposed SEMI-MARKOV DECISION PROCESS FRAMEWORK is based on the SEMI-MARKOV DECISION PROCESS FRAMEWORK . the proposed SEMI-MARKOV DECISION PROCESS FRAMEWORK is based on the SEMI-MARKOV DECISION PROCESS FRAMEWORK . the proposed SEMI-MARKOV DECISION PROCESS FRAMEWORK is based on the SEMI-MARKOV DECISION PROCESS FRAMEWORK .\n",
            "\n",
            "159 1000\n",
            "this paper investigates the problem of how to partition UNKNOWN SPEECH UTTERANCES into CLUSTERS , such that the overall WITHIN-CLUSTER HOMOGENEITY of speakers ' voice characteristics can be maximized . the WITHIN-CLUSTER HOMOGENEITY is characterized by the LIKELIHOOD PROBABILITY that a CLUSTER MODEL , trained using all the utterances within a CLUSTER , matches each of the WITHIN-CLUSTER UTTERANCES . such probability is then maximized by using a GENETIC ALGORITHM , which determines the best CLUSTER where each utterance should be located . for greater COMPUTATIONAL EFFICIENCY , also proposed is an alternative solution that approximates the LIKELIHOOD PROBABILITY with a DIVERGENCE-BASED MODEL SIMILARITY . the method is further designed to estimate the optimal number of CLUSTERS automatically . \n",
            "this paper presents a new method for DIVERGENCE-BASED MODEL SIMILARITY in UNKNOWN SPEECH UTTERANCES . the proposed method is based on the CLUSTER MODEL of the CLUSTER MODEL . the proposed method is based on the GENETIC ALGORITHM and the GENETIC ALGORITHM . the proposed method is based on the GENETIC ALGORITHM . the proposed method is based on the GENETIC ALGORITHM . the proposed method is based on the GENETIC ALGORITHM and the GENETIC ALGORITHM .\n",
            "\n",
            "160 1000\n",
            "we describe our implementation of a PARALLEL DEPTH RECOVERY SCHEME for a FOUR-CAMERA MULTIBASELINE STEREO in a CONVER-GENT CONFIGURATION . our PARALLEL DEPTH RECOVERY SCHEME is capable of IMAGE CAPTURE at VIDEO RATE . this is critical in applications that require THREE-DIMENSIONAL TRACKING . we obtain DENSE STEREO DEPTH DATA by projecting a light pattern of frequency modulated <unk> varying intensity onto the scene , thus increasing the LOCAL DISCRIMINABILITY at each pixel and facilitating matches . in addition , we make most of the CAMERA VIEW AREAS by converging them at a volume of interest . results show that we are able to extract STEREO DEPTH DATA that are , on the average , less than 1 mm in error at distances between 1.5 to 3.5 m away from the cameras . \n",
            "this paper presents a novel PARALLEL DEPTH RECOVERY SCHEME for DENSE STEREO DEPTH DATA . the PARALLEL DEPTH RECOVERY SCHEME is based on a PARALLEL DEPTH RECOVERY SCHEME . the proposed PARALLEL DEPTH RECOVERY SCHEME is based on a PARALLEL DEPTH RECOVERY SCHEME . the proposed PARALLEL DEPTH RECOVERY SCHEME is based on a PARALLEL DEPTH RECOVERY SCHEME . the proposed PARALLEL DEPTH RECOVERY SCHEME is evaluated on the DENSE STEREO DEPTH DATA . the proposed PARALLEL DEPTH RECOVERY SCHEME is compared with the conventional PARALLEL DEPTH RECOVERY SCHEME in terms of VIDEO RATE . the proposed PARALLEL DEPTH RECOVERY SCHEME is shown to outperform the conventional PARALLEL DEPTH RECOVERY SCHEME in terms of VIDEO RATE .\n",
            "\n",
            "161 1000\n",
            "practical schemes for DISTRIBUTED VIDEO CODING with SIDE INFORMATION at the DECODER need to consider NON-STANDARD CORRELATION MODELS in order to take NON-STATIONARITIES into account . in this paper we introduce two CORRELATION MODELS for GAUSSIAN SOURCES , the <unk> -lrb- <unk> -rrb- and the GAUSSIAN-ERASURE MODELS , and evaluate LOWER AND UPPER BOUNDS on their RATE-DISTORTION FUNCTIONS . provided that the probability of IMPULSE NOISE or of ERASURES remains small , these bounds remain close to the RATE-DISTORTION FUNCTION for GAUSSIAN CORRELATION . two practical schemes for the GE CORRELATION MODEL are also presented , with performance about 1.5 db away from the upper bound . \n",
            "this paper addresses the problem of DISTRIBUTED VIDEO CODING in DISTRIBUTED VIDEO CODING . in this paper , we propose a method for DISTRIBUTED VIDEO CODING based on the GE CORRELATION MODEL . the proposed method is based on the use of NON-STANDARD CORRELATION MODELS for DISTRIBUTED VIDEO CODING . the proposed method is based on the use of NON-STANDARD CORRELATION MODELS , which is based on the GE CORRELATION MODEL . the proposed method is based on the use of NON-STANDARD CORRELATION MODELS for DISTRIBUTED VIDEO CODING . the proposed method is based on the use of NON-STANDARD CORRELATION MODELS for DISTRIBUTED VIDEO CODING . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "162 1000\n",
            "our work deals with the important problem of globally characterizing truthful mechanisms where players have <unk> , ADDITIVE VALUATIONS , like scheduling unrelated machines or ADDITIVE COMBINATORIAL AUCTIONS . very few mechanisms are known for these settings and the question is : can we prove that no other truthful mechanisms exist ? we characterize truthful mechanisms for n players and 2 tasks or items , as either TASK-INDEPENDENT , or a PLAYER-GROUPING MINIMIZER , a new class of mechanisms we discover , which generalizes AFFINE MIN-IMIZERS . we assume <unk> , strong <unk> and that the truthful payments 1 are continuous functions of players ' bids . \n",
            "this paper presents a new method for ADDITIVE COMBINATORIAL AUCTIONS , which is based on a PLAYER-GROUPING MINIMIZER and a PLAYER-GROUPING MINIMIZER . the proposed method is based on the PLAYER-GROUPING MINIMIZER and the PLAYER-GROUPING MINIMIZER .\n",
            "\n",
            "163 1000\n",
            "sparse signal approximation can be used to design efficient LOW BIT-RATE CODING SCHEMES . SPARSE SIGNAL APPROXIMATION heavily relies on the ability to design appropriate dictionaries and corresponding DECOMPOSITION ALGORITHMS . the size of the dictionary , and therefore its resolution , is a key parameter that handles the tradeoff between SPARSITY and tractability . this work proposes the use of a non adaptive random sequence of <unk> in a GREEDY DECOMPOSITION PROCESS , thus browsing a larger DICTIONARY SPACE in a PROBABILISTIC FASHION with no additional projection cost nor PARAMETER ESTIMATION . this SPARSE SIGNAL APPROXIMATION leads to very SPARSE DECOMPOSITIONS , at a CONTROLLED COMPUTATIONAL COMPLEXITY . experimental evaluation is provided as proof of concept for LOW BIT RATE COMPRESSION OF AUDIO SIGNALS . \n",
            "this paper addresses the problem of LOW BIT RATE COMPRESSION OF AUDIO SIGNALS in DICTIONARY SPACE . we propose a GREEDY DECOMPOSITION PROCESS for LOW BIT RATE COMPRESSION OF AUDIO SIGNALS , which is based on the GREEDY DECOMPOSITION PROCESS . the proposed DECOMPOSITION ALGORITHMS is based on the GREEDY DECOMPOSITION PROCESS . the proposed DECOMPOSITION ALGORITHMS is based on the GREEDY DECOMPOSITION PROCESS . the proposed DECOMPOSITION ALGORITHMS is based on the GREEDY DECOMPOSITION PROCESS and is shown to outperform the conventional DECOMPOSITION ALGORITHMS .\n",
            "\n",
            "164 1000\n",
            "the goal of SPEECH EMOTION RECOGNITION is to identify the emotional or physical state of a human being from his or her voice . one of the most important things in a SPEECH EMOTION RECOGNITION is to extract and select relevant SPEECH FEATURES with which most emotions could be recognized . in this paper , we present a SMOOTHED NONLINEAR ENERGY OPERATOR - based amplitude modulation cepstral coefficients -lrb- <unk> -rrb- feature for RECOGNIZING EMOTIONS from SPEECH SIGNALS . SMOOTHED NONLINEAR ENERGY OPERATOR estimates the energy required to produce the AM-FM SIGNAL , and then the estimated energy is separated into its AMPLITUDE AND FREQUENCY COMPONENTS using an ENERGY SEPARATION ALGORITHM . AMCC FEATURES are obtained by first decomposing a SPEECH SIGNAL using a C-CHANNEL GAMMATONE FILTERBANK , computing the AM POWER SPECTRUM , and taking a DISCRETE COSINE TRANSFORM of the ROOT COMPRESSED AM POWER SPECTRUM . conventional MFCC -lrb- mel-frequency cepstral coefficients -rrb- and <unk> dft -lrb- discrete fourier transform -rrb- spectrum based cepstral coefficients -lrb- <unk> -rrb- FEATURES are used for comparing the RECOGNITION performances of the proposed FEATURES . EMOTION RECOGNITION experiments are conducted on the FAU AIBO SPONTANEOUS EMOTION CORPUS . it is observed from the experimental results that the AMCC FEATURES provide a relative improvement of approximately 3.5 % over the baseline MFCC . \n",
            "this paper addresses the problem of SPEECH EMOTION RECOGNITION from SPEECH SIGNALS . we propose a C-CHANNEL GAMMATONE FILTERBANK based on the DISCRETE COSINE TRANSFORM . the ENERGY SEPARATION ALGORITHM is based on the DISCRETE COSINE TRANSFORM of the DISCRETE COSINE TRANSFORM . the proposed ENERGY SEPARATION ALGORITHM is based on the DISCRETE COSINE TRANSFORM . the proposed ENERGY SEPARATION ALGORITHM is based on the DISCRETE COSINE TRANSFORM . the proposed ENERGY SEPARATION ALGORITHM is based on the DISCRETE COSINE TRANSFORM . the proposed ENERGY SEPARATION ALGORITHM is based on the DISCRETE COSINE TRANSFORM . the proposed ENERGY SEPARATION ALGORITHM is based on a C-CHANNEL GAMMATONE FILTERBANK and is shown to be more robust to SPEECH EMOTION RECOGNITION than the conventional MFCC .\n",
            "\n",
            "165 1000\n",
            "this paper describes our recent work in developing an UNIFIED DUTCH AND GERMAN SPEECH RECOGNITION SYSTEM in the SPEECHDAT DOMAIN . the ACOUSTIC COMPONENT of the UNIFIED DUTCH AND GERMAN SPEECH RECOGNITION SYSTEM is accomplished through sharing COMMON PHONEMES without preserving any information about the languages . we propose a more robust MCE-BASED TRAINING ALGORITHM , where only the LANGUAGE DEPENDENT PHONEME MODELS are allowed to be adjusted , according to the type of training data . experimental results on DUTCH AND GERMAN SUBWORD RECOGNITION TASKS clearly show an OVERALL STRING ERROR RATE REDUCTION of about 7 % and 13 % obtained by the newly trained UNIFIED DUTCH AND GERMAN SPEECH RECOGNITION SYSTEM in comparison with the conventional MCE-TRAINED MULTILINGUAL SYSTEM . \n",
            "this paper presents a UNIFIED DUTCH AND GERMAN SPEECH RECOGNITION SYSTEM for DUTCH AND GERMAN SUBWORD RECOGNITION TASKS . the proposed MCE-TRAINED MULTILINGUAL SYSTEM is based on the MCE-BASED TRAINING ALGORITHM and the MCE-BASED TRAINING ALGORITHM . the MCE-BASED TRAINING ALGORITHM is evaluated on the DUTCH AND GERMAN SUBWORD RECOGNITION TASKS and the UNIFIED DUTCH AND GERMAN SPEECH RECOGNITION SYSTEM . the proposed UNIFIED DUTCH AND GERMAN SPEECH RECOGNITION SYSTEM is evaluated on the DUTCH AND GERMAN SUBWORD RECOGNITION TASKS . the proposed UNIFIED DUTCH AND GERMAN SPEECH RECOGNITION SYSTEM is evaluated on the DUTCH AND GERMAN SUBWORD RECOGNITION TASKS . the proposed UNIFIED DUTCH AND GERMAN SPEECH RECOGNITION SYSTEM is evaluated on the DUTCH AND GERMAN SUBWORD RECOGNITION TASKS . the proposed UNIFIED DUTCH AND GERMAN SPEECH RECOGNITION SYSTEM is evaluated on the DUTCH AND GERMAN SUBWORD RECOGNITION TASKS .\n",
            "\n",
            "166 1000\n",
            "spectral EMBEDDING based on the SINGULAR VALUE DECOMPOSITION is a widely used '' preprocessing '' step in many LEARNING TASKS , typically leading to DI-MENSIONALITY REDUCTION by projecting onto a number of DOMINANT SINGULAR VECTORS and <unk> the COORDINATE AXES -lrb- by a PREDEFINED FUNCTION of the singular value -rrb- . however , the number of such vectors required to capture PROBLEM STRUCTURE grows with PROBLEM SIZE , and even PARTIAL SVD COMPUTATION becomes a bottleneck . in this paper , we propose a LOW-COMPLEXITY COMPRESSIVE SPECTRAL EMBEDDING ALGORITHM , which employs RANDOM PROJECTIONS and FINITE ORDER POLYNOMIAL EXPANSIONS to compute approximations to SVD-BASED EMBEDDING . for an M × N MATRIX with T NON-ZEROS , its TIME COMPLEXITY is o -lrb- -lrb- t + m + n -rrb- log -lrb- m + n -rrb- -rrb- , and the EMBEDDING DIMENSION is o -lrb- log -lrb- m + n -rrb- -rrb- , both of which are independent of the number of SINGULAR VECTORS whose effect we wish to capture . to the best of our knowledge , this is the first work to circumvent this dependence on the number of SINGULAR VECTORS for GENERAL SVD-BASED EMBEDDINGS . the key to <unk> the SINGULAR VALUE DECOMPOSITION is the observation that , for DOWNSTREAM INFERENCE TASKS such as CLUSTERING and classification , we are only interested in using the resulting EMBEDDING to evaluate PAIRWISE SIMILARITY METRICS derived from the ℓ 2-norm , rather than capturing the effect of the underlying MATRIX on ARBITRARY VECTORS as a partial SINGULAR VALUE DECOMPOSITION tries to do . our numerical results on NETWORK DATASETS demonstrate the efficacy of the proposed LOW-COMPLEXITY COMPRESSIVE SPECTRAL EMBEDDING ALGORITHM , and motivate further exploration of its application to DOWNSTREAM INFERENCE TASKS . \n",
            "this paper presents a novel LOW-COMPLEXITY COMPRESSIVE SPECTRAL EMBEDDING ALGORITHM for DI-MENSIONALITY REDUCTION . the LOW-COMPLEXITY COMPRESSIVE SPECTRAL EMBEDDING ALGORITHM is based on the PROBLEM STRUCTURE and the PROBLEM STRUCTURE . the proposed LOW-COMPLEXITY COMPRESSIVE SPECTRAL EMBEDDING ALGORITHM is based on the SINGULAR VALUE DECOMPOSITION and the LOW-COMPLEXITY COMPRESSIVE SPECTRAL EMBEDDING ALGORITHM . the proposed LOW-COMPLEXITY COMPRESSIVE SPECTRAL EMBEDDING ALGORITHM is based on the SINGULAR VALUE DECOMPOSITION and the LOW-COMPLEXITY COMPRESSIVE SPECTRAL EMBEDDING ALGORITHM . the proposed LOW-COMPLEXITY COMPRESSIVE SPECTRAL EMBEDDING ALGORITHM is based on the LOW-COMPLEXITY COMPRESSIVE SPECTRAL EMBEDDING ALGORITHM and the LOW-COMPLEXITY COMPRESSIVE SPECTRAL EMBEDDING ALGORITHM . the proposed LOW-COMPLEXITY COMPRESSIVE SPECTRAL EMBEDDING ALGORITHM is based on a LOW-COMPLEXITY COMPRESSIVE SPECTRAL EMBEDDING ALGORITHM and is shown to be robust to TIME COMPLEXITY and TIME COMPLEXITY . the proposed LOW-COMPLEXITY COMPRESSIVE SPECTRAL EMBEDDING ALGORITHM is based on a LOW-COMPLEXITY COMPRESSIVE SPECTRAL EMBEDDING ALGORITHM and is shown to be robust to TIME COMPLEXITY and TIME COMPLEXITY .\n",
            "\n",
            "167 1000\n",
            "this paper describes a DATA-DRIVEN TECHNIQUE for optimizing the ACOUSTIC MODELS for SPEECH RECOGNITION SYSTEMS that target COMMERCIAL APPLICATIONS over <unk> . FRAME-AVERAGED FOREGROUND LOG-LIKELIHOODS -LRB- FOREGROUND SCORES -rrb- correlate to RECOGNITION ERRORS . these scores are used together with gender to optimize DATA WEIGHTING for the ACOUSTIC MODELS . this process is interpreted as increasing the PRIORS and associated parameters for POORLY MODELED DATA . the SCORE-BASED OPTIMIZATION leads to about 7 % fewer SEMANTIC ERRORS on a LIVE EVALUATION SET collected after the last data used to estimate the ACOUSTIC MODELS . \n",
            "this paper presents a novel DATA-DRIVEN TECHNIQUE for SPEECH RECOGNITION SYSTEMS for SPEECH RECOGNITION SYSTEMS . the proposed DATA-DRIVEN TECHNIQUE is based on the DATA-DRIVEN TECHNIQUE of the ACOUSTIC MODELS . the DATA-DRIVEN TECHNIQUE is based on the DATA-DRIVEN TECHNIQUE . the proposed DATA-DRIVEN TECHNIQUE is based on the use of POORLY MODELED DATA , and is based on the DATA-DRIVEN TECHNIQUE . the proposed DATA-DRIVEN TECHNIQUE is based on the DATA-DRIVEN TECHNIQUE . the proposed DATA-DRIVEN TECHNIQUE is evaluated on the LIVE EVALUATION SET . the results show that the proposed DATA-DRIVEN TECHNIQUE can improve the performance of SPEECH RECOGNITION SYSTEMS for COMMERCIAL APPLICATIONS .\n",
            "\n",
            "168 1000\n",
            "a STATISTICAL PARAMETRIC APPROACH to SINGING VOICE SYNTHESIS based on HIDDEN MARKOV MODELS has been growing in popularity over the last few years . the spectrum , <unk> , VIBRATO , and duration of the singing voice in this STATISTICAL PARAMETRIC APPROACH are simultaneously modeled with CONTEXT-DEPENDENT HMMS and waveforms are generated from the HMMS themselves . since HMM-BASED SINGING VOICE SYNTHESIS SYSTEMS are '' corpus-based , '' the HMMS corresponding to CONTEXTUAL FACTORS that rarely appear in the training data can not be <unk> . however , it may be difficult to prepare a large enough quantity of SINGING VOICE DATA sung by one singer . furthermore , the pitch included in each song is imbalanced , and there is the vocal range of the singer . in this paper , we propose '' SINGER ADAPTIVE TRAINING '' which can solve the DATA SPARSE-NESS PROBLEM . experimental results demonstrated that the proposed STATISTICAL PARAMETRIC APPROACH improved the quality of the SYNTHESIZED SINGING VOICES . \n",
            "this paper presents a novel STATISTICAL PARAMETRIC APPROACH for SINGING VOICE SYNTHESIS . the STATISTICAL PARAMETRIC APPROACH is based on a STATISTICAL PARAMETRIC APPROACH of the DATA SPARSE-NESS PROBLEM . the STATISTICAL PARAMETRIC APPROACH is based on the STATISTICAL PARAMETRIC APPROACH . the proposed STATISTICAL PARAMETRIC APPROACH is based on a STATISTICAL PARAMETRIC APPROACH , which is based on the STATISTICAL PARAMETRIC APPROACH . the proposed STATISTICAL PARAMETRIC APPROACH is based on a STATISTICAL PARAMETRIC APPROACH . the proposed STATISTICAL PARAMETRIC APPROACH is based on a STATISTICAL PARAMETRIC APPROACH . the proposed STATISTICAL PARAMETRIC APPROACH is based on a STATISTICAL PARAMETRIC APPROACH and is applied to the DATA SPARSE-NESS PROBLEM .\n",
            "\n",
            "169 1000\n",
            "this paper presents a method for constructing DE-TERMINISTIC PROLOG PARSERS from CORPORA OF PARSED SENTENCES . our approach uses recent MACHINE LEARNING METHODS for inducing PROLOG RULES from examples -lrb- INDUCTIVE LOGIC PROGRAMMING -rrb- . we discuss several advantages of this method compared to recent STATISTICAL METHODS and present results on learning complete PARSERS from portions of the ATIS CORPUS . \n",
            "this paper addresses the problem of INDUCTIVE LOGIC PROGRAMMING for CORPORA OF PARSED SENTENCES . we propose a new method for INDUCTIVE LOGIC PROGRAMMING based on INDUCTIVE LOGIC PROGRAMMING . the proposed method is based on the use of PROLOG RULES and the PROLOG RULES . the proposed method is based on the use of INDUCTIVE LOGIC PROGRAMMING . the proposed method is compared with state-of-the-art MACHINE LEARNING METHODS .\n",
            "\n",
            "170 1000\n",
            "we pose UNSEEN VIEW SYNTHESIS as a PROBABILISTIC TENSOR COMPLETION PROBLEM . given images of people organized by their ROUGH VIEWPOINT , we form a 3D APPEARANCE TENSOR indexed by images -lrb- pose examples -rrb- , VIEWPOINTS , and IMAGE POSITIONS . after discovering the LOW-DIMENSIONAL LATENT FACTORS that approximate that tensor , we can impute its missing entries . in this way , we generate novel SYNTHETIC VIEWS OF PEOPLE -- even when they are observed from just one CAMERA VIEWPOINT . we show that the INFERRED VIEWS are both visually and quantitatively accurate . furthermore , we demonstrate their value for recognizing actions in UNSEEN VIEWS and ESTIMATING VIEWPOINT in novel images . while existing methods are often forced to choose between data that is either realistic or multi-view , our VIRTUAL VIEWS offer both , thereby allowing greater ROBUSTNESS to viewpoint in novel images . \n",
            "this paper presents a method for ESTIMATING VIEWPOINT in UNSEEN VIEWS . the proposed method is based on a PROBABILISTIC TENSOR COMPLETION PROBLEM . the proposed method is based on a PROBABILISTIC TENSOR COMPLETION PROBLEM . the proposed method is based on a PROBABILISTIC TENSOR COMPLETION PROBLEM , which is based on a PROBABILISTIC TENSOR COMPLETION PROBLEM . the proposed method is based on a PROBABILISTIC TENSOR COMPLETION PROBLEM and a PROBABILISTIC TENSOR COMPLETION PROBLEM . the proposed method is based on a PROBABILISTIC TENSOR COMPLETION PROBLEM , which is based on the PROBABILISTIC TENSOR COMPLETION PROBLEM . the ROBUSTNESS of the proposed method is demonstrated by simulations .\n",
            "\n",
            "171 1000\n",
            "in this paper , we present a KERNEL-BASED APPROACH to the CLUSTERING OF DIFFUSION TENSORS and FIBER TRACTS . we propose to use a MERCER KERNEL over the TENSOR SPACE where both SPATIAL AND DIFFUSION INFORMATION are taken into account . this MERCER KERNEL highlights implicitly the connectivity along FIBER TRACTS . TENSOR SEGMENTATION is performed using KERNEL-PCA compounded with a LANDMARK-ISOMAP EMBEDDING and K-MEANS CLUSTERING . based on a SOFT FIBER REPRESENTATION , we extend the TENSOR KERNEL to deal with FIBER TRACTS using the MULTI-INSTANCE KERNEL that reflects not only interactions between points along FIBER TRACTS , but also the interactions between DIFFUSION TENSORS . this KERNEL-BASED APPROACH is further extended by way of an ATLAS-BASED REGISTRATION OF DIFFUSION-FREE IMAGES , followed by a CLASSIFICATION OF FIBERS based on NONLINEAR KERNEL SUPPORT VECTOR MACHINES . promising experimental results of tensor and <unk> classification of the human skeletal muscle over a significant set of HEALTHY AND DISEASED SUBJECTS demonstrate the potential of our KERNEL-BASED APPROACH . \n",
            "this paper proposes a new KERNEL-BASED APPROACH for ATLAS-BASED REGISTRATION OF DIFFUSION-FREE IMAGES . the KERNEL-BASED APPROACH is based on a SOFT FIBER REPRESENTATION and a SOFT FIBER REPRESENTATION . the proposed KERNEL-BASED APPROACH is based on a SOFT FIBER REPRESENTATION , which is based on the CLUSTERING OF DIFFUSION TENSORS . the proposed KERNEL-BASED APPROACH is based on the CLUSTERING OF DIFFUSION TENSORS . the proposed KERNEL-BASED APPROACH is based on the CLUSTERING OF DIFFUSION TENSORS . the proposed KERNEL-BASED APPROACH is based on the CLUSTERING OF DIFFUSION TENSORS . the proposed KERNEL-BASED APPROACH is based on the CLUSTERING OF DIFFUSION TENSORS . the proposed KERNEL-BASED APPROACH is based on the CLUSTERING OF DIFFUSION TENSORS . the proposed KERNEL-BASED APPROACH is compared with the conventional KERNEL-BASED APPROACH and the KERNEL-BASED APPROACH .\n",
            "\n",
            "172 1000\n",
            "a QUERY SPELLER is crucial to SEARCH ENGINE in improving WEB SEARCH RELEVANCE . this paper describes novel methods for use of DISTRIBUTIONAL SIMILARITY estimated from QUERY LOGS in learning improved QUERY SPELLING CORRECTION MODELS . the key to our methods is the property of DIS-TRIBUTIONAL SIMILARITY between two terms : it is high between a frequently occurring <unk> and its CORRECTION , and low between two irrelevant terms only with similar spellings . we present two models that are able to take advantage of this property . experimental results demonstrate that the DISTRIBUTIONAL SIMILARITY BASED MODELS can significantly outper-form their baseline systems in the WEB QUERY SPELLING CORRECTION TASK . \n",
            "this paper addresses the problem of CORRECTION in QUERY LOGS . we propose a method for CORRECTION based on DISTRIBUTIONAL SIMILARITY . the SEARCH ENGINE is formulated as a WEB QUERY SPELLING CORRECTION TASK . the SEARCH ENGINE is formulated as a WEB QUERY SPELLING CORRECTION TASK . the SEARCH ENGINE is formulated as a WEB QUERY SPELLING CORRECTION TASK . the proposed method is applied to the WEB QUERY SPELLING CORRECTION TASK . the experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "173 1000\n",
            "sparse learning framework , which is very popular in the field of NATURE LANGUAGE PROCESSING recently due to the advantages of efficiency and GENERALIZABILITY , can be applied to CONDITIONAL RANDOM FIELDS with L1 REGULARIZATION METHOD . STOCHASTIC GRADIENT DESCENT METHOD has been used in training L1-REGULARIZED CRFS , because STOCHASTIC GRADIENT DESCENT METHOD often requires much less TRAINING TIME than the BATCH TRAINING ALGORITHM like QUASI-NEWTON METHOD in practice . nevertheless , STOCHASTIC GRADIENT DESCENT METHOD sometimes fails to converge to the optimum , and STOCHASTIC GRADIENT DESCENT METHOD can be very sensitive to the LEARNING RATE PARAMETER SETTINGS . we present a TWO-STAGE TRAINING ALGORITHM which guarantees the CONVERGENCE , and use HEURIS-TIC LINE SEARCH STRATEGY to make the first stage of STOCHASTIC GRADIENT DESCENT METHOD more robust and stable . experimental evaluations on CHINESE WORD SEGMENTATION and NAME ENTITY RECOGNITION TASKS demonstrate that our TWO-STAGE TRAINING ALGORITHM can produce more accurate and compact model with less TRAINING TIME for L1 REGULARIZATION . \n",
            "this paper proposes a new TWO-STAGE TRAINING ALGORITHM for CHINESE WORD SEGMENTATION . the proposed HEURIS-TIC LINE SEARCH STRATEGY is based on a HEURIS-TIC LINE SEARCH STRATEGY , which is a HEURIS-TIC LINE SEARCH STRATEGY for CHINESE WORD SEGMENTATION . the proposed TWO-STAGE TRAINING ALGORITHM is based on a HEURIS-TIC LINE SEARCH STRATEGY , which is a HEURIS-TIC LINE SEARCH STRATEGY . the proposed TWO-STAGE TRAINING ALGORITHM is based on a HEURIS-TIC LINE SEARCH STRATEGY , which is a HEURIS-TIC LINE SEARCH STRATEGY for CHINESE WORD SEGMENTATION . the proposed TWO-STAGE TRAINING ALGORITHM is evaluated on the NAME ENTITY RECOGNITION TASKS . the experimental results show that the proposed TWO-STAGE TRAINING ALGORITHM achieves better performance than the conventional BATCH TRAINING ALGORITHM .\n",
            "\n",
            "174 1000\n",
            "in this paper we investigate the problem of CHANNEL TRACKING and DETECTION for MIMO-OFDM SYSTEMS over fast varying channels obeying a GAUSS-MARKOV MODEL . we consider TIME DOMAIN TRACKING of the CHANNEL MATRIX TAPS with kalman ¿ <unk> , whereas SYMBOLS DETECTION is carried out by a ZERO-FORCING SOFT DETECTOR . a key assumption of the theory of KALMAN ¿ LTER is that the GAUSS-MARKOV MODEL is perfectly known , while COMMUNICATION SYSTEMS make use of the DETECTED SYMBOLS as an input to the KALMAN ¿ LTER in order to form a suitable GAUSS-MARKOV MODEL . this gives rise to ERROR PROPAGATION due to MIS-DETECTED SYMBOLS -lrb- model mismatch -rrb- and is usually solved by using frequently inserted pilot symbols , resulting in a REDUCED SPECTRAL EF ¿ CIENCY . to overcome this problem , we suggest a novel approach to mitigate the ERROR PROPAGATION due to MIS-DETECTIONS without using frequent pilot symbols . in particular , we consider the reliability of the detections based on the SOFT DETECTOR and use only those outputs that have robust reliability to track the CHANNEL MATRIX TAPS , minimizing the effect of KALMAN ¿ LTER MISMODELING . this method can signi ¿ cantly reduce the ERROR PROPAGATION EFFECT , leading to an improved BIT ERROR PROBABILITY . \n",
            "this paper presents a new method for TIME DOMAIN TRACKING based on REDUCED SPECTRAL EF ¿ CIENCY . the proposed method is based on a ZERO-FORCING SOFT DETECTOR of the CHANNEL MATRIX TAPS . the proposed method is based on the KALMAN ¿ LTER MISMODELING . the proposed method is based on the KALMAN ¿ LTER MISMODELING . the proposed method is based on the KALMAN ¿ LTER MISMODELING . the proposed method is based on the KALMAN ¿ LTER MISMODELING . the proposed method is based on the KALMAN ¿ LTER MISMODELING . the proposed method is based on the KALMAN ¿ LTER MISMODELING . the proposed method is based on the KALMAN ¿ LTER MISMODELING . the proposed method is based on the KALMAN ¿ LTER MISMODELING . the proposed method is based on the KALMAN ¿ LTER MISMODELING . the proposed method is based on the KALMAN ¿ LTER MISMODELING . the proposed method is based on the KALMAN ¿ LTER MISMODELING . the proposed method is based on the KALMAN ¿ LTER MISMODELING . the proposed method is based on the KALMAN ¿ LTER MISMODELING . the proposed method is based on the KALMAN ¿ LTER MISMODELING . the proposed method is based on the KALMAN ¿ LTER MISMODELING . the proposed method is based on the KALMAN ¿ LTER MISMODELING . the proposed method is based on the KALMAN ¿ LTER MISMODELING . the proposed method is based on the KALMAN ¿ LTER MISMODELING .\n",
            "\n",
            "175 1000\n",
            "we introduce a new CLASS OF DISTINGUISHED REGIONS based on detecting the most salient CONVEX LOCAL ARRANGEMENTS OF CONTOURS in the image . the regions are used in a similar way to the LOCAL INTEREST POINTS extracted from GRAY-LEVEL IMAGES , but they capture shape rather than texture . LOCAL CONVEXITY is characterized by measuring the extent to which the detected image contours support circle or <unk> local structures at each position and scale in the image . our CLASS OF DISTINGUISHED REGIONS combines two COST FUNCTIONS defined on the TANGENTIAL EDGES near the circle : a TANGENTIAL-GRADIENT ENERGY TERM , and an ENTROPY TERM that ensures LOCAL SUPPORT from a wide range of ANGULAR POSITIONS around the circle . the DETECTED REGIONS are invariant to SCALE CHANGES and ROTATIONS , and robust against CLUTTER , OCCLUSIONS and SPURIOUS EDGE DETECTIONS . experimental results show very good performance for both SHAPE MATCHING and recognition of object categories . \n",
            "this paper presents a new method for SHAPE MATCHING in GRAY-LEVEL IMAGES . the method is based on the CLASS OF DISTINGUISHED REGIONS of the LOCAL INTEREST POINTS and the CLASS OF DISTINGUISHED REGIONS . the proposed method is based on the CLASS OF DISTINGUISHED REGIONS of the LOCAL INTEREST POINTS and the CLASS OF DISTINGUISHED REGIONS . the proposed method is based on the CLASS OF DISTINGUISHED REGIONS of the LOCAL INTEREST POINTS and the ANGULAR POSITIONS . the proposed method is based on the CLASS OF DISTINGUISHED REGIONS and the CLASS OF DISTINGUISHED REGIONS of the GRAY-LEVEL IMAGES . the proposed method is evaluated on the GRAY-LEVEL IMAGES and the results show that the proposed method is robust to OCCLUSIONS and OCCLUSIONS .\n",
            "\n",
            "176 1000\n",
            "large-scale 1-regularized loss minimization problems arise in HIGH-DIMENSIONAL APPLICATIONS such as COMPRESSED SENSING and HIGH-DIMENSIONAL SUPERVISED LEARNING , including CLASSIFICATION and REGRESSION PROBLEMS . HIGH-PERFORMANCE ALGORITHMS and implementations are critical to efficiently solving these problems . building upon previous work on COORDINATE DESCENT ALGORITHMS for 1-REGULARIZED PROBLEMS , we introduce a novel family of algorithms called BLOCK-GREEDY COORDINATE DESCENT that includes , as special cases , several existing algorithms such as SCD , GREEDY CD , SHOTGUN , and THREAD-GREEDY . we give a UNIFIED CONVERGENCE ANALYSIS for the family of BLOCK-GREEDY ALGORITHMS . the analysis suggests that BLOCK-GREEDY COORDINATE DESCENT can better exploit PARALLELISM if FEATURES are clustered so that the MAXIMUM INNER PRODUCT between FEATURES in different blocks is small . our THEORETICAL CONVERGENCE ANALYSIS is supported with experimental results using data from diverse REAL-WORLD APPLICATIONS . we hope that algorithmic approaches and CONVERGENCE ANALYSIS we provide will not only advance the field , but will also encourage researchers to systematically explore the design space of algorithms for solving LARGE-SCALE 1-REGULARIZATION PROBLEMS . \n",
            "this paper addresses the problem of HIGH-DIMENSIONAL SUPERVISED LEARNING in HIGH-DIMENSIONAL APPLICATIONS . we propose a method for HIGH-DIMENSIONAL SUPERVISED LEARNING based on BLOCK-GREEDY COORDINATE DESCENT . the proposed method is based on the use of BLOCK-GREEDY COORDINATE DESCENT and BLOCK-GREEDY COORDINATE DESCENT . the proposed method is based on the use of BLOCK-GREEDY COORDINATE DESCENT and COORDINATE DESCENT ALGORITHMS . the proposed method is based on the use of BLOCK-GREEDY COORDINATE DESCENT and COORDINATE DESCENT ALGORITHMS . the proposed method is compared with state-of-the-art BLOCK-GREEDY ALGORITHMS and COORDINATE DESCENT ALGORITHMS .\n",
            "\n",
            "177 1000\n",
            "the social nature of laughter <unk> people to <unk> together . this JOINT VOCAL ACTION often results in OVERLAPPING LAUGHTER . in this paper , we show that the ACOUSTICS OF OVERLAPPING LAUGHS are different from NON-OVERLAPPING LAUGHS . we found that overlapping <unk> are stronger prosodically marked than NON-OVERLAPPING ONES , in terms of higher values for duration , mean f0 , mean and MAXIMUM INTENSITY , and the amount of voicing . this effect is <unk> by the number of people joining in the LAUGHTER EVENT , which suggests that ENTRAINMENT is at work . we also found that GROUP SIZE affects the number of overlapping <unk> which illustrates the CONTAGIOUS NATURE OF LAUGHTER . finally , people appear to join laughter simultaneously at a delay of approximately 500 ms ; a delay that must be considered when developing SPOKEN DIALOGUE SYSTEMS that are able to respond to users ' <unk> . \n",
            "this paper presents a method for JOINT VOCAL ACTION in SPOKEN DIALOGUE SYSTEMS . the proposed method is based on the CONTAGIOUS NATURE OF LAUGHTER of the LAUGHTER EVENT and the GROUP SIZE . the proposed method is based on the CONTAGIOUS NATURE OF LAUGHTER of the LAUGHTER EVENT . the proposed method is based on the CONTAGIOUS NATURE OF LAUGHTER and the CONTAGIOUS NATURE OF LAUGHTER . the proposed method is based on the CONTAGIOUS NATURE OF LAUGHTER .\n",
            "\n",
            "178 1000\n",
            "a HALF-DUPLEX DISTRIBUTED BEAMFORMING TECHNIQUE for RELAY NETWORKS with FREQUENCY SELECTIVE FADING CHANNELS is developed . the NETWORK RELAYS use the FILTER-AND-FORWARD STRATEGY to compensate for the TRANSMITTER-TO-RELAY AND RELAY-TO-DESTINATION CHANNELS using FINITE IMPULSE RESPONSE FILTERS . with the channel state information -lrb- csi -rrb- being available at the RECEIVER , the TRANSMIT RELAY POWER is minimized subject to the DESTINATION QUALITY-OF-SERVICE CONSTRAINT . this DISTRIBUTED BEAMFORMING PROBLEM is shown to have a CLOSED-FORM SOLUTION . simulation results demonstrate substantial improvements in terms of the RELAY TRANSMITTED POWER and feasibility of the DESTINATION QOS CONSTRAINT as compared to AMPLIFY-AND-FORWARD DISTRIBUTED BEAMFORMING TECHNIQUES . \n",
            "this paper presents a HALF-DUPLEX DISTRIBUTED BEAMFORMING TECHNIQUE for RELAY NETWORKS . the HALF-DUPLEX DISTRIBUTED BEAMFORMING TECHNIQUE is based on the FILTER-AND-FORWARD STRATEGY . the FILTER-AND-FORWARD STRATEGY is based on the FILTER-AND-FORWARD STRATEGY . the FILTER-AND-FORWARD STRATEGY is based on the FILTER-AND-FORWARD STRATEGY . the proposed HALF-DUPLEX DISTRIBUTED BEAMFORMING TECHNIQUE is based on the FILTER-AND-FORWARD STRATEGY . the proposed HALF-DUPLEX DISTRIBUTED BEAMFORMING TECHNIQUE is based on the FILTER-AND-FORWARD STRATEGY . the proposed HALF-DUPLEX DISTRIBUTED BEAMFORMING TECHNIQUE is based on the FILTER-AND-FORWARD STRATEGY . the proposed HALF-DUPLEX DISTRIBUTED BEAMFORMING TECHNIQUE is based on a HALF-DUPLEX DISTRIBUTED BEAMFORMING TECHNIQUE . the proposed HALF-DUPLEX DISTRIBUTED BEAMFORMING TECHNIQUE is based on the HALF-DUPLEX DISTRIBUTED BEAMFORMING TECHNIQUE . the proposed HALF-DUPLEX DISTRIBUTED BEAMFORMING TECHNIQUE is based on a HALF-DUPLEX DISTRIBUTED BEAMFORMING TECHNIQUE . the proposed HALF-DUPLEX DISTRIBUTED BEAMFORMING TECHNIQUE is based on a HALF-DUPLEX DISTRIBUTED BEAMFORMING TECHNIQUE .\n",
            "\n",
            "179 1000\n",
            "our goal in this work was to develop an accurate method to identify LAUGHTER SEGMENTS , ultimately for the purpose of SPEAKER RECOGNITION . our previous work used MLPS to perform FRAME LEVEL DETECTION OF LAUGHTER using SHORT-TERM FEATURES , including MFCCS and pitch , and achieved a 7.9 % EER on our test set . we improved upon our previous results by including HIGH-LEVEL AND LONG-TERM FEATURES , MEDIAN FILTERING , and performing segmentation via a HYBRID MLP/HMM SYSTEM with VITERBI DECODING . upon including the LONG-TERM FEATURES and MEDIAN FILTERING , our results improved to 5.4 % EER on our test set and 2.7 % EER on an EQUAL-PRIOR TEST SET used by others . after attaining segmentation results by incorporating the HYBRID MLP/HMM SYSTEM and VITERBI DECODING , we had a <unk> % PRECISION RATE and <unk> % RECALL RATE on our test set . to our knowledge these are the best known LAUGHTER DETECTION results on the ICSI MEETING RECORDER CORPUS to date . \n",
            "this paper presents a new method for SPEAKER RECOGNITION based on MEDIAN FILTERING . the proposed method is based on the use of LONG-TERM FEATURES and MEDIAN FILTERING . the proposed method is based on the use of SHORT-TERM FEATURES and MEDIAN FILTERING . the proposed method is based on the use of SHORT-TERM FEATURES and MEDIAN FILTERING . the proposed method is evaluated on the ICSI MEETING RECORDER CORPUS . the results show that the proposed method outperforms the conventional HYBRID MLP/HMM SYSTEM in terms of RECALL RATE and RECALL RATE .\n",
            "\n",
            "180 1000\n",
            "we propose a simple NEURAL ARCHITECTURE for NATURAL LANGUAGE INFERENCE . our NEURAL ARCHITECTURE uses attention to decompose the problem into <unk> that can be solved separately , thus making it trivially parallelizable . on the STANFORD NATURAL LANGUAGE INFERENCE DATASET , we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any WORD-ORDER INFORMATION . adding INTRA-SENTENCE ATTENTION that takes a minimum amount of order into account yields further improvements . \n",
            "this paper presents a NEURAL ARCHITECTURE for NATURAL LANGUAGE INFERENCE . the NEURAL ARCHITECTURE is based on a NEURAL ARCHITECTURE . the NEURAL ARCHITECTURE is based on the NEURAL ARCHITECTURE . the proposed NEURAL ARCHITECTURE is based on the NEURAL ARCHITECTURE .\n",
            "\n",
            "181 1000\n",
            "we identify and study two types of '' ACCIDENTAL '' IMAGES that can be formed in scenes . the first is an ACCIDENTAL PIN-HOLE CAMERA IMAGE . these images are often mistaken for SHADOWS , but can reveal structures outside a room , or the unseen shape of the light APERTURE into the room . the second class of ACCIDENTAL '' IMAGES are '' inverse '' <unk> camera images , formed by subtracting an image with a small <unk> present from a REFERENCE IMAGE without the <unk> . the REFERENCE IMAGE can be an earlier frame of a VIDEO SEQUENCE . both types of ACCIDENTAL '' IMAGES happen in a variety of different situations -lrb- an INDOOR SCENE illuminated by NATURAL LIGHT , a street with a person walking under the shadow of a building , etc. -rrb- . ACCIDENTAL CAMERAS can reveal information about the scene outside the image , the LIGHTING CONDITIONS , or the APERTURE by which light enters the scene . \n",
            "this paper presents a method for ACCIDENTAL '' IMAGES , which is based on the APERTURE and the APERTURE . the proposed method is based on the APERTURE and the APERTURE . the proposed method is based on the APERTURE and the APERTURE . the proposed method is based on the APERTURE and the APERTURE .\n",
            "\n",
            "182 1000\n",
            "we address the DETECTION OF VEHICLES in a VIDEO STREAM obtained from a MOVING AIRBORNE PLATFORM . our approach is based on robust OPTICAL FLOW ALGORITHM applied on STABILIZED FRAMES . STABILIZATION OF THE FRAMES compensates for GROSS AFFINE BACKGROUND MOTION prior to running robust optical flow to compute DENSE RESIDUAL FLOW . based on the flow and the previous BACKGROUND APPEARANCE MODEL , the new frame is separated into background and foreground oc-clusion layers using an EM-BASED MOTION SEGMENTATION . the proposed framework shows that GROUND VEHICLES can be detected and segmented from AIRBORNE VIDEO SEQUENCES while building a MOSAIC OF THE BACKGROUND LAYER . \n",
            "this paper presents a novel method for DETECTION OF VEHICLES from AIRBORNE VIDEO SEQUENCES . the OPTICAL FLOW ALGORITHM is based on the DENSE RESIDUAL FLOW of the VIDEO STREAM . the OPTICAL FLOW ALGORITHM is based on the DENSE RESIDUAL FLOW of the VIDEO STREAM . the DETECTION OF VEHICLES is formulated as a VIDEO STREAM . the DETECTION OF VEHICLES is formulated as a VIDEO STREAM . the DETECTION OF VEHICLES is formulated as a MOVING AIRBORNE PLATFORM . the proposed OPTICAL FLOW ALGORITHM is applied to the DETECTION OF VEHICLES .\n",
            "\n",
            "183 1000\n",
            "the traditional PARTIAL-MODEL SELECTION SEARCHING METHOD for MODEL-ORDER SELECTION in LINEAR REGRESSION is a NESTED FULL-PARAMETERS-SET SEARCHING PROCEDURE over the desired orders , which we call FULL-MODEL ORDER SELECTION . on the other hand , a method for MODEL-SELECTION SEARCHES for the best <unk> within each order . in this paper , we propose using the PARTIAL-MODEL SELECTION SEARCHING METHOD for MODEL-ORDER SELECTION , which we call FULL-MODEL ORDER SELECTION . we show by simulations that the proposed PARTIAL-MODEL SELECTION SEARCHING METHOD gives better ACCURACIES than the traditional one , especially for LOW SIGNAL-TO-NOISE RATIOS over a wide range of MODEL-ORDER SELECTION CRITERIA -lrb- both information <unk> and <unk> -rrb- . also , we show that for some models the performance of the BOOTSTRAP-BASED CRITERION improves significantly by using the proposed PARTIAL-MODEL SELECTION SEARCHING METHOD . \n",
            "this paper proposes a new PARTIAL-MODEL SELECTION SEARCHING METHOD for MODEL-ORDER SELECTION . the proposed PARTIAL-MODEL SELECTION SEARCHING METHOD is based on the NESTED FULL-PARAMETERS-SET SEARCHING PROCEDURE . the proposed PARTIAL-MODEL SELECTION SEARCHING METHOD is based on the NESTED FULL-PARAMETERS-SET SEARCHING PROCEDURE . the proposed PARTIAL-MODEL SELECTION SEARCHING METHOD is based on the NESTED FULL-PARAMETERS-SET SEARCHING PROCEDURE . the proposed PARTIAL-MODEL SELECTION SEARCHING METHOD is based on the NESTED FULL-PARAMETERS-SET SEARCHING PROCEDURE . the proposed PARTIAL-MODEL SELECTION SEARCHING METHOD is compared with the conventional PARTIAL-MODEL SELECTION SEARCHING METHOD . the proposed PARTIAL-MODEL SELECTION SEARCHING METHOD is shown to outperform the conventional PARTIAL-MODEL SELECTION SEARCHING METHOD in terms of ACCURACIES and ACCURACIES .\n",
            "\n",
            "184 1000\n",
            "many REAL-WORLD DECISION-THEORETIC PLANNING PROBLEMS are naturally modeled using both CONTINUOUS STATE AND ACTION SPACES , yet little work has provided exact solutions for the case of continuous actions . in this work , we propose a SYMBOLIC DYNAMIC PROGRAMMING SOLUTION to obtain the optimal CLOSED-FORM VALUE FUNCTION and POLICY for CSA-MDPS with MUL-TIVARIATE CONTINUOUS STATE AND ACTIONS , DISCRETE NOISE , PIECEWISE LINEAR DYNAMICS , and piecewise linear -lrb- or RESTRICTED PIECEWISE QUADRATIC -RRB- REWARD . our key contribution over previous SDP work is to show how the CONTINUOUS ACTION MAXIMIZATION STEP in the DYNAMIC PROGRAMMING BACKUP can be evaluated optimally and <unk> -- a task which amounts to SYMBOLIC CONSTRAINED OPTIMIZATION subject to UNKNOWN STATE PARAMETERS ; we further integrate this technique to work with an efficient and compact data structure for SDP -- the extended algebraic decision diagram -lrb- <unk> -rrb- . we demonstrate empirical results on a DIDACTIC NONLINEAR PLANNING EXAMPLE and two domains from operations research to show the first AUTOMATED EXACT SOLUTION to these problems . \n",
            "this paper presents a new method for SYMBOLIC CONSTRAINED OPTIMIZATION based on SYMBOLIC CONSTRAINED OPTIMIZATION . the proposed method is based on a DIDACTIC NONLINEAR PLANNING EXAMPLE and a SYMBOLIC DYNAMIC PROGRAMMING SOLUTION . the proposed method is based on a DIDACTIC NONLINEAR PLANNING EXAMPLE , which is based on SYMBOLIC CONSTRAINED OPTIMIZATION and SYMBOLIC CONSTRAINED OPTIMIZATION . the proposed method is based on SYMBOLIC CONSTRAINED OPTIMIZATION and SYMBOLIC CONSTRAINED OPTIMIZATION . the proposed method is based on SYMBOLIC CONSTRAINED OPTIMIZATION and SYMBOLIC CONSTRAINED OPTIMIZATION . the proposed method is based on the DYNAMIC PROGRAMMING BACKUP and the AUTOMATED EXACT SOLUTION . the proposed method is based on the DYNAMIC PROGRAMMING BACKUP and the AUTOMATED EXACT SOLUTION . the proposed method is compared with the conventional SDP and the AUTOMATED EXACT SOLUTION .\n",
            "\n",
            "185 1000\n",
            "we enrich a <unk> resource of COMMON-SENSE KNOWLEDGE by formulating the problem as one of KNOWLEDGE BASE COMPLETION . most work in KNOWLEDGE BASE COMPLETION focuses on KNOWLEDGE BASES like FREEBASE that relate entities drawn from a fixed set . however , the tuples in CONCEPTNET -lrb- <unk> and <unk> , 2012 -rrb- define relations between an unbounded set of phrases . we develop NEURAL NETWORK MODELS for scoring tuples on arbitrary phrases and evaluate NEURAL NETWORK MODELS by their ability to distinguish TRUE HELD-OUT TUPLES from false ones . we find strong performance from a BILINEAR MODEL using a simple ADDITIVE ARCHITECTURE to BILINEAR MODEL phrases . we manually evaluate our trained BILINEAR MODEL 's ability to assign quality scores to novel tuples , finding that BILINEAR MODEL can propose TU-PLES at the same quality level as MEDIUM-CONFIDENCE TUPLES from CONCEPTNET . \n",
            "this paper presents a new BILINEAR MODEL for KNOWLEDGE BASE COMPLETION . the BILINEAR MODEL is based on the BILINEAR MODEL and the BILINEAR MODEL . the BILINEAR MODEL is based on the BILINEAR MODEL . the BILINEAR MODEL is based on the BILINEAR MODEL . the BILINEAR MODEL is based on the BILINEAR MODEL . the BILINEAR MODEL is based on the BILINEAR MODEL . the BILINEAR MODEL is based on the BILINEAR MODEL . the proposed BILINEAR MODEL is based on a BILINEAR MODEL and is shown to be useful for KNOWLEDGE BASE COMPLETION .\n",
            "\n",
            "186 1000\n",
            "using a recently developed REAL TIME AUDIO CAMERA , that uses the output of a SPHERICAL MICROPHONE ARRAY BEAMFORMER steered in all directions to create CENTRAL PROJECTION to create ACOUSTIC INTENSITY IMAGES , we present a technique to measure the ACOUSTICS OF ROOMS AND HALLS . a PANORAMIC MOSAICED VISUAL IMAGE OF THE SPACE is also create . since both the VISUAL AND THE AUDIO CAMERA IMAGES are CENTRAL PROJECTION , REGISTRATION of the acquired AUDIO AND VIDEO IMAGES can be performed using standard COMPUTER VISION TECHNIQUES . we describe the technique , and apply it to the examine the relation between ACOUSTICAL FEATURES and architectural details of the <unk> concert hall at the <unk> smith performing arts center in <unk> <unk> , <unk> . \n",
            "this paper presents a SPHERICAL MICROPHONE ARRAY BEAMFORMER for AUDIO AND VIDEO IMAGES . the proposed SPHERICAL MICROPHONE ARRAY BEAMFORMER is based on a SPHERICAL MICROPHONE ARRAY BEAMFORMER and a SPHERICAL MICROPHONE ARRAY BEAMFORMER . the proposed SPHERICAL MICROPHONE ARRAY BEAMFORMER is based on the ACOUSTICS OF ROOMS AND HALLS and the ACOUSTICAL FEATURES . the REGISTRATION is performed using the SPHERICAL MICROPHONE ARRAY BEAMFORMER . the REGISTRATION is performed using the SPHERICAL MICROPHONE ARRAY BEAMFORMER . the proposed method is evaluated on the AUDIO AND VIDEO IMAGES .\n",
            "\n",
            "187 1000\n",
            "<unk> audio IMMERSIVE AUDIO SYSTEMS are being envisioned for applications that include TELECONFERENCING and <unk> ; AUGMENTED AND VIRTUAL REALITY for MANUFACTURING AND ENTERTAINMENT ; AIR TRAFFIC CONTROL , PILOT WARNING , and GUIDANCE SYSTEMS ; DISPLAYS for the <unk> ; DISTANCE LEARNING ; and professional sound and picture editing for television and film . the principal function of such IMMERSIVE AUDIO SYSTEMS is to synthesize , manipulate and render SOUND FIELDS in real time . in this paper we examine several SIGNAL PROCESSING CONSIDERATIONS in SPATIAL SOUND RENDERING over loudspeakers . we propose two methods that can be used to implement the necessary filters for generating VIRTUAL SOUND SOURCES based on SYNTHETIC HEAD-RELATED TRANSFER FUNCTIONS with the same SPECTRAL CHARACTERISTICS as those of the REAL SOURCE . \n",
            "this paper addresses the problem of SPATIAL SOUND RENDERING in IMMERSIVE AUDIO SYSTEMS . we propose a method for AIR TRAFFIC CONTROL based on DISTANCE LEARNING and DISTANCE LEARNING . the proposed method is based on the use of SYNTHETIC HEAD-RELATED TRANSFER FUNCTIONS and DISTANCE LEARNING . the proposed method is based on the use of SYNTHETIC HEAD-RELATED TRANSFER FUNCTIONS , TELECONFERENCING , and DISTANCE LEARNING . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "188 1000\n",
            "though both DOCUMENT SUMMARIZATION and KEYWORD EXTRACTION aim to extract concise representations from documents , these two tasks have usually been investigated independently . this paper proposes a novel ITERATIVE REINFORCEMENT APPROACH to simultaneously extracting SUMMARY AND KEYWORDS from SINGLE DOCUMENT under the assumption that the SUMMARY AND KEYWORDS of a document can be mutually boosted . the ITERATIVE REINFORCEMENT APPROACH can naturally make full use of the reinforcement between sentences and KEYWORDS by fusing three kinds of relationships between sentences and words , either homogeneous or heterogeneous . experimental results show the effectiveness of the proposed ITERATIVE REINFORCEMENT APPROACH for both tasks . the ITERATIVE REINFORCEMENT APPROACH is validated to work almost as well as the KNOWLEDGE-BASED APPROACH for COMPUTING WORD SEMANTICS . \n",
            "this paper presents a new KNOWLEDGE-BASED APPROACH for KEYWORD EXTRACTION . the proposed ITERATIVE REINFORCEMENT APPROACH is based on the KNOWLEDGE-BASED APPROACH and the KNOWLEDGE-BASED APPROACH . the proposed ITERATIVE REINFORCEMENT APPROACH is based on the KNOWLEDGE-BASED APPROACH and the KNOWLEDGE-BASED APPROACH . the proposed KNOWLEDGE-BASED APPROACH is compared with the conventional KNOWLEDGE-BASED APPROACH and the KNOWLEDGE-BASED APPROACH .\n",
            "\n",
            "189 1000\n",
            "eeg connectivity measures could provide a new type of FEATURE SPACE for inferring a subject 's intention in BRAIN-COMPUTER INTERFACES . however , very little is known on EEG CONNECTIVITY PATTERNS for BRAIN-COMPUTER INTERFACES . in this study , EEG CONNECTIVITY MEASURES during MOTOR IMAGERY of the left and right is investigated in a broad frequency range across the whole scalp by combining BEAMFORMING with TRANSFER ENTROPY and taking into account possible volume <unk> effects . observed <unk> patterns indicate that MODULATION intentionally induced by MOTOR IMAGERY is strongest in the Γ-BAND , i.e. , above 35 hz . furthermore , MODULATION between MOTOR IMAGERY and rest is found to be more pronounced than between MOTOR IMAGERY of different hands . this is in contrast to results on MOTOR IMAGERY obtained with BANDPOWER FEATURES , and might provide an explanation for the so far only moderate success of CONNECTIVITY FEATURES in BRAIN-COMPUTER INTERFACES . it is concluded that future studies on CONNECTIVITY BASED BCIS should focus on HIGH FREQUENCY BANDS and consider experimental paradigms that maximally vary COGNITIVE DEMANDS between conditions . \n",
            "this paper presents a new method for BRAIN-COMPUTER INTERFACES in MOTOR IMAGERY . the method is based on the use of CONNECTIVITY FEATURES and TRANSFER ENTROPY . the proposed method is based on the use of BANDPOWER FEATURES and TRANSFER ENTROPY . the proposed method is based on the use of BANDPOWER FEATURES and MODULATION . the proposed method is based on the use of BANDPOWER FEATURES and MODULATION . the proposed method is evaluated on MOTOR IMAGERY and MOTOR IMAGERY .\n",
            "\n",
            "190 1000\n",
            "symbolic bidirectional <unk> SEARCH is a prominent technique for COST-OPTIMAL PLANNING . thus , the question whether it can be further improved by making use of HEURISTIC FUNCTIONS raises naturally . however , the use of HEURISTICS in BIDI-RECTIONAL SEARCH does not always improve its performance . we propose a novel way to use ABSTRACTION HEURISTICS in SYMBOLIC BIDIRECTIONAL SEARCH in which the SEARCH only <unk> to HEURISTICS when it becomes unfeasible . we adapt the definition of PARTIAL AND PERIMETER ABSTRACTIONS to BIDIRECTIONAL SEARCH , where a ⇤ is used to traverse the ABSTRACT STATE SPACES AND/OR generate the PERIMETER . the results show that ABSTRACTION HEURISTICS can further improve SYMBOLIC BIDIRECTIONAL SEARCH in some domains . in fact , the resulting planner , SYMBA ⇤ , was the winner of the <unk> of the last IPC . \n",
            "this paper addresses the problem of COST-OPTIMAL PLANNING in ABSTRACT STATE SPACES AND/OR . we propose a method for COST-OPTIMAL PLANNING based on SYMBOLIC BIDIRECTIONAL UNIFORM-COST SEARCH . the proposed method is based on the use of HEURISTIC FUNCTIONS , which is a BIDIRECTIONAL SEARCH . the proposed method is based on the use of HEURISTIC FUNCTIONS and a BIDIRECTIONAL SEARCH . the proposed method is based on the use of HEURISTIC FUNCTIONS , which are used for COST-OPTIMAL PLANNING . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "191 1000\n",
            "we present an UNSUPERVISED ALGORITHM for the discovery of words and WORD-LIKE FRAGMENTS from the SPEECH SIGNAL , without using an UPFRONT DEFINED LEXICON or ACOUSTIC PHONE MODELS . the UNSUPERVISED ALGORITHM is based on a combination of ACOUSTIC PATTERN DISCOVERY , CLUSTERING , and TEMPORAL SEQUENCE LEARNING . UNSUPERVISED ALGORITHM exploits the ACOUSTIC SIMILARITY between multiple acoustic tokens of the same words or WORD-LIKE FRAGMENTS . in its current form , the UNSUPERVISED ALGORITHM is able to discover words in speech with low <unk> -lrb- connected digits -rrb- . although its performance still falls off compared to MAINSTREAM ASR APPROACHES , the value of the UNSUPERVISED ALGORITHM is its potential to serve as a COMPUTATIONAL MODEL in two research directions . first , the UNSUPERVISED ALGORITHM may lead to an approach for SPEECH RECOGNITION that is fundamentally <unk> from the MODELLING CONSTRAINTS in conventional ASR . second , the proposed UNSUPERVISED ALGORITHM can be interpreted as a COMPUTATIONAL MODEL of LANGUAGE ACQUISITION that takes actual speech as input and is able to find words as 'em <unk> ' properties from RAW INPUT . \n",
            "this paper presents a COMPUTATIONAL MODEL for SPEECH RECOGNITION . the UNSUPERVISED ALGORITHM is based on the UNSUPERVISED ALGORITHM and the UNSUPERVISED ALGORITHM . the UNSUPERVISED ALGORITHM is based on the UNSUPERVISED ALGORITHM and the COMPUTATIONAL MODEL . the UNSUPERVISED ALGORITHM is based on the UNSUPERVISED ALGORITHM and the UNSUPERVISED ALGORITHM . the UNSUPERVISED ALGORITHM is based on the UNSUPERVISED ALGORITHM and the UNSUPERVISED ALGORITHM . the UNSUPERVISED ALGORITHM is based on the UNSUPERVISED ALGORITHM and the UNSUPERVISED ALGORITHM . the proposed UNSUPERVISED ALGORITHM is based on the UNSUPERVISED ALGORITHM and the UNSUPERVISED ALGORITHM . the UNSUPERVISED ALGORITHM is applied to SPEECH RECOGNITION and SPEECH RECOGNITION .\n",
            "\n",
            "192 1000\n",
            "we present a novel approach for unsu-pervised induction of a REORDERING GRAMMAR using a modified form of PERMUTATION TREES -lrb- zhang and <unk> , 2007 -rrb- , which we apply to PREORDERING in PHRASE-BASED MACHINE TRANSLATION . unlike previous approaches , we induce in one step both the HIERARCHICAL STRUCTURE and the TRANSDUCTION FUNCTION over it from WORD-ALIGNED PARALLEL CORPORA . furthermore , our model -lrb- 1 -rrb- handles NON-ITG REORDERING PATTERNS -lrb- up to <unk> branching -rrb- , -lrb- 2 -rrb- is learned from all derivations by treating not only LABELING but also BRACKETING as LATENT VARIABLE , -lrb- 3 -rrb- is entirely <unk> at the level of REORDERING RULES , and -lrb- 4 -rrb- requires no LINGUISTIC ANNOTATION . our model is evaluated both for ACCURACY in PREDICTING TARGET ORDER , and for its impact on TRANSLATION QUALITY . we report significant performance gains over PHRASE REORDERING , and over two known PREORDERING BASELINES for ENGLISH-JAPANESE . \n",
            "this paper presents a new method for PHRASE-BASED MACHINE TRANSLATION in PHRASE-BASED MACHINE TRANSLATION . the method is based on the use of a LATENT VARIABLE and a LATENT VARIABLE . the proposed method is based on the HIERARCHICAL STRUCTURE and the HIERARCHICAL STRUCTURE . the proposed method is based on the use of PERMUTATION TREES and PERMUTATION TREES . the proposed method is based on the use of PERMUTATION TREES and PERMUTATION TREES . the proposed method is based on the use of PERMUTATION TREES and PREORDERING BASELINES . experimental results show the effectiveness of the proposed method in terms of TRANSLATION QUALITY and TRANSLATION QUALITY .\n",
            "\n",
            "193 1000\n",
            "this paper describes some improvements in SPEECH RECOGNITION OF BROADCAST NEWS COMMENTARY in JAPANESE . since NEWS COMMENTARY SPEECH has different LINGUISTIC AND ACOUSTIC FEATURES from READ SPEECH , it gives lower WORD RECOGNITION ACCURACY . in this paper we apply to news <unk> some RULES which represent the LINGUISTIC FEATURES OF NEWS COMMENTARIES , and generate WORD SEQUENCES for LANGUAGE MODEL ADAPTATION . we also use a large volume of transcriptions of NEWS PROGRAMS as training texts . ACOUSTIC MODELS are <unk> and their structures are changed so as to recognize relatively short phonemes , because we found the SPEECH RATE of NEWS COMMENTARY is sometimes much faster than that of READ SPEECH . furthermore , by using a DECODER that can handle CROSSWORD TRIPHONE MODELS , we reduced the WORD ERROR RATE by 32 % . \n",
            "this paper presents a new method for SPEECH RECOGNITION OF BROADCAST NEWS COMMENTARY in NEWS COMMENTARY SPEECH . the proposed method is based on the use of LINGUISTIC AND ACOUSTIC FEATURES in the DECODER . the proposed method is based on the use of LINGUISTIC AND ACOUSTIC FEATURES in the DECODER . the proposed method is based on the LINGUISTIC FEATURES OF NEWS COMMENTARIES and the DECODER . the proposed method is compared with conventional CROSSWORD TRIPHONE MODELS . the results show that the proposed method outperforms the conventional ACOUSTIC MODELS in terms of WORD ERROR RATE and WORD ERROR RATE .\n",
            "\n",
            "194 1000\n",
            "this study is a contribution to the field of VISUAL SPEECH PROCESSING . it focuses on the AUTOMATIC EXTRACTION OF SPEECH LIP FEATURES from NATURAL LIPS . the method is based on the direct prediction of these FEATURES from predictors derived from an adequate transformation of the pixels of the lip region of interest . the transformation is made of a 2-D DISCRETE COSINE TRANSFORM combined with a PRINCIPAL COMPONENT ANALYSIS applied to a subset of the DCT COEFFICIENTS corresponding to about 1 % of the total <unk> . the results show the possibility to estimate the GEOMETRIC LIP FEATURES with a good ACCURACY -lrb- a root mean square of 1 to 1.4 mm for the LIP APERTURE and the lip width -rrb- using a reduce set of predictors derived from the PCA . \n",
            "this paper addresses the problem of AUTOMATIC EXTRACTION OF SPEECH LIP FEATURES in VISUAL SPEECH PROCESSING . in this paper , we propose a new method for AUTOMATIC EXTRACTION OF SPEECH LIP FEATURES based on PRINCIPAL COMPONENT ANALYSIS . the proposed method is based on the 2-D DISCRETE COSINE TRANSFORM . the proposed method is based on the 2-D DISCRETE COSINE TRANSFORM of the 2-D DISCRETE COSINE TRANSFORM . the proposed method is based on the 2-D DISCRETE COSINE TRANSFORM . the proposed method is based on the use of the 2-D DISCRETE COSINE TRANSFORM . the proposed method is based on the 2-D DISCRETE COSINE TRANSFORM .\n",
            "\n",
            "195 1000\n",
            "descriptor learning has recently drawn increasing attention in COMPUTER VISION , existing algorithms are mainly developed for CLASSIFICATION rather than for REGRESSION which however has recently emerged as a powerful tool to solve a broad range of problems , e.g. , HEAD POSE ESTIMATION . in this paper , we propose a novel SUPERVISED DESCRIPTOR LEARNING ALGORITHM to establish a DISCRIMINATIVE AND COMPACT FEATURE REPRESENTATION for MULTI-OUTPUT REGRESSION . by formulating as GENERALIZED LOW-RANK APPROXIMATIONS OF MATRICES with a SUPERVISED MANIFOLD REGULARIZATION , the SUPERVISED DESCRIPTOR LEARNING ALGORITHM removes irrelevant and redundant information from RAW FEATURES by transforming into a LOW-DIMENSIONAL SPACE under the SUPERVISION OF MULTIVARIATE TARGETS . the obtained discriminative while compact descriptor largely reduces the variability and AMBIGUITY in MULTI-OUTPUT REGRESSION , and therefore enables more accurate and efficient MULTIVARIATE ESTIMATION . we demonstrate the effectiveness of the proposed SUPERVISED DESCRIPTOR LEARNING ALGORITHM on a REPRESENTATIVE MULTI-OUTPUT REGRESSION TASK : HEAD POSE ESTIMATION using the BENCHMARK POINTING '04 DATASET . experimental results show that the SUPERVISED DESCRIPTOR LEARNING ALGORITHM can achieve high pose ESTIMATION ACCURACY and significantly outperforms state-of-the-art algorithms by an ERROR REDUCTION up to <unk> % . the proposed SUPERVISED DESCRIPTOR LEARNING ALGORITHM provides a general DESCRIPTOR LEARNING FRAMEWORK in a supervised way for MULTI-OUTPUT REGRESSION which can largely boost the performance of existing MULTI-OUTPUT REGRESSION TASKS . \n",
            "this paper presents a SUPERVISED DESCRIPTOR LEARNING ALGORITHM for MULTIVARIATE ESTIMATION . the DESCRIPTOR LEARNING FRAMEWORK is based on the GENERALIZED LOW-RANK APPROXIMATIONS OF MATRICES . the DESCRIPTOR LEARNING FRAMEWORK is based on the GENERALIZED LOW-RANK APPROXIMATIONS OF MATRICES . the SUPERVISED DESCRIPTOR LEARNING ALGORITHM is based on the GENERALIZED LOW-RANK APPROXIMATIONS OF MATRICES . the SUPERVISED DESCRIPTOR LEARNING ALGORITHM is based on the GENERALIZED LOW-RANK APPROXIMATIONS OF MATRICES . the proposed SUPERVISED DESCRIPTOR LEARNING ALGORITHM is based on the GENERALIZED LOW-RANK APPROXIMATIONS OF MATRICES . the proposed SUPERVISED DESCRIPTOR LEARNING ALGORITHM is based on the DESCRIPTOR LEARNING FRAMEWORK . the proposed SUPERVISED DESCRIPTOR LEARNING ALGORITHM is based on the SUPERVISED DESCRIPTOR LEARNING ALGORITHM . the proposed SUPERVISED DESCRIPTOR LEARNING ALGORITHM is evaluated on the BENCHMARK POINTING '04 DATASET . the proposed SUPERVISED DESCRIPTOR LEARNING ALGORITHM is compared with the conventional SUPERVISED DESCRIPTOR LEARNING ALGORITHM and the SUPERVISED DESCRIPTOR LEARNING ALGORITHM .\n",
            "\n",
            "196 1000\n",
            "<unk> an audio mixture containing multiple SIMULTANEOUS BIRD SOUNDS is a challenging task . however , BIRDSONG often contains RAPID PITCH MODULATIONS , and these RAPID PITCH MODULATIONS carry information which may be of use in AUTOMATIC RECOGNITION . in this paper we demonstrate that an improved SPEC-TROGRAM REPRESENTATION , based on the DISTRIBUTION DERIVATIVE METHOD , leads to improved performance of a SEGREGATION ALGORITHM which uses a MARKOV RENEWAL PROCESS MODEL to track VOCALISATION PATTERNS consisting of singing and <unk> . \n",
            "this paper presents a new SEGREGATION ALGORITHM for AUTOMATIC RECOGNITION . the MARKOV RENEWAL PROCESS MODEL is based on the DISTRIBUTION DERIVATIVE METHOD . the SEGREGATION ALGORITHM is based on the DISTRIBUTION DERIVATIVE METHOD . the proposed SEGREGATION ALGORITHM is based on the DISTRIBUTION DERIVATIVE METHOD . the proposed SEGREGATION ALGORITHM is based on the DISTRIBUTION DERIVATIVE METHOD . the proposed SEGREGATION ALGORITHM is based on the DISTRIBUTION DERIVATIVE METHOD . the proposed SEGREGATION ALGORITHM is based on the DISTRIBUTION DERIVATIVE METHOD .\n",
            "\n",
            "197 1000\n",
            "we present a novel approach to represent TRANSIENTS using SPECTRAL-DOMAIN AMPLITUDE-MODULATED/FREQUENCY-MODULATED FUNCTIONS . the model is applied to the real and imaginary parts of the fourier transform -lrb- <unk> -rrb- of the transient . the suitability of the model lies in the observation that since TRANSIENTS are <unk> in time , the real and imaginary parts of the FOURIER SPECTRUM have a MODULATION STRUCTURE . the SPECTRAL AM is the ENVELOPE and the SPECTRAL FM is the GROUP DELAY FUNCTION . the GROUP DELAY is estimated using SPECTRAL ZERO-CROSSINGS and the SPECTRAL ENVELOPE is estimated using a COHERENT DEMODULATOR . we show that the proposed technique is robust to ADDITIVE NOISE . we present applications of the proposed technique to <unk> and <unk> in speech . \n",
            "this paper addresses the problem of ADDITIVE NOISE in ADDITIVE NOISE . in this paper , we propose a new method for ADDITIVE NOISE , which is based on the GROUP DELAY FUNCTION and the GROUP DELAY FUNCTION . the proposed method is based on the GROUP DELAY FUNCTION and the MODULATION STRUCTURE . the proposed method is based on the GROUP DELAY FUNCTION and the MODULATION STRUCTURE . the proposed method is based on the FOURIER SPECTRUM and the MODULATION STRUCTURE . the proposed method is compared with other SPECTRAL-DOMAIN AMPLITUDE-MODULATED/FREQUENCY-MODULATED FUNCTIONS and is shown to be robust to ADDITIVE NOISE and ADDITIVE NOISE .\n",
            "\n",
            "198 1000\n",
            "mixed probabilistic and deterministic graphical models are ubiquitous in REAL-WORLD APPLICATIONS . unfortunately , GIBBS SAMPLING , a popular MCMC TECHNIQUE , does not converge to the correct answers in presence of DETERMINISM and therefore can not be used for INFERENCE in such models . in this paper , we propose to remedy this problem by combining GIBBS SAMPLING with SAMPLESEARCH , an advanced IMPORTANCE SAMPLING TECHNIQUE which leverages complete SAT/CSP SOLVERS to generate high quality samples from HARD DETERMINISTIC SPACES . we call the resulting algorithm , SAMPLESEARCH . unlike GIBBS SAMPLING which yields UNWEIGHTED SAMPLES , SAMPLESEARCH yields weighted samples . computing these weights exactly can be computationally expensive and therefore we propose several approximations . we show that our APPROXIMATE WEIGHTING SCHEMES yield consistent estimates and demonstrate experimentally that SAMPLESEARCH is competitive in terms of ACCURACY with state-of-the-art algorithms such as SAMPLESEARCH , MC-SAT and BELIEF PROPAGATION . \n",
            "this paper addresses the problem of INFERENCE in HARD DETERMINISTIC SPACES . we propose a new MCMC TECHNIQUE , called GIBBS SAMPLING , which is based on GIBBS SAMPLING and GIBBS SAMPLING . the proposed MCMC TECHNIQUE is based on the use of GIBBS SAMPLING and GIBBS SAMPLING . the proposed MCMC TECHNIQUE is based on the IMPORTANCE SAMPLING TECHNIQUE and the IMPORTANCE SAMPLING TECHNIQUE . experimental results on REAL-WORLD APPLICATIONS show that the proposed method achieves better performance than the conventional MCMC TECHNIQUE .\n",
            "\n",
            "199 1000\n",
            "the goal of LOW-LEVEL VISION is to estimate an underlying scene , given an OBSERVED IMAGE . REAL-WORLD SCENES -lrb- eg , <unk> or shapes -rrb- can be very complex , conventionally requiring HIGH DIMENSIONAL REPRESENTATIONS which are hard to estimate and store . we propose a LOW-DIMENSIONAL REPRESENTATION , called a SCENE RECIPE , that relies on the image itself to describe the COMPLEX SCENE CONFIGURATIONS . SHAPE RECIPES are an example : these are the REGRESSION COEFFICIENTS that predict the BANDPASSED SHAPE from IMAGE DATA . we describe the benefits of this LOW-DIMENSIONAL REPRESENTATION , and show two uses illustrating their properties : -lrb- 1 -rrb- we improve STEREO SHAPE ESTIMATES by learning shape recipes at low resolution and applying STEREO SHAPE ESTIMATES at full resolution ; -lrb- 2 -rrb- SHAPE RECIPES implicitly contain information about lighting and materials and we use STEREO SHAPE ESTIMATES for MATERIAL SEGMENTATION . \n",
            "this paper presents a method for MATERIAL SEGMENTATION from IMAGE DATA . the method is based on a LOW-DIMENSIONAL REPRESENTATION of the OBSERVED IMAGE . the method is based on a LOW-DIMENSIONAL REPRESENTATION of the OBSERVED IMAGE . the proposed method is based on a LOW-DIMENSIONAL REPRESENTATION of the OBSERVED IMAGE . the proposed method is based on a LOW-DIMENSIONAL REPRESENTATION of the OBSERVED IMAGE . the proposed method is based on a LOW-DIMENSIONAL REPRESENTATION of the OBSERVED IMAGE .\n",
            "\n",
            "200 1000\n",
            "the GUITAR FEEDBACK EFFECT , or HOWLING , is well known to the general public and identified with many <unk> music genres and it is the only case of ACOUSTIC FEEDBACK employed for musical purposes . VIRTUAL ACOUSTIC FEEDBACK , is regarded as the extension of this phenomenon to any INSTRUMENT OR SOUND SOURCE by means of VIRTUAL ACOUSTICS and is meant to enrich the SOUND PALETTE of a musician . the study of the ACOUSTIC FEEDBACK as a MUSICAL TOOL and COMPUTATIONAL TECHNIQUES for its <unk> have been <unk> addressed in literature . in this paper a NONLINEAR FEEDBACK OSCILLATOR is proposed and its properties derived . the NONLINEAR FEEDBACK OSCILLATOR does not necessarily need to be connected to a VIRTUAL INSTRUMENT , thus enables to process any kind of PITCHED REAL-TIME INPUT . \n",
            "this paper presents a new method for PITCHED REAL-TIME INPUT . the method is based on a NONLINEAR FEEDBACK OSCILLATOR and a NONLINEAR FEEDBACK OSCILLATOR . the proposed method is based on a NONLINEAR FEEDBACK OSCILLATOR and a NONLINEAR FEEDBACK OSCILLATOR . the proposed method is based on the NONLINEAR FEEDBACK OSCILLATOR and the MUSICAL TOOL . the proposed method is based on the NONLINEAR FEEDBACK OSCILLATOR and the MUSICAL TOOL . the proposed method is compared with the conventional COMPUTATIONAL TECHNIQUES and COMPUTATIONAL TECHNIQUES .\n",
            "\n",
            "201 1000\n",
            "the ACCURACY of PARSING has <unk> 90 % recently , but this is not high enough to use PARSING results practically in NATURAL LANGUAGE PROCESSING APPLICATIONS such as PARAPHRASE ACQUISITION and RELATION EXTRACTION . we present a method for detecting reliable parses out of the outputs of a single DEPENDENCY PARSER . this technique is also applied to DOMAIN ADAPTATION OF DEPENDENCY PARSING . our goal was to improve the performance of a state-of-the-art DEPENDENCY PARSER on the data set of the domain adaptation track of the CONLL 2007 SHARED TASK , a formidable challenge . \n",
            "this paper presents a new method for DOMAIN ADAPTATION OF DEPENDENCY PARSING . the proposed method is based on the DEPENDENCY PARSER and the DEPENDENCY PARSER . the proposed DEPENDENCY PARSER is based on the DEPENDENCY PARSER and the DEPENDENCY PARSER . experimental results on the CONLL 2007 SHARED TASK show that the proposed DEPENDENCY PARSER achieves a significant improvement in ACCURACY compared to the conventional DEPENDENCY PARSER .\n",
            "\n",
            "202 1000\n",
            "deep convolutional neural networks -lrb- CNNS -rrb- are successfully used in a number of applications . however , their STORAGE AND COMPUTATIONAL REQUIREMENTS have largely prevented their widespread use on MOBILE DEVICES . here we present an effective CNN COMPRESSION APPROACH in the FREQUENCY DOMAIN , which focuses not only on smaller weights but on all the weights and their underlying connections . by treating CONVOLUTIONAL FILTERS as IMAGES , we decompose their representations in the FREQUENCY DOMAIN as common parts -lrb- i.e. , CLUSTER CENTERS -rrb- shared by other similar filters and their individual private parts -lrb- i.e. , individual residuals -rrb- . a large number of LOW-ENERGY FREQUENCY COEFFICIENTS in both parts can be discarded to produce HIGH COMPRESSION without significantly compromising ACCURACY . we relax the computational burden of CONVOLUTION OPERATIONS in CNNS by linearly combining the convolution responses of DISCRETE COSINE TRANSFORM BASES . the COMPRESSION AND SPEED-UP RATIOS of the proposed CNN COMPRESSION APPROACH are thoroughly analyzed and evaluated on BENCHMARK IMAGE DATASETS to demonstrate its superiority over state-of-the-art methods . \n",
            "this paper presents a new CNN COMPRESSION APPROACH based on DEEP CONVOLUTIONAL NEURAL NETWORKS . the proposed CNN COMPRESSION APPROACH is based on the DISCRETE COSINE TRANSFORM BASES of the DEEP CONVOLUTIONAL NEURAL NETWORKS . the proposed CNN COMPRESSION APPROACH is based on the DISCRETE COSINE TRANSFORM BASES of the DISCRETE COSINE TRANSFORM BASES . the proposed CNN COMPRESSION APPROACH is evaluated on the BENCHMARK IMAGE DATASETS and on the BENCHMARK IMAGE DATASETS . experimental results show that the proposed CNN COMPRESSION APPROACH outperforms the conventional CNN COMPRESSION APPROACH in terms of ACCURACY and ACCURACY .\n",
            "\n",
            "203 1000\n",
            "-- TRANSMIT-REFERENCE ULTRA-WIDEBAND SYSTEMS are attractive due to their relatively low COMPLEXITY at both the TRANSMITTER and the receiver . partly , this is achieved by making RESTRICTIVE ASSUMPTIONS such as a FRAME LENGTH which should be much larger than the CHANNEL LENGTH . this limits their use to low data rate applications . in this paper , we lift this restriction and allow INTER-FRAME INTERFERENCE to occur . we propose a suitable SIGNAL PROCESSING DATA MODEL and corresponding RECEIVER ALGORITHMS which take the INTER-FRAME INTERFERENCE into account . the performance of the SIGNAL PROCESSING DATA MODEL are verified using simulations . \n",
            "this paper presents a new method for TRANSMIT-REFERENCE ULTRA-WIDEBAND SYSTEMS that is based on the SIGNAL PROCESSING DATA MODEL . the method is based on a SIGNAL PROCESSING DATA MODEL of the TRANSMITTER . the proposed method is based on the use of RESTRICTIVE ASSUMPTIONS such as the TRANSMITTER , and the COMPLEXITY of the TRANSMIT-REFERENCE ULTRA-WIDEBAND SYSTEMS . the proposed method is compared with other RECEIVER ALGORITHMS such as the SIGNAL PROCESSING DATA MODEL and the RECEIVER ALGORITHMS .\n",
            "\n",
            "204 1000\n",
            "we introduce a new application of ONLINE DIALOGUE ANALYSIS : supporting PEDAGOGICAL ASSESSMENT OF ONLINE Q&A DISCUSSIONS . extending the existing SPEECH ACT FRAMEWORK , we capture common EMOTIONAL EXPRESSIONS that often appear in STUDENT DISCUSSIONS , such as frustration and degree of certainty , and present a viable approach for the CLASSIFICATION . we demonstrate how such dialogue information can be used in analyzing STUDENT DISCUSSIONS and identifying difficulties . in particular , the DIFFICULTY EXPRESSIONS are aligned to DISCUSSION PATTERNS and student performance . we found that frustration occurs more frequently in longer discussions . the students who frequently express frustration tend to get lower <unk> than others . on the other hand , frequency of HIGH CERTAINTY EXPRESSIONS is positively correlated with the performance . we expect such dialogue analyses can become a powerful ASSESSMENT TOOL for instructors and education researchers . \n",
            "this paper presents a new method for ONLINE DIALOGUE ANALYSIS . the proposed method is based on a ASSESSMENT TOOL . the proposed method is based on the PEDAGOGICAL ASSESSMENT OF ONLINE Q&A DISCUSSIONS . the proposed method is based on a ASSESSMENT TOOL . the proposed method is based on the PEDAGOGICAL ASSESSMENT OF ONLINE Q&A DISCUSSIONS . the proposed method is based on the PEDAGOGICAL ASSESSMENT OF ONLINE Q&A DISCUSSIONS . the proposed method is based on the PEDAGOGICAL ASSESSMENT OF ONLINE Q&A DISCUSSIONS . the experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "205 1000\n",
            "we present a novel TRANSLATION MODEL , which simultaneously exploits the CONSTITUENCY AND DEPENDENCY TREES on the source side , to combine the advantages of two types of trees . we take HEAD-DEPENDENTS RELATIONS OF DEPENDENCY TREES as backbone and incorporate PHRASAL NODES OF CONSTITUENCY TREES as the source side of our TRANSLATION RULES , and the target side as strings . our RULES hold the property of long distance reorderings and the compatibility with phrases . large-scale experimental results show that our TRANSLATION MODEL achieves significantly improvements over the <unk> -lrb- <unk> BLEU on average -rrb- and <unk> -lrb- <unk> BLEU on average -rrb- models , which only employ single type of trees , and significantly outperforms the state-of-the-art HIERARCHICAL PHRASE-BASED MODEL -lrb- <unk> BLEU on average -rrb- , on three CHINESE-ENGLISH NIST TEST SETS . \n",
            "this paper presents a new TRANSLATION MODEL for HEAD-DEPENDENTS RELATIONS OF DEPENDENCY TREES . the proposed TRANSLATION MODEL is based on a HIERARCHICAL PHRASE-BASED MODEL . the proposed TRANSLATION MODEL is based on a HIERARCHICAL PHRASE-BASED MODEL . the proposed TRANSLATION MODEL is evaluated on the CHINESE-ENGLISH NIST TEST SETS and compared with the conventional HIERARCHICAL PHRASE-BASED MODEL . the proposed TRANSLATION MODEL outperforms the conventional TRANSLATION MODEL in terms of BLEU and BLEU .\n",
            "\n",
            "206 1000\n",
            "the AUTOMATIC CLASSIFICATION OF ENVIRONMENTAL NOISE SOURCES from their ACOUSTIC SIGNATURES recorded at the microphone of a NOISE MONITORING SYSTEM -lrb- <unk> -rrb- is an active subject of research nowadays . this paper shows how HIDDEN MARKOV MODELS -LRB- HMM 'S -RRB- can be used to build an ENVIRONMENTAL NOISE RECOGNITION SYSTEM based on a TIME-FREQUENCY ANALYSIS of the noise signal . the performance of the proposed HMM-BASED APPROACH is evaluated experimentally for the classification of five types of NOISE EVENTS -lrb- car , TRUCK , MOPED , AIRCRAFT , train -rrb- . the HMM-BASED APPROACH is found to outperform previously proposed CLASSIFIERS based on the AVERAGE SPECTRUM OF NOISE EVENT with more than 95 % of correct classifications . for comparison , a CLASSIFICATION TEST is performed with HUMAN LISTENERS for the same data which shows that the best HMM-BASED APPROACH outper-forms the '' average '' human listener who achieves only <unk> % of correct classification for the same task . \n",
            "this paper presents a novel HMM-BASED APPROACH for AUTOMATIC CLASSIFICATION OF ENVIRONMENTAL NOISE SOURCES . the proposed HMM-BASED APPROACH is based on the AVERAGE SPECTRUM OF NOISE EVENT and the AVERAGE SPECTRUM OF NOISE EVENT of the NOISE MONITORING SYSTEM . the proposed HMM-BASED APPROACH is based on the AVERAGE SPECTRUM OF NOISE EVENT of the HIDDEN MARKOV MODELS -LRB- HMM 'S -RRB- and the ACOUSTIC SIGNATURES . the proposed HMM-BASED APPROACH is compared with the conventional HMM-BASED APPROACH and the HMM-BASED APPROACH . the proposed HMM-BASED APPROACH is compared with the conventional HMM-BASED APPROACH and the HMM-BASED APPROACH .\n",
            "\n",
            "207 1000\n",
            "in this paper , we propose a novel framework to integrate ARTIC-ULATORY FEATURES into HMM-BASED ASR SYSTEM . this is achieved by using POSTERIOR PROBABILITIES of different AFS -lrb- estimated by MULTILAYER PERCEPTRONS -rrb- directly as OBSERVATION FEATURES in KULLBACK-LEIBLER DIVERGENCE BASED HMM SYSTEM . on the TIMIT PHONEME RECOGNITION TASK , the proposed framework yields a PHONEME RECOGNITION ACCURACY of <unk> % which is comparable to KULLBACK-LEIBLER DIVERGENCE BASED HMM SYSTEM using POSTERIOR PROBABILITIES OF PHONEMES as FEATURES -lrb- <unk> % -rrb- . furthermore , a best performance of <unk> % PHONEME RECOGNITION ACCURACY is achieved by jointly modeling AF PROBABILITIES and PHONEME PROBABILITIES as FEATURES . this shows the efficacy and flexibility of the proposed approach . \n",
            "this paper presents a novel HMM-BASED ASR SYSTEM for MULTILAYER PERCEPTRONS . the proposed KULLBACK-LEIBLER DIVERGENCE BASED HMM SYSTEM is based on the POSTERIOR PROBABILITIES OF PHONEMES and the POSTERIOR PROBABILITIES OF PHONEMES . the proposed KULLBACK-LEIBLER DIVERGENCE BASED HMM SYSTEM is based on the POSTERIOR PROBABILITIES OF PHONEMES and the POSTERIOR PROBABILITIES . the proposed KULLBACK-LEIBLER DIVERGENCE BASED HMM SYSTEM is evaluated on the TIMIT PHONEME RECOGNITION TASK . the proposed HMM-BASED ASR SYSTEM is evaluated on the TIMIT PHONEME RECOGNITION TASK and the TIMIT PHONEME RECOGNITION TASK . the proposed HMM-BASED ASR SYSTEM is evaluated on the TIMIT PHONEME RECOGNITION TASK .\n",
            "\n",
            "208 1000\n",
            "we propose a new VECTOR ENCODING SCHEME -LRB- TREE QUAN-TIZATION -RRB- that obtains LOSSY COMPACT CODES for HIGH-DIMENSIONAL VECTORS via TREE-BASED DYNAMIC PROGRAMMING . similarly to several previous schemes such as PRODUCT QUANTIZATION , these PRODUCT QUANTIZATION correspond to CODEWORD NUMBERS within multiple codebooks . we propose an INTEGER PROGRAMMING-BASED OPTIMIZATION that jointly recovers the CODING TREE STRUCTURE and the codebooks by minimizing the COMPRESSION ERROR on a TRAINING DATASET . in the experiments with diverse visual descriptors -lrb- sift , NEURAL CODES , FISHER VECTORS -rrb- , TREE QUANTIZATION is shown to combine FAST ENCODING and state-of-the-art ACCURACY in terms of the COMPRESSION ERROR , the RETRIEVAL PERFORMANCE , and the IMAGE CLASSIFICATION ERROR . \n",
            "this paper addresses the problem of FAST ENCODING in HIGH-DIMENSIONAL VECTORS . we propose a VECTOR ENCODING SCHEME -LRB- TREE QUAN-TIZATION -RRB- based on VECTOR ENCODING SCHEME -LRB- TREE QUAN-TIZATION -RRB- and VECTOR ENCODING SCHEME -LRB- TREE QUAN-TIZATION -RRB- . the proposed method is based on the use of NEURAL CODES and TREE QUANTIZATION . the proposed method is based on the VECTOR ENCODING SCHEME -LRB- TREE QUAN-TIZATION -RRB- and the VECTOR ENCODING SCHEME -LRB- TREE QUAN-TIZATION -RRB- . the proposed method is based on the TRAINING DATASET and the TRAINING DATASET . the proposed method is evaluated on the TRAINING DATASET and the TRAINING DATASET . the proposed method is compared with the conventional TRAINING DATASET and the TRAINING DATASET .\n",
            "\n",
            "209 1000\n",
            "this paper proposes a novel technique of incorporating FACTOR-ANALYSIS-BASED INTER-SESSION VARIABILITY MODELLING in SPEAKER VERIFICATION SYSTEMS that employ CONTINUOUS PROGRESSIVE SPEAKER MODEL ADAPTATION . CONTINUOUS MODEL ADAPTATION involves the use of all encountered trials in the ADAPTATION PROCESS through the assignment of CONFIDENCE MEASURES . the proposed approach incorporates these CONFIDENCE MEASURES in the general statistics used in the ISV MODELLING PROCESS . PROGRESSIVE SVM-BASED CLASSIFICATION was integrated into the system through the <unk> of GMM MEAN SUPERVECTORS . the proposed system demonstrated a gain of 50 % over baseline results when <unk> on the NIST 2005 SRE CORPUS . ADAPTATIVE SCORE NORMALISATION TECHNIQUES were found to be beneficial to both GMM AND SVM CONFIGURATIONS alleviating the detrimental effects of SCORE SHIFT in PROGRESSIVE SYSTEMS . \n",
            "this paper addresses the problem of FACTOR-ANALYSIS-BASED INTER-SESSION VARIABILITY MODELLING for SPEAKER VERIFICATION SYSTEMS . in this paper , we propose a new method for FACTOR-ANALYSIS-BASED INTER-SESSION VARIABILITY MODELLING based on GMM MEAN SUPERVECTORS . the proposed method is based on the use of GMM MEAN SUPERVECTORS for FACTOR-ANALYSIS-BASED INTER-SESSION VARIABILITY MODELLING . the proposed method is based on the use of CONFIDENCE MEASURES as a ADAPTATION PROCESS . the proposed method is based on the use of GMM MEAN SUPERVECTORS . the proposed method is based on the use of GMM MEAN SUPERVECTORS . the proposed method is based on the use of GMM MEAN SUPERVECTORS for FACTOR-ANALYSIS-BASED INTER-SESSION VARIABILITY MODELLING . experimental results on NIST 2005 SRE CORPUS show that the proposed method is effective for FACTOR-ANALYSIS-BASED INTER-SESSION VARIABILITY MODELLING .\n",
            "\n",
            "210 1000\n",
            "we propose to include SYNCHRONY EFFECTS , known to exist in the AUDITORY SYSTEM , to represent VOICED PARTS OF THE SPEECH SIGNAL in a robust way . the system decomposes the input signal by means of a BAND-PASS FILTER BANK , and utilizes a bank of phase <unk> loops -lrb- <unk> -rrb- to obtain information on the frequencies present at a specific time . this information about the FREQUENCY DISTRIBUTION is transformed into a SPECTRAL-LIKE REPRESENTATION based on SYNCHRONY EFFECTS . NOISY SPEECH RECOGNITION experiments are performed using this SYNCHRONY-BASED SPECTRUM , which is transformed into a small set of COEFFICIENTS by using a transformation similar to that utilized for MEL CEPSTRUM FEATURES . we show that RECOGNITION performance compared to MEL CEPSTRUM FEATURES is advantageous , when measured over a range of SNR CONDITIONS , especially in the HIGH NOISE LEVEL CASE . \n",
            "this paper addresses the problem of NOISY SPEECH RECOGNITION in NOISY SPEECH RECOGNITION . the AUDITORY SYSTEM is based on a BAND-PASS FILTER BANK , which is a SPECTRAL-LIKE REPRESENTATION of the VOICED PARTS OF THE SPEECH SIGNAL . the proposed method is based on a BAND-PASS FILTER BANK , which is based on the BAND-PASS FILTER BANK . the proposed method is based on a BAND-PASS FILTER BANK , which is based on MEL CEPSTRUM FEATURES . the proposed method is based on a BAND-PASS FILTER BANK , which is based on the BAND-PASS FILTER BANK . the proposed method is tested on a HIGH NOISE LEVEL CASE .\n",
            "\n",
            "211 1000\n",
            "we consider the problem of quickly localizing multiple targets by asking questions of the form '' how many targets are within this set '' while obtaining noisy answers . this setting is a generalization to multiple targets of the game of 20 questions in which only a single target is queried . we assume that the targets are points on the real line , or in a two dimensional plane for the experiments , drawn independently from a KNOWN DISTRIBUTION . we evaluate the performance of a policy using the expected entropy of the POSTERIOR DISTRIBUTION after a fixed number of questions with noisy answers . we derive a lower bound for the value of this problem and study a specific policy , named the DYADIC POLICY . we show that this policy achieves a value which is no more than twice this lower bound when answers are <unk> , and show a more general CONSTANT FACTOR APPROXIMATION GUARANTEE for the NOISY SETTING . we present an empirical evaluation of this policy on SIMULATED DATA for the problem of detecting multiple instances of the same object in an image . finally , we present experiments on LOCALIZING MULTIPLE FACES simultaneously on REAL IMAGES . \n",
            "this paper presents a new method for LOCALIZING MULTIPLE FACES based on CONSTANT FACTOR APPROXIMATION GUARANTEE . the proposed method is based on a CONSTANT FACTOR APPROXIMATION GUARANTEE , which is based on the CONSTANT FACTOR APPROXIMATION GUARANTEE . the proposed method is based on a CONSTANT FACTOR APPROXIMATION GUARANTEE , which is based on the CONSTANT FACTOR APPROXIMATION GUARANTEE . the proposed method is based on a CONSTANT FACTOR APPROXIMATION GUARANTEE , which is based on a CONSTANT FACTOR APPROXIMATION GUARANTEE .\n",
            "\n",
            "212 1000\n",
            "in this paper , we address the problem of precisely recovering the 3-D POSE OF 3-D SHAPE FIDUCIALS from IMAGES obtained by means of an UNCALIBRATED COMPUTED TOMOGRA-PHY IMAGING DEVICE . the main goal in this work is to model and estimate the GEOMETRIC TRANSFORMATION relating line <unk> to their projections in CROSS-SECTIONAL IMAGES . to do so , we propose techniques which solve the points to lines correspondence using CLOSED-FORM AND NUMERICAL ALGORITHMS . a GEOMETRIC TRANSFORMATION with eight degrees of FREEDOM -LRB- ROTATION , TRANSLATION and ANISOTROPIC SCALING -rrb- is used to model both a RIGID-BODY TRANSFORMATION and a SCALING TRANSFORMATION ACCOUNTING for CT SCAN INTRINSIC PARAMETERS . furthermore , an ESTIMATION OF ERROR BOUNDS in space is given when IMAGE DATA are affected by NOISE . real experiments show that the proposed method provides good results on a set of CT IMAGES from many viewpoints . \n",
            "this paper presents a new method for 3-D POSE OF 3-D SHAPE FIDUCIALS in CT IMAGES . the proposed method is based on the SCALING TRANSFORMATION ACCOUNTING and SCALING TRANSFORMATION ACCOUNTING . the proposed method is based on the 3-D POSE OF 3-D SHAPE FIDUCIALS and the GEOMETRIC TRANSFORMATION . the proposed method is based on the 3-D POSE OF 3-D SHAPE FIDUCIALS and the ESTIMATION OF ERROR BOUNDS . the proposed method is based on the 3-D POSE OF 3-D SHAPE FIDUCIALS and the ESTIMATION OF ERROR BOUNDS . the proposed method is compared with the conventional CLOSED-FORM AND NUMERICAL ALGORITHMS and the CLOSED-FORM AND NUMERICAL ALGORITHMS .\n",
            "\n",
            "213 1000\n",
            "the phonetic translation of <unk> speech -lrb- cs -rrb- gestures needs to mix the MANUAL CS INFORMATION together with the lips , taking into account the desynchronization delay -lrb- <unk> et al. -lsb- 2 -rsb- , <unk> et al. -lsb- 4 -rsb- -rrb- between these two flows of information . this contribution focuses on the LIP FLOW MODELING in the case of FRENCH VOWELS . previously , CLASSIFICATION MODELS have been developed for a PROFESSIONAL NORMAL-HEARING CS SPEAKER -lrb- <unk> et al. , -lsb- 7 -rsb- -rrb- . these CLASSIFICATION MODELS are used as a reference . in this study , we process the case of a DEAF CS SPEAKER and discuss the possibilities of CLASSIFICATION . the best performance -lrb- <unk> % -rrb- is obtained with the adaptation of the DEAF DATA to the REFERENCE MODELS . \n",
            "this paper addresses the problem of CLASSIFICATION in FRENCH VOWELS . we propose a method for LIP FLOW MODELING based on LIP FLOW MODELING . the proposed method is based on the use of MANUAL CS INFORMATION in the PROFESSIONAL NORMAL-HEARING CS SPEAKER . the proposed method is based on the use of MANUAL CS INFORMATION and the MANUAL CS INFORMATION . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "214 1000\n",
            "the classical way of ENCODING PREFERENCES in DECISION THEORY is by means of UTILITY OR VALUE FUNCTIONS . however agents are not always able to deliver such a function directly . in this paper , we relate three different ways of specifying preferences , namely by means of a set of particular types of CONSTRAINTS on the UTILITY FUNCTION , by means of an ordered set of PRIORITIZED GOALS expressed by LOGICAL PROPOSITIONS , and by means of an ordered set of subsets of possible candidates reaching the same level of satisfaction . these different EXPRESSION MODES can be handled in a WEIGHTED LOGICAL SETTING , here the one of POSSIBILISTIC LOGIC . the AGGREGATION OF PREFERENCES pertaining to different criteria can then be handled by fusing sets of PRIORITIZED GOALS . apart from a better EXPRESSIVITY , the benefits of a LOGICAL REPRESENTATION OF PREFERENCES are to put them in a suitable format for REASONING PURPOSES , or for modifying or revising them . \n",
            "this paper addresses the problem of REASONING PURPOSES in REASONING PURPOSES . we propose a DECISION THEORY based on the LOGICAL REPRESENTATION OF PREFERENCES of the UTILITY FUNCTION . the proposed method is based on the LOGICAL REPRESENTATION OF PREFERENCES of the DECISION THEORY . the proposed method is based on the LOGICAL REPRESENTATION OF PREFERENCES of the DECISION THEORY . the proposed method is based on the LOGICAL REPRESENTATION OF PREFERENCES of the DECISION THEORY . the proposed method is based on the AGGREGATION OF PREFERENCES . the proposed method is based on the LOGICAL REPRESENTATION OF PREFERENCES and the AGGREGATION OF PREFERENCES .\n",
            "\n",
            "215 1000\n",
            "we consider the problem of designing a LINEAR TRANSFORMATION such as to achieve MINIMUM BAYES ERROR -lrb- or probability of misclassification -rrb- . two avenues will be explored : the first is to maximize the cents - average divergence between the CLASS DENSITIES and the second is to minimize the union <unk> bound in the range of cents . while both approaches yield similar performance in practice , they out-perform standard LDA FEATURES and show a 10 % relative improvement in the WORD ERROR RATE over state-of-the-art CEPSTRAL FEATURES on a large vocabulary telephony speech recognition task . \n",
            "this paper proposes a new method for estimating the parameters of the LINEAR TRANSFORMATION . the proposed method is based on the use of the LDA FEATURES . the proposed method is based on the use of the LDA FEATURES and the LDA FEATURES . experimental results show that the proposed method outperforms the conventional LDA FEATURES in terms of WORD ERROR RATE and WORD ERROR RATE .\n",
            "\n",
            "216 1000\n",
            "we address the problem of MINIMUM MEAN-SQUARED ERROR ESTIMATION where the ESTIMATOR is constrained to belong to a <unk> set of functions . we derive a simple CLOSED FORM FORMULA that reveals the structure of the RESTRICTED ESTIMATOR for a wide class of constraints . using this CLOSED FORM FORMULA we study various types of CONSTRAINED ESTIMATION PROBLEMS that arise commonly in the ELDS OF SIGNAL PROCESSING and communication . \n",
            "this paper presents a new method for MINIMUM MEAN-SQUARED ERROR ESTIMATION based on the RESTRICTED ESTIMATOR . the proposed method is based on a CLOSED FORM FORMULA . the proposed method is based on the RESTRICTED ESTIMATOR . the proposed method is based on the RESTRICTED ESTIMATOR . the proposed method is compared with the conventional ESTIMATOR .\n",
            "\n",
            "217 1000\n",
            "an important problem in IMAGE LABELING concerns learning with IMAGES labeled at varying levels of SPECIFICITY . we propose an approach that can incorporate IMAGES with labels drawn from a SEMANTIC HIERARCHY , and can also readily cope with MISSING LABELS , and ROUGHLY-SPECIFIED OBJECT BOUNDARIES . we introduce a new form of LATENT TOPIC MODEL , learning a novel CONTEXT REPRESENTATION in the JOINT LABEL-AND-IMAGE SPACE by capturing CO-OCCURRING PATTERNS within and between IMAGE FEATURES and object labels . given a topic , the LATENT TOPIC MODEL generates the INPUT DATA , as well as a TOPIC-DEPENDENT PROBABILISTIC CLASSIFIER to predict labels for IMAGE REGIONS . we present results on two REAL-WORLD DATASETS , demonstrating significant improvements gained by including the COARSELY LABELED IMAGES . \n",
            "this paper presents a method for IMAGE LABELING in IMAGES . the LATENT TOPIC MODEL is based on the LATENT TOPIC MODEL . the LATENT TOPIC MODEL is based on the LATENT TOPIC MODEL and the LATENT TOPIC MODEL . the proposed method is based on the LATENT TOPIC MODEL and the LATENT TOPIC MODEL . the proposed method is based on the LATENT TOPIC MODEL . the proposed method is based on the LATENT TOPIC MODEL . the proposed method is based on the LATENT TOPIC MODEL . the proposed method is based on the LATENT TOPIC MODEL . the proposed method is based on the LATENT TOPIC MODEL . the proposed method is based on the LATENT TOPIC MODEL . the proposed method is based on the LATENT TOPIC MODEL and the LATENT TOPIC MODEL .\n",
            "\n",
            "218 1000\n",
            "in this paper , we present a complete platform for the SEMIAUTOMATIC GENERATION OF HUMAN-MACHINE DIALOG SYSTEMS , that using as input a description of the database of the service , a FLOW MODEL with the different states of the final application and a GUIDED INTERACTION step by step with the DESIGNER 'S INTERVENTION , generates dialogs to access the SERVICE DATA in different languages and two modalities , SPEECH AND WEB , simultaneously . we describe in detail several strategies that have been followed to reduce the time needed to do the design using the mentioned information . we also address important issues in DIALOG APPLICATIONS as mixed initiative and OVERANSWERING DIALOGS , CONFIRMATION HANDLING and how to provide the user long lists of information . \n",
            "this paper addresses the problem of SEMIAUTOMATIC GENERATION OF HUMAN-MACHINE DIALOG SYSTEMS in DIALOG APPLICATIONS , such as CONFIRMATION HANDLING , CONFIRMATION HANDLING , and SEMIAUTOMATIC GENERATION OF HUMAN-MACHINE DIALOG SYSTEMS . we propose a method for the SEMIAUTOMATIC GENERATION OF HUMAN-MACHINE DIALOG SYSTEMS . the proposed method is based on the FLOW MODEL and the FLOW MODEL . the experimental results show that the proposed method is robust and robust to DESIGNER 'S INTERVENTION such as CONFIRMATION HANDLING , CONFIRMATION HANDLING , and CONFIRMATION HANDLING .\n",
            "\n",
            "219 1000\n",
            "a NON-PARAMETRIC BAYESIAN MODEL is proposed for processing multiple images . the analysis employs IMAGE FEATURES and , when present , the words associated with accompanying annotations . the NON-PARAMETRIC BAYESIAN MODEL clusters the images into classes , and each image is segmented into a set of objects , also allowing the opportunity to assign a word to each object -lrb- localized labeling -rrb- . each object is assumed to be represented as a HETEROGENEOUS MIX OF COMPONENTS , with this realized via MIXTURE MODELS linking IMAGE FEATURES to OBJECT TYPES . the number of IMAGE CLASSES , number of OBJECT TYPES , and the characteristics of the OBJECT-FEATURE MIXTURE MODELS are inferred nonparametrically . to constitute SPATIALLY CONTIGUOUS OBJECTS , a new LOGISTIC STICK-BREAKING PROCESS is developed . INFERENCE is performed efficiently via VARIATIONAL BAYESIAN ANALYSIS , with example results presented on two IMAGE DATABASES . \n",
            "this paper addresses the problem of INFERENCE in IMAGE DATABASES . we propose a method for INFERENCE based on VARIATIONAL BAYESIAN ANALYSIS . the NON-PARAMETRIC BAYESIAN MODEL is based on the NON-PARAMETRIC BAYESIAN MODEL . the proposed NON-PARAMETRIC BAYESIAN MODEL is based on the NON-PARAMETRIC BAYESIAN MODEL . the proposed NON-PARAMETRIC BAYESIAN MODEL is based on the NON-PARAMETRIC BAYESIAN MODEL . the proposed NON-PARAMETRIC BAYESIAN MODEL is based on the NON-PARAMETRIC BAYESIAN MODEL . the proposed NON-PARAMETRIC BAYESIAN MODEL is based on the NON-PARAMETRIC BAYESIAN MODEL .\n",
            "\n",
            "220 1000\n",
            "this paper deals with NOISY PHASE MONOCOMPONENT SIGNALS in ADDITIVE NOISE . this model is more appropriate for REAL WORLD APPLICATIONS in particular for RADAR AND COMMUNICATIONS . the problem is introduced and a MAXIMUM LIKELIHOOD SOLUTION is proposed . specifically , the CRAMÈR-RAO BOUND is explicitly derived and compared to the case of NOISE FREE PHASE . \n",
            "this paper addresses the problem of NOISY PHASE MONOCOMPONENT SIGNALS in the presence of ADDITIVE NOISE . in this paper , we propose a method for NOISY PHASE MONOCOMPONENT SIGNALS in the NOISE FREE PHASE . the proposed method is based on the use of NOISY PHASE MONOCOMPONENT SIGNALS in the NOISE FREE PHASE . experimental results show that the proposed method outperforms the conventional method in terms of ADDITIVE NOISE .\n",
            "\n",
            "221 1000\n",
            "localization and synchronization are critical challenges for a WIRELESS NETWORK , which are conventionally solved independently . recently , various estimators have been proposed to jointly synchronize and localize a NODE in a STATIC NETWORK based on two way communication . in this paper , we present a novel and generic model based on two way communication between nodes , which are in RELATIVE MOTION with respect to each other . furthermore , for the entire STATIC NETWORK we propose a closed form extended global least squares -lrb- <unk> -rrb- solution to solve for all the UNKNOWN CLOCK SKEWS , CLOCK OFFSETS , INITIAL PAIRWISE DISTANCES and RELATIVE RADIAL VELOCITIES using a SINGLE CLOCK REFERENCE within the STATIC NETWORK . a new CRAMER RAO BOUND is derived and the proposed fusion center based extended global least squares -lrb- <unk> -rrb- solution is shown to be asymptotically optimal . \n",
            "this paper presents a new method for LOCALIZATION AND SYNCHRONIZATION in WIRELESS NETWORK . the proposed method is based on the CRAMER RAO BOUND and the CRAMER RAO BOUND . the proposed method is based on the CRAMER RAO BOUND and the CRAMER RAO BOUND . the proposed method is based on the CRAMER RAO BOUND and the CRAMER RAO BOUND . the proposed method is compared with the conventional CRAMER RAO BOUND and the CRAMER RAO BOUND .\n",
            "\n",
            "222 1000\n",
            "the COMPACT DESCRIPTION OF A VIDEO SEQUENCE through a single IMAGE MAP and a DOMINANT MOTION has applications in several domains , including VIDEO BROWSING AND RETRIEVAL , COMPRESSION , MOSAICING , and VISUAL SUMMARIZATION . building such a COMPACT DESCRIPTION OF A VIDEO SEQUENCE requires the capability to register all the frames with respect to the dominant object in the scene , a task which has been , in the past , addressed through TEMPORALLY LOCALIZED MOTION ESTIMATES . in this paper , we show how the lack of TEMPORAL CONSISTENCY associated with such estimates can <unk> the validity of the DOMINANT MOTION ASSUMPTION , leading to <unk> between different scene interpretations and poor registration . to avoid this <unk> , we augment the MOTION MODEL with a GENERIC TEMPORAL CONSTRAINT which increases the ROBUSTNESS against competing interpretations , leading to more meaningful CONTENT SUMMARIZATION . \n",
            "this paper presents a new MOTION MODEL for VIDEO BROWSING AND RETRIEVAL . the MOTION MODEL is based on the DOMINANT MOTION ASSUMPTION and the DOMINANT MOTION ASSUMPTION . the MOTION MODEL is based on the DOMINANT MOTION ASSUMPTION and the DOMINANT MOTION ASSUMPTION . the ROBUSTNESS of the proposed MOTION MODEL is compared with the conventional MOTION MODEL and the MOTION MODEL . the ROBUSTNESS of the proposed MOTION MODEL is compared with the conventional MOTION MODEL and the MOTION MODEL .\n",
            "\n",
            "223 1000\n",
            "expert finding for QUESTION ANSWERING is a challenging QUESTION ANSWERING in COMMUNITY-BASED QUESTION ANSWERING SITE , arising in many applications such as QUESTION ROUTING and the identification of best answers . in order to provide high-quality experts , many existing approaches learn the USER MODEL mainly from their past QUESTION-ANSWERING ACTIVITIES in cqa sites , which suffer from the spar-sity QUESTION ANSWERING of CQA DATA . in this paper , we consider the QUESTION ANSWERING of expert finding from the viewpoint of LEARNING RANKING METRIC EMBEDDING . we propose a novel RANKING METRIC NETWORK LEARNING FRAMEWORK for expert finding by exploiting both users ' RELATIVE QUALITY RANK to given questions and their SOCIAL RELATIONS . we then develop a RANDOM-WALK BASED LEARNING METHOD with RECURRENT NEURAL NETWORKS for RANKING METRIC NETWORK EMBEDDING . the extensive experiments on a LARGE-SCALE DATASET from a real world cqa site show that our RANDOM-WALK BASED LEARNING METHOD achieves better performance than other state-of-the-art solutions to the QUESTION ANSWERING . \n",
            "this paper presents a RANDOM-WALK BASED LEARNING METHOD for QUESTION ANSWERING . the RANKING METRIC NETWORK LEARNING FRAMEWORK is based on a RANKING METRIC NETWORK LEARNING FRAMEWORK . the proposed RANKING METRIC NETWORK LEARNING FRAMEWORK is based on the LEARNING RANKING METRIC EMBEDDING . the proposed RANDOM-WALK BASED LEARNING METHOD is based on the LEARNING RANKING METRIC EMBEDDING . the proposed RANDOM-WALK BASED LEARNING METHOD is based on the LEARNING RANKING METRIC EMBEDDING . the proposed RANDOM-WALK BASED LEARNING METHOD is based on the LEARNING RANKING METRIC EMBEDDING . the proposed RANDOM-WALK BASED LEARNING METHOD is evaluated on the LARGE-SCALE DATASET . the experimental results show that the proposed RANDOM-WALK BASED LEARNING METHOD outperforms the conventional RANDOM-WALK BASED LEARNING METHOD in terms of RELATIVE QUALITY RANK and RELATIVE QUALITY RANK .\n",
            "\n",
            "224 1000\n",
            "in this work , the novel task of DETECTING DELETIONS within automatic speech recognition -lrb- asr -rrb- system output is investigated . DELETION-INFORMED CONFIDENCE ESTIMATION is proposed as an approach which simultaneously yields a CONFIDENCE SCORE in a word being correct , as well as a DELETION CONFIDENCE SCORE which indicates whether a DELETION is likely to occur in the output . the sequential nature of CONDITIONAL RANDOM FIELD MODELS is exploited as a means through which this can be achieved . it is shown that this SEQUENCE STRUCTURE is crucial in yielding useful DELETION DETECTION SCORES , with an equivalent NON-SEQUENTIAL MODEL proven to be unsuitable for the task . the DELETION-INFORMED CONFIDENCE ESTIMATION APPROACH is also shown to outperform one where DELETION CONFIDENCE SCORES are estimated as a CLASSIFICATION TASK separate from that of OVERALL CONFIDENCE ESTIMATION . \n",
            "this paper proposes a new method for DETECTING DELETIONS based on CONDITIONAL RANDOM FIELD MODELS . the proposed method is based on a DELETION-INFORMED CONFIDENCE ESTIMATION APPROACH , which is based on the DELETION CONFIDENCE SCORE of the CONDITIONAL RANDOM FIELD MODELS . the proposed method is based on a DELETION-INFORMED CONFIDENCE ESTIMATION APPROACH . the proposed method is based on a DELETION-INFORMED CONFIDENCE ESTIMATION APPROACH . the proposed method is based on the DELETION-INFORMED CONFIDENCE ESTIMATION APPROACH . the proposed method is based on the DELETION-INFORMED CONFIDENCE ESTIMATION APPROACH . the proposed method is based on the DELETION-INFORMED CONFIDENCE ESTIMATION APPROACH . the proposed method is based on the DELETION-INFORMED CONFIDENCE ESTIMATION APPROACH . the proposed method is based on the DELETION-INFORMED CONFIDENCE ESTIMATION APPROACH . the proposed method is compared with the conventional DELETION-INFORMED CONFIDENCE ESTIMATION APPROACH .\n",
            "\n",
            "225 1000\n",
            "hybrid <unk> combine multiple types of reasoning , usually subsumption and <unk> resolution . we outline a system which combines NATURAL DEDUCTION and SUBSUMPTION REASONING using INFERENCE GRAPHS implementing a LOGIC OF ARBITRARY AND INDEFINITE OBJECTS . \n",
            "this paper addresses the problem of LOGIC OF ARBITRARY AND INDEFINITE OBJECTS for LOGIC OF ARBITRARY AND INDEFINITE OBJECTS . we propose a method for SUBSUMPTION REASONING that is based on HYBRID REASONERS and SUBSUMPTION REASONING . we demonstrate the effectiveness of our approach on a variety of INFERENCE GRAPHS and SUBSUMPTION REASONING .\n",
            "\n",
            "226 1000\n",
            "active learning can lead to a dramatic reduction in labeling effort . however , in many practical implementations -lrb- such as CROWDSOURCING , surveys , high-throughput experimental design -rrb- , it is preferable to query labels for batches of examples to be labelled in parallel . while several HEURISTICS have been proposed for BATCH-MODE ACTIVE LEARNING , little is known about their theoretical performance . we consider BATCH MODE ACTIVE LEARNING and more general <unk> stochastic optimization problems that exhibit ADAPTIVE SUBMODULARITY , a NATURAL DIMINISHING RETURNS CONDITION . we prove that for such problems , a simple GREEDY STRATEGY is competitive with the OPTIMAL BATCH-MODE POLICY . in some cases , surprisingly , the use of batches incurs competitively low cost , even when compared to a fully SEQUENTIAL STRATEGY . we demonstrate the effectiveness of our approach on BATCH-MODE ACTIVE LEARNING TASKS , where it outperforms the state of the art , as well as the novel problem of MULTI-STAGE INFLUENCE MAXIMIZATION in SOCIAL NETWORKS . \n",
            "this paper addresses the problem of MULTI-STAGE INFLUENCE MAXIMIZATION in SOCIAL NETWORKS . in this paper , we propose a new SEQUENTIAL STRATEGY based on the GREEDY STRATEGY . the proposed method is based on the use of MULTI-STAGE INFLUENCE MAXIMIZATION in SOCIAL NETWORKS . the proposed method is based on the use of MULTI-STAGE INFLUENCE MAXIMIZATION in SOCIAL NETWORKS . the proposed method is based on the GREEDY STRATEGY . the proposed method is based on the GREEDY STRATEGY . the proposed method is based on the SEQUENTIAL STRATEGY . the proposed method is based on the GREEDY STRATEGY . the experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "227 1000\n",
            "in this paper , we investigate DETECTION AND LOCALIZATION OF GENERAL 3D OBJECT CLASSES by relating LOCAL SCALE-INVARIANT FEATURES to a VIEWPOINT INVARIANT REFERENCE FRAME . this can generally be achieved by either a MULTI-VIEW REPRESENTATION , where FEATURES and REFERENCE FRAME are modeled as a collection of distinct views , or by a VIEWPOINT INVARIANT REPRESENTATION , where FEATURES and REFERENCE FRAME are mod-eled independently of VIEWPOINT . we compare MULTI-VIEW AND VIEWPOINT INVARIANT REPRESENTATIONS trained and tested on the same data , where the VIEWPOINT INVARIANT APPROACH results in fewer FALSE POSITIVE DETECTIONS and higher AVERAGE PRECISION . we present a new , ITERATIVE LEARNING ALGORITHM to determine an optimal VIEWPOINT INVARIANT REFERENCE FRAME from training images in a DATA-DRIVEN MANNER . the learned optimal REFERENCE FRAME is centrally located with respect to the 3D OBJECT CLASS and to IMAGE FEATURES in a given view , thereby minimizing REFERENCE FRAME LOCALIZATION ERROR as predicted by theory and maintaining a CONSISTENT GEOMETRICAL INTERPRETATION with respect to the underlying OBJECT CLASS . MODELING AND DETECTION based on the optimal REFERENCE FRAME improves DETECTION performance for both MULTIVIEW AND VIEWPOINT INVARIANT REPRESENTATIONS . experimentation is performed on the CLASS OF 3D FACES , using the PUBLIC COLOR FERET DATABASE for training , the CMU PROFILE DATABASE for testing and sift IMAGE FEATURES . \n",
            "this paper presents a VIEWPOINT INVARIANT APPROACH for DETECTION AND LOCALIZATION OF GENERAL 3D OBJECT CLASSES . the proposed ITERATIVE LEARNING ALGORITHM is based on a VIEWPOINT INVARIANT APPROACH and a VIEWPOINT INVARIANT REPRESENTATION . the proposed ITERATIVE LEARNING ALGORITHM is based on the VIEWPOINT INVARIANT APPROACH and the ITERATIVE LEARNING ALGORITHM . the proposed ITERATIVE LEARNING ALGORITHM is based on the VIEWPOINT INVARIANT APPROACH and the ITERATIVE LEARNING ALGORITHM . the proposed ITERATIVE LEARNING ALGORITHM is based on a VIEWPOINT INVARIANT APPROACH and a VIEWPOINT INVARIANT REPRESENTATION . the proposed ITERATIVE LEARNING ALGORITHM is evaluated on a PUBLIC COLOR FERET DATABASE and a PUBLIC COLOR FERET DATABASE . the results show that the proposed ITERATIVE LEARNING ALGORITHM is robust and robust to FALSE POSITIVE DETECTIONS and AVERAGE PRECISION .\n",
            "\n",
            "228 1000\n",
            "confusion matrices have been widely used to increase the ACCURACY of SPEECH RECOGNISERS , but usually a MEAN CONFUSION MATRIX , averaged over many speakers , is used . however , analysis shows that CONFUSION MATRICES for individual speakers vary considerably , and so there is benefit in obtaining estimates of CONFUSION MATRICES for individual speakers . unfortunately , there is rarely enough data to make reliable estimates . we present a technique for estimating the elements of a SPEAKER 'S CONFUSION MATRIX given only SPARSE DATA from the speaker . it utilizes NON-NEGATIVE MATRIX FACTORISATION to find structure within CONFUSION MATRICES , and this structure is exploited to make improved estimates . results show that under certain conditions , this technique can give estimates that are as good as those obtained with twice the number of utterances available from the speaker . \n",
            "this paper addresses the problem of SPEECH RECOGNISERS for SPEECH RECOGNISERS . we show that the MEAN CONFUSION MATRIX of the SPEAKER 'S CONFUSION MATRIX can be reduced to the MEAN CONFUSION MATRIX of the SPEAKER 'S CONFUSION MATRIX . we also propose a method for estimating the parameters of the MEAN CONFUSION MATRIX . we show that the proposed method can be applied to SPEECH RECOGNISERS with a MEAN CONFUSION MATRIX .\n",
            "\n",
            "229 1000\n",
            "this paper presents our work on '' SNACK , '' a LOW-DIMENSIONAL CONCEPT EMBEDDING ALGORITHM that combines HUMAN EXPERTISE with AUTOMATIC MACHINE SIMILARITY KERNELS . both parts are complimentary : HUMAN INSIGHT can capture relationships that are not apparent from the object 's visual similarity and the machine can help relieve the human from having to exhaustively specify many constraints . we show that our SNACK EMBEDDINGS are useful in several tasks : distinguishing PRIME AND NONPRIME NUMBERS on MNIST , discovering labeling mistakes in the CALTECH UCSD BIRDS DATASET with the help of DEEP-LEARNED FEATURES , creating TRAINING DATASETS for BIRD CLASSIFIERS , capturing SUBJECTIVE HUMAN TASTE on a new dataset of 10,000 <unk> , and qualitatively exploring an UNSTRUCTURED SET OF PICTOGRAPHIC CHARACTERS . comparisons with the state-of-the-art in these tasks show that SNACK produces better CONCEPT EMBEDDINGS that require less HUMAN SUPERVISION than the leading methods . \n",
            "this paper addresses the problem of AUTOMATIC MACHINE SIMILARITY KERNELS in MNIST . we propose a new LOW-DIMENSIONAL CONCEPT EMBEDDING ALGORITHM based on CONCEPT EMBEDDINGS and CONCEPT EMBEDDINGS . the proposed method is based on a LOW-DIMENSIONAL CONCEPT EMBEDDING ALGORITHM and a LOW-DIMENSIONAL CONCEPT EMBEDDING ALGORITHM . the proposed method is based on a LOW-DIMENSIONAL CONCEPT EMBEDDING ALGORITHM and a LOW-DIMENSIONAL CONCEPT EMBEDDING ALGORITHM . the proposed method is based on a LOW-DIMENSIONAL CONCEPT EMBEDDING ALGORITHM and a LOW-DIMENSIONAL CONCEPT EMBEDDING ALGORITHM . the proposed method is based on a LOW-DIMENSIONAL CONCEPT EMBEDDING ALGORITHM and a LOW-DIMENSIONAL CONCEPT EMBEDDING ALGORITHM . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "230 1000\n",
            "we present a NEURAL NETWORK SIMULATION which we implemented on the massively parallel connection machine 2 . in contrast to previous work , this NEURAL NETWORK SIMULATION is based on BIOLOGICALLY REALISTIC NEU-RONS with NONTRIVIAL SINGLE-CELL DYNAMICS , high connectivity with a STRUCTURE modelled in agreement with BIOLOGICAL DATA , and preservation of the TEMPORAL DYNAMICS OF SPIKE INTERACTIONS . we simulate NEURAL NETWORKS of <unk> neurons coupled by about 1000 synapses per neuron , and estimate the performance for much larger systems . communication between neurons is identified as the COMPUTATION-ALLY MOST DEMANDING TASK and we present a novel method to overcome this bottleneck . the NEURAL NETWORK SIMULATION has already been used to study the PRIMARY VISUAL SYSTEM of the cat . \n",
            "this paper presents a novel method for TEMPORAL DYNAMICS OF SPIKE INTERACTIONS in NEURAL NETWORKS . the proposed method is based on a NEURAL NETWORK SIMULATION , which is based on a NEURAL NETWORK SIMULATION . the proposed method is based on a NEURAL NETWORK SIMULATION , which is based on the NEURAL NETWORK SIMULATION . the proposed method is based on a NEURAL NETWORK SIMULATION . the proposed method is based on a NEURAL NETWORK SIMULATION . the proposed method is based on a NEURAL NETWORK SIMULATION . the proposed method is evaluated on the COMPUTATION-ALLY MOST DEMANDING TASK .\n",
            "\n",
            "231 1000\n",
            "<unk> ± RB ± ATL is an extension of RB ± ATL where it is possible to model consumption and production of several resources by a set of agents . the MODEL-CHECKING PROBLEM for RB ± ATL is known to be decidable . however the only available MODEL-CHECKING ALGORITHM for RB ± ATL uses a FORWARD SEARCH OF THE STATE SPACE , and hence does not have an efficient SYMBOLIC IMPLEMENTATION . in this paper , we consider a fragment of RB ± ATL , <unk> ± RB ± ATL , that allows only one resource type . we give a SYMBOLIC MODEL-CHECKING ALGORITHM for this fragment of RB ± ATL , and evaluate the performance of an <unk> implementation of the SYMBOLIC MODEL-CHECKING ALGORITHM on an example problem that can be scaled to large state spaces . \n",
            "this paper presents a new MODEL-CHECKING ALGORITHM for RB ± ATL . the MODEL-CHECKING ALGORITHM is based on the MODEL-CHECKING ALGORITHM . the MODEL-CHECKING ALGORITHM is based on the MODEL-CHECKING ALGORITHM . the MODEL-CHECKING ALGORITHM is based on the MODEL-CHECKING ALGORITHM . the MODEL-CHECKING ALGORITHM is applied to the MODEL-CHECKING PROBLEM . the experimental results show the effectiveness of the proposed MODEL-CHECKING ALGORITHM .\n",
            "\n",
            "232 1000\n",
            "this paper explores in detail the use of ERROR CORRECTING OUTPUT CODING for learning TEXT CLASSIFIERS . we show that the ACCURACY of a naive bayes classifier over TEXT CLASSIFICATION TASKS can be significantly improved by taking advantage of the error-correcting properties of the code . we also explore the use of different kinds of codes , namely ERROR-CORRECTING CODES , RANDOM CODES , and domain and <unk> codes and give experimental results for each of them . the ERROR CORRECTING OUTPUT CODING scales well to large data sets with a large number of classes . experiments on a REAL-WORLD DATA SET show a reduction in classification error by up to 66 % over the traditional naive bayes classifier . we also compare our empirical results to <unk> results and find that the two closely agree . \n",
            "this paper addresses the problem of ERROR CORRECTING OUTPUT CODING in TEXT CLASSIFICATION TASKS . we propose a method for ERROR CORRECTING OUTPUT CODING , which is based on RANDOM CODES and RANDOM CODES . we demonstrate the effectiveness of our approach on TEXT CLASSIFICATION TASKS and TEXT CLASSIFICATION TASKS . we also show that our method outperforms state-of-the-art methods in terms of ACCURACY and ACCURACY .\n",
            "\n",
            "233 1000\n",
            "traditional ACOUSTIC SOURCE LOCALIZATION uses a TWO-STEP PROCEDURE requiring INTERMEDIATE TIME-DELAY ESTIMATES from pairs of microphones . an alternative SINGLE-STEP APPROACH is proposed in this paper in which PARTICLE FILTERING is used to estimate the SOURCE LOCATION through STEERED BEAMFORMING . this SINGLE-STEP APPROACH is especially attractive in SPEECH ENHANCEMENT APPLICATIONS , where the LOCALIZATION ESTIMATES are typically used to steer a BEAMFORMER at a later stage . simulation results show that the SINGLE-STEP APPROACH is robust to REVERBERATION , and is able to accurately follow the SOURCE TRAJECTORY . \n",
            "this paper presents a TWO-STEP PROCEDURE for ACOUSTIC SOURCE LOCALIZATION based on PARTICLE FILTERING . the SINGLE-STEP APPROACH is based on the SINGLE-STEP APPROACH . the SINGLE-STEP APPROACH is based on a TWO-STEP PROCEDURE . the BEAMFORMER is based on a TWO-STEP PROCEDURE . the proposed SINGLE-STEP APPROACH is based on a TWO-STEP PROCEDURE , which is based on the SINGLE-STEP APPROACH . the proposed SINGLE-STEP APPROACH is based on a TWO-STEP PROCEDURE . the proposed SINGLE-STEP APPROACH is based on a TWO-STEP PROCEDURE and is shown to be useful for SPEECH ENHANCEMENT APPLICATIONS .\n",
            "\n",
            "234 1000\n",
            "this paper introduces a DIAGNOSIS SCHEME of a RAILWAY INFRASTRUCTURE COMPONENT based on a combined use of EMPIRICAL MODE DECOMPOSITION and HILBERT TRANSFORM . this RAILWAY INFRASTRUCTURE COMPONENT is dedicated to TRACK/VEHICLE TRANSMISSION referred as TRACK CIRCUIT . the aim is to detect its working state from one MEASUREMENT SIGNAL which can be viewed as a superposition of several oscillations and PERIODIC PATTERNS called INTRINSIC MODE FUNCTIONS . for this application , it will be shown that PHYSICAL MEANING can be assigned to each mode that EMD tries to extract . furthermore , when the HILBERT TRANSFORM of the INTRINSIC MODE FUNCTIONS is performed , we show that the changing of INSTANTANEOUS FREQUENCY can be linked to the existence of defect . the performances are illustrated on both SIMULATED AND EXPERIMENTAL SIGNALS . \n",
            "this paper presents a novel DIAGNOSIS SCHEME for TRACK/VEHICLE TRANSMISSION . the proposed DIAGNOSIS SCHEME is based on the HILBERT TRANSFORM of the HILBERT TRANSFORM . the proposed DIAGNOSIS SCHEME is based on the HILBERT TRANSFORM . the proposed DIAGNOSIS SCHEME is based on the HILBERT TRANSFORM . the proposed DIAGNOSIS SCHEME is based on the HILBERT TRANSFORM . the proposed DIAGNOSIS SCHEME is based on the HILBERT TRANSFORM . the proposed DIAGNOSIS SCHEME is based on the HILBERT TRANSFORM . the proposed DIAGNOSIS SCHEME is based on the HILBERT TRANSFORM . the proposed DIAGNOSIS SCHEME is based on the HILBERT TRANSFORM .\n",
            "\n",
            "235 1000\n",
            "we present a VOICE MORPHING STRATEGY that can be used to generate a CONTINUUM OF ACCENT TRANSFORMATIONS between a FOREIGN SPEAKER and a NATIVE SPEAKER . the VOICE MORPHING STRATEGY performs a cepstral decomposition of speech into SPECTRAL SLOPE and SPECTRAL DETAIL . ACCENT CONVERSIONS are then generated by combining the SPECTRAL SLOPE of the FOREIGN SPEAKER with a morph of the SPECTRAL DETAIL of the NATIVE SPEAKER . SPECTRAL MORPHING is achieved by representing the SPECTRAL DETAIL through PULSE DENSITY MODULATION and AVERAGING PULSES in a PAIR-WISE FASHION . the VOICE MORPHING STRATEGY is validated on PARALLEL RECORDINGS from two ARCTIC SPEAKERS using both objective and subjective measures of ACOUSTIC QUALITY , SPEAKER IDENTITY and FOREIGN ACCENT . \n",
            "this paper presents a VOICE MORPHING STRATEGY for ARCTIC SPEAKERS . the VOICE MORPHING STRATEGY is based on a VOICE MORPHING STRATEGY and a VOICE MORPHING STRATEGY . the proposed VOICE MORPHING STRATEGY is based on a VOICE MORPHING STRATEGY and a VOICE MORPHING STRATEGY . the proposed VOICE MORPHING STRATEGY is based on a VOICE MORPHING STRATEGY and a VOICE MORPHING STRATEGY . the ACOUSTIC QUALITY of the proposed VOICE MORPHING STRATEGY is demonstrated by the use of PARALLEL RECORDINGS and ARCTIC SPEAKERS . the ACOUSTIC QUALITY of the proposed method is demonstrated by simulations .\n",
            "\n",
            "236 1000\n",
            "problem context open problem : exact <unk> nearest neighbor search in HAMMING DISTANCE on BINARY CODES . \n",
            "this paper presents a new method for the PROBLEM CONTEXT OPEN PROBLEM in BINARY CODES . the proposed method is based on the PROBLEM CONTEXT OPEN PROBLEM in the presence of BINARY CODES . the proposed method is applied to the PROBLEM CONTEXT OPEN PROBLEM in BINARY CODES .\n",
            "\n",
            "237 1000\n",
            "for the purpose of efficient AUDIO CODING at LOW RATES , a new BIT ALLOCATION STRATEGY is proposed in this paper . the basic idea behind this BIT ALLOCATION STRATEGY is '' give bits to the band with the maximum <unk> '' or '' retrieve bits from the band with the maximum <unk> '' . the notion of '' <unk> efficiency '' is suggested and BIT ALLOCATION STRATEGY can be employed to construct a BIT ASSIGNMENT ALGORITHM operated at BAND-LEVEL as compared to the traditional FRAME-LEVEL BIT ASSIGNMENT METHODS . based on this BIT ASSIGNMENT ALGORITHM a new BIT ALLOCATION STRATEGY , called MAX-BNLR SCHEME , is designed for the MPEG-4 AAC . simulation results show that the performance of the MAX-BNLR SCHEME is significantly better than that of the MPEG-4 AAC VERIFICATION MODEL and is close to that of TB-ANMR -lsb- 3 -rsb- , which is the -lrb- nearly -rrb- optimal solution . moreover , the MAX-BNLR SCHEME has the advantages of LOW COMPUTATIONAL COMPLEXITY comparing to TB-ANMR . \n",
            "this paper proposes a new MPEG-4 AAC VERIFICATION MODEL for AUDIO CODING . the proposed MPEG-4 AAC VERIFICATION MODEL is based on the BIT ASSIGNMENT ALGORITHM , which is based on the BIT ASSIGNMENT ALGORITHM . the proposed BIT ALLOCATION STRATEGY is based on the BIT ASSIGNMENT ALGORITHM and the BIT ASSIGNMENT ALGORITHM . the proposed MPEG-4 AAC VERIFICATION MODEL is compared with the conventional MAX-BNLR SCHEME . the proposed MPEG-4 AAC VERIFICATION MODEL is compared with the conventional MAX-BNLR SCHEME . the proposed MPEG-4 AAC VERIFICATION MODEL is compared with the conventional MAX-BNLR SCHEME .\n",
            "\n",
            "238 1000\n",
            "hashing has been widely applied to approximate nearest neighbor search for LARGE-SCALE MULTIMEDIA RETRIEVAL . SUPERVISED HASHING improves the quality of HASH CODING by exploiting the SEMANTIC SIMILARITY on DATA PAIRS and has received increasing attention recently . for most existing SUPERVISED HASHING METHODS for IMAGE RETRIEVAL , an image is first represented as a vector of HAND-CRAFTED OR MACHINE-LEARNED FEATURES , then quantized by a separate QUANTIZATION STEP that generates BINARY CODES . however , SUBOPTIMAL HASH CODING may be produced , since the QUANTIZATION ERROR is not statistically minimized and the QUANTIZATION STEP is not optimally compatible with the HASH CODING . in this paper , we propose a novel DEEP QUANTIZATION NETWORK ARCHITECTURE for SUPERVISED HASHING , which learns IMAGE REPRESENTATION for HASH CODING and formally control the QUANTIZATION ERROR . the DEEP QUANTIZATION NETWORK ARCHITECTURE constitutes four key components : -lrb- 1 -rrb- a SUB-NETWORK with multiple CONVOLUTION-POOLING LAYERS to capture DEEP IMAGE REPRESENTATIONS ; -lrb- 2 -rrb- a fully connected bottleneck layer to generate DIMENSION-REDUCED REPRESENTATION optimal for HASH CODING ; -lrb- 3 -rrb- a PAIRWISE COSINE LOSS LAYER for SIMILARITY-PRESERVING LEARNING ; and -lrb- 4 -rrb- a PRODUCT QUANTIZA-TION LOSS for controlling HASHING QUALITY and the <unk> of BOTTLENECK REPRESENTATION . extensive experiments on standard IMAGE RETRIEVAL DATASETS show the proposed DEEP QUANTIZATION NETWORK ARCHITECTURE yields substantial boosts over latest state-of-the-art HASHING METHODS . \n",
            "this paper presents a new DEEP QUANTIZATION NETWORK ARCHITECTURE for LARGE-SCALE MULTIMEDIA RETRIEVAL . the DEEP QUANTIZATION NETWORK ARCHITECTURE is based on a DEEP QUANTIZATION NETWORK ARCHITECTURE and a PRODUCT QUANTIZA-TION LOSS based on the PAIRWISE COSINE LOSS LAYER . the proposed DEEP QUANTIZATION NETWORK ARCHITECTURE is based on the BOTTLENECK REPRESENTATION , which is based on the PAIRWISE COSINE LOSS LAYER . the proposed DEEP QUANTIZATION NETWORK ARCHITECTURE is based on the BOTTLENECK REPRESENTATION and the DEEP QUANTIZATION NETWORK ARCHITECTURE . the proposed DEEP QUANTIZATION NETWORK ARCHITECTURE is based on the PAIRWISE COSINE LOSS LAYER and the DEEP QUANTIZATION NETWORK ARCHITECTURE . the proposed DEEP QUANTIZATION NETWORK ARCHITECTURE is compared with conventional SUPERVISED HASHING METHODS and the DEEP QUANTIZATION NETWORK ARCHITECTURE . the proposed DEEP QUANTIZATION NETWORK ARCHITECTURE is compared with conventional HASHING METHODS and the DEEP QUANTIZATION NETWORK ARCHITECTURE .\n",
            "\n",
            "239 1000\n",
            "we address the problem of segmenting and RECOGNIZING OBJECTS in REAL WORLD IMAGES , focusing on challenging ARTICULATED CATEGORIES such as humans and other animals . for this purpose , we propose a novel design for REGION-BASED OBJECT DETECTORS that integrates efficiently top-down information from SCANNING-WINDOWS PART MODELS and GLOBAL APPEARANCE CUES . our detectors produce CLASS-SPECIFIC SCORES for BOTTOM-UP REGIONS , and then aggregate the votes of multiple overlapping candidates through PIXEL CLASSIFICATION . we evaluate our approach on the PASCAL SEGMENTATION CHALLENGE , and report competitive performance with respect to current leading techniques . on VOC2010 , our method obtains the best results in <unk> categories and the highest performance on ARTICULATED OBJECTS . \n",
            "this paper presents a method for RECOGNIZING OBJECTS from REAL WORLD IMAGES . the method is based on the use of SCANNING-WINDOWS PART MODELS and SCANNING-WINDOWS PART MODELS . the proposed method is based on the use of SCANNING-WINDOWS PART MODELS and SCANNING-WINDOWS PART MODELS . the proposed method is based on the use of SCANNING-WINDOWS PART MODELS and SCANNING-WINDOWS PART MODELS . the proposed method is based on SCANNING-WINDOWS PART MODELS and SCANNING-WINDOWS PART MODELS . the proposed method is evaluated on REAL WORLD IMAGES and REAL WORLD IMAGES .\n",
            "\n",
            "240 1000\n",
            "this paper presents a LOW-POWER BIT-SERIAL VITERBI DECODER CHIP with the CODING RATE r = 1 = 3 and the constraint length k = 9 -lrb- 256 states -rrb- . this LOW-POWER BIT-SERIAL VITERBI DECODER CHIP has been implemented using 0.5 m THREE-LAYER METAL CMOS TECHNOLOGY and is targeted for HIGH SPEED CONVOLU-TIONAL DECODING for next generation wireless applications such as WIDE-BAND CDMA MOBILE SYSTEMS and WIRELESS ATM LANS . the LOW-POWER BIT-SERIAL VITERBI DECODER CHIP is expected to operate at <unk> under 3.3 v and at <unk> under 1.8 v . the ADD-COMPARE-SELECT UNITS have been designed using BIT-SERIAL ARITHMETIC , which has made it feasible to execute 256 ACS OPERATIONS in parallel . for TRACE-BACK OPERATIONS , we have developed a novel POWER-EFFICIENT TRACE-BACK SCHEME and an APPLICATION-SPECIFIC MEMORY , which was designed considering that 256 bits should be written simultaneously for write operations but only one bit needs to be accessed for read operations . we have estimated that the LOW-POWER BIT-SERIAL VITERBI DECODER CHIP dissipates only <unk> at 2MBPS OPERATION under 1.8 v. \n",
            "this paper presents a POWER-EFFICIENT TRACE-BACK SCHEME for HIGH SPEED CONVOLU-TIONAL DECODING . the LOW-POWER BIT-SERIAL VITERBI DECODER CHIP is based on the LOW-POWER BIT-SERIAL VITERBI DECODER CHIP and the LOW-POWER BIT-SERIAL VITERBI DECODER CHIP . the LOW-POWER BIT-SERIAL VITERBI DECODER CHIP is based on the LOW-POWER BIT-SERIAL VITERBI DECODER CHIP and the POWER-EFFICIENT TRACE-BACK SCHEME . the proposed POWER-EFFICIENT TRACE-BACK SCHEME is based on the LOW-POWER BIT-SERIAL VITERBI DECODER CHIP and the POWER-EFFICIENT TRACE-BACK SCHEME . the CODING RATE of the proposed POWER-EFFICIENT TRACE-BACK SCHEME is demonstrated by simulation results .\n",
            "\n",
            "241 1000\n",
            "in current SPEECH RECOGNITION SYSTEMS mainly SHORT-TIME FOURIER TRANSFORM BASED FEATURES like MFCC are applied . dropping the short-time stationarity assumption of the VOICED SPEECH , this paper introduces the NON-STATIONARY SIGNAL ANALYSIS into the ASR FRAMEWORK . we present new ACOUSTIC FEATURES extracted by a PITCH-ADAPTIVE GAMMATONE FILTER BANK . the NOISE ROBUSTNESS was proved on aurora 2 and 4 tasks , where the proposed ACOUSTIC FEATURES outperform the standard MFCC . furthermore , successful combination experiments via MFCC indicate the differences between the new ACOUSTIC FEATURES and MFCC . \n",
            "this paper presents a new method for NON-STATIONARY SIGNAL ANALYSIS . the PITCH-ADAPTIVE GAMMATONE FILTER BANK is based on a PITCH-ADAPTIVE GAMMATONE FILTER BANK and a PITCH-ADAPTIVE GAMMATONE FILTER BANK . the proposed ASR FRAMEWORK is based on the PITCH-ADAPTIVE GAMMATONE FILTER BANK and the SHORT-TIME FOURIER TRANSFORM BASED FEATURES . experimental results show that the proposed ASR FRAMEWORK outperforms the conventional MFCC and the MFCC .\n",
            "\n",
            "242 1000\n",
            "statistical approaches for building NON-RIGID DEFORMABLE MODELS , such as the ACTIVE APPEARANCE MODEL , have enjoyed great popularity in recent years , but typically require tedious MANUAL ANNOTATION OF TRAINING IMAGES . in this paper , a LEARNING BASED APPROACH for the AUTOMATIC ANNOTATION OF VISUALLY DEFORMABLE OBJECTS from a single ANNOTATED FRONTAL IMAGE is presented and demonstrated on the example of AUTOMATICALLY ANNOTATING FACE IMAGES that can be used for building ACTIVE APPEARANCE MODEL for FITTING and TRACKING . this LEARNING BASED APPROACH employs the idea of initially learning the CORRESPONDENCES BETWEEN LANDMARKS in a FRONTAL IMAGE and a set of training images with a face in arbitrary poses . using this learner , VIRTUAL IMAGES OF UNSEEN FACES at any ARBITRARY POSE for which the learner was trained can be reconstructed by predicting the new LANDMARK LOCATIONS and warping the TEXTURE from the FRONTAL IMAGE . VIEW-BASED AAMS are then built from the VIRTUAL IMAGES and used for automatically annotating UNSEEN IMAGES , including images of different facial expressions , at any random pose within the MAXIMUM RANGE spanned by the virtually RECONSTRUCTED IMAGES . the LEARNING BASED APPROACH is experimentally validated by AUTOMATICALLY ANNOTATING FACE IMAGES from three different databases . \n",
            "this paper presents a LEARNING BASED APPROACH for AUTOMATICALLY ANNOTATING FACE IMAGES . the LEARNING BASED APPROACH is based on a LEARNING BASED APPROACH and a LEARNING BASED APPROACH . the ACTIVE APPEARANCE MODEL is based on a LEARNING BASED APPROACH and a LEARNING BASED APPROACH . the ACTIVE APPEARANCE MODEL is based on a LEARNING BASED APPROACH , which is based on a LEARNING BASED APPROACH . the proposed LEARNING BASED APPROACH is based on a LEARNING BASED APPROACH and a LEARNING BASED APPROACH for the AUTOMATIC ANNOTATION OF VISUALLY DEFORMABLE OBJECTS . the LEARNING BASED APPROACH is applied to AUTOMATICALLY ANNOTATING FACE IMAGES , and the results show that the proposed LEARNING BASED APPROACH is robust to ARBITRARY POSE and is robust to ARBITRARY POSE and ARBITRARY POSE .\n",
            "\n",
            "243 1000\n",
            "in this paper , we present a new method for FACIAL AGE ESTIMATION based on ORDINAL DISCRIMINATIVE FEATURE LEARNING . considering the temporally ordinal and continuous characteristic of AGING PROCESS , the proposed method not only aims at preserving the LOCAL MANIFOLD STRUCTURE OF FACIAL IMAGES , but also it wants to keep the ORDINAL INFORMATION among AGING FACES . moreover , we try to remove REDUNDANT INFORMATION from both the LOCALITY INFORMATION and ORDINAL INFORMATION as much as possible by minimizing NONLINEAR CORRELATION and RANK CORRELATION . finally , we formulate these two issues into a unified optimization problem of FEATURE SELECTION and present an efficient solution . the experiments are conducted on the PUBLIC AVAILABLE IMAGES OF GROUPS DATASET and the FG-NET DATASET , and the experimental results demonstrate the power of the proposed method against the state-of-the-art methods . \n",
            "this paper addresses the problem of FACIAL AGE ESTIMATION in the presence of AGING FACES . we propose a method for FACIAL AGE ESTIMATION based on ORDINAL DISCRIMINATIVE FEATURE LEARNING . the proposed method is based on the LOCAL MANIFOLD STRUCTURE OF FACIAL IMAGES and the LOCALITY INFORMATION . the proposed method is based on the LOCAL MANIFOLD STRUCTURE OF FACIAL IMAGES and the RANK CORRELATION . the proposed method is based on the use of the LOCALITY INFORMATION and the LOCALITY INFORMATION . the proposed method is compared with the conventional FG-NET DATASET and the FG-NET DATASET .\n",
            "\n",
            "244 1000\n",
            "we examine the SET COVERING MACHINE when SET COVERING MACHINE uses DATA-DEPENDENT HALF-SPACES for its set of FEATURES and bound its GENERALIZATION ERROR in terms of the number of training errors and the number of <unk> SET COVERING MACHINE achieves on the training data . we show that SET COVERING MACHINE provides a favorable alternative to DATA-DEPENDENT BALLS on some NATURAL DATA SETS . compared to the SUPPORT VECTOR MACHINE , the SET COVERING MACHINE with DATA-DEPENDENT HALF-SPACES produces substantially sparser clas-sifiers with comparable -lrb- and sometimes better -rrb- generalization . furthermore , we show that our bound on the GENERALIZATION ERROR provides an effective guide for MODEL SELECTION . \n",
            "this paper addresses the problem of MODEL SELECTION in the presence of DATA-DEPENDENT BALLS . we propose a new SET COVERING MACHINE that is based on the SUPPORT VECTOR MACHINE . the proposed SUPPORT VECTOR MACHINE is based on the SUPPORT VECTOR MACHINE . the proposed SET COVERING MACHINE is compared with the conventional SET COVERING MACHINE and the SET COVERING MACHINE . the experimental results show that the proposed SET COVERING MACHINE outperforms the conventional SET COVERING MACHINE .\n",
            "\n",
            "245 1000\n",
            "in the learning process of SPEECH MODELING , many choices or settings are defined '' a priori '' or are resulting from years of experimental work . in this paper , instead , a GLOBAL LEARNING SCHEME is proposed based on a DISTRIBUTED GENETIC ALGORITHM combined with a standard SPEECH-MODELING ALGORITHM . the SPEECH RECOGNITION MODELS are now created out of a pre-defined SPACE of solutions . furthermore , this GLOBAL LEARNING SCHEME enables to learn the SPEECH MODELS as well as the best FEATURE EXTRACTION MODULE . experimental validation is performed on the task of discovering the WAVELET PACKET BEST BASIS DECOMPOSITION , knowing that the '' a PRIORI '' REFERENCE is the MEL-SCALED SUBBAND DECOMPOSITION . two experiments are presented , a REFERENCE SYSTEM using a SIMULATED FITNESS and a second one that uses the SPEECH RECOGNITION performance as FITNESS VALUE . in the latter , each element of the SPACE is a CONNECTIONIST SYSTEM defined by a WAVELET TOPOLOGY and its associated NEURAL NETWORK . \n",
            "this paper presents a GLOBAL LEARNING SCHEME for SPEECH RECOGNITION . the SPEECH-MODELING ALGORITHM is based on a PRIORI '' REFERENCE and a PRIORI '' REFERENCE . the SPEECH-MODELING ALGORITHM is based on a PRIORI '' REFERENCE and a PRIORI '' REFERENCE . the SPEECH-MODELING ALGORITHM is based on a PRIORI '' REFERENCE and a PRIORI '' REFERENCE . the SPEECH-MODELING ALGORITHM is applied to a PRIORI '' REFERENCE and a PRIORI '' REFERENCE . the SPEECH-MODELING ALGORITHM is applied to a PRIORI '' REFERENCE and a PRIORI '' REFERENCE . the experimental results show that the proposed SPEECH-MODELING ALGORITHM is effective for SPEECH RECOGNITION .\n",
            "\n",
            "246 1000\n",
            "in this paper we report our results concerning the study of multivariate functions of <unk> signals . in particular we show that MULTILINEAR TENSOR FORMS of the decomposed signal yield a CLASS OF LTERS that we propose to call PIECEWISE VOLTERRA FILTERS . a LTER can be viewed as a transformation of r n ! r , where n is the number of LTER TAPS . PWV LTERS PARTITION R N using a HY-PER-RECTANGULAR LATTICE , and assign a VOLTERRA LTER to each of the partition regions . at the PARTITION BOUNDARIES CONTINUITY between the MULTIVARIATE POLYNOMIALS is preserved resulting in class c 0 piecewise polynomials . PIECEWISE VOLTERRA FILTERS constitute an eecient alternative for describing some systems rich in HARD NONLINEAR STRUCTURES , especially since PARAMETER ESTIMATION remains a LINEAR PROBLEM for PWV 'S . \n",
            "this paper presents a new method for PARAMETER ESTIMATION based on PIECEWISE VOLTERRA FILTERS . the proposed method is based on the CLASS OF LTERS of the VOLTERRA LTER . the proposed method is based on the CLASS OF LTERS of the VOLTERRA LTER . the proposed method is based on the CLASS OF LTERS of the VOLTERRA LTER . the proposed method is based on the CLASS OF LTERS of the LTER . the proposed method is based on the PWV LTERS PARTITION R N . the proposed method is based on the PWV LTERS PARTITION R N . the proposed method is based on the PWV LTERS PARTITION R N . the proposed method is based on the PWV LTERS PARTITION R N .\n",
            "\n",
            "247 1000\n",
            "semi-supervised learning , which uses UNLABELED DATA to help learn a DISCRIMINATIVE MODEL , is especially important for STRUCTURED OUTPUT PROBLEMS , as considerably more effort is needed to label its MULTI-DIMENSIONAL OUTPUTS versus standard single output problems . we propose a new MAX-MARGIN FRAMEWORK for SEMI-SUPERVISED STRUCTURED OUTPUT LEARNING , that allows the use of powerful DISCRETE OPTIMIZATION ALGORITHMS and HIGH ORDER REGULAR-IZERS defined directly on MODEL PREDICTIONS for the UNLABELED EXAMPLES . we show that our MAX-MARGIN FRAMEWORK is closely related to POSTERIOR REGULARIZA-TION , and the two MAX-MARGIN FRAMEWORK optimize special cases of the same objective . the new MAX-MARGIN FRAMEWORK is instantiated on two IMAGE SEGMENTATION TASKS , using both a GRAPH REGULARIZER and a CARDINALITY REGULARIZER . experiments also demonstrate that this MAX-MARGIN FRAMEWORK can utilize UNLABELED DATA from a different source than the LABELED DATA to significantly improve performance while saving labeling effort . \n",
            "this paper presents a novel DISCRIMINATIVE MODEL for SEMI-SUPERVISED STRUCTURED OUTPUT LEARNING . the DISCRIMINATIVE MODEL is based on a MAX-MARGIN FRAMEWORK and a DISCRIMINATIVE MODEL . the proposed DISCRIMINATIVE MODEL is based on a DISCRIMINATIVE MODEL and a DISCRIMINATIVE MODEL . the proposed DISCRIMINATIVE MODEL is based on the GRAPH REGULARIZER and the DISCRIMINATIVE MODEL . the proposed DISCRIMINATIVE MODEL is based on a MAX-MARGIN FRAMEWORK and a DISCRIMINATIVE MODEL . the proposed DISCRIMINATIVE MODEL is based on the GRAPH REGULARIZER and the DISCRIMINATIVE MODEL . the proposed DISCRIMINATIVE MODEL is evaluated on the IMAGE SEGMENTATION TASKS . the experimental results show that the proposed DISCRIMINATIVE MODEL is effective for IMAGE SEGMENTATION TASKS .\n",
            "\n",
            "248 1000\n",
            "in this paper , we explore the problem of MULTIVIEW SUB-SPACE CLUSTERING . we introduce a LOW-RANK TENSOR CONSTRAINT to explore the COMPLEMENTARY INFORMATION from multiple views and , accordingly , establish a novel method called low-rank tensor constrained <unk> subspace clustering -lrb- <unk> -rrb- . our method regards the SUBSPACE REPRESENTATION MATRICES of different views as a LOW-RANK TENSOR CONSTRAINT , which captures <unk> the HIGH ORDER CORRELATIONS underlying MULTI-VIEW DATA . then the LOW-RANK TENSOR CONSTRAINT is equipped with a LOW-RANK CONSTRAINT , which models elegantly the cross information among different views , reduces <unk> the redundancy of the learned SUBSPACE REPRESENTATIONS , and improves the ACCURACY of CLUSTERING as well . the INFERENCE PROCESS of the AFFINITY MATRIX for CLUSTERING is formulated as a TEN-SOR NUCLEAR NORM MINIMIZATION PROBLEM , constrained with an additional ℓ 2,1-NORM REGULARIZER and some LINEAR EQUALITIES . the MINIMIZATION PROBLEM is convex and thus can be solved efficiently by an augmented <unk> alternating direction minimization -lrb- <unk> -rrb- method . extensive experimental results on four BENCHMARK IMAGE DATASETS show the effectiveness of the proposed LT-MSC METHOD . \n",
            "this paper presents a new method for MULTIVIEW SUB-SPACE CLUSTERING based on SUBSPACE REPRESENTATION MATRICES . the proposed method is based on a LOW-RANK TENSOR CONSTRAINT and a LOW-RANK CONSTRAINT . the proposed method is based on a LOW-RANK TENSOR CONSTRAINT and a LOW-RANK CONSTRAINT . the proposed method is based on a LOW-RANK TENSOR CONSTRAINT and a LOW-RANK TENSOR CONSTRAINT . the proposed method is based on a LOW-RANK TENSOR CONSTRAINT and a LOW-RANK TENSOR CONSTRAINT . the proposed LT-MSC METHOD is based on a LOW-RANK TENSOR CONSTRAINT and a LOW-RANK CONSTRAINT . the ACCURACY of the proposed LT-MSC METHOD is evaluated using the BENCHMARK IMAGE DATASETS and the BENCHMARK IMAGE DATASETS .\n",
            "\n",
            "249 1000\n",
            "planning landmarks are facts that must be true at some point in every SOLUTION PLAN . previous work has very successfully exploited PLANNING LANDMARKS in SATISFICING PLANNING . we propose a methodology for deriving ADMISSIBLE HEURISTIC ESTIMATES for COST-OPTIMAL PLANNING from a set of PLANNING LANDMARKS . the resulting HEURISTICS fall into a novel class of MULTI-PATH DEPENDENT HEURISTICS , and we present a simple BEST-FIRST SEARCH PROCEDURE exploiting such HEURISTICS . our empirical evaluation shows that this framework favorably competes with the state-of-the-art of COST-OPTIMAL HEURISTIC SEARCH . \n",
            "this paper addresses the problem of COST-OPTIMAL PLANNING in SATISFICING PLANNING . we propose a new BEST-FIRST SEARCH PROCEDURE based on MULTI-PATH DEPENDENT HEURISTICS . the proposed method is based on the use of MULTI-PATH DEPENDENT HEURISTICS to estimate the SOLUTION PLAN . the proposed method is based on the use of MULTI-PATH DEPENDENT HEURISTICS . the proposed method is based on the use of MULTI-PATH DEPENDENT HEURISTICS . the proposed method is based on the use of MULTI-PATH DEPENDENT HEURISTICS . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "250 1000\n",
            "this paper considers the BLIND SEPARATION OF NONSTATIONARY SOURCES in the UNDERDETERMINED CONVOLUTIVE MIXTURE CASE . we introduce two methods based on the sparsity assumption of the sources in the TIME-FREQUENCY DOMAIN . the first one assumes that the sources are DISJOINT in the TF DOMAIN ; i.e. there is at most one source signal present at a given point in the TF DOMAIN . in the second method , we relax this assumption by allowing the sources to be TF-NONDISJOINT to a certain extent . in particular , the number of sources present -lrb- active -rrb- at a TF POINT should be strictly less than the number of sensors . in that case , the BLIND SEPARATION OF NONSTATIONARY SOURCES can be achieved thanks to SUBSPACE PROJECTION which allows us to identify the ACTIVE SOURCES and to estimate their corresponding TIME-FREQUENCY DISTRIBUTION VALUES . \n",
            "this paper presents a method for BLIND SEPARATION OF NONSTATIONARY SOURCES from a TIME-FREQUENCY DOMAIN . the proposed method is based on a SUBSPACE PROJECTION , which is based on the SUBSPACE PROJECTION . the proposed method is based on the SUBSPACE PROJECTION . the proposed method is based on the SUBSPACE PROJECTION . the proposed method is based on the SUBSPACE PROJECTION . the proposed method is based on the SUBSPACE PROJECTION . the proposed method is based on the SUBSPACE PROJECTION .\n",
            "\n",
            "251 1000\n",
            "in this paper , we propose a new RATE CONTROL SCHEME designed for the <unk> high efficiency video coding -lrb- <unk> -rrb- standard , and aimed at enhancing the quality of regions of interest -lrb- roi -rrb- . our RATE CONTROL SCHEME allocates a higher bit rate to the region of interest while keeping the GLOBAL BIT RATE close to the assigned target value . this RATE CONTROL SCHEME is developed for a VIDEOCONFERENCING SYSTEM , where the ROIS -lrb- typically , faces -rrb- are automatically detected and each CODING UNIT is classified in a region of the interest map . this map is given as input to the RATE CONTROL ALGORITHM and the BIT ALLOCATION is made accordingly . experimental results show that the proposed RATE CONTROL SCHEME achieves accurate TARGET BIT RATES and provides an improvement in the REGION OF INTEREST QUALITY , both in OBJECTIVE METRICS and based on SUBJECTIVE QUALITY EVALUATION . \n",
            "this paper presents a new method for BIT ALLOCATION in the VIDEOCONFERENCING SYSTEM . the proposed method is based on the use of OBJECTIVE METRICS as the CODING UNIT . the proposed method is based on the RATE CONTROL ALGORITHM . the proposed method is based on the RATE CONTROL ALGORITHM . the proposed method is based on the RATE CONTROL ALGORITHM . the proposed method is based on the RATE CONTROL ALGORITHM . the proposed method is compared with conventional OBJECTIVE METRICS .\n",
            "\n",
            "252 1000\n",
            "<unk> object BOUNDING BOX is a simple and popular INTERACTION PARADIGM considered by many existing INTERACTIVE IMAGE SEGMENTATION FRAMEWORKS . however , these frameworks tend to exploit the provided BOUNDING BOX merely to exclude its <unk> from consideration and sometimes to initialize the ENERGY MINIMIZATION . in this paper , we discuss how the BOUNDING BOX can be further used to impose a powerful TOPOLOGICAL PRIOR , which prevents the solution from EXCESSIVE SHRINKING and ensures that the USER-PROVIDED BOX bounds the segmentation in a sufficiently tight way . the TOPOLOGICAL PRIOR is expressed using HARD CONSTRAINTS incorporated into the GLOBAL ENERGY MINIMIZATION FRAMEWORK leading to an NP-HARD INTEGER PROGRAM . we then investigate the possible OPTIMIZATION STRATEGIES including LINEAR RELAXATION as well as a new GRAPH CUT ALGORITHM called PINPOINTING . the latter can be used either as a ROUNDING METHOD for the FRACTIONAL LP SOLUTION , which is provably better than THRESHOLDING-BASED ROUNDING , or as a fast STANDALONE HEURISTIC . we evaluate the proposed algorithms on a PUBLICLY AVAILABLE DATASET , and demonstrate the practical benefits of the new TOPOLOGICAL PRIOR both qualitatively and quantitatively . \n",
            "this paper proposes a new ROUNDING METHOD for INTERACTIVE IMAGE SEGMENTATION FRAMEWORKS . the proposed GRAPH CUT ALGORITHM is based on a GRAPH CUT ALGORITHM , which is a STANDALONE HEURISTIC with a FRACTIONAL LP SOLUTION . the proposed ROUNDING METHOD is based on a GRAPH CUT ALGORITHM , which is based on the GRAPH CUT ALGORITHM . the proposed ROUNDING METHOD is based on the GRAPH CUT ALGORITHM . the proposed ROUNDING METHOD is based on the GRAPH CUT ALGORITHM . the proposed ROUNDING METHOD is compared with conventional OPTIMIZATION STRATEGIES such as ENERGY MINIMIZATION . the proposed ROUNDING METHOD is compared with other OPTIMIZATION STRATEGIES such as ENERGY MINIMIZATION . the proposed ROUNDING METHOD is compared with conventional OPTIMIZATION STRATEGIES such as the GLOBAL ENERGY MINIMIZATION FRAMEWORK .\n",
            "\n",
            "253 1000\n",
            "in this paper , the application of the SINUSOIDAL MODEL for AUDIO/SPEECH SIGNALS to the WATERMARKING TASK is proposed . the basic idea is that adequate MODULATION of medium rank partials -lrb- frequency -rrb- trajectories is not perceptible and thus this MODULATION may contain the data to be embedded in the signal . the MODULATION -lrb- encoding -rrb- and estimation -lrb- decoding -rrb- of the message are described in this paper and preliminary promising results are given in the case of SPEECH SIGNALS . \n",
            "this paper presents a new SINUSOIDAL MODEL for SPEECH SIGNALS . the SINUSOIDAL MODEL is based on a SINUSOIDAL MODEL for the WATERMARKING TASK . the SINUSOIDAL MODEL is based on a SINUSOIDAL MODEL . the SINUSOIDAL MODEL is applied to the WATERMARKING TASK .\n",
            "\n",
            "254 1000\n",
            "* <unk> -rrb- <unk> in this paper , a new FILTER that is performing COLOR IMAGE ENHANCEMENT is presented . the FILTER is achieving this through the minimization of a WEIGHTED COST FUNCTION . the weights are determined using potential functions which are calculated in such a way as to convey SPATIAL INFORMATION . application of the proposed FILTER on a REAL BLURRED AND NOISY COLOR IMAGE is performed to verify its ENHANCEMENT CAPABILITIES . \n",
            "this paper presents a FILTER for COLOR IMAGE ENHANCEMENT . the FILTER is based on a WEIGHTED COST FUNCTION and a WEIGHTED COST FUNCTION . the proposed FILTER is based on a WEIGHTED COST FUNCTION , which is based on a WEIGHTED COST FUNCTION .\n",
            "\n",
            "255 1000\n",
            "we address two open theoretical questions in POLICY GRADIENT REINFORCEMENT LEARNING . the first concerns the efficacy of using FUNCTION APPROXIMATION to represent the STATE ACTION VALUE FUNCTION , <unk> theory is presented showing that LINEAR FUNCTION APPROXIMATION REPRESENTATIONS OF Q can degrade the rate of convergence of PERFORMANCE GRADIENT ESTIMATES by a factor of o -lrb- m l -rrb- relative to when no FUNCTION APPROXIMATION of q is used , where m is the number of possible actions and l is the number of BASIS FUNCTIONS in the FUNCTION APPROXIMATION REPRESENTATION . the second concerns the use of a BIAS TERM in estimating the STATE ACTION VALUE FUNCTION . theory is presented showing that a NON-ZERO BIAS TERM can improve the rate of convergence of PERFORMANCE GRADIENT ESTIMATES by o -lrb- 1 − -lrb- <unk> -rrb- -rrb- , where m is the number of possible actions . experimental evidence is presented showing that these theoretical results lead to significant improvement in the convergence properties of POLICY GRADIENT REINFORCEMENT LEARNING ALGORITHMS . \n",
            "this paper proposes a new method for POLICY GRADIENT REINFORCEMENT LEARNING based on a LINEAR FUNCTION APPROXIMATION REPRESENTATIONS OF Q . the proposed method is based on the LINEAR FUNCTION APPROXIMATION REPRESENTATIONS OF Q . the proposed method is based on the LINEAR FUNCTION APPROXIMATION REPRESENTATIONS OF Q . the proposed method is based on the LINEAR FUNCTION APPROXIMATION REPRESENTATIONS OF Q . the proposed method is based on the FUNCTION APPROXIMATION REPRESENTATION . the proposed method is based on the LINEAR FUNCTION APPROXIMATION REPRESENTATIONS OF Q . the proposed method is based on the LINEAR FUNCTION APPROXIMATION REPRESENTATIONS OF Q . the proposed method is based on the LINEAR FUNCTION APPROXIMATION REPRESENTATIONS OF Q . the proposed method is based on the LINEAR FUNCTION APPROXIMATION REPRESENTATIONS OF Q . the proposed method is based on the LINEAR FUNCTION APPROXIMATION REPRESENTATIONS OF Q . the proposed method is based on the FUNCTION APPROXIMATION REPRESENTATION .\n",
            "\n",
            "256 1000\n",
            "good DIALOGUE STRATEGIES in SPOKEN DIALOGUE SYSTEMS help to ensure and maintain MUTUAL UNDERSTANDING and thus play a crucial role in robust CONVERSATIONAL INTERACTION . we focus on CLARIFICATION STRATEGIES and build USER SIMULATIONS which are critical for REINFORCEMENT LEARNING , which is a cheap and principled way to automatically optimise DIALOGUE MANAGEMENT . in this paper we present a novel CLUSTER-BASED TECHNIQUE for building USER SIMULATIONS which show varying , but complete and consistent behaviour with respect to real users . we use this CLUSTER-BASED TECHNIQUE to build USER SIMULATIONS and we also introduce the CLUSTER-BASED USER SIMULATION TECHNIQUE which allows us to evaluate USER SIMULATIONS with respect to these desiderata . we show that the CLUSTER-BASED USER SIMULATION TECHNIQUE performs significantly better -lrb- at p < 0.01 -rrb- than decisions made using either the one most likely action or a RANDOM BASE-LINE . the CLUSTER-BASED USER SIMULATIONS reduce the average error of these other CLUSTER-BASED USER SIMULATION TECHNIQUE by 53 % and 34 % respectively . \n",
            "this paper proposes a new CLUSTER-BASED USER SIMULATION TECHNIQUE for DIALOGUE MANAGEMENT . the proposed CLUSTER-BASED USER SIMULATION TECHNIQUE is based on the use of CLARIFICATION STRATEGIES for DIALOGUE MANAGEMENT . the proposed CLUSTER-BASED USER SIMULATION TECHNIQUE is based on the use of CLARIFICATION STRATEGIES for DIALOGUE MANAGEMENT . the proposed CLUSTER-BASED USER SIMULATION TECHNIQUE is based on the use of CLARIFICATION STRATEGIES for DIALOGUE MANAGEMENT . experimental results show that the proposed CLUSTER-BASED USER SIMULATION TECHNIQUE improves the performance of the proposed CLUSTER-BASED USER SIMULATION TECHNIQUE .\n",
            "\n",
            "257 1000\n",
            "this paper examines the effect of sensor performance on SPEAKER DIARISATION in meetings and investigates the use of more advanced BEAMFORMING TECHNIQUES , beyond the typically employed DELAY-SUM BEAMFORMER , for mitigating the effects of poorer sensor performance . we present SUPER-DIRECTIVE BEAMFORMING and investigate how different time difference of ARRIVAL SMOOTHING and BEAMFORMING TECHNIQUES influence the performance of state-of-the-art DIAR-ISATION SYSTEMS . we produced and transcribed a new CORPUS OF MEETINGS recorded in the INSTRUMENTED MEETING ROOM using a high SNR ANALOGUE and a newly developed low snr DIGITAL MEMS MICROPHONE ARRAY -lrb- <unk> .2 -rrb- . this research demonstrates that DELAY-SUM BEAMFORMER has a significant effect on the DIARISATION ERROR RATE and that simple NOISE REDUCTION and BEAMFORMING SCHEMES suffice to overcome AUDIO SIGNAL DEGRADATION due to the lower snr of modern MEMS MICROPHONES . INDEX TERMS -- speaker <unk> in meetings , DIGITAL MEMS MICROPHONE ARRAY , time difference of arrival -lrb- tdoa -rrb- , <unk> beamforming \n",
            "this paper addresses the problem of NOISE REDUCTION in DIAR-ISATION SYSTEMS . in this paper , we propose a new DELAY-SUM BEAMFORMER based on the DELAY-SUM BEAMFORMER . the proposed method is based on a CORPUS OF MEETINGS , which is based on the DELAY-SUM BEAMFORMER . the proposed method is based on the DELAY-SUM BEAMFORMER and the DELAY-SUM BEAMFORMER . the proposed method is based on the DELAY-SUM BEAMFORMER and the DELAY-SUM BEAMFORMER . the proposed DELAY-SUM BEAMFORMER is evaluated on the CORPUS OF MEETINGS . the results show the effectiveness of the proposed DELAY-SUM BEAMFORMER in terms of DIARISATION ERROR RATE and DIARISATION ERROR RATE .\n",
            "\n",
            "258 1000\n",
            "balancing between COMPUTATIONAL EFFICIENCY and SAMPLE EFFICIENCY is an important goal in REINFORCEMENT LEARNING . TEMPORAL DIFFERENCE LEARNING ALGORITHMS stochastically update the VALUE FUNCTION , with a LINEAR TIME COMPLEXITY in the number of FEATURES , whereas LEAST-SQUARES TEMPORAL DIFFERENCE ALGORITHMS are sample efficient but can be quadratic in the number of FEATURES . in this work , we develop an efficient INCREMENTAL LOW-RANK LSTD -LRB- -RRB- ALGORITHM that progresses towards the goal of better balancing computation and SAMPLE EFFICIENCY . the INCREMENTAL LOW-RANK LSTD -LRB- -RRB- ALGORITHM reduces the COMPUTATION AND STORAGE COMPLEXITY to the number of FEATURES times the chosen RANK PARAMETER while summarizing past samples efficiently to nearly obtain the SAMPLE EFFICIENCY of LSTD . we derive a simulation bound on the solution given by TRUNCATED LOW-RANK APPROXIMATION , illustrating a BIAS-VARIANCE TRADE-OFF dependent on the choice of RANK . we demonstrate that the INCREMENTAL LOW-RANK LSTD -LRB- -RRB- ALGORITHM effectively balances COMPUTATIONAL COMPLEXITY and SAMPLE EFFICIENCY for POLICY EVALUATION in a benchmark task and a HIGH-DIMENSIONAL ENERGY ALLOCATION DOMAIN . \n",
            "this paper proposes a new INCREMENTAL LOW-RANK LSTD -LRB- -RRB- ALGORITHM for REINFORCEMENT LEARNING . the proposed INCREMENTAL LOW-RANK LSTD -LRB- -RRB- ALGORITHM is based on a TRUNCATED LOW-RANK APPROXIMATION and a TRUNCATED LOW-RANK APPROXIMATION . the proposed INCREMENTAL LOW-RANK LSTD -LRB- -RRB- ALGORITHM is based on a TRUNCATED LOW-RANK APPROXIMATION and a TRUNCATED LOW-RANK APPROXIMATION . the proposed INCREMENTAL LOW-RANK LSTD -LRB- -RRB- ALGORITHM is based on the TRUNCATED LOW-RANK APPROXIMATION and the VALUE FUNCTION . the proposed INCREMENTAL LOW-RANK LSTD -LRB- -RRB- ALGORITHM is evaluated on both COMPUTATION AND STORAGE COMPLEXITY and POLICY EVALUATION . the experimental results show that the proposed INCREMENTAL LOW-RANK LSTD -LRB- -RRB- ALGORITHM outperforms the conventional INCREMENTAL LOW-RANK LSTD -LRB- -RRB- ALGORITHM in terms of both COMPUTATION AND STORAGE COMPLEXITY and COMPUTATIONAL EFFICIENCY .\n",
            "\n",
            "259 1000\n",
            "this paper describes the application of EXPLANATION-BASED LEARNING , a MACHINE LEARNING TECHNIQUE , to the SRI CORE LANGUAGE ENGINE , a large scale general purpose natural language analysis system . the idea is to bypass normal morphological , syntactic and -lrb- partly -rrb- semantic processing , for most input sentences , instead using a set of learned MACHINE LEARNING TECHNIQUE . EXPLANATION-BASED LEARNING is used to extract the learned MACHINE LEARNING TECHNIQUE automatically from sample sentences submitted by a user and thus tune the system for that particular user . by indexing the learned MACHINE LEARNING TECHNIQUE efficiently , it is possible to achieve dramatic speed-ups . performance measurements were carried out using a training set of 1500 sentences and a separate test set of 100 sentences , all from the ATIS CORPUS . a set of <unk> learned MACHINE LEARNING TECHNIQUE was derived from the training set . these MACHINE LEARNING TECHNIQUE covered 90 percent of the test sentences and reduced the TOTAL PROCESSING TIME to a third . an overall speed-up of 50 percent was accomplished using a set of only 250 learned MACHINE LEARNING TECHNIQUE . \n",
            "this paper presents a MACHINE LEARNING TECHNIQUE for EXPLANATION-BASED LEARNING . the proposed MACHINE LEARNING TECHNIQUE is based on a MACHINE LEARNING TECHNIQUE and a MACHINE LEARNING TECHNIQUE . the proposed MACHINE LEARNING TECHNIQUE is based on a MACHINE LEARNING TECHNIQUE and a MACHINE LEARNING TECHNIQUE . the experimental results show that the proposed MACHINE LEARNING TECHNIQUE outperforms the conventional MACHINE LEARNING TECHNIQUE in terms of TOTAL PROCESSING TIME and TOTAL PROCESSING TIME .\n",
            "\n",
            "260 1000\n",
            "this paper addresses the problem of merging SPEECH ENHANCEMENT and coding in the context of an AUDITORY MODELING . the NOISY SIGNAL is rst processed by a fast WAVELET PACKET TRANSFORM ALGORITHM to obtain an AUDITORY SPECTRUM , from which a ROUGH MASKING MODEL is estimated . then , this model is used to <unk> a SUBTRACTIVE-TYPE ENHANCEMENT ALGORITHM . the enhanced SPEECH COECIENTS are then encoded in the same TIME-FREQUENCY TRANSFORM DOMAIN using MASKING THRESHOLD CONSTRAINTS for QUANTIZATION NOISE . the advantage of the proposed method is that both enhancement and coding are performed with the transform <unk> , without making use of the additional FFT PROCESSING . \n",
            "this paper addresses the problem of SPEECH ENHANCEMENT in SPEECH ENHANCEMENT . we propose a ROUGH MASKING MODEL based on the WAVELET PACKET TRANSFORM ALGORITHM . the proposed method is based on a ROUGH MASKING MODEL , which is based on the WAVELET PACKET TRANSFORM ALGORITHM . the proposed method is based on the WAVELET PACKET TRANSFORM ALGORITHM . the proposed method is based on the WAVELET PACKET TRANSFORM ALGORITHM . the proposed method is based on the WAVELET PACKET TRANSFORM ALGORITHM . the proposed method is based on the WAVELET PACKET TRANSFORM ALGORITHM . the proposed method is evaluated on the TIME-FREQUENCY TRANSFORM DOMAIN .\n",
            "\n",
            "261 1000\n",
            "the development of an AUTOMATIC SPEECH RECOGNITION SYSTEM for the BILINGUAL MEDIAPARL CORPUS is challenging for several reasons : -lrb- 1 -rrb- REVERBERANT RECORDINGS , -lrb- 2 -rrb- ACCENTED SPEECH , and -lrb- 3 -rrb- no prior information about the language . in that context , we employ FREQUENCY DOMAIN LINEAR PREDICTION-BASED FEATURES to reduce the effect of REVERBERATION , exploit BILINGUAL DEEP NEURAL NETWORKS applied in TANDEM AND HYBRID ACOUSTIC MODELING APPROACHES to significantly improve AUTOMATIC SPEECH RECOGNITION SYSTEM for AC-CENTED SPEECH and develop a fully AUTOMATIC SPEECH RECOGNITION SYSTEM using ENTROPY-BASED DECODING-GRAPH SELECTION . our experiments indicate that the proposed AUTOMATIC SPEECH RECOGNITION SYSTEM performs similar to a LANGUAGE-SPECIFIC ASR SYSTEM if approximately five seconds of speech are available . \n",
            "this paper presents a novel AUTOMATIC SPEECH RECOGNITION SYSTEM for AC-CENTED SPEECH . the AUTOMATIC SPEECH RECOGNITION SYSTEM is based on a BILINGUAL MEDIAPARL CORPUS and a BILINGUAL MEDIAPARL CORPUS . the AUTOMATIC SPEECH RECOGNITION SYSTEM is based on a BILINGUAL MEDIAPARL CORPUS . the AUTOMATIC SPEECH RECOGNITION SYSTEM is based on the BILINGUAL MEDIAPARL CORPUS . the LANGUAGE-SPECIFIC ASR SYSTEM is based on the BILINGUAL MEDIAPARL CORPUS and the BILINGUAL DEEP NEURAL NETWORKS . the AUTOMATIC SPEECH RECOGNITION SYSTEM is evaluated on the BILINGUAL MEDIAPARL CORPUS and compared with the conventional AUTOMATIC SPEECH RECOGNITION SYSTEM . the proposed AUTOMATIC SPEECH RECOGNITION SYSTEM is compared with the conventional LANGUAGE-SPECIFIC ASR SYSTEM and the LANGUAGE-SPECIFIC ASR SYSTEM .\n",
            "\n",
            "262 1000\n",
            "the recent proliferation of LARGE MULTIMEDIA COLLECTIONS has gathered immense attention from the speech research community , because SPEECH RECOGNITION enables the transcription and indexing of such LARGE MULTIMEDIA COLLECTIONS . TOPICALITY INFORMATION can be used to improve TRANSCRIPTION QUALITY and enable CONTENT NAVIGATION . in this paper , we give a novel QUALITY MEASURE for TOPIC SEGMEN-TATION ALGORITHMS that improves over previously used measures . our QUALITY MEASURE takes into account not only the presence or absence of TOPIC BOUNDARIES but also the content of the text or SPEECH SEGMENTS labeled as <unk> . additionally , we demonstrate that TOPIC SEGMENTATION QUALITY of SPOKEN LANGUAGE can be improved using SPEECH RECOGNITION LATTICES . using LATTICES , improvements over the BASELINE ONE-BEST TOPIC MODEL are observed when measured with the previously existing TOPIC SEGMENTATION QUALITY MEASURE , as well as the new QUALITY MEASURE proposed in this paper -lrb- 9.4 % and <unk> % RELATIVE ERROR REDUCTION , respectively -rrb- . \n",
            "this paper proposes a new TOPIC SEGMENTATION QUALITY MEASURE for SPEECH RECOGNITION based on TOPICALITY INFORMATION . the proposed TOPIC SEGMENTATION QUALITY MEASURE is based on the TOPICALITY INFORMATION of the SPEECH SEGMENTS and the TOPICALITY INFORMATION . the TOPIC SEGMENTATION QUALITY MEASURE of the proposed BASELINE ONE-BEST TOPIC MODEL is evaluated in terms of TOPIC SEGMENTATION QUALITY and RELATIVE ERROR REDUCTION . the proposed TOPIC SEGMENTATION QUALITY MEASURE is evaluated on the LARGE MULTIMEDIA COLLECTIONS and compared with the conventional BASELINE ONE-BEST TOPIC MODEL .\n",
            "\n",
            "263 1000\n",
            "we present a method for REGION IDENTIFICATION in multiple images . a set of regions in different images and the correspondences on their boundaries can be thought of as a boundary in the MULTI-DIMENSIONAL SPACE formed by the product of the individual image domains . we minimize an ENERGY FUNCTIONAL on the space of such boundaries , thereby identifying simultaneously both the optimal regions in each image and the optimal correspondences on their boundaries . we use a RATIO FORM for the ENERGY FUNCTIONAL , thus enabling the GLOBAL MINIMIZATION OF THE ENERGY FUNCTIONAL using a POLYNOMIAL TIME GRAPH ALGORITHM , among other desirable properties . we choose a simple form for this energy that <unk> boundaries that lie on HIGH INTENSITY GRADIENTS in each image , while encouraging correspondences between boundaries in different images that match INTENSITY VALUES . the latter tendency is weighted by a novel HEURISTIC ENERGY that encourages the boundaries to lie on disparity or OPTICAL FLOW DISCONTINUITIES , although no DENSE OPTICAL FLOW or DISPARITY MAP is computed . \n",
            "this paper presents a new method for REGION IDENTIFICATION . the proposed method is based on the GLOBAL MINIMIZATION OF THE ENERGY FUNCTIONAL . the proposed method is based on a GLOBAL MINIMIZATION OF THE ENERGY FUNCTIONAL . the proposed method is based on the GLOBAL MINIMIZATION OF THE ENERGY FUNCTIONAL . the proposed method is based on the GLOBAL MINIMIZATION OF THE ENERGY FUNCTIONAL . the proposed method is based on the GLOBAL MINIMIZATION OF THE ENERGY FUNCTIONAL . the proposed method is based on the GLOBAL MINIMIZATION OF THE ENERGY FUNCTIONAL . the proposed method is based on the GLOBAL MINIMIZATION OF THE ENERGY FUNCTIONAL . the proposed method is based on the GLOBAL MINIMIZATION OF THE ENERGY FUNCTIONAL . the proposed method is based on the GLOBAL MINIMIZATION OF THE ENERGY FUNCTIONAL . the proposed method is compared with the conventional POLYNOMIAL TIME GRAPH ALGORITHM and the POLYNOMIAL TIME GRAPH ALGORITHM .\n",
            "\n",
            "264 1000\n",
            "assessing similarity between FEATURES is a key step in OBJECT RECOGNITION and SCENE CATEGORIZATION TASKS . we argue that knowledge on the DISTRIBUTION OF DISTANCES generated by SIMILARITY FUNCTIONS is crucial in deciding whether FEATURES are similar or not . intuitively one would expect that similarities between FEATURES could arise from any DISTRIBUTION . in this paper , we will derive the contrary , and report the theoretical result that L P-NORMS -- a class of commonly applied distance metrics -- from one FEATURE VECTOR to other vectors are <unk> if the FEATURE VALUES are correlated and <unk> distributed . besides these assumptions being realistic for IMAGES , we experimentally show them to hold for various popular FEATURE EXTRACTION ALGORITHMS , for a diverse range of IMAGES . this fundamental insight opens new directions in the assessment of FEATURE SIMILARITY , with projected improvements in OBJECT AND SCENE RECOGNITION ALGORITHMS . \n",
            "this paper addresses the problem of OBJECT RECOGNITION in IMAGES . we propose a method for OBJECT RECOGNITION based on ASSESSING SIMILARITY BETWEEN FEATURES and DISTRIBUTION OF DISTANCES . the proposed method is based on the DISTRIBUTION OF DISTANCES of the FEATURE VECTOR and the DISTRIBUTION OF DISTANCES . the proposed method is based on ASSESSING SIMILARITY BETWEEN FEATURES and ASSESSING SIMILARITY BETWEEN FEATURES . the proposed method is based on ASSESSING SIMILARITY BETWEEN FEATURES and ASSESSING SIMILARITY BETWEEN FEATURES . the proposed method is evaluated on SCENE CATEGORIZATION TASKS and SCENE CATEGORIZATION TASKS .\n",
            "\n",
            "265 1000\n",
            "we present a new algorithm for COMPUTING UPPER BOUNDS for an optimization version of the E-MAJSAT PROBLEM called FUNCTIONAL E-MAJSAT . the algorithm utilizes the COMPILATION LANGUAGE D-DNNF which underlies several state-of-the-art algorithms for solving related problems . this bound computation can be used in a BRANCH-AND-BOUND SOLVER for solving FUNCTIONAL E-MAJSAT . we then present a technique for pruning values from the BRANCH-AND-BOUND SEARCH TREE based on the information available after each bound computation . we evaluated the proposed techniques in a MAP SOLVER and a PROBABILISTIC CONFORMANT PLANNER . in both cases , our experiments showed that the new techniques improved the efficiency of state-of-the-art solvers by orders of magnitude . \n",
            "this paper presents a new method for COMPILATION LANGUAGE D-DNNF based on a PROBABILISTIC CONFORMANT PLANNER . the proposed method is based on a PROBABILISTIC CONFORMANT PLANNER and a PROBABILISTIC CONFORMANT PLANNER . the proposed method is based on the MAP SOLVER and the MAP SOLVER . the proposed method is based on the MAP SOLVER and the MAP SOLVER . the experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "266 1000\n",
            "the problem of assigning m points in the <unk> real space r n to k clusters is formulated as that of determining k centers in r n such that the sum of distances of each point to the nearest center is minimized . if a POLYHEDRAL DISTANCE is used , the problem can be formulated as that of minimizing a PIECEWISE-LINEAR CONCAVE FUNCTION on a POLYHEDRAL SET which is shown to be equivalent to a BILINEAR PROGRAM : minimizing a BILINEAR FUNCTION on a POLYHE-DRAL SET . a FAST NITE K-MEDIAN ALGORITHM consisting of solving few LINEAR PROGRAMS in CLOSED FORM leads to a stationary point of the BILINEAR PROGRAM . computational testing on a number of REAL-WORLD DATABASES was carried out . on the WISCONSIN DIAGNOSTIC BREAST CANCER DATABASE , K-MEDIAN TRAINING SET CORRECT-NESS was comparable to that of the K-MEDIAN ALGORITHM , however its testing set correctness was better . additionally , on the WISCONSIN PROGNOSTIC BREAST CANCER DATABASE , distinct and clinically important SURVIVAL CURVES were extracted by the K-MEDIAN ALGORITHM , whereas the K-MEDIAN ALGORITHM failed to obtain such distinct SURVIVAL CURVES for the same database . \n",
            "this paper presents a novel FAST NITE K-MEDIAN ALGORITHM for LINEAR PROGRAMS . the proposed FAST NITE K-MEDIAN ALGORITHM is based on a BILINEAR PROGRAM . the proposed K-MEDIAN ALGORITHM is based on a BILINEAR PROGRAM . the proposed FAST NITE K-MEDIAN ALGORITHM is based on a FAST NITE K-MEDIAN ALGORITHM . the proposed FAST NITE K-MEDIAN ALGORITHM is compared with the conventional K-MEDIAN ALGORITHM . the proposed FAST NITE K-MEDIAN ALGORITHM is compared with the conventional K-MEDIAN ALGORITHM . the proposed FAST NITE K-MEDIAN ALGORITHM is compared with the conventional K-MEDIAN ALGORITHM and the K-MEDIAN ALGORITHM . the proposed FAST NITE K-MEDIAN ALGORITHM is compared with the conventional K-MEDIAN ALGORITHM and the K-MEDIAN ALGORITHM .\n",
            "\n",
            "267 1000\n",
            "we investigate the use of TIME-FREQUENCY METHODS to query BIOLOGICAL SEQUENCES in search of regions of similarity or critical relationships among the sequences . existing QUERYING APPROACHES are insensitive to repeats , especially in LOW-COMPLEXITY REGIONS , and do not provide much support for <unk> querying sub-sequences with inserts and <unk> -lrb- or gaps -rrb- . our TIME-FREQUENCY METHODS uses HIGHLY-LOCALIZED BASIS FUNCTIONS and multiple transformations in the TF PLANE to map characters in a sequence as well as different properties of a SUB-SEQUENCE , such as its position in the sequence or number of gaps between sub-sequences . we analyze GAPPED QUERY-BASED ALIGNMENT METHODS using transformations in the TF PLANE while demonstrating the TIME-FREQUENCY METHODS 's possible operation in real-time without PRE-PROCESSING . the TIME-FREQUENCY METHODS 's performance is compared to the WIDELY-ACCEPTED BLAST ALIGNMENT APPROACH , and a SIGNICANCE IMPROVEMENT is observed for queries with REPETITIVE SEGMENTS . \n",
            "this paper addresses the problem of PRE-PROCESSING for BIOLOGICAL SEQUENCES . we propose a WIDELY-ACCEPTED BLAST ALIGNMENT APPROACH for BIOLOGICAL SEQUENCES , which is based on the WIDELY-ACCEPTED BLAST ALIGNMENT APPROACH . the WIDELY-ACCEPTED BLAST ALIGNMENT APPROACH is based on the WIDELY-ACCEPTED BLAST ALIGNMENT APPROACH . the proposed WIDELY-ACCEPTED BLAST ALIGNMENT APPROACH is based on the WIDELY-ACCEPTED BLAST ALIGNMENT APPROACH . the proposed WIDELY-ACCEPTED BLAST ALIGNMENT APPROACH is based on the WIDELY-ACCEPTED BLAST ALIGNMENT APPROACH . the proposed WIDELY-ACCEPTED BLAST ALIGNMENT APPROACH is based on a WIDELY-ACCEPTED BLAST ALIGNMENT APPROACH and is shown to be robust to REPETITIVE SEGMENTS .\n",
            "\n",
            "268 1000\n",
            "a recent trend in SALIENCY ALGORITHM DEVELOPMENT is LARGE-SCALE BENCHMARKING and ALGORITHM RANKING with ground truth provided by DATASETS OF HUMAN FIXATIONS . in order to accommodate the strong BIAS humans have toward CENTRAL FIXATIONS , SHUFFLED ROC METRIC is common to replace traditional ROC METRICS with a SHUFFLED ROC METRIC which uses RANDOMLY SAMPLED FIXATIONS from other images in the database as the negative set . however , the <unk> roc introduces a number of problematic elements , including a fundamental assumption that SHUFFLED ROC METRIC is possible to separate VISUAL SALIENCE and image spatial arrangement . we argue that SHUFFLED ROC METRIC is more informative to directly measure the effect of SPATIAL BIAS on algorithm performance rather than try to correct for SHUFFLED ROC METRIC . to capture and quantify these known sources of BIAS , we propose a novel metric for measuring SALIENCY ALGORITHM performance : the spatially <unk> roc -lrb- <unk> -rrb- . this metric provides direct insight into the SPATIAL BIASES of a SALIENCY ALGORITHM without sacrificing the intuitive raw performance evaluation of traditional ROC METRICS . by quantitatively measuring the BIAS in SALIENCY ALGORITHMS , researchers will be better equipped to select and optimize the most appropriate algorithm for a given task . we use a baseline measure of inherent algorithm BIAS to show that ADAPTIVE WHITENING SALIENCY -lsb- 14 -rsb- , attention by INFORMATION MAXIMIZA-TION -lsb- 8 -rsb- , and DYNAMIC VISUAL ATTENTION -lsb- 20 -rsb- provide the least spatially biased results , <unk> them for tasks in which there is no information about the underlying SPATIAL BIAS of the stimuli , whereas algorithms such as GRAPH BASED VISUAL SALIENCY -lsb- 18 -rsb- and CONTEXT-AWARE SALIENCY -lsb- 15 -rsb- have a significant inherent central BIAS . \n",
            "this paper presents a new method for ADAPTIVE WHITENING SALIENCY based on GRAPH BASED VISUAL SALIENCY . the proposed method is based on the SALIENCY ALGORITHM and the SALIENCY ALGORITHM . the proposed method is based on the SALIENCY ALGORITHM and the SALIENCY ALGORITHM . the proposed method is based on the SALIENCY ALGORITHM and the SALIENCY ALGORITHM . the proposed SALIENCY ALGORITHM is based on the SALIENCY ALGORITHM and the SALIENCY ALGORITHM . the proposed method is compared with conventional SALIENCY ALGORITHMS such as ADAPTIVE WHITENING SALIENCY and ADAPTIVE WHITENING SALIENCY .\n",
            "\n",
            "269 1000\n",
            "this paper presents a GEOMETRIC BASED APPROACH for multiple MOBILE ROBOT MOTION COORDINATION . all the ROBOT PATHS being computed independently , we address the problem of coordinating the motion of the robots along their own path in such a way they do not <unk> each other . the proposed GEOMETRIC BASED APPROACH is based on a BOUNDING BOX REPRESENTATION of the obstacles in the SO-CALLED COORDINATION DIAGRAM . the GEOMETRIC BASED APPROACH is <unk> . its efficiency is illustrated by examples involving more than 100 robots . \n",
            "this paper presents a new method for MOBILE ROBOT MOTION COORDINATION . the method is based on a GEOMETRIC BASED APPROACH , which is based on the BOUNDING BOX REPRESENTATION . the proposed method is based on the BOUNDING BOX REPRESENTATION . the proposed method is based on a GEOMETRIC BASED APPROACH . the proposed method is based on a GEOMETRIC BASED APPROACH .\n",
            "\n",
            "270 1000\n",
            "the ONE-SHOT SIMILARITY MEASURE has recently been introduced in the context of FACE RECOGNITION where ONE-SHOT SIMILARITY MEASURE was used to produce state-of-the-art results . given two vectors , their ONE-SHOT SIMILARITY SCORE reflects the likelihood of each vector belonging in the same class as the other vector and not in a class defined by a fixed set of '' negative '' examples . the potential of this approach has thus far been largely <unk> . in this paper we analyze the ONE-SHOT SCORE and show that : -lrb- 1 -rrb- when using a version of LDA as the underlying CLASSIFIER , this score is a CONDITIONALLY POSITIVE DEFINITE KERNEL and may be used within <unk> -lrb- e.g. , svm -rrb- , -lrb- 2 -rrb- ONE-SHOT SIMILARITY MEASURE can be efficiently computed , and -lrb- 3 -rrb- that ONE-SHOT SIMILARITY MEASURE is effective as an underlying mechanism for IMAGE REPRESENTATION . we further demonstrate the effectiveness of the ONE-SHOT SIMILARITY SCORE in a number of applications including MULTI-CLASS IDENTIFICATION and DESCRIPTOR GENERATION . \n",
            "this paper presents a new method for MULTI-CLASS IDENTIFICATION based on a CONDITIONALLY POSITIVE DEFINITE KERNEL . the proposed method is based on the ONE-SHOT SIMILARITY MEASURE of the CONDITIONALLY POSITIVE DEFINITE KERNEL and the ONE-SHOT SIMILARITY SCORE of the CLASSIFIER . the proposed method is based on the ONE-SHOT SIMILARITY MEASURE of the CONDITIONALLY POSITIVE DEFINITE KERNEL and the ONE-SHOT SIMILARITY SCORE . the proposed method is based on the ONE-SHOT SIMILARITY MEASURE and the ONE-SHOT SIMILARITY SCORE of the CLASSIFIER .\n",
            "\n",
            "271 1000\n",
            "multi-task learning attempts to simultaneously leverage data from multiple domains in order to estimate related functions on each domain . for example , a special case of MULTI-TASK LEARNING , TRANSFER LEARNING , is often employed when one has a good estimate of a function on a SOURCE DOMAIN , but is unable to estimate a related function well on a target domain using only target data . MULTI-TASK/TRANSFER LEARNING PROBLEMS are usually solved by imposing some kind of '' smooth '' relationship <unk> tasks . in this paper , we study how different SMOOTHNESS ASSUMPTIONS on TASK RELATIONS affect the upper bounds of algorithms proposed for these MULTI-TASK/TRANSFER LEARNING PROBLEMS under different settings . for general MULTI-TASK LEARNING , we study a family of algorithms which utilize a REWEIGHTING MATRIX on task weights to capture the smooth relationship among tasks , which has many instantiations in existing literature . furthermore , for MULTI-TASK LEARNING in a TRANSFER LEARNING FRAMEWORK , we study the recently proposed algorithms for the '' model shift '' , where the conditional distribution p -lrb- y | x -rrb- is allowed to change across tasks but the change is assumed to be smooth . in addition , we illustrate our results with experiments on both SIMULATED AND REAL DATA . \n",
            "this paper presents a TRANSFER LEARNING FRAMEWORK for MULTI-TASK/TRANSFER LEARNING PROBLEMS . the TRANSFER LEARNING FRAMEWORK is based on the REWEIGHTING MATRIX . the TRANSFER LEARNING FRAMEWORK is based on the REWEIGHTING MATRIX of the REWEIGHTING MATRIX . the proposed TRANSFER LEARNING FRAMEWORK is based on the TRANSFER LEARNING FRAMEWORK . the proposed TRANSFER LEARNING FRAMEWORK is based on the TRANSFER LEARNING FRAMEWORK . the proposed TRANSFER LEARNING FRAMEWORK is based on the TRANSFER LEARNING FRAMEWORK and the TRANSFER LEARNING FRAMEWORK .\n",
            "\n",
            "272 1000\n",
            "symmetry reduction has significantly contributed to the success of CLASSICAL PLANNING as HEURISTIC SEARCH . however , it is an open question if SYMMETRY REDUCTION TECHNIQUES can be lifted to fully observable NONDETERMINISTIC PLANNING . we generalize the concepts of STRUCTURAL SYMMETRIES and SYMMETRY REDUCTION to FOND PLANNING and specifically to the LAO ⇤ ALGORITHM . our base implementation of LAO ⇤ ALGORITHM in the FAST DOWNWARD PLANNER is competitive with the LAO ⇤ ALGORITHM - based <unk> planner <unk> . our experiments further show that SYMMETRY REDUCTION can yield strong performance gains compared to our base implementation of LAO ⇤ ALGORITHM . \n",
            "this paper presents a new FAST DOWNWARD PLANNER for NONDETERMINISTIC PLANNING . the proposed LAO ⇤ ALGORITHM is based on the LAO ⇤ ALGORITHM and the LAO ⇤ ALGORITHM . the proposed LAO ⇤ ALGORITHM is based on the use of STRUCTURAL SYMMETRIES and SYMMETRY REDUCTION TECHNIQUES . the proposed FAST DOWNWARD PLANNER is compared with the conventional LAO ⇤ ALGORITHM and the LAO ⇤ ALGORITHM . the proposed LAO ⇤ ALGORITHM is compared with the conventional LAO ⇤ ALGORITHM and the LAO ⇤ ALGORITHM .\n",
            "\n",
            "273 1000\n",
            "audio coding at LOW BITRATES suffers from artifacts due to SPECTRUM TRUNCATION . typical AUDIO CODECS CODE MULTI-CHANNEL SOURCES using transforms across the channels to remove REDUNDANCY such as middle -lrb- mid -rrb- - side -lrb- <unk> -rrb- coding . at LOW BITRATES , the spectrum of the CODED CHANNELS is truncated and the spectrum of the channels with lower energy , such as the SIDE CHANNEL , is truncated severely , sometimes entirely . this results in a MUFFLED SOUND due to truncation of all CODED CHANNELS beyond a certain frequency . it also results in a LOSS OF SPATIAL IMAGE even at LOW FREQUENCIES due to SEVERE TRUNCATION OF THE SIDE CHANNEL . previously we have developed a LOW BITRATE CODING METHOD to combat the LOSS OF HIGHER FREQUENCIES caused by SPECTRUM TRUNCATION . in this paper , we present a novel LOW BITRATE AUDIO CODING SCHEME to mitigate the LOSS OF SPATIAL IMAGE . listening tests show that the combination of the two LOW BITRATE CODING METHODS results in a AUDIO CODEC that can get good quality even at bitrates as low as <unk> for STEREO CONTENT with LOW DECODER COMPLEXITY . \n",
            "this paper presents a LOW BITRATE CODING METHOD for AUDIO CODECS CODE MULTI-CHANNEL SOURCES . the proposed LOW BITRATE AUDIO CODING SCHEME is based on the LOSS OF HIGHER FREQUENCIES and the LOSS OF HIGHER FREQUENCIES . the AUDIO CODEC is based on the LOSS OF HIGHER FREQUENCIES and the LOSS OF HIGHER FREQUENCIES . the proposed LOW BITRATE AUDIO CODING SCHEME is based on the LOSS OF HIGHER FREQUENCIES and the LOSS OF HIGHER FREQUENCIES . the proposed LOW BITRATE AUDIO CODING SCHEME is based on the LOSS OF HIGHER FREQUENCIES and the LOSS OF HIGHER FREQUENCIES . the proposed LOW BITRATE AUDIO CODING SCHEME is based on a LOW BITRATE CODING METHOD . the proposed LOW BITRATE AUDIO CODING SCHEME is based on a LOW BITRATE CODING METHOD . the proposed LOW BITRATE AUDIO CODING SCHEME is applied to the AUDIO CODEC and is shown to be robust to LOW FREQUENCIES in CODED CHANNELS .\n",
            "\n",
            "274 1000\n",
            "in this paper , we propose a novel RADIUS CONTROL STRATEGY for SPHERE DECODING referred to as INTER SEARCH RADIUS CONTROL that provides further improvement of the COMPUTATIONAL COMPLEXITY with minimal extra cost and NEGLIGIBLE PERFORMANCE PENALTY . the proposed RADIUS CONTROL STRATEGY focuses on the SPHERE RADIUS CONTROL STRATEGY when a CANDIDATE LATTICE POINT is found . for this purpose , the DYNAMIC RADIUS UPDATE STRATEGY as well as the LATTICE INDEPENDENT RADIUS SELECTION SCHEME are jointly exploited . from simulations in MULTIPLE-INPUT AND MULTIPLE-OUTPUT CHANNELS , it is shown that the proposed RADIUS CONTROL STRATEGY provides a substantial improvement in COMPLEXITY with <unk> performance . \n",
            "this paper proposes a new DYNAMIC RADIUS UPDATE STRATEGY for INTER SEARCH RADIUS CONTROL . the proposed DYNAMIC RADIUS UPDATE STRATEGY is based on the DYNAMIC RADIUS UPDATE STRATEGY . the proposed DYNAMIC RADIUS UPDATE STRATEGY is based on the DYNAMIC RADIUS UPDATE STRATEGY . the proposed DYNAMIC RADIUS UPDATE STRATEGY is based on the DYNAMIC RADIUS UPDATE STRATEGY . the proposed DYNAMIC RADIUS UPDATE STRATEGY is evaluated on the MULTIPLE-INPUT AND MULTIPLE-OUTPUT CHANNELS and on the MULTIPLE-INPUT AND MULTIPLE-OUTPUT CHANNELS . experimental results show that the proposed DYNAMIC RADIUS UPDATE STRATEGY achieves a better performance than the conventional LATTICE INDEPENDENT RADIUS SELECTION SCHEME .\n",
            "\n",
            "275 1000\n",
            "in SPEAKER RECOGNITION , it is a problem that variation of SPEECH FEATURES is caused by sentences and TIME DIFFERENCE . SPEECH DATA includes a PHONETIC INFORMATION and a SPEAKER INFORMATION . if they are separated each other , ROBUST SPEAKER VERIFICATION will be realized by using only the SPEAKER INFORMATION . however , it is difficult to separate the SPEAKER INFORMATION from the PHONETIC INFORMATION included in SPEECH DATA at present . from this viewpoint , we propose a SPEAKER VERIFICATION METHOD using a SUBSPACE METHOD based on PRINCIPAL COMPONENT ANALYSIS in order to extract only the SPEAKER INFORMATION included in SPEECH DATA . we also propose DYNAMIC AND STATIC FEATURES of each speaker presented in the SPEAKER EIGENSPACE as well as their integration for ROBUST NORMALIZATION OF SPEECH FEATURE VARIATIONS . we carried out comparative experiments between the proposed SPEAKER VERIFICATION METHOD and conventional GMM to show an effectiveness of our proposed SPEAKER VERIFICATION METHOD . as a result , integrated DYNAMIC AND STATIC FEATURES in SPEAKER EIGENSPACE were shown to be effective for SPEAKER VERIFICATION . \n",
            "this paper presents a novel SPEAKER VERIFICATION METHOD for ROBUST SPEAKER VERIFICATION . the SPEAKER VERIFICATION METHOD is based on a SUBSPACE METHOD and a SUBSPACE METHOD . the SUBSPACE METHOD is based on the PHONETIC INFORMATION and the PHONETIC INFORMATION . the proposed SPEAKER VERIFICATION METHOD is based on PRINCIPAL COMPONENT ANALYSIS and PRINCIPAL COMPONENT ANALYSIS . the proposed SPEAKER VERIFICATION METHOD is based on PRINCIPAL COMPONENT ANALYSIS and PRINCIPAL COMPONENT ANALYSIS . the proposed SPEAKER VERIFICATION METHOD is based on PRINCIPAL COMPONENT ANALYSIS and PRINCIPAL COMPONENT ANALYSIS . experimental results on SPEECH DATA show that the proposed SPEAKER VERIFICATION METHOD outperforms the conventional GMM in terms of TIME DIFFERENCE and TIME DIFFERENCE .\n",
            "\n",
            "276 1000\n",
            "previous RESOLUTION-BASED APPROACHES to THEORY-GUIDED INDUCTION OF LOGIC PROGRAMS produce hypotheses in the form of a set of <unk> of a theory , where the RESOLVENTS represent allowed sequences of RESOLUTION STEPS for the initial theory . there are , however , many characterizations of allowed sequences of RESOLUTION STEPS that can not be expressed by a set of RESOLVENTS . one approach to this THEORY-GUIDED INDUCTION OF LOGIC PROGRAMS is presented , the system <unk> , which is based on an earlier technique for learning NITE-STATE AUTOMATA that represent allowed sequences of RESOLUTION STEPS . <unk> extends the previous technique in three ways : i -rrb- negative examples are considered in addition to positive examples , ii -rrb- a new strategy for performing GENERALIZATION is used , and iii -rrb- a technique for converting the learned automaton to a LOGIC PROGRAM is included . results from experiments are presented in which <unk> outperforms both a system using the old strategy for performing GENERALIZATION , and a traditional COVERING TECHNIQUE . the latter result can be explained by the limited expressiveness of hypotheses produced by covering and also by the fact that covering needs to produce the correct BASE CLAUSES for a RECURSIVE DENITION before producing the RECURSIVE CLAUSES . <unk> on the other hand does not require that particular examples of the base cases are given , since both BASE CLAUSES and RECURSIVE CLAUSES can be inferred from a single example . \n",
            "this paper addresses the problem of THEORY-GUIDED INDUCTION OF LOGIC PROGRAMS in BASE CLAUSES . we propose a method for THEORY-GUIDED INDUCTION OF LOGIC PROGRAMS based on NITE-STATE AUTOMATA . the proposed method is based on the use of RECURSIVE CLAUSES and NITE-STATE AUTOMATA . the proposed method is based on the use of RECURSIVE CLAUSES and RECURSIVE CLAUSES . the proposed method is based on the use of RECURSIVE CLAUSES and NITE-STATE AUTOMATA . the experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "277 1000\n",
            "many COMPUTER VISION PROBLEMS can be formulated as BINARY QUADRATIC PROGRAMS . two classic RELAXATION METHODS are widely used for solving BINARY QUADRATIC PROGRAMS , namely , SPECTRAL METHODS and SEMIDEFINITE PROGRAMMING , each with their own advantages and disadvantages . BINARY QUADRATIC PROGRAMS is simple and easy to implement , but its bound is loose . BINARY QUADRATIC PROGRAMS has a tighter bound , but its COMPUTATIONAL COMPLEXITY is high for LARGE SCALE PROBLEMS . we present a new SDP FORMULATION for BINARY QUADRATIC PROGRAMS , with two desirable properties . first , SDP FORMULATION has a similar relaxation bound to conventional SDP FORMULATIONS . second , compared with conventional SDP FORMULATIONS , the new SDP FORMULATION leads to a significantly more efficient and scalable DUAL OPTIMIZATION APPROACH , which has the same degree of COMPLEXITY as SPECTRAL METHODS . extensive experiments on various applications including CLUSTERING , IMAGE SEGMEN-TATION , CO-SEGMENTATION and REGISTRATION demonstrate the usefulness of our SDP FORMULATION for solving LARGE-SCALE BQPS . \n",
            "this paper presents a new DUAL OPTIMIZATION APPROACH for IMAGE SEGMEN-TATION . the proposed DUAL OPTIMIZATION APPROACH is based on the DUAL OPTIMIZATION APPROACH and the DUAL OPTIMIZATION APPROACH . the proposed DUAL OPTIMIZATION APPROACH is based on the DUAL OPTIMIZATION APPROACH and the DUAL OPTIMIZATION APPROACH . the proposed DUAL OPTIMIZATION APPROACH is based on the DUAL OPTIMIZATION APPROACH and the DUAL OPTIMIZATION APPROACH . the proposed DUAL OPTIMIZATION APPROACH is compared with other RELAXATION METHODS and RELAXATION METHODS . the proposed DUAL OPTIMIZATION APPROACH is compared with other RELAXATION METHODS and RELAXATION METHODS .\n",
            "\n",
            "278 1000\n",
            "we propose a compact , low power VLSI NETWORK of spiking neurons which can learn to classify COMPLEX PATTERNS OF MEAN FIRING RATES on -- line and in real -- time . the NETWORK OF INTEGRATE-AND-FIRE NEURONS is connected by BISTABLE SYNAPSES that can change their weight using a LOCAL SPIKE -- based <unk> mechanism . LEARNING is supervised by a teacher which provides an extra input to the output neurons during training . the SYNAPTIC WEIGHTS are updated only if the current generated by the PLASTIC SYNAPSES does not match the output desired by the teacher -lrb- as in the PERCEPTRON LEARNING RULE -rrb- . we present experimental results that demonstrate how this VLSI NETWORK is able to robustly classify uncorrelated linearly separable spatial patterns of mean firing rates . \n",
            "this paper presents a new method for LEARNING based on the NETWORK OF INTEGRATE-AND-FIRE NEURONS . the proposed method is based on the PERCEPTRON LEARNING RULE of the LOCAL SPIKE . the proposed method is based on the PERCEPTRON LEARNING RULE of the LOCAL SPIKE . the proposed method is based on the NETWORK OF INTEGRATE-AND-FIRE NEURONS . the proposed method is based on the NETWORK OF INTEGRATE-AND-FIRE NEURONS . the proposed method is based on the NETWORK OF INTEGRATE-AND-FIRE NEURONS . the proposed method is based on the NETWORK OF INTEGRATE-AND-FIRE NEURONS .\n",
            "\n",
            "279 1000\n",
            "it is known that the deformations of the APPARENT CONTOURS of a surface under PERSPECTIVE PROJECTION and VIEWER MOTION enable the recovery of the GEOMETRY OF THE SURFACE , for example by utilising the EPIPOLAR PARAMETRIZATION . these methods break down with APPARENT CONTOURS that are singular i.e. with <unk> . in this paper we study this situation in detail and show how , nevertheless , the SURFACE GEOMETRY -lrb- including the GAUSS CURVATURE and mean curvature of the surface -rrb- can be recovered by following the <unk> . indeed the FOR-MULAE are much simpler in this case and require lower SPATIO-TEMPORAL DERIVATIVES than in the general case of NONSINGULAR APPARENT CONTOURS . we give a simulated example , and also show that following <unk> does not by itself provide us with information on EGO-MOTION . \n",
            "this paper presents a new method for NONSINGULAR APPARENT CONTOURS in NONSINGULAR APPARENT CONTOURS . the method is based on a PERSPECTIVE PROJECTION , which is a PERSPECTIVE PROJECTION of the GEOMETRY OF THE SURFACE . the method is based on a PERSPECTIVE PROJECTION , which is a PERSPECTIVE PROJECTION of the GEOMETRY OF THE SURFACE . the proposed method is based on the EPIPOLAR PARAMETRIZATION . the proposed method is based on the EPIPOLAR PARAMETRIZATION . the proposed method is based on the EPIPOLAR PARAMETRIZATION . the proposed method is based on the EPIPOLAR PARAMETRIZATION .\n",
            "\n",
            "280 1000\n",
            "we present a VARIATIONAL BAYESIAN FRAMEWORK for performing INFERENCE , DENSITY ESTIMATION and MODEL SELECTION in a special class of GRAPHICAL MODELS -- HIDDEN MARKOV RANDOM FIELDS . HIDDEN MARKOV RANDOM FIELDS are particularly well suited to IMAGE MODELLING and in this paper , we apply HIDDEN MARKOV RANDOM FIELDS to the problem of IMAGE SEGMENTATION . unfortunately , HIDDEN MARKOV RANDOM FIELDS are notoriously hard to train and use because the exact INFERENCE PROBLEMS they create are intractable . our main contribution is to introduce an efficient VARIATIONAL APPROACH for performing approximate INFERENCE of the VARIATIONAL BAYESIAN FRAMEWORK of HIDDEN MARKOV RANDOM FIELDS , which we can then apply to the DENSITY ESTIMATION and MODEL SELECTION PROBLEMS that arise when learning IMAGE MODELS from data . with this VARIATIONAL APPROACH , we can conveniently tackle the problem of IMAGE SEGMENTATION . we present experimental results which show that our VARIATIONAL BAYESIAN FRAMEWORK outperforms recent HMRF-BASED SEGMENTATION METHODS on REAL WORLD IMAGES . \n",
            "this paper presents a new VARIATIONAL APPROACH for IMAGE MODELLING . the VARIATIONAL BAYESIAN FRAMEWORK is based on a VARIATIONAL BAYESIAN FRAMEWORK and a VARIATIONAL APPROACH for IMAGE MODELLING . the VARIATIONAL BAYESIAN FRAMEWORK is based on the VARIATIONAL BAYESIAN FRAMEWORK and the VARIATIONAL APPROACH . the proposed VARIATIONAL APPROACH is based on the VARIATIONAL BAYESIAN FRAMEWORK and the VARIATIONAL APPROACH . the proposed VARIATIONAL APPROACH is evaluated on the REAL WORLD IMAGES and on the REAL WORLD IMAGES . the proposed VARIATIONAL APPROACH is compared with state-of-the-art HMRF-BASED SEGMENTATION METHODS and HMRF-BASED SEGMENTATION METHODS .\n",
            "\n",
            "281 1000\n",
            "we consider the problem of ESTIMATING 3-D STRUCTURE from a single still image of an outdoor urban scene . our goal is to efficiently create 3-D MODEL STRUCTURE which are visually pleasant . we <unk> an appropriate 3-D MODEL STRUCTURE and formulate the task of 3-D RECONSTRUCTION as MODEL FITTING PROBLEM . our 3-D MODEL STRUCTURE are composed of a number of VERTICAL WALLS and a GROUND PLANE , where GROUND-VERTICAL BOUNDARY is a CONTINUOUS POLYLINE . we achieve COMPUTATIONAL EFFICIENCY by special preprocessing together with STEPWISE SEARCH OF 3-D MODEL PARAMETERS dividing the problem into two smaller sub-problems on CHAIN GRAPHS . the use of CONDITIONAL RANDOM FIELD MODELS for both problems allows to various cues . we infer orientation of VERTICAL WALLS of 3-d model vanishing points . \n",
            "this paper presents a new method for ESTIMATING 3-D STRUCTURE from CHAIN GRAPHS . the proposed method is based on a MODEL FITTING PROBLEM , which is based on the 3-D MODEL STRUCTURE . the proposed method is based on the STEPWISE SEARCH OF 3-D MODEL PARAMETERS and the 3-D MODEL STRUCTURE . the proposed method is based on the STEPWISE SEARCH OF 3-D MODEL PARAMETERS and the 3-D MODEL STRUCTURE . the proposed method is based on the STEPWISE SEARCH OF 3-D MODEL PARAMETERS and the 3-D MODEL STRUCTURE . the proposed method is based on the STEPWISE SEARCH OF 3-D MODEL PARAMETERS and the 3-D MODEL STRUCTURE .\n",
            "\n",
            "282 1000\n",
            "it has been shown that the combination of MULTI-MODAL MRI IMAGES can improve the DISCRIMINATION OF DISEASED TISSUE . the fusion of DISSIMILAR IMAGING DATA for CLASSIFICATION AND SEG-MENTATION PURPOSES however , is not a trivial task , as there is an inherent difference in INFORMATION DOMAINS , DIMENSION-ALITY and scales . this work proposes a MULTI-VIEW CONSENSUS CLUSTERING METHODOLOGY for the integration of MULTI-MODAL MRI IMAGES into a UNIFIED SEGMENTATION OF TUMORAL LESIONS for HETEROGENEITY ASSESSMENT . using a variety of metrics and distance functions this MULTI-VIEW CONSENSUS CLUSTERING METHODOLOGY calculates multiple VECTORIAL DISSIMILARITY-SPACES for each MRI MODALITY and makes use of CLUSTER ENSEMBLES to combine a set of UN-SUPERVISED BASE SEGMENTATIONS into an unified partition of the VOXEL-BASED DATA . the MULTI-VIEW CONSENSUS CLUSTERING METHODOLOGY is demonstrated in application to DCE-MRI and DTI-MR , for which a MANIFOLD LEARNING STEP is implemented in order to account for the GEOMETRIC CONSTRAINS of the HIGH DIMENSIONAL DIFFUSION INFORMATION . \n",
            "this paper presents a novel MULTI-VIEW CONSENSUS CLUSTERING METHODOLOGY for CLASSIFICATION AND SEG-MENTATION PURPOSES . the MULTI-VIEW CONSENSUS CLUSTERING METHODOLOGY is based on a MANIFOLD LEARNING STEP and a MANIFOLD LEARNING STEP . the proposed MULTI-VIEW CONSENSUS CLUSTERING METHODOLOGY is based on a UNIFIED SEGMENTATION OF TUMORAL LESIONS , which is based on the MANIFOLD LEARNING STEP and the DISCRIMINATION OF DISEASED TISSUE . the proposed MULTI-VIEW CONSENSUS CLUSTERING METHODOLOGY is based on a MANIFOLD LEARNING STEP and a UNIFIED SEGMENTATION OF TUMORAL LESIONS . the proposed MULTI-VIEW CONSENSUS CLUSTERING METHODOLOGY is based on a MULTI-VIEW CONSENSUS CLUSTERING METHODOLOGY and a UNIFIED SEGMENTATION OF TUMORAL LESIONS . the proposed MULTI-VIEW CONSENSUS CLUSTERING METHODOLOGY is based on a MANIFOLD LEARNING STEP and a UNIFIED SEGMENTATION OF TUMORAL LESIONS . the proposed MULTI-VIEW CONSENSUS CLUSTERING METHODOLOGY is applied to CLASSIFICATION AND SEG-MENTATION PURPOSES and CLASSIFICATION AND SEG-MENTATION PURPOSES . the experimental results show that the proposed MULTI-VIEW CONSENSUS CLUSTERING METHODOLOGY outperforms the conventional DTI-MR and the MULTI-VIEW CONSENSUS CLUSTERING METHODOLOGY .\n",
            "\n",
            "283 1000\n",
            "we introduce PARMA , a PARMA for CROSS-DOCUMENT , SEMANTIC PREDICATE and argument alignment . our PARMA combines a number of LINGUISTIC RESOURCES familiar to researchers in areas such as recognizing TEXTUAL ENTAILMENT and QUESTION ANSWERING , integrating LINGUISTIC RESOURCES into a simple DISCRIMINA-TIVE MODEL . PARMA achieves state of the art results on an existing and a new dataset . we suggest that previous efforts have focussed on data that is biased and too easy , and we provide a more difficult dataset based on TRANSLATION DATA with a low base-line which we beat by 17 % F1 . \n",
            "this paper presents a new method for QUESTION ANSWERING in QUESTION ANSWERING . the DISCRIMINA-TIVE MODEL is based on the DISCRIMINA-TIVE MODEL . the proposed method is based on the DISCRIMINA-TIVE MODEL . the proposed method is based on the DISCRIMINA-TIVE MODEL and the DISCRIMINA-TIVE MODEL . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "284 1000\n",
            "we propose a TRANSITION-BASED MODEL for JOINT WORD SEGMENTATION , POS TAGGING and TEXT NORMALIZATION . different from previous methods , the TRANSITION-BASED MODEL can be trained on standard TEXT CORPORA , overcoming the lack of ANNOTATED MICROBLOG CORPORA . to evaluate our TRANSITION-BASED MODEL , we develop an ANNOTATED CORPUS based on MICROBLOGS . experimental results show that our TRANSITION-BASED MODEL can help improve the performance of WORD SEGMENTATION on MICROBLOGS , giving an ERROR REDUCTION in SEGMENTATION ACCURACY of <unk> % , compared to the traditional approach . \n",
            "this paper presents a novel TRANSITION-BASED MODEL for JOINT WORD SEGMENTATION . the TRANSITION-BASED MODEL is based on the TRANSITION-BASED MODEL and the TRANSITION-BASED MODEL . the TRANSITION-BASED MODEL is based on the TRANSITION-BASED MODEL and the TRANSITION-BASED MODEL . the proposed TRANSITION-BASED MODEL is based on the TRANSITION-BASED MODEL and the TRANSITION-BASED MODEL . the SEGMENTATION ACCURACY of the proposed TRANSITION-BASED MODEL is demonstrated by the use of ANNOTATED MICROBLOG CORPORA and MICROBLOGS .\n",
            "\n",
            "285 1000\n",
            "efficient decoding for SYNTACTIC PARSING has become a necessary research area as STATISTICAL GRAMMARS grow in ACCURACY and size and as more NLP APPLICATIONS leverage SYNTACTIC ANALYSES . we review prior methods for PRUNING and then present a new framework that unifies their strengths into a single approach . using a LOG LINEAR MODEL , we learn the optimal BEAM-SEARCH PRUNING PARAMETERS for each CYK CHART CELL , effectively predicting the most promising areas of the MODEL SPACE to explore . we demonstrate that our method is faster than COARSE-TO-FINE PRUNING , exemplified in both the CHARNIAK AND BERKELEY PARSERS , by empirically comparing our PARSER to the BERKELEY PARSER using the same GRAMMAR and under identical operating conditions . \n",
            "this paper addresses the problem of SYNTACTIC PARSING in NLP APPLICATIONS . in this paper , we propose a new PARSER based on the LOG LINEAR MODEL . the proposed method is based on a LOG LINEAR MODEL , which is based on a LOG LINEAR MODEL . the proposed method is based on the use of a LOG LINEAR MODEL and a LOG LINEAR MODEL . the proposed PARSER is compared with the conventional PARSER and the PARSER . the ACCURACY of the proposed PARSER is compared with the conventional PARSER .\n",
            "\n",
            "286 1000\n",
            "the goal of BANDWIDTH EXTENSION OF SPEECH is to extrapolate the MISSING LOW OR HIGH FREQUENCY COMPONENTS of the wide-band speech -lrb- <unk> hz -rrb- based entirely on information contained in a NARROW-BAND SIGNAL -lrb- <unk> hz -rrb- . in this paper we propose a new method for HIGH-FREQUENCY REGENERATION OF THE EXCITATION SIGNAL , using the correlation between the SHAPE of the GLOTTAL FLOW WAVEFORM and the spectrum of the voice source . the HIGH-BAND EXCITATION is generated by performing a PITCH-SYNCHRONOUS TIMESCALE TRANSFORMATION on the LINEAR PREDICTION NARROW-BAND RESIDUAL to generate an HIGH-PASS SIGNAL that retains the PERIODIC CHARACTERISTICS of the original signal but with a larger open quotient . this method is easy to implement and does not introduce discontinuities in the spectrum of the REGENERATED EXCITATION . it can be used in applications for BANDWIDTH EXTENSION OF SPEECH where no SIDE INFORMATION is transmitted or for LOW BIT CODING OF WIDE-BAND SPEECH . \n",
            "this paper presents a PITCH-SYNCHRONOUS TIMESCALE TRANSFORMATION for LOW BIT CODING OF WIDE-BAND SPEECH in LOW BIT CODING OF WIDE-BAND SPEECH . the PITCH-SYNCHRONOUS TIMESCALE TRANSFORMATION is based on a PITCH-SYNCHRONOUS TIMESCALE TRANSFORMATION of the GLOTTAL FLOW WAVEFORM . the PITCH-SYNCHRONOUS TIMESCALE TRANSFORMATION is based on a PITCH-SYNCHRONOUS TIMESCALE TRANSFORMATION of the GLOTTAL FLOW WAVEFORM and the GLOTTAL FLOW WAVEFORM . the SHAPE are estimated by the PITCH-SYNCHRONOUS TIMESCALE TRANSFORMATION . the proposed PITCH-SYNCHRONOUS TIMESCALE TRANSFORMATION is applied to LOW BIT CODING OF WIDE-BAND SPEECH and is shown to be robust to MISSING LOW OR HIGH FREQUENCY COMPONENTS in LOW BIT CODING OF WIDE-BAND SPEECH .\n",
            "\n",
            "287 1000\n",
            "execution speed of programs on modern COMPUTER ARCHITECTURES is sensitive , by a factor of two or more , to the order in which instructions are presented to the processor . to realize potential EXECUTION EFFICIENCY , it is now <unk> for an optimizing COMPILER to employ a HEURISTIC ALGORITHM for INSTRUCTION SCHEDULING . these HEURISTIC ALGORITHM are <unk> hand-crafted , which is <unk> and time-consuming . we show how to cast the INSTRUCTION SCHEDULING PROBLEM as a LEARNING TASK , so that one obtains the HEURISTIC SCHEDULING ALGORITHM automatically . our focus is the narrower problem of SCHEDULING STRAIGHT-LINE CODE , also known as a basic block of instructions . our empirical results show that just a few FEATURES are adequate for quite good performance at this task for a real modern processor , and that any of several SUPERVISED LEARNING METHODS perform nearly optimally with respect to the FEATURES used . \n",
            "this paper proposes a new HEURISTIC SCHEDULING ALGORITHM for INSTRUCTION SCHEDULING . the proposed HEURISTIC SCHEDULING ALGORITHM is based on the HEURISTIC ALGORITHM . the proposed HEURISTIC ALGORITHM is based on the HEURISTIC ALGORITHM . the proposed HEURISTIC ALGORITHM is based on the HEURISTIC ALGORITHM . the proposed HEURISTIC ALGORITHM is based on the HEURISTIC ALGORITHM . the proposed HEURISTIC SCHEDULING ALGORITHM is based on the HEURISTIC ALGORITHM . the proposed HEURISTIC SCHEDULING ALGORITHM is based on the HEURISTIC ALGORITHM and is shown to be more robust to EXECUTION SPEED than the conventional HEURISTIC ALGORITHM .\n",
            "\n",
            "288 1000\n",
            "we propose a novel , LARGE-SCALE , STRUCTURE-FROM-MOTION FRAMEWORK that advances the state of the art in DATA SCAL-ABILITY from CITY-SCALE MODELING -lrb- millions of IMAGES -rrb- to WORLD-SCALE MODELING -lrb- several tens of millions of IMAGES -rrb- using just a single computer . the main enabling technology is the use of a STREAMING-BASED FRAMEWORK for CONNECTED COMPONENT DISCOVERY . moreover , our LARGE-SCALE , STRUCTURE-FROM-MOTION FRAMEWORK employs an ADAPTIVE , ONLINE , ICONIC IMAGE CLUSTERING APPROACH based on an AUGMENTED BAG-OF-WORDS REPRESENTATION , in order to balance the goals of REGISTRATION , <unk> , and DATA COMPACTNESS . we demonstrate our proposal by operating on a recent publicly available 100 million IMAGE CROWD-SOURCED PHOTO COLLECTION containing IMAGES <unk> distributed throughout the entire world . results illustrate that our LARGE-SCALE , STRUCTURE-FROM-MOTION FRAMEWORK does not compromise MODEL COMPLETENESS , but achieves unprecedented levels of efficiency and SCALABILITY . \n",
            "this paper presents a LARGE-SCALE , STRUCTURE-FROM-MOTION FRAMEWORK for WORLD-SCALE MODELING . the STREAMING-BASED FRAMEWORK is based on a LARGE-SCALE , STRUCTURE-FROM-MOTION FRAMEWORK of the AUGMENTED BAG-OF-WORDS REPRESENTATION . the proposed ADAPTIVE , ONLINE , ICONIC IMAGE CLUSTERING APPROACH is based on the AUGMENTED BAG-OF-WORDS REPRESENTATION . the proposed ADAPTIVE , ONLINE , ICONIC IMAGE CLUSTERING APPROACH is based on the AUGMENTED BAG-OF-WORDS REPRESENTATION . the proposed ADAPTIVE , ONLINE , ICONIC IMAGE CLUSTERING APPROACH is based on the AUGMENTED BAG-OF-WORDS REPRESENTATION . the proposed ADAPTIVE , ONLINE , ICONIC IMAGE CLUSTERING APPROACH is based on a LARGE-SCALE , STRUCTURE-FROM-MOTION FRAMEWORK . the SCALABILITY of the proposed method is demonstrated by simulation results .\n",
            "\n",
            "289 1000\n",
            "<unk> '' knows '' that NEURAL NETWORKS need more than a single layer of NONLINEAR UNITS to compute interesting functions . we show that this is false if one employs WINNER-TAKE-ALL as NONLINEAR UNIT : any BOOLEAN FUNCTION can be computed by a single ¡ - WINNER-TAKE-ALL unit applied to WEIGHTED SUMS OF THE INPUT VARIABLES . any CONTINUOUS FUNCTION can be approximated arbitrarily well by a single SOFT WINNER-TAKE-ALL UNIT applied to WEIGHTED SUMS OF THE INPUT VARIABLES . only POSITIVE WEIGHTS are needed in these -LRB- LINEAR -RRB- WEIGHTED SUMS . this may be of interest from the point of view of NEUROPHYSIOLOGY , since only 15 % of the SYNAPSES in the cortex are INHIBITORY . in addition it is widely believed that there are special MICROCIRCUITS in the cortex that compute WINNER-TAKE-ALL . our results support the view that WINNER-TAKE-ALL is a very useful basic COMPUTATIONAL UNIT in NEURAL VLSI : cents it is <unk> that WINNER-TAKE-ALL of # input variables can be computed very efficiently with $ ¥ # <unk> -lrb- and a total <unk> length and area that is linear in # -rrb- in analog vlsi -lsb- <unk> et al. , 1989 -rsb- cents we show that WINNER-TAKE-ALL is not just useful for special purpose computations , but may serve as the only NONLINEAR UNIT for NEURAL CIRCUITS with UNIVERSAL COMPUTATIONAL POWER CENTS we show that any MULTI-LAYER PERCEPTRON needs quadratically in # many gates to compute WINNER-TAKE-ALL for # input variables , hence WINNER-TAKE-ALL provides a substantially more powerful COMPUTATIONAL UNIT than a PERCEPTRON -lrb- at about the same cost of implementation in analog vlsi -rrb- . complete proofs and further details to these results can be found in -lsb- <unk> , 2000 -rsb- . \n",
            "this paper presents a new method for NEURAL VLSI , which is based on the PERCEPTRON . the proposed method is based on the use of a MULTI-LAYER PERCEPTRON and a MULTI-LAYER PERCEPTRON . the proposed method is based on a MULTI-LAYER PERCEPTRON , which is based on the PERCEPTRON . the proposed method is based on a MULTI-LAYER PERCEPTRON , which is based on the PERCEPTRON . the proposed method is based on a MULTI-LAYER PERCEPTRON , which is based on the PERCEPTRON . the proposed method is based on a MULTI-LAYER PERCEPTRON , which is based on the PERCEPTRON . the proposed method is compared with the conventional PERCEPTRON and the PERCEPTRON . the results show that the proposed method outperforms the conventional PERCEPTRON in terms of UNIVERSAL COMPUTATIONAL POWER CENTS and the UNIVERSAL COMPUTATIONAL POWER CENTS .\n",
            "\n",
            "290 1000\n",
            "a DESCRIPTION CLASSIFIER organizes concepts and relations into a taxonomy based on the results of SUBSUMPTION COMPUTATIONS applied to pairs of relation definitions . until now , DESCRIPTION CLASSIFIER have only been designed to operate over definitions <unk> in highly restricted subsets of the PREDICATE CALCULUS . this paper describes a CLASSIFIER able to reason with definitions <unk> in the FULL FIRST ORDER PREDICATE CALCULUS , extended with sets , CARDINALITY , EQUALITY , SCALAR INEQUALITIES , and PREDICATE VARIABLES . the performance of the new CLASSIFIER is comparable to that of existing DESCRIPTION CLASSIFIER . our CLASSIFIER introduces two new techniques , DUAL REPRESENTATIONS and AUTO-SOCRATIC ELABORATION , that may be expected to improve the performance of existing DESCRIPTION CLASSIFIER . \n",
            "this paper presents a new method for AUTO-SOCRATIC ELABORATION , EQUALITY , and a DESCRIPTION CLASSIFIER . the proposed method is based on a DESCRIPTION CLASSIFIER and a DESCRIPTION CLASSIFIER . the proposed method is based on a DESCRIPTION CLASSIFIER and a DESCRIPTION CLASSIFIER . the proposed method is compared with the conventional CLASSIFIER and the DESCRIPTION CLASSIFIER . the results show that the proposed DESCRIPTION CLASSIFIER outperforms the conventional CLASSIFIER and the CLASSIFIER .\n",
            "\n",
            "291 1000\n",
            "information extraction -lrb- INFORMATION EXTRACTION -rrb- is the problem of <unk> out PRE-DEENED STRUCTURED SUMMARIES from TEXT DOCUMENTS . we are interested in performing INFORMATION EXTRACTION in NON-TRADITIONAL DOMAINS , where much of the text is often <unk> , such as ELECTRONIC BULLETIN BOARD POSTS and WEB PAGES . we suggest that the best approach is one that takes into account many diierent kinds of information , and argue for the suitability of a MULTISTRAT-EGY APPROACH . we describe LEARNERS for INFORMATION EXTRACTION drawn from three separate MACHINE LEARNING PARADIGMS : ROTE MEMORIZATION , TERM-SPACE TEXT CLASSIICATION , and RELATIONAL RULE INDUCTION . by building REGRESSION MODELS mapping from LEARNER CONNDENCE to PROBABILITY OF COR-RECTNESS and combining probabilities appropriately , it is possible to improve EXTRACTION ACCURACY over that achieved by any individual learner . we describe three diierent MUL-TISTRATEGY APPROACHES . experiments on two IE DOMAINS , a collection of ELECTRONIC SEMINAR ANNOUNCEMENTS from a university computer science department and a set of NEWSWIRE ARTICLES describing <unk> <unk> from the REUTERS COLLECTION , demonstrate the <unk> of all three MUL-TISTRATEGY APPROACHES . \n",
            "this paper addresses the problem of INFORMATION EXTRACTION in TEXT DOCUMENTS . we propose a method for INFORMATION EXTRACTION based on the PROBABILITY OF COR-RECTNESS . the proposed method is based on the PROBABILITY OF COR-RECTNESS and the PROBABILITY OF COR-RECTNESS . the proposed method is based on the PROBABILITY OF COR-RECTNESS and the PROBABILITY OF COR-RECTNESS . the proposed method is based on the MULTISTRAT-EGY APPROACH and the MULTISTRAT-EGY APPROACH . the proposed method is based on the PROBABILITY OF COR-RECTNESS and the MULTISTRAT-EGY APPROACH . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "292 1000\n",
            "despite considerable progress in HDR IMAGE TONE MAPPING for the past decade , little work has been done for HDR VIDEO . for applications such as FILM POST-PRODUCTION , the capability of LOCAL TONE MANIPULATION is highly regarded by the content <unk> . this paper presents an INTERACTIVE TONE MAPPING SCHEME for HDR VIDEO SEQUENCES . INTERACTIVE TONE MAPPING SCHEME provides a simple SCRIB-BLE/STROKE BASED INTERFACE for LOCAL TONE MANIPULATION and is capable of propagating USER INPUT INFORMATION throughout a VIDEO SEQUENCE by using GAUSSIAN MIXTURE MODEL and EDGE PRESERVING FILTERING . the experimental results demonstrated its effectiveness for HDR IMAGE TONE MAPPING as well as its flexibility for users to easily and intuitively manipulate the appearance of the video while maintaining TEMPORAL CONSISTENCY . \n",
            "this paper presents a SCRIB-BLE/STROKE BASED INTERFACE for HDR VIDEO SEQUENCES . the INTERACTIVE TONE MAPPING SCHEME is based on a GAUSSIAN MIXTURE MODEL for the HDR IMAGE TONE MAPPING . the INTERACTIVE TONE MAPPING SCHEME is based on a GAUSSIAN MIXTURE MODEL . the proposed INTERACTIVE TONE MAPPING SCHEME is based on the GAUSSIAN MIXTURE MODEL . the INTERACTIVE TONE MAPPING SCHEME is based on the GAUSSIAN MIXTURE MODEL . the proposed INTERACTIVE TONE MAPPING SCHEME is based on a GAUSSIAN MIXTURE MODEL . the proposed INTERACTIVE TONE MAPPING SCHEME is based on a GAUSSIAN MIXTURE MODEL . the proposed INTERACTIVE TONE MAPPING SCHEME is based on a SCRIB-BLE/STROKE BASED INTERFACE . the proposed INTERACTIVE TONE MAPPING SCHEME is based on a SCRIB-BLE/STROKE BASED INTERFACE .\n",
            "\n",
            "293 1000\n",
            "-- COOPERATIVE COMMUNICATIONS leverages the SPATIAL DIVERSITY available in a WIRELESS NETWORK enabling multiple RADIO NODES work together to improve the overall system performance . when a DESTINATION RECEIVER combines the signal from an originating source with the associated signals from RELAY NODES , significant improvements in the BIT ERROR RATE performance can be achieved . this paper details the MEASURED BIT ERROR RATE performance of a THREE-NODE COOPERATIVE COMMUNICATION SYSTEM operating in a SOFTWARE DEFINED RADIO TESTBED . the measured performances of several types of COOPERATIVE PHYSICAL LAYER PROTOCOLS are compared to similar systems operating over a single WIRELESS LINK . the measured results include COOPERATIVE SYSTEMS operating with a MAXIMUM RATIO COMBINING TECHNIQUE and two COOPERATIVE CODED SYSTEMS using HARD DECISION DECODING . \n",
            "this paper addresses the problem of HARD DECISION DECODING for COOPERATIVE COMMUNICATIONS . the THREE-NODE COOPERATIVE COMMUNICATION SYSTEM is based on the MAXIMUM RATIO COMBINING TECHNIQUE and the MAXIMUM RATIO COMBINING TECHNIQUE . the proposed MAXIMUM RATIO COMBINING TECHNIQUE is based on the MAXIMUM RATIO COMBINING TECHNIQUE and the MAXIMUM RATIO COMBINING TECHNIQUE . the THREE-NODE COOPERATIVE COMMUNICATION SYSTEM is based on the MAXIMUM RATIO COMBINING TECHNIQUE and the MAXIMUM RATIO COMBINING TECHNIQUE . the proposed MAXIMUM RATIO COMBINING TECHNIQUE is evaluated on a WIRELESS LINK and a WIRELESS LINK . the THREE-NODE COOPERATIVE COMMUNICATION SYSTEM is evaluated on a WIRELESS LINK and a WIRELESS LINK . the results show that the proposed THREE-NODE COOPERATIVE COMMUNICATION SYSTEM is robust to SPATIAL DIVERSITY and SPATIAL DIVERSITY .\n",
            "\n",
            "294 1000\n",
            "we present a general approach to formally modelling corpora with MULTI-LAYERED ANNOTATION , thereby inducing a LEXICON MODEL in a TYPED LOGICAL REPRESENTATION LANGUAGE , OWL DL . this model can be interpreted as a GRAPH STRUCTURE that offers flexible querying <unk> beyond current XML-BASED QUERY LANGUAGES and powerful methods for CONSISTENCY CONTROL . we illustrate our approach by applying it to the SYNTACTICALLY AND SEMANTICALLY ANNOTATED SALSA/TIGER CORPUS . \n",
            "this paper presents a new method for XML-BASED QUERY LANGUAGES , which is based on a LEXICON MODEL . the proposed method is based on a LEXICON MODEL , which is based on the LEXICON MODEL . the proposed method is based on the SYNTACTICALLY AND SEMANTICALLY ANNOTATED SALSA/TIGER CORPUS . the proposed method is based on the SYNTACTICALLY AND SEMANTICALLY ANNOTATED SALSA/TIGER CORPUS . the proposed method is based on the SYNTACTICALLY AND SEMANTICALLY ANNOTATED SALSA/TIGER CORPUS .\n",
            "\n",
            "295 1000\n",
            "in this paper , we describe a method for SEGMENTING FIBER BUNDLES from DIFFUSION-WEIGHTED MAGNETIC RESONANCE IMAGES using a LOCALLY-CONSTRAINED REGION BASED APPROACH . from a pre-computed optimal path , the algorithm propagates <unk> capturing only those VOXELS which are locally connected to the FIBER BUNDLE . rather than attempting to find large numbers of OPEN CURVES or single fibers , which individually have questionable meaning , this method segments the FULL FIBER BUNDLE REGION . the strengths of this approach include its EASE-OF-USE , COMPUTATIONAL SPEED , and applicability to a wide range of FIBER BUNDLES . in this work , we show results for segmenting the CINGULUM BUNDLE . finally , we explain how this approach and extensions <unk> overcome a major problem that typical REGION-BASED FLOWS experience when attempting to segment neural FIBER BUNDLES . \n",
            "this paper presents a new method for SEGMENTING FIBER BUNDLES in DIFFUSION-WEIGHTED MAGNETIC RESONANCE IMAGES . the method is based on the LOCALLY-CONSTRAINED REGION BASED APPROACH and the LOCALLY-CONSTRAINED REGION BASED APPROACH . the proposed method is based on the LOCALLY-CONSTRAINED REGION BASED APPROACH and the LOCALLY-CONSTRAINED REGION BASED APPROACH . the proposed method is based on the LOCALLY-CONSTRAINED REGION BASED APPROACH and the LOCALLY-CONSTRAINED REGION BASED APPROACH . the proposed method is based on the LOCALLY-CONSTRAINED REGION BASED APPROACH and the LOCALLY-CONSTRAINED REGION BASED APPROACH . the proposed method is compared with the conventional LOCALLY-CONSTRAINED REGION BASED APPROACH and the LOCALLY-CONSTRAINED REGION BASED APPROACH .\n",
            "\n",
            "296 1000\n",
            "online <unk> handwriting recognition is currently one of the most intriguing challenges in PATTERN RECOGNITION . this study presents a novel approach to this ONLINE CURSIVE HANDWRITING RECOGNITION which is composed of two complementary phases . the first is DYNAMIC ENCODING of the WRITING TRA-JECTORY into a compact sequence of DISCRETE MOTOR CONTROL SYMBOLS . in this COMPACT REPRESENTATION we largely remove the redundancy of the script , while preserving most of its INTELLIGIBLE COMPONENTS . in the second phase these CONTROL SEQUENCES are used to train ADAPTIVE PROBABILISTIC ACYCLIC AUTOMATA for the important ingredients of the WRITING TRAJECTORIES , e.g. letters . we present a new and efficient LEARNING ALGORITHM for such STOCHASTIC AUTOMATA , and demonstrate its utility for SPOTTING AND SEGMENTATION OF CURSIVE SCRIPTS . our experiments show that over 90 % of the letters are correctly spotted and identified , prior to any higher LEVEL LANGUAGE MODEL . moreover , both the TRAINING AND RECOGNITION ALGORITHMS are very efficient compared to other MODELING METHODS , and the TRAINING AND RECOGNITION ALGORITHMS are ` on-line ' adaptable to other writers and styles . \n",
            "this paper addresses the problem of SPOTTING AND SEGMENTATION OF CURSIVE SCRIPTS in ONLINE CURSIVE HANDWRITING RECOGNITION . in this paper , we propose a novel LEARNING ALGORITHM for ONLINE CURSIVE HANDWRITING RECOGNITION . the proposed LEARNING ALGORITHM is based on the LEARNING ALGORITHM . the proposed LEARNING ALGORITHM is based on the LEARNING ALGORITHM , which is based on the LEARNING ALGORITHM . the proposed LEARNING ALGORITHM is based on the LEARNING ALGORITHM . the proposed LEARNING ALGORITHM is compared with other TRAINING AND RECOGNITION ALGORITHMS such as ONLINE CURSIVE HANDWRITING RECOGNITION . the proposed method is compared with other TRAINING AND RECOGNITION ALGORITHMS such as the TRAINING AND RECOGNITION ALGORITHMS .\n",
            "\n",
            "297 1000\n",
            "<unk> r-d optimized rate control for VIDEO CODING is investigated in this work . a FRAME LEVEL BIT ALLOCATION is first presented based on a model of the relationship between the rate -lrb- r -rrb- and NONZERO COEFFICIENTS . with the modelled <unk> relationship , a QUALITY FEEDBACK SCHEME is proposed to generate <unk> -lrb- video buffer <unk> -rrb- compliant bitstream with ASSURED VIDEO QUALITY . then , a R-D OPTIMIZED MACROBLOCK LEVEL RATE CONTROL is described by jointly selecting the QUANTIZATION PARAMETER and the CODING MODE OF MACROBLOCKS in i , b and p pictures for both PROGRESSIVE AND INTERLACED VIDEO . to avoid the <unk> large <unk> or one single ISOLATED COEFFICIENT , we extend the set of CODING modes of mb by including zero <unk> and zero texture bits as two more candidates . finally , FAST HEURISTICS are developed to reduce the COMPUTATIONAL COMPLEXITY of R-D DATA GENERATION and the VITERBI ALGORITHM in R-D OPTIMIZATION , which achieves CODING results close to the optimal one at a much lower COMPUTATIONAL COST . \n",
            "this paper proposes a new QUALITY FEEDBACK SCHEME for R-D DATA GENERATION . the VITERBI ALGORITHM is based on the VITERBI ALGORITHM and the VITERBI ALGORITHM . the proposed QUALITY FEEDBACK SCHEME is based on the VITERBI ALGORITHM and the VITERBI ALGORITHM . the proposed VITERBI ALGORITHM is based on the VITERBI ALGORITHM and the VITERBI ALGORITHM . the proposed QUALITY FEEDBACK SCHEME is evaluated on the PROGRESSIVE AND INTERLACED VIDEO . the results show that the proposed QUALITY FEEDBACK SCHEME can improve the COMPUTATIONAL COMPLEXITY of the VIDEO CODING .\n",
            "\n",
            "298 1000\n",
            "temporal processing and FILTERING in SPEECH FEATURE EXTRACTION are commonly used to aid in performance and ROBUSTNESS in AUTOMATIC SPEECH RECOGNITION . among the techniques successfully employed are RASTA FILTERING , DELTA CALCULATION , and CEPSTRAL MEAN SUBTRACTION . the work here explores the use of TEMPORAL FILTER DESIGN using LDA FILTERS to further enhance performance using a few PREPROCESSING CONFIGURATIONS . in addition to RASTA FILTERING , we apply the TEMPORAL FILTER DESIGN to MODULATION-SPECTRAL FEATURES and <unk> while making sure that the assumptions of LDA FILTERS are observed . we additionally test the use of TEMPORAL FILTER DESIGN that have been trained in different REVERBERATION CONDITIONS , noting from previous work that the presence of REVERBERATION alters the preferred frequency range of the derived TEMPORAL FILTER DESIGN . our tests indicate a consistent advantage in PHONE CLASSIFICATION . WORD RECOGNITION TESTS , in contrast , reveal that the LDA FILTERS often do not improve upon the existing TEMPORAL FILTER DESIGN previously used . they can also be made less <unk> by allowing CONTEXTUAL FRAMES to a TRAINED PROBABILITY ESTIMATOR . \n",
            "this paper addresses the problem of SPEECH FEATURE EXTRACTION in AUTOMATIC SPEECH RECOGNITION . we propose a TRAINED PROBABILITY ESTIMATOR for AUTOMATIC SPEECH RECOGNITION , which is based on the TRAINED PROBABILITY ESTIMATOR and the TRAINED PROBABILITY ESTIMATOR . the proposed TRAINED PROBABILITY ESTIMATOR is based on the TRAINED PROBABILITY ESTIMATOR and the TRAINED PROBABILITY ESTIMATOR . the ROBUSTNESS of the proposed TEMPORAL FILTER DESIGN is demonstrated by the use of LDA FILTERS and CEPSTRAL MEAN SUBTRACTION . the ROBUSTNESS of the proposed method is demonstrated by simulation results .\n",
            "\n",
            "299 1000\n",
            "in this paper , we investigate the practical applicability of CO-TRAINING for the task of building a CLASSIFIER for REFERENCE RESOLUTION . we are concerned with the question if CO-TRAINING can significantly reduce the amount of MANUAL LABELING work and still produce a CLASSIFIER with an acceptable performance . \n",
            "this paper presents a new CLASSIFIER for MANUAL LABELING . the CLASSIFIER is based on the use of CO-TRAINING for CO-TRAINING . the proposed CLASSIFIER is based on the CLASSIFIER and the CLASSIFIER is applied to the CLASSIFIER .\n",
            "\n",
            "300 1000\n",
            "the progress of DIGITAL ELECTROENCEPHALOGRAPHY gave rise to the problem of EEG DATA RECORDING . in this paper a DPCM SCHEME for EEG DATA RECORDING is discussed . in particular the performance of a class of PRE-DICTORS based on RECURRENT NEURAL NETWORKS is presented . the TRAINING STRATEGY is accurately described and the results of a comparison with some other classical linear and static neural predictors are given . the proposed RECURRENT NEURAL PREDICTOR demonstrates to be competitive with the others in <unk> good performance at a very low COMPUTATIONAL COST . \n",
            "this paper presents a new method for RECURRENT NEURAL NETWORKS based on RECURRENT NEURAL NETWORKS . the TRAINING STRATEGY is based on the TRAINING STRATEGY . the TRAINING STRATEGY is based on the TRAINING STRATEGY . the proposed TRAINING STRATEGY is based on the TRAINING STRATEGY . the TRAINING STRATEGY is based on the TRAINING STRATEGY . the TRAINING STRATEGY is applied to the EEG DATA RECORDING . the experimental results show that the proposed TRAINING STRATEGY can achieve a significant improvement in COMPUTATIONAL COST .\n",
            "\n",
            "301 1000\n",
            "spoken queries are a natural medium for searching the MOBILE WEB . LANGUAGE MODELING for VOICE SEARCH RECOGNITION offers different challenges compared to more conventional SPEECH APPLICATIONS . the challenges arise from the fact that SPOKEN QUERIES are usually a set of KEYWORDS and do not have a SYNTACTIC AND GRAMMATICAL STRUCTURE . this paper describes a CO-OCCURRENCE BASED APPROACH to improve the ACCURACY of VOICE QUERIES AUTOMATIC TRANSCRIPTION . with the right choice of SCORING FUNCTION and CO-OCCURRENCE LEVEL , we show that CO-OCCURRENCE INFORMATION gives a 2 % RELATIVE ACCURACY improvement over a state of the art system . \n",
            "this paper presents a CO-OCCURRENCE BASED APPROACH for VOICE SEARCH RECOGNITION . the proposed CO-OCCURRENCE BASED APPROACH is based on the CO-OCCURRENCE INFORMATION and the CO-OCCURRENCE INFORMATION . the proposed CO-OCCURRENCE BASED APPROACH is based on the CO-OCCURRENCE INFORMATION and the CO-OCCURRENCE INFORMATION . the proposed CO-OCCURRENCE BASED APPROACH is based on the CO-OCCURRENCE BASED APPROACH and the CO-OCCURRENCE BASED APPROACH . experimental results show that the proposed CO-OCCURRENCE BASED APPROACH can improve the ACCURACY of the VOICE QUERIES AUTOMATIC TRANSCRIPTION .\n",
            "\n",
            "302 1000\n",
            "one of the most difficult tasks in VALUE FUNCTION APPROXIMATION for MARKOV DECISION PROCESSES is finding an APPROXIMATION ARCHITECTURE that is expressive enough to capture the important structure in the VALUE FUNCTION , while at the same time not overfitting the training samples . recent results in NON-PARAMETRIC APPROXIMATE LINEAR PROGRAMMING , have demonstrated that this can be done effectively using nothing more than a SMOOTHNESS ASSUMPTION on the VALUE FUNCTION . in this paper we extend these results to the case where samples come from REAL WORLD TRANSITIONS instead of the full BELLMAN EQUATION , adding ROBUSTNESS to NOISE . in addition , we provide the first <unk> , finite sample performance guarantees for any form of ALP . NON-PARAMETRIC APPROXIMATE LINEAR PROGRAMMING is amenable to problems with large -lrb- multidimensional -rrb- or even INFINITE ACTION SPACES , and does not require a model to select actions using the resulting APPROXIMATE SOLUTION . \n",
            "this paper addresses the problem of MARKOV DECISION PROCESSES for INFINITE ACTION SPACES in INFINITE ACTION SPACES . we propose a APPROXIMATION ARCHITECTURE based on the VALUE FUNCTION APPROXIMATION . the proposed method is based on the VALUE FUNCTION APPROXIMATION . the proposed method is based on the VALUE FUNCTION APPROXIMATION . the proposed method is based on the APPROXIMATION ARCHITECTURE . the ROBUSTNESS of the proposed method is demonstrated by simulation results .\n",
            "\n",
            "303 1000\n",
            "in this paper , we use COMPUTER SIMULATION to better understand MIXED-INITIATIVE DIALOGUES . we compare two models of MIXED-INITIATIVE : UNRESTRICTED INITIATIVE , where either participant can take over control at any point ; and RESTRICTED INITIATIVE MODEL where one participant keeps control and the other plays a secondary role , but greater than what <unk> allows . we find that RESTRICTED INITIATIVE MODEL results in similar SOLUTION QUALITY as unrestricted , less communication effort , and similar or less reasoning effort . these results agree with our empirical studies on HUMAN-HUMAN DIALOGUES , in which we find that participants seem to follow the RESTRICTED INITIATIVE MODEL . \n",
            "this paper presents a new method for HUMAN-HUMAN DIALOGUES . the RESTRICTED INITIATIVE MODEL is based on the RESTRICTED INITIATIVE MODEL . the proposed RESTRICTED INITIATIVE MODEL is based on the RESTRICTED INITIATIVE MODEL . the proposed method is based on the RESTRICTED INITIATIVE MODEL and the RESTRICTED INITIATIVE MODEL . experimental results show that the proposed method outperforms the conventional method in terms of SOLUTION QUALITY and SOLUTION QUALITY .\n",
            "\n",
            "304 1000\n",
            "this paper introduces MATRIX FILTERS as a tool for LOCALIZA-TION AND DETECTION PROBLEMS in PASSIVE SONAR . the outputs of an array of sensors , at some given frequency , can be represented by a vector of complex numbers . a LINEAR FILTERING OPERATION on the SENSOR OUTPUTS can be expressed as the multiplication of a MATRIX -lrb- called a MATRIX FILTERS -rrb- times this vector . the purpose of a MATRIX FILTERS is to attenuate UNWANTED COMPONENTS in the MEASURED SENSOR DATA while passing desired components with MINIMAL DISTORTION . MATRIX FILTERS are designed by defining an appropriate pass band and stop band and solving a CONVEX OPTIMIZATION PROBLEM . this paper formulates the design of MATRIX FILTERS for PASSIVE SONAR and gives two examples . \n",
            "this paper addresses the problem of LOCALIZA-TION AND DETECTION PROBLEMS in PASSIVE SONAR . we propose a method for LOCALIZA-TION AND DETECTION PROBLEMS based on MATRIX FILTERS . the proposed method is based on a CONVEX OPTIMIZATION PROBLEM , which is based on the MATRIX FILTERS . the proposed method is based on the MATRIX FILTERS . the proposed method is based on the MATRIX FILTERS . the proposed method is based on the MATRIX FILTERS . the proposed method is based on the MATRIX FILTERS . the proposed method is based on a CONVEX OPTIMIZATION PROBLEM . the proposed method is based on the MATRIX FILTERS . the proposed method is evaluated on the LOCALIZA-TION AND DETECTION PROBLEMS .\n",
            "\n",
            "305 1000\n",
            "<unk> <unk> poses a particularly diicult challenge for conventional MONOPULSE RADARS . in such cases SPATIALLY ADAPTIVE PROCESSING provides some INTERFERENCE SUPPRESSION when the target and JAMMER are not exactly <unk> , but the resulting ARRAY PATTERN is too distorted to be suitable for MONOPULSE PROCESSING . the presence of COHERENT MULTI-PATH in the form of TERRAIN SCATTERED INTERFERENCE is normally considered a nuisance source of interference . however , it can also be exploited to suppress MAINBEAM JAMMING with SPACE-TIME PROCESSING . here we present a method for incorporating SPACE-TIME PROCESSING into MONOPULSE PROCESSING to yield a SPACE-TIME MONOPULSE PROCESSOR with DIS-TORTIONLESS SPATIAL ARRAY PATTERNS that can achieve far better MAINBEAM JAMMING CANCELATION and TARGET ANGLE ESTIMATION than has been previously possible . performance results for the SPACE-TIME MONOPULSE PROCESSOR are obtained for MOUNTAINTOP DATA containing a JAMMER and TERRAIN SCATTERED INTERFERENCE , that demonstrate a dramatic improvement in performance over conventional MONOPULSE AND SPATIALLY ADAPTIVE MONOPULSE . \n",
            "this paper addresses the problem of INTERFERENCE SUPPRESSION in SPACE-TIME PROCESSING . we propose a method for INTERFERENCE SUPPRESSION based on the DIS-TORTIONLESS SPATIAL ARRAY PATTERNS . the proposed method is based on a SPACE-TIME MONOPULSE PROCESSOR and a SPACE-TIME MONOPULSE PROCESSOR . the proposed method is based on a SPACE-TIME MONOPULSE PROCESSOR and a SPACE-TIME MONOPULSE PROCESSOR . the proposed method is based on a SPACE-TIME MONOPULSE PROCESSOR and a SPACE-TIME MONOPULSE PROCESSOR . the proposed method is based on a SPACE-TIME MONOPULSE PROCESSOR . the proposed method is based on a SPACE-TIME MONOPULSE PROCESSOR . the proposed method is based on a SPACE-TIME MONOPULSE PROCESSOR and a SPACE-TIME MONOPULSE PROCESSOR . experimental results show that the proposed method is robust and robust to INTERFERENCE SUPPRESSION and INTERFERENCE SUPPRESSION .\n",
            "\n",
            "306 1000\n",
            "however , permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists , or to reuse any COPYRIGHTED COMPONENT of this work in other works must be obtained from the IEEE . abstract a novel family of 2d and 3d geometrically invariant FEATURES , called SUMMATION INVARIANTS is proposed for the RECOGNITION OF THE 3D SURFACE OF HUMAN FACES . focusing on a RECTANGULAR REGION surrounding the NOSE of a 3D FACIAL DEPTH MAP , a subset of the so called SEMI-LOCAL SUM-MATION INVARIANT FEATURES is extracted . then the similarity between a pair of 3D FACIAL DEPTH MAPS is computed to determine whether SEMI-LOCAL SUM-MATION INVARIANT FEATURES belong to the same person . out of many possible combinations of these set of FEATURES , we select , through careful experimentation , a subset of FEATURES that yields best combined performance . tested with the 3D FACIAL DATA from the ongoing FACE RECOGNITION GRAND CHALLENGE V1 .0 DATASET , the proposed new FEATURES exhibit significant performance improvement over the baseline algorithm distributed with the dataset . \n",
            "this paper presents a method for RECOGNITION OF THE 3D SURFACE OF HUMAN FACES from 3D FACIAL DATA . the proposed method is based on a RECTANGULAR REGION , which is based on the COPYRIGHTED COMPONENT . the proposed method is based on the SEMI-LOCAL SUM-MATION INVARIANT FEATURES of the IEEE . the proposed method is based on the SEMI-LOCAL SUM-MATION INVARIANT FEATURES of the IEEE . the proposed method is based on the RECOGNITION OF THE 3D SURFACE OF HUMAN FACES . the proposed method is based on a FACE RECOGNITION GRAND CHALLENGE V1 .0 DATASET .\n",
            "\n",
            "307 1000\n",
            "this paper presents a novel SPECTRAL CONVERSION METHOD by considering the glottal effect on STRAIGHT SPECTRUM to improve the performance of FORMER VOICE CONVERSION SYSTEM based on CODEBOOK MAPPING . by introducing MOG MODEL into SPECTRAL REPRESENTATION , STRAIGHT SPECTRUM is decomposed into EXCITATION-DEPENDENT AND EXCITATION-INDEPENDENT COMPONENTS , which are transformed separately . besides , MOG MODEL is adopted to measure the PROSODIC CHARACTERISTICS of different speakers and realize PROSODIC CONVERSION . listening test proves that proposed SPECTRAL CONVERSION METHOD can effectively improve the DISCRIMINATION AND SPEECH QUALITY of CONVERTED SPEECH at the same time . \n",
            "this paper presents a SPECTRAL CONVERSION METHOD for PROSODIC CONVERSION . the FORMER VOICE CONVERSION SYSTEM is based on the CODEBOOK MAPPING . the MOG MODEL is based on the CODEBOOK MAPPING . the MOG MODEL is based on the CODEBOOK MAPPING . the MOG MODEL is based on the CODEBOOK MAPPING . the proposed SPECTRAL CONVERSION METHOD is based on the SPECTRAL CONVERSION METHOD . the proposed SPECTRAL CONVERSION METHOD is based on the MOG MODEL . the proposed SPECTRAL CONVERSION METHOD is based on the SPECTRAL CONVERSION METHOD . the SPECTRAL CONVERSION METHOD is applied to the FORMER VOICE CONVERSION SYSTEM .\n",
            "\n",
            "308 1000\n",
            "why does placing an object from one photograph into another often make the colors of that object suddenly look wrong ? one possibility is that humans prefer DISTRIBUTIONS OF COLORS that are often found in nature ; that is , we find pleasing these COLOR COMBINATIONS that we see often . another possibility is that humans simply prefer colors to be consistent within an image , regardless of what they are . in this paper , we explore some of these issues by studying the color statistics of a large dataset of NATURAL IMAGES , and by looking at differences in COLOR DISTRIBUTION in REALISTIC AND UNREALISTIC IMAGES . we apply our findings to two problems : 1 -rrb- CLASSIFYING COMPOSITE IMAGES into realistic vs. <unk> , and 2 -rrb- RECOLORING IMAGE REGIONS for REALISTIC COM-POSITING . \n",
            "this paper presents a method for CLASSIFYING COMPOSITE IMAGES from NATURAL IMAGES . the proposed method is based on the use of COLOR COMBINATIONS and COLOR COMBINATIONS . the proposed method is based on the DISTRIBUTIONS OF COLORS and the DISTRIBUTIONS OF COLORS . the proposed method is based on the DISTRIBUTIONS OF COLORS and the DISTRIBUTIONS OF COLORS .\n",
            "\n",
            "309 1000\n",
            "in this paper , it is shown that an appropriate model for VOICED SPEECH is an ALL-POLE FILTER excited by a BLOCK SPARSE EXCITATION SEQUENCE . the modeling approach is generalized in a novel manner to deal with a wide spectrum of SPEECH SIGNAL ; VOICED SPEECH , UNVOICED SPEECH and MIXED EXCITATION SPEECH . in this context , the input sequence to the all-pole model is modeled as a suitable WEIGHTED LINEAR COMBINATION of a block sparse signal and WHITE NOISE . we develop the corresponding ESTIMATION PROCEDURE to reconstruct the GENERALIZED INPUT SEQUENCE and MODEL PARAMETERS via SPARSE BAYESIAN LEARNING METHODS employing the EXPECTATION-MAXIMIZATION BASED PROCEDURE . rigorous experiments have been performed to show the efficacy of our proposed model for the SPEECH MODELING TASK . by imposing a BLOCK SPARSE STRUCTURE on the input sequence , the problems associated with the commonly used LINEAR PREDICTION APPROACH is alleviated leading to a more robust modeling scheme . \n",
            "this paper presents a new method for UNVOICED SPEECH . the proposed method is based on a WEIGHTED LINEAR COMBINATION , which is based on WEIGHTED LINEAR COMBINATION and SPARSE BAYESIAN LEARNING METHODS . the proposed method is based on a WEIGHTED LINEAR COMBINATION , which is based on a WEIGHTED LINEAR COMBINATION . the proposed method is based on a WEIGHTED LINEAR COMBINATION and a WEIGHTED LINEAR COMBINATION . the proposed method is based on the LINEAR PREDICTION APPROACH and the EXPECTATION-MAXIMIZATION BASED PROCEDURE . the proposed method is based on the EXPECTATION-MAXIMIZATION BASED PROCEDURE and the LINEAR PREDICTION APPROACH . the proposed method is based on WEIGHTED LINEAR COMBINATION and SPARSE BAYESIAN LEARNING METHODS . the proposed method is evaluated on the SPEECH MODELING TASK and SPEECH MODELING TASK .\n",
            "\n",
            "310 1000\n",
            "standard AUTOMATIC SPEECH RECOGNITION SYSTEMS use PHONEME-BASED PRONUNCIATION LEXICON prepared by linguistic experts . when the HAND CRAFTED PRONUNCIATIONS fail to cover the vocabulary of a new domain , a GRAPHEME-TO-PHONEME CONVERTER is used to extract PRONUNCIATIONS for new words and then a PHONEME-BASED ASR SYSTEM is trained . G2P CONVERTERS are typically trained only on the existing lexicons . in this paper , we propose a GRAPHEME-BASED ASR APPROACH in the framework of PROBABILISTIC LEXICAL MOD-ELING that integrates PRONUNCIATION LEARNING as a stage in ASR SYSTEM TRAINING , and exploits both ACOUSTIC AND LEXICAL RESOURCES -lrb- not necessarily from the domain or language of interest -rrb- . the proposed GRAPHEME-BASED ASR APPROACH is evaluated on four LEXICAL RESOURCE CONSTRAINED ASR TASKS and compared with the conventional two STAGE APPROACH where G2P TRAINING is followed by ASR SYSTEM DEVELOPMENT . \n",
            "this paper presents a novel GRAPHEME-BASED ASR APPROACH for ASR SYSTEM DEVELOPMENT . the proposed GRAPHEME-BASED ASR APPROACH is based on a GRAPHEME-TO-PHONEME CONVERTER , which is based on a GRAPHEME-TO-PHONEME CONVERTER . the proposed GRAPHEME-BASED ASR APPROACH is based on the STAGE APPROACH . the proposed GRAPHEME-BASED ASR APPROACH is based on the STAGE APPROACH . the proposed GRAPHEME-BASED ASR APPROACH is compared with the conventional GRAPHEME-BASED ASR APPROACH and the GRAPHEME-BASED ASR APPROACH . the proposed GRAPHEME-BASED ASR APPROACH is compared with the conventional GRAPHEME-BASED ASR APPROACH and the GRAPHEME-BASED ASR APPROACH . the proposed GRAPHEME-BASED ASR APPROACH is compared with the conventional GRAPHEME-BASED ASR APPROACH and the GRAPHEME-BASED ASR APPROACH .\n",
            "\n",
            "311 1000\n",
            "we present a GAME-THEORETIC MODEL of <unk> over a metaphor in the context of POLITICAL COMMUNICATION , find its equilibrium , and use GAME-THEORETIC MODEL to <unk> OBSERVED LINGUISTIC BEHAVIOR . we argue that GAME THEORY is well suited for MODELING DISCOURSE as a dynamic resulting from a number of conflicting <unk> , and suggest applications of interest to COMPUTATIONAL LINGUISTS . \n",
            "this paper presents a new GAME-THEORETIC MODEL for MODELING DISCOURSE . the GAME-THEORETIC MODEL is based on the GAME-THEORETIC MODEL . the GAME-THEORETIC MODEL is based on the GAME-THEORETIC MODEL . the GAME-THEORETIC MODEL is based on the GAME-THEORETIC MODEL and the GAME-THEORETIC MODEL is applied to the MODELING DISCOURSE . the experimental results show that the proposed GAME-THEORETIC MODEL can improve the performance of the GAME-THEORETIC MODEL .\n",
            "\n",
            "312 1000\n",
            "environmental MONITORING APPLICATIONS present a challenge to current BACKGROUND SUBTRACTION ALGORITHMS that analyze the TEMPORAL VARIABILITY OF PIXEL INTENSITIES , due to the complex texture and motion of the scene . they also present a challenge to SEGMENTATION ALGORITHMS that compare INTENSITY OR COLOR DISTRIBUTIONS between the foreground and the background in each image independently , because objects of interest such as animals have adapted to blend in . therefore , we have developed a BACKGROUND MODELING AND SUBTRACTION SCHEME that analyzes the temporal variation of INTENSITY OR COLOR DISTRIBUTIONS , instead of either looking at TEMPORAL VARIATION OF POINT STATISTICS , or the SPATIAL VARIATION OF REGION STATISTICS in isolation . DISTRIBUTIONAL SIGNATURES are less sensitive to movements of the TEX-TURED BACKGROUND , and at the same time DISTRIBUTIONAL SIGNATURES are more robust than individual pixel statistics in DETECTING FOREGROUND OBJECTS . they also enable slow background update , which is crucial in MONITORING APPLICATIONS where PROCESSING POWER comes at a <unk> , and where FOREGROUND OBJECTS , when present , may move less than the background and therefore disappear into it when a fast update scheme is used . our BACKGROUND MODELING AND SUBTRACTION SCHEME compares favorably with the state of the art both in GENERIC LOW-LEVEL DETECTION METRICS , as well as in APPLICATION-DEPENDENT CRITERIA . \n",
            "this paper presents a new BACKGROUND MODELING AND SUBTRACTION SCHEME for DETECTING FOREGROUND OBJECTS . the proposed BACKGROUND MODELING AND SUBTRACTION SCHEME is based on the TEMPORAL VARIATION OF POINT STATISTICS and the TEMPORAL VARIATION OF INTENSITY . the proposed BACKGROUND MODELING AND SUBTRACTION SCHEME is based on the SPATIAL VARIATION OF REGION STATISTICS and the TEMPORAL VARIATION OF POINT STATISTICS . the proposed BACKGROUND MODELING AND SUBTRACTION SCHEME is based on the SPATIAL VARIATION OF REGION STATISTICS and the SPATIAL VARIATION OF REGION STATISTICS . the proposed BACKGROUND MODELING AND SUBTRACTION SCHEME is evaluated on the BACKGROUND MODELING AND SUBTRACTION SCHEME and the BACKGROUND MODELING AND SUBTRACTION SCHEME . the results show that the proposed BACKGROUND MODELING AND SUBTRACTION SCHEME improves the performance of the proposed BACKGROUND MODELING AND SUBTRACTION SCHEME .\n",
            "\n",
            "313 1000\n",
            "consumer DEPTH CAMERAS have dramatically improved our ability to track rigid , articulated , and deformable 3d objects in real-time . however , DEPTH CAMERAS have a limited temporal resolution -lrb- <unk> -rrb- that restricts the ACCURACY and ROBUSTNESS of TRACKING , especially for FAST OR UNPREDICTABLE MOTION . in this paper , we show how to perform MODEL-BASED OBJECT TRACKING which allows to reconstruct the object 's depth at an order of magnitude higher <unk> through simple modifications to an OFF-THE-SHELF DEPTH CAMERA . we focus on PHASE-BASED TIME-OF-FLIGHT SENSING , which reconstructs each LOW FRAME-RATE DEPTH IMAGE from a set of short exposure ` raw ' infrared captures . these RAW CAPTURES are taken in quick <unk> near the beginning of each depth frame , and differ in the modulation of their ACTIVE ILLUMINATION . we make two contributions . first , we detail how to perform MODEL-BASED OBJECT TRACKING against these RAW CAPTURES . second , we show that by <unk> the camera to space the RAW CAPTURES uniformly in time , we obtain a <unk> higher <unk> , and thereby improve the ability to track FAST-MOVING OBJECTS . \n",
            "this paper presents a method for MODEL-BASED OBJECT TRACKING in CONSUMER DEPTH CAMERAS . the proposed method is based on a LOW FRAME-RATE DEPTH IMAGE , which is based on the RAW CAPTURES . the proposed method is based on the RAW CAPTURES . the proposed method is based on the RAW CAPTURES . the proposed method is based on the PHASE-BASED TIME-OF-FLIGHT SENSING . the proposed method is based on the PHASE-BASED TIME-OF-FLIGHT SENSING . the proposed method is based on the PHASE-BASED TIME-OF-FLIGHT SENSING . the proposed method is based on the PHASE-BASED TIME-OF-FLIGHT SENSING .\n",
            "\n",
            "314 1000\n",
            "decision making under uncertainty is commonly modelled as a process of COMPETITIVE STOCHASTIC EVIDENCE ACCUMULATION to threshold -lrb- the DRIFT-DIFFUSION MODEL -rrb- . however , it is unknown how animals learn these DECISION THRESHOLDS . we examine THRESHOLD LEARNING by constructing a REWARD FUNCTION that averages over many trials to WALD 'S COST FUNCTION that defines DECISION OPTIMALITY . these REWARD FUNCTION are highly stochastic and hence challenging to optimize , which we address in two ways : first , a simple TWO-FACTOR REWARD-MODULATED LEARNING RULE derived from WILLIAMS ' REINFORCE METHOD for NEURAL NETWORKS ; and second , BAYESIAN OPTIMIZATION of the REWARD FUNCTION with a GAUSSIAN PROCESS . BAYESIAN OPTIMIZATION converges in fewer trials than REINFORCE but is slower computationally with greater variance . the WILLIAMS ' REINFORCE METHOD is also a better model of ACQUISITION BEHAVIOUR in animals and a similar LEARNING RULE has been proposed for MODELLING BASAL GANGLIA FUNCTION . \n",
            "this paper proposes a new DRIFT-DIFFUSION MODEL for DECISION MAKING . the proposed WILLIAMS ' REINFORCE METHOD is based on a DRIFT-DIFFUSION MODEL , which is based on the LEARNING RULE . the proposed WILLIAMS ' REINFORCE METHOD is based on the LEARNING RULE . the proposed WILLIAMS ' REINFORCE METHOD is based on the LEARNING RULE . the proposed WILLIAMS ' REINFORCE METHOD is based on the WILLIAMS ' REINFORCE METHOD . the proposed WILLIAMS ' REINFORCE METHOD is based on the WILLIAMS ' REINFORCE METHOD . the proposed WILLIAMS ' REINFORCE METHOD is based on a DRIFT-DIFFUSION MODEL . the proposed WILLIAMS ' REINFORCE METHOD is compared with conventional NEURAL NETWORKS .\n",
            "\n",
            "315 1000\n",
            "this paper deals with MULTIPLE INPUT MULTIPLE OUTPUT SYSTEMS for ACTIVE CONTROL OF ACOUSTIC SIGNALS . these MULTIPLE INPUT MULTIPLE OUTPUT SYSTEMS are used when the ACOUSTIC ELD is complex and therefore a number of sensors are necessary to estimate the sound eld a n d a n <unk> of sources to create the CANCELLING ELD . a STEEPEST DESCENT ITERATIVE ALGORITHM is applied to minimise the p-norm of a MULTIPLE INPUT MULTIPLE OUTPUT SYSTEMS composed by the output signals of a microphone array . the existing algorithms deal with the 2-norm of this MULTIPLE INPUT MULTIPLE OUTPUT SYSTEMS . this paper describes a general framework that covers the existing MULTIPLE INPUT MULTIPLE OUTPUT SYSTEMS and then it focuses on the 1-NORM MINIMISATION ALGORITHM . the 1-NORM MINIMISATION ALGORITHM based on the 1-norm minimises the output signal which has the greatest power . it is shown by means of simulations using MEASURED DATA from a real room that the 1-NORM MINIMISATION ALGORITHM leads to a more UNIFORM NAL NOISE ELD than the existing algorithms . \n",
            "this paper proposes a new 1-NORM MINIMISATION ALGORITHM for ACTIVE CONTROL OF ACOUSTIC SIGNALS . the 1-NORM MINIMISATION ALGORITHM is based on the 1-NORM MINIMISATION ALGORITHM . the proposed 1-NORM MINIMISATION ALGORITHM is based on the 1-NORM MINIMISATION ALGORITHM . the proposed 1-NORM MINIMISATION ALGORITHM is based on the 1-NORM MINIMISATION ALGORITHM . the proposed 1-NORM MINIMISATION ALGORITHM is based on the 1-NORM MINIMISATION ALGORITHM . the proposed 1-NORM MINIMISATION ALGORITHM is based on the 1-NORM MINIMISATION ALGORITHM .\n",
            "\n",
            "316 1000\n",
            "in this paper , we address the problem of learning compact , <unk> , realistic 3d models of human actions recorded with multiple cameras , for the purpose of recognizing those same actions from a single or few cameras , without PRIOR KNOWLEDGE about the RELATIVE ORIENTA-TIONS between the cameras and the subjects . to this aim , we propose a new framework where we model actions using three DIMENSIONAL OCCUPANCY GRIDS , built from multiple viewpoints , in an EXEMPLAR-BASED HMM . the novelty is , that a 3D RECONSTRUCTION is not required during the RECOGNITION PHASE , instead learned 3D RECONSTRUCTION are used to produce 2D IMAGE INFORMATION that is compared to the observations . PARAMETERS that describe IMAGE PROJECTIONS are added as LATENT VARIABLES in the RECOGNITION PROCESS . in addition , the TEMPORAL MARKOV DEPENDENCY applied to VIEW PARAMETERS allows them to evolve during RECOGNITION as with a SMOOTHLY MOVING CAMERA . the effectiveness of the framework is demonstrated with experiments on REAL DATASETS and with challenging RECOGNITION SCENARIOS . \n",
            "this paper addresses the problem of 3D RECONSTRUCTION in DIMENSIONAL OCCUPANCY GRIDS . we propose a method for RECOGNITION based on PRIOR KNOWLEDGE . the proposed method is based on a SMOOTHLY MOVING CAMERA , which is based on the PRIOR KNOWLEDGE of the EXEMPLAR-BASED HMM . the proposed method is based on a SMOOTHLY MOVING CAMERA , which is based on the PRIOR KNOWLEDGE . the proposed method is based on the EXEMPLAR-BASED HMM . the proposed method is based on the use of LATENT VARIABLES and PRIOR KNOWLEDGE . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "317 1000\n",
            "a main issue in SOURCE SEPARATION is to deal with the INDE-TERMINACIES . well known are the ORDERING AND SCALE AMBIGUITIES , but other types of INDETERMINACIES may also occur . in this paper we address these INDETERMINACIES in the case of NON-NEGATIVE SOURCES and NON-NEGATIVE MIXING COEFFICIENTS . on the one hand , we fully develop the case of two sources . on the other hand , in the general case we formulate necessary conditions for the uniqueness of the solution -lrb- up to ORDERING AND SCALE AMBIGUITIES -rrb- . \n",
            "this paper presents a new method for SOURCE SEPARATION in NON-NEGATIVE SOURCES . the method is based on SOURCE SEPARATION and SOURCE SEPARATION . the method is based on the use of NON-NEGATIVE MIXING COEFFICIENTS and NON-NEGATIVE MIXING COEFFICIENTS . the proposed method is tested on NON-NEGATIVE SOURCES and NON-NEGATIVE SOURCES . the results show that the proposed method is robust and robust to SCALE AMBIGUITIES and SCALE AMBIGUITIES .\n",
            "\n",
            "318 1000\n",
            "this paper proposes a COMPUTATIONAL SYSTEM OF OBJECT CATEGORIZATION based on decomposition and ADAPTIVE FUSION OF VISUAL INFORMATION . a coupled CONDITIONAL RANDOM FIELD is developed to model the interaction between LOW LEVEL CUES OF CONTOUR and texture , and to decompose contour and texture in NATURAL IMAGES . the advantages of using coupled rather than SINGLE-LAYER RANDOM FIELDS are demonstrated with MODEL LEARNING and evaluation . MULTIPLE DECOMPOSED VISUAL CUES are adaptively combined for OBJECT CATEGORIZATION to fully leverage different DISCRIMINA-TIVE CUES for different classes . experimental results show that the proposed COMPUTATIONAL SYSTEM OF OBJECT CATEGORIZATION of '' <unk> '' achieves better performance than most of the state-of-the-art methods , especially when only a limited number of training samples are available . \n",
            "this paper presents a method for COMPUTATIONAL SYSTEM OF OBJECT CATEGORIZATION based on MULTIPLE DECOMPOSED VISUAL CUES . the proposed method is based on the ADAPTIVE FUSION OF VISUAL INFORMATION . the proposed method is based on the ADAPTIVE FUSION OF VISUAL INFORMATION . the proposed method is based on the ADAPTIVE FUSION OF VISUAL INFORMATION . the proposed method is based on the ADAPTIVE FUSION OF VISUAL INFORMATION . the proposed method is based on the ADAPTIVE FUSION OF VISUAL INFORMATION . the proposed method is based on the ADAPTIVE FUSION OF VISUAL INFORMATION . the proposed method is based on the ADAPTIVE FUSION OF VISUAL INFORMATION .\n",
            "\n",
            "319 1000\n",
            "cortical AMPLIFICATION has been proposed as a mechanism for enhancing the selectivity of neurons in the primary visual cortex . less <unk> is the fact that the same form of AMPLIFICATION can also be used to <unk> or broaden selectivity . using a NETWORK MODEL with RECURRENT CORTICAL CIRCUITRY , we propose that the spatial phase invariance of COMPLEX CELL RESPONSES arises through RECURRENT AMPLIFICATION OF FEEDFORWARD INPUT . neurons in the NETWORK MODEL respond like simple cells at low gain and complex cells at high gain . similar RECURRENT MECHANISMS may play a role in generating invariant representations of feedforward input elsewhere in the VISUAL PROCESSING PATHWAY . \n",
            "this paper addresses the problem of RECURRENT AMPLIFICATION OF FEEDFORWARD INPUT in COMPLEX CELL RESPONSES . we propose a NETWORK MODEL based on the NETWORK MODEL . the proposed method is based on the RECURRENT AMPLIFICATION OF FEEDFORWARD INPUT of the NETWORK MODEL . the proposed method is based on a NETWORK MODEL . the proposed method is based on a NETWORK MODEL .\n",
            "\n",
            "320 1000\n",
            "<unk> pure exploration in MULTI-ARMED BANDITS is a key component of MONTE-CARLO TREE SEARCH ALGORITHMS for SEQUENTIAL DECISION PROBLEMS . we introduce DISCRIMINATIVE BUCKETING , a novel family of strategies for pure exploration in MULTI-ARMED BANDITS , which allows for adapting recent advances in NON-INTERRUPTIBLE STRATEGIES to the INTERRUPTIBLE SETTING , while guaranteeing <unk> performance improvement over time . our experimental evaluation demonstrates that the corresponding instances of DISCRIMINATIVE BUCKETING favorably compete both with the currently popular strategies <unk> and Ε-GREEDY , as well as with the CONSERVATIVE UNIFORM SAMPLING . \n",
            "this paper addresses the problem of MULTI-ARMED BANDITS for SEQUENTIAL DECISION PROBLEMS . in this paper , we propose a new DISCRIMINATIVE BUCKETING based on CONSERVATIVE UNIFORM SAMPLING . the proposed MONTE-CARLO TREE SEARCH ALGORITHMS is based on the use of CONSERVATIVE UNIFORM SAMPLING , and is shown to outperform the conventional Ε-GREEDY in terms of Ε-GREEDY .\n",
            "\n",
            "321 1000\n",
            "we study the problem of identifying individuals based on their CHARACTERISTIC GAZE PATTERNS during reading of arbitrary text . the motivation for this problem is an <unk> <unk> setting in which a user is observed during access to a document , but no specific challenge protocol requiring the user 's time and attention is carried out . existing models of individual differences in GAZE CONTROL during reading are either based on simple AGGREGATE FEATURES OF EYE MOVEMENTS , or rely on PARAMET-RIC DENSITY MODELS to describe , for instance , SACCADE AMPLITUDES or WORD FIXATION DURATIONS . we develop FLEXIBLE SEMIPARAMETRIC MODELS of eye movements during reading in which densities are inferred under a GAUSSIAN PROCESS prior centered at a PARAMETRIC DISTRIBUTION FAMILY that is expected to approximate the true distribution well . an empirical study on reading data from <unk> individuals shows significant improvements over the state of the art . \n",
            "this paper addresses the problem of GAZE CONTROL in the presence of SACCADE AMPLITUDES . we propose a method for estimating the parameters of the PARAMETRIC DISTRIBUTION FAMILY . the proposed method is based on the AGGREGATE FEATURES OF EYE MOVEMENTS and the PARAMETRIC DISTRIBUTION FAMILY . the proposed method is based on the AGGREGATE FEATURES OF EYE MOVEMENTS and the CHARACTERISTIC GAZE PATTERNS . the proposed method is based on the AGGREGATE FEATURES OF EYE MOVEMENTS and the CHARACTERISTIC GAZE PATTERNS .\n",
            "\n",
            "322 1000\n",
            "in this paper , we propose a BILINGUAL LEXICAL COHESION TRIGGER MODEL to capture LEXICAL COHESION for DOCUMENT-LEVEL MACHINE TRANSLATION . we integrate the BILINGUAL LEXICAL COHESION TRIGGER MODEL into HIERARCHICAL PHRASE-BASED MACHINE TRANSLATION and achieve an absolute improvement of 0.85 bleu points on average over the baseline on NIST CHINESE-ENGLISH TEST SETS . \n",
            "this paper presents a new BILINGUAL LEXICAL COHESION TRIGGER MODEL for DOCUMENT-LEVEL MACHINE TRANSLATION . the BILINGUAL LEXICAL COHESION TRIGGER MODEL is based on the BILINGUAL LEXICAL COHESION TRIGGER MODEL . the proposed BILINGUAL LEXICAL COHESION TRIGGER MODEL is based on the BILINGUAL LEXICAL COHESION TRIGGER MODEL . the BILINGUAL LEXICAL COHESION TRIGGER MODEL is based on the BILINGUAL LEXICAL COHESION TRIGGER MODEL . the proposed BILINGUAL LEXICAL COHESION TRIGGER MODEL is applied to the NIST CHINESE-ENGLISH TEST SETS .\n",
            "\n",
            "323 1000\n",
            "model-based methods for SEQUENTIAL ORGANIZATION in COCHANNEL SPEECH require PRETRAINED SPEAKER MODELS and often prior knowledge of participating speakers . we propose an UNSUPERVISED APPROACH to SEQUENTIAL ORGANIZATION OF COCHANNEL SPEECH . based on CEPSTRAL FEATURES , we first cluster voiced speech into two speaker groups by maximizing the ratio of <unk> <unk> distances penalized by WITHIN-GROUP CONCURRENT PITCHES . to group UNVOICED SPEECH , we employ an ONSET/OFFSET BASED ANALYSIS to generate TIME-FREQUENCY SEGMENTS . UNVOICED SEGMENTS are then labeled by the complementary portions of SEGREGATED VOICED SPEECH . our UNSUPERVISED APPROACH does not require any PRETRAINED MODEL and is computationally simple . evaluations and comparisons show that the proposed UNSUPERVISED APPROACH outperforms a MODEL-BASED METHOD in terms of SPEECH SEGREGATION . \n",
            "this paper presents a new MODEL-BASED METHOD for SEQUENTIAL ORGANIZATION OF COCHANNEL SPEECH . the proposed MODEL-BASED METHOD is based on a MODEL-BASED METHOD of the TIME-FREQUENCY SEGMENTS . the proposed MODEL-BASED METHOD is based on the PRETRAINED MODEL . the proposed MODEL-BASED METHOD is based on the PRETRAINED MODEL . the proposed MODEL-BASED METHOD is based on the PRETRAINED MODEL . the proposed UNSUPERVISED APPROACH is compared with the conventional MODEL-BASED METHOD and the UNSUPERVISED APPROACH .\n",
            "\n",
            "324 1000\n",
            "context decision trees are widely used in the SPEECH RECOGNITION COMMUNITY . besides questions about <unk> classes of a phone 's context , questions about their position within a word <unk> -rsb- and questions about the gender of the current speaker <unk> -rsb- have been used so far . in this paper we additionally incorporate questions about current modalities of the SPOKEN UTTERANCE like the SPEAKER 'S DIALECT , the SPEAKING RATE , the signal to noise ratio , the latter two of which may change while speaking one utterance . we present a framework that treats all these modalities in a uniform way . experiments with the JANUS SPEECH RECOGNIZER have produced ERROR RATE REDUCTIONS of up to 10 % when compared to systems that do not use MODALITY QUESTIONS . \n",
            "this paper presents a JANUS SPEECH RECOGNIZER for SPEECH RECOGNITION COMMUNITY . the proposed JANUS SPEECH RECOGNIZER is based on the SPEAKER 'S DIALECT of the JANUS SPEECH RECOGNIZER . the proposed JANUS SPEECH RECOGNIZER is based on a JANUS SPEECH RECOGNIZER . the ERROR RATE REDUCTIONS of the proposed JANUS SPEECH RECOGNIZER is compared with the ERROR RATE REDUCTIONS of the JANUS SPEECH RECOGNIZER .\n",
            "\n",
            "325 1000\n",
            "we introduce a new CLASSIFICATION ALGORITHM based on the concept of SYMMETRIC MAXIMIZED MINIMAL DISTANCE in subspace -lrb- SMMS -rrb- . given the training data of authentic samples and IMPOSTER SAMPLES in the FEATURE SPACE , SMMS tries to identify a SUBSPACE in which all the authentic samples are close to each other and all the IMPOSTER SAMPLES are far away from the authentic samples . the OPTIMALITY of the SUBSPACE is determined by maximizing the minimal distance between the authentic samples and the IMPOSTER SAMPLES in the SUBSPACE . we present a procedure to achieve such OPTIMALITY and to identify the DECISION BOUNDARY . the verification procedure is simple since we only need to project the test sample to the SUBSPACE and compare it against the DECISION BOUNDARY . using FACE AUTHENTICATION as an example , we show that the proposed algorithm outperforms several other algorithms based on SUPPORT VECTOR MACHINES . \n",
            "this paper proposes a new method for FACE AUTHENTICATION based on a SYMMETRIC MAXIMIZED MINIMAL DISTANCE . the proposed method is based on a SYMMETRIC MAXIMIZED MINIMAL DISTANCE . the proposed method is based on the use of a SYMMETRIC MAXIMIZED MINIMAL DISTANCE . the proposed method is based on a SYMMETRIC MAXIMIZED MINIMAL DISTANCE . the proposed method is based on a SYMMETRIC MAXIMIZED MINIMAL DISTANCE . the proposed method is based on a SYMMETRIC MAXIMIZED MINIMAL DISTANCE . the proposed method is based on a SYMMETRIC MAXIMIZED MINIMAL DISTANCE . the proposed method is based on a SYMMETRIC MAXIMIZED MINIMAL DISTANCE .\n",
            "\n",
            "326 1000\n",
            "speech can be divided into DISCOURSE GENRES based on the CONTEXTUAL ENVIRONMENT it occurs in -lrb- e.g. POLITICAL SPEECH , SPORT COMMENTARY SPEECH , etc. -rrb- . the present study investigated whether listeners can distinguish between speech from different DISCOURSE GENRES on the basis of ACOUSTIC PROSODIC CUES only 1 . in a perception experiment with <unk> speech 70 listeners with varying experience in FRENCH -LRB- NATIVE SPEAKERS , NON-NATIVE SPEAKERS , and <unk> -rrb- were asked to identify four different types of DISCOURSE GENRES -lrb- <unk> service , political , journal , and SPORT COMMENTARY -rrb- . results revealed a fair IDENTIFICATION ABILITY with a significant increase in performance with increasing experience in FRENCH . IDENTIFICATION ABILITY was used to cluster DISCOURSE GENRES according to their PERCEPTUAL SIMILARITY . the possible application of the results for the evaluation of SPEAKING STYLE SPEECH SYNTHESIS will be discussed . \n",
            "this paper presents a new method for SPEAKING STYLE SPEECH SYNTHESIS in POLITICAL SPEECH . the proposed method is based on a CONTEXTUAL ENVIRONMENT , which is a CONTEXTUAL ENVIRONMENT and a PERCEPTUAL SIMILARITY . the IDENTIFICATION ABILITY is formulated as a CONTEXTUAL ENVIRONMENT , which is a CONTEXTUAL ENVIRONMENT . the IDENTIFICATION ABILITY is formulated as a CONTEXTUAL ENVIRONMENT , which is a CONTEXTUAL ENVIRONMENT . the IDENTIFICATION ABILITY is formulated as a CONTEXTUAL ENVIRONMENT . the proposed method is evaluated on the CONTEXTUAL ENVIRONMENT and the IDENTIFICATION ABILITY .\n",
            "\n",
            "327 1000\n",
            "this paper presents a VOICE CONVERSION TECHNIQUE using DEEP BELIEF NETS to build HIGH-ORDER EIGEN SPACES of the <unk> speakers , where VOICE CONVERSION TECHNIQUE is easier to convert the source speech to the target speech than in the traditional CEPSTRUM SPACE . DEEP BELIEF NETS have a DEEP ARCHITECTURE that automatically discovers ABSTRACTIONS to maximally express the original input features . if we train the DEEP BELIEF NETS using only the speech of an individual speaker , VOICE CONVERSION TECHNIQUE can be considered that there is less PHONOLOGI-CAL INFORMATION and relatively more SPEAKER INDIVIDUALITY in the output features at the highest layer . training the DEEP BELIEF NETS for a source speaker and a target speaker , we can then connect and convert the SPEAKER INDIVIDUALITY ABSTRACTIONS using NEURAL NETWORKS . the converted abstraction of the source speaker is then brought back to the CEPSTRUM SPACE using an INVERSE PROCESS of the DEEP BELIEF NETS of the target speaker . we conducted <unk> conversion experiments and confirmed the efficacy of our VOICE CONVERSION TECHNIQUE with respect to SUBJECTIVE AND OBJECTIVE CRITERIA , comparing VOICE CONVERSION TECHNIQUE with the conventional GAUSSIAN MIXTURE MODEL-BASED METHOD . \n",
            "this paper presents a new VOICE CONVERSION TECHNIQUE based on DEEP BELIEF NETS . the proposed VOICE CONVERSION TECHNIQUE is based on a GAUSSIAN MIXTURE MODEL-BASED METHOD . the proposed VOICE CONVERSION TECHNIQUE is based on the use of DEEP BELIEF NETS . the proposed GAUSSIAN MIXTURE MODEL-BASED METHOD is based on a GAUSSIAN MIXTURE MODEL-BASED METHOD . the proposed GAUSSIAN MIXTURE MODEL-BASED METHOD is compared with the conventional GAUSSIAN MIXTURE MODEL-BASED METHOD and the GAUSSIAN MIXTURE MODEL-BASED METHOD . the proposed GAUSSIAN MIXTURE MODEL-BASED METHOD is compared with the conventional GAUSSIAN MIXTURE MODEL-BASED METHOD and the GAUSSIAN MIXTURE MODEL-BASED METHOD .\n",
            "\n",
            "328 1000\n",
            "this paper proposes a PRIOR DISTRIBUTION DETERMINATION TECHNIQUE using CROSS VALIDATION for SPEECH RECOGNITION based on the BAYESIAN APPROACH . the DISTRIBUTION DETERMINATION TECHNIQUE is a STATISTICAL TECHNIQUE for estimating reliable PREDICTIVE DISTRIBUTIONS by marginalizing MODEL PARAMETERS and its APPROXIMATE VERSION , the VARIATIONAL BAYESIAN METHOD has been applied to HMM-BASED SPEECH RECOGNITION . since PRIOR DISTRIBUTIONS representing PRIOR INFORMATION about MODEL PARAMETERS affect the POSTERIOR DISTRIBUTIONS and MODEL SELECTION , the determination of PRIOR DISTRIBUTIONS is an important problem . however , it has not been thoroughly investigate in SPEECH RECOGNITION . the proposed DISTRIBUTION DETERMINATION TECHNIQUE can determine reliable PRIOR DISTRIBUTIONS without TUNING PARAMETERS and select an appropriate MODEL STRUCTURE <unk> on the amount of training data . continuous phoneme recognition experiments show that the proposed DISTRIBUTION DETERMINATION TECHNIQUE achieved a higher performance than the conventional methods . \n",
            "this paper proposes a PRIOR DISTRIBUTION DETERMINATION TECHNIQUE for SPEECH RECOGNITION . the VARIATIONAL BAYESIAN METHOD is based on the VARIATIONAL BAYESIAN METHOD and the VARIATIONAL BAYESIAN METHOD . the VARIATIONAL BAYESIAN METHOD is based on the PRIOR DISTRIBUTION DETERMINATION TECHNIQUE and the VARIATIONAL BAYESIAN METHOD . the proposed VARIATIONAL BAYESIAN METHOD is based on the PRIOR DISTRIBUTION DETERMINATION TECHNIQUE and the VARIATIONAL BAYESIAN METHOD . the proposed VARIATIONAL BAYESIAN METHOD is based on the PRIOR DISTRIBUTION DETERMINATION TECHNIQUE and the VARIATIONAL BAYESIAN METHOD . the proposed VARIATIONAL BAYESIAN METHOD is based on the PRIOR DISTRIBUTION DETERMINATION TECHNIQUE and the VARIATIONAL BAYESIAN METHOD . the proposed VARIATIONAL BAYESIAN METHOD is based on the VARIATIONAL BAYESIAN METHOD and the VARIATIONAL BAYESIAN METHOD . the proposed VARIATIONAL BAYESIAN METHOD is based on a PRIOR DISTRIBUTION DETERMINATION TECHNIQUE , and is shown to be useful for HMM-BASED SPEECH RECOGNITION .\n",
            "\n",
            "329 1000\n",
            "most HMM-BASED SPEECH RECOGNITION SYSTEMS use GAUSSIAN MIXTURES as OBSERVATION PROBABILITY DENSITY FUNCTIONS . an important goal in all such HMM-BASED SPEECH RECOGNITION SYSTEMS is to improve PARSIMONY . one method is to adjust the type of COVARIANCE MATRICES used . in this work , FAC-TORED SPARSE INVERSE COVARIANCE MATRICES are introduced . based on Í 1/4 Í FACTORIZATION , the INVERSE COVARIANCE MATRIX can be represented using LINEAR REGRESSIVE COEFFICIENTS which 1 -rrb- correspond to SPARSE PATTERNS in the INVERSE COVARIANCE MATRIX -lrb- and therefore represent CONDITIONAL INDEPENDENCE PROPERTIES of the gaussian -rrb- , and 2 -rrb- , result in a method of partial tying of the COVARIANCE MATRICES without requiring NON-LINEAR EM UPDATE EQUATIONS . results show that the performance of FULL-COVARIANCE GAUSSIANS can be matched by FACTORED SPARSE INVERSE COVARIANCE GAUSSIANS having significantly fewer parameters . \n",
            "this paper addresses the problem of FAC-TORED SPARSE INVERSE COVARIANCE MATRICES in HMM-BASED SPEECH RECOGNITION SYSTEMS . we propose a Í 1/4 Í FACTORIZATION based on FACTORED SPARSE INVERSE COVARIANCE GAUSSIANS . the proposed method is based on a Í 1/4 Í FACTORIZATION of the INVERSE COVARIANCE MATRIX . the proposed method is based on the use of FACTORED SPARSE INVERSE COVARIANCE GAUSSIANS to estimate the CONDITIONAL INDEPENDENCE PROPERTIES from the INVERSE COVARIANCE MATRIX . the proposed method is based on the use of FACTORED SPARSE INVERSE COVARIANCE GAUSSIANS to estimate the INVERSE COVARIANCE MATRIX from the INVERSE COVARIANCE MATRIX . the proposed method is compared with other HMM-BASED SPEECH RECOGNITION SYSTEMS in terms of CONDITIONAL INDEPENDENCE PROPERTIES .\n",
            "\n",
            "330 1000\n",
            "this paper addresses the problem of WALL CLUTTER MITIGATION in COMPRESSED SENSING THROUGH-THE-WALL RADAR IMAGING , where a different set of frequencies is sensed at different antenna locations . a JOINT BAYESIAN SPARSE APPROXIMATION FRAMEWORK is first employed to reconstruct all the signals simultaneously by exploiting SIGNAL SPARSITY and correlations between antenna signals . this is in contrast to previous approaches where the signal at each ANTENNA LOCATION is reconstructed independently . furthermore , to promote SPARSITY and improve RECONSTRUCTION ACCURACY , a SPARSIFYING WAVELET DICTIONARY is employed in the SPARSE SIGNAL RECOVERY . following WALL CLUTTER MITIGATION , a SUBSPACE PROJECTION TECHNIQUE is applied to remove WALL CLUTTER , prior to IMAGE FORMATION . experimental results on real data show that the proposed approach produces significantly higher RECONSTRUCTION ACCURACY and requires far fewer measurements for forming HIGH-QUALITY IMAGES , compared to the SINGLE-SIGNAL COMPRESSED SENSING MODEL , where each ANTENNA SIGNAL is reconstructed independently . \n",
            "this paper presents a JOINT BAYESIAN SPARSE APPROXIMATION FRAMEWORK for COMPRESSED SENSING THROUGH-THE-WALL RADAR IMAGING . the SINGLE-SIGNAL COMPRESSED SENSING MODEL is based on the SUBSPACE PROJECTION TECHNIQUE of the SPARSIFYING WAVELET DICTIONARY . the SINGLE-SIGNAL COMPRESSED SENSING MODEL is based on the SUBSPACE PROJECTION TECHNIQUE of the SPARSIFYING WAVELET DICTIONARY . the proposed SUBSPACE PROJECTION TECHNIQUE is based on the SUBSPACE PROJECTION TECHNIQUE . the proposed SUBSPACE PROJECTION TECHNIQUE is based on the SUBSPACE PROJECTION TECHNIQUE . the RECONSTRUCTION ACCURACY of the proposed SINGLE-SIGNAL COMPRESSED SENSING MODEL is demonstrated by simulation results . the RECONSTRUCTION ACCURACY of the proposed SINGLE-SIGNAL COMPRESSED SENSING MODEL is compared with the conventional SUBSPACE PROJECTION TECHNIQUE .\n",
            "\n",
            "331 1000\n",
            "we have recently proposed a new PLDA-BASED ACOUSTIC MODEL based on PROB-ABILISTIC LINEAR DISCRIMINANT ANALYSIS which enjoys the flexibility of using higher DIMENSIONAL ACOUSTIC FEATURES , and is more capable to capture the INTRA-FRAME FEATURE CORRELATIONS . in this paper , we investigate the use of BOTTLENECK FEATURES obtained from a DEEP NEURAL NETWORK for the PLDA-BASED ACOUSTIC MODEL . experiments were performed on the SWITCHBOARD DATASET -- a LARGE VOCABULARY CONVERSATIONAL TELEPHONE SPEECH CORPUS . we observe significant WORD ERROR REDUCTION by using the BOTTLENECK FEATURES . in addition , we have also compared the PLDA-BASED ACOUSTIC MODEL to three others using GAUSSIAN MIXTURE MODELS , SUBSPACE GMMS and HYBRID DEEP NEURAL NETWORKS , and PLDA can achieve comparable or slightly higher RECOGNITION ACCURACY from our experiments . \n",
            "this paper presents a novel PLDA-BASED ACOUSTIC MODEL for HYBRID DEEP NEURAL NETWORKS . the PLDA-BASED ACOUSTIC MODEL is based on a DEEP NEURAL NETWORK and a DEEP NEURAL NETWORK . the proposed PLDA-BASED ACOUSTIC MODEL is based on a DEEP NEURAL NETWORK and a DEEP NEURAL NETWORK . the PLDA-BASED ACOUSTIC MODEL is based on a DEEP NEURAL NETWORK and a DEEP NEURAL NETWORK . the RECOGNITION ACCURACY of the proposed PLDA-BASED ACOUSTIC MODEL is evaluated using the SWITCHBOARD DATASET and the SWITCHBOARD DATASET . the RECOGNITION ACCURACY of the proposed PLDA-BASED ACOUSTIC MODEL is compared with the conventional HYBRID DEEP NEURAL NETWORKS and the SWITCHBOARD DATASET .\n",
            "\n",
            "332 1000\n",
            "we address an ANOMALY DETECTION SETTING in which TRAINING SEQUENCES are unavailable and ANOMALIES are scored independently of TEMPORAL ORDERING . current algorithms in ANOMALY DETECTION SETTING are based on the CLASSICAL DENSITY ESTIMATION APPROACH of learning HIGH-DIMENSIONAL MODELS and finding LOW-PROBABILITY EVENTS . these algorithms are sensitive to the order in which ANOMALIES appear and require either TRAINING DATA or EARLY CONTEXT ASSUMPTIONS that do not hold for longer , more COMPLEX VIDEOS . by defining ANOMALIES as examples that can be distinguished from other examples in the same video , our definition inspires a shift in approaches from CLASSICAL DENSITY ESTIMATION to simple DISCRIMINATIVE LEARNING . our contributions include a novel framework for ANOMALY DETECTION SETTING that is -lrb- 1 -rrb- independent of TEMPORAL ORDERING OF ANOMALIES , and -lrb- 2 -rrb- unsupervised , requiring no separate TRAINING SEQUENCES . we show that our algorithm can achieve state-of-the-art results even when we adjust the setting by removing TRAINING SEQUENCES from standard datasets . \n",
            "this paper presents a new method for TEMPORAL ORDERING OF ANOMALIES from COMPLEX VIDEOS . the proposed method is based on the use of EARLY CONTEXT ASSUMPTIONS and DISCRIMINATIVE LEARNING . the proposed method is based on the CLASSICAL DENSITY ESTIMATION APPROACH and the CLASSICAL DENSITY ESTIMATION APPROACH . the proposed method is based on the CLASSICAL DENSITY ESTIMATION APPROACH and the CLASSICAL DENSITY ESTIMATION APPROACH . the proposed method is based on the CLASSICAL DENSITY ESTIMATION APPROACH and the CLASSICAL DENSITY ESTIMATION APPROACH . experimental results show the effectiveness of the proposed CLASSICAL DENSITY ESTIMATION APPROACH .\n",
            "\n",
            "333 1000\n",
            "we address the problem of answering new questions in COMMUNITY FORUMS , by selecting suitable answers to already asked questions . we approach the task as an ANSWER RANKING PROBLEM , adopting a PAIRWISE NEURAL NETWORK ARCHITECTURE that selects which of two competing answers is better . we focus on the utility of the three types of similarities occurring in the triangle formed by the original question , the related question , and an answer to the related comment , which we call relevance , RELATEDNESS , and appropriateness . our proposed PAIRWISE NEURAL NETWORK ARCHITECTURE models the interactions among all INPUT COMPONENTS using SYNTACTIC AND SEMANTIC EMBEDDINGS , LEXICAL MATCHING , and DOMAIN-SPECIFIC FEATURES . it achieves state-of-the-art results , showing that the three similarities are important and need to be mod-eled together . our experiments demonstrate that all FEATURE TYPES are relevant , but the most important ones are the LEXICAL SIMILARITY FEATURES , the DOMAIN-SPECIFIC FEATURES , and the SYNTACTIC AND SEMANTIC EMBEDDINGS . \n",
            "this paper presents a novel PAIRWISE NEURAL NETWORK ARCHITECTURE for COMMUNITY FORUMS . the PAIRWISE NEURAL NETWORK ARCHITECTURE is based on the PAIRWISE NEURAL NETWORK ARCHITECTURE and the PAIRWISE NEURAL NETWORK ARCHITECTURE . the PAIRWISE NEURAL NETWORK ARCHITECTURE is based on the PAIRWISE NEURAL NETWORK ARCHITECTURE and the PAIRWISE NEURAL NETWORK ARCHITECTURE . the proposed PAIRWISE NEURAL NETWORK ARCHITECTURE is based on the PAIRWISE NEURAL NETWORK ARCHITECTURE and the PAIRWISE NEURAL NETWORK ARCHITECTURE . the proposed PAIRWISE NEURAL NETWORK ARCHITECTURE is based on the PAIRWISE NEURAL NETWORK ARCHITECTURE and the PAIRWISE NEURAL NETWORK ARCHITECTURE . experimental results show the effectiveness of the proposed PAIRWISE NEURAL NETWORK ARCHITECTURE .\n",
            "\n",
            "334 1000\n",
            "language modeling for AUTOMATIC SPEECH RECOGNITION SYSTEMS has been traditionally in the VERBAL DOMAIN . in this paper , we present FINITE-STATE MODELING TECHNIQUES that we developed for LANGUAGE MODELING in the WRITTEN DOMAIN . the first FINITE-STATE MODELING TECHNIQUES we describe is for the VERBALIZATION OF WRITTEN-DOMAIN VOCABULARY ITEMS , which include LEXICAL AND NON-LEXICAL ENTITIES . the second FINITE-STATE MODELING TECHNIQUES is the DECOMPOSITION -- RECOMPOSITION APPROACH to address the OUT-OF-VOCABULARY and the DATA SPARSITY PROBLEMS with NON-LEXICAL ENTITIES such as URLS , E-MAIL ADDRESSES , PHONE NUMBERS , and DOLLAR AMOUNTS . we evaluate the proposed WRITTEN-DOMAIN LANGUAGE MODELING APPROACHES on a very large vocabulary SPEECH RECOGNITION system for EN-GLISH . we show that the WRITTEN-DOMAIN LANGUAGE MODELING APPROACHES improves the SPEECH RECOGNITION and the ASR TRANSCRIPT RENDERING ACCURACY in the WRITTEN DOMAIN over a baseline system using a VERBAL-DOMAIN LANGUAGE MODEL . in addition , the WRITTEN-DOMAIN LANGUAGE MODELING APPROACHES is much simpler since WRITTEN-DOMAIN LANGUAGE MODELING APPROACHES does not require complex and error-prone text normalization and DENORMALIZATION RULES , which are generally required for VERBAL-DOMAIN LANGUAGE MODELING . \n",
            "this paper addresses the problem of SPEECH RECOGNITION in AUTOMATIC SPEECH RECOGNITION SYSTEMS . we propose a DECOMPOSITION -- RECOMPOSITION APPROACH based on the DECOMPOSITION -- RECOMPOSITION APPROACH and the DECOMPOSITION -- RECOMPOSITION APPROACH . the proposed WRITTEN-DOMAIN LANGUAGE MODELING APPROACHES is based on the DECOMPOSITION -- RECOMPOSITION APPROACH and the DECOMPOSITION -- RECOMPOSITION APPROACH . the proposed WRITTEN-DOMAIN LANGUAGE MODELING APPROACHES is based on the DECOMPOSITION -- RECOMPOSITION APPROACH and the DECOMPOSITION -- RECOMPOSITION APPROACH . the proposed WRITTEN-DOMAIN LANGUAGE MODELING APPROACHES is based on the DECOMPOSITION -- RECOMPOSITION APPROACH and the DECOMPOSITION -- RECOMPOSITION APPROACH . the proposed DECOMPOSITION -- RECOMPOSITION APPROACH is based on the DECOMPOSITION -- RECOMPOSITION APPROACH and the DECOMPOSITION -- RECOMPOSITION APPROACH . experimental results are presented to demonstrate the effectiveness of the proposed WRITTEN-DOMAIN LANGUAGE MODELING APPROACHES .\n",
            "\n",
            "335 1000\n",
            "this paper gives a practical and accurate algorithm for the computation of the QUADRIFOCAL TENSOR and EXTRACTION OF CAMERA MATRICES from it . previous methods for using the QUADRIFOCAL TENSOR in PROJECTIVE SCENE RECONSTRUCTION have not emphasized ACCURACY of the algorithm in conditions of noise . methods given in this paper minimize ALGEBRAIC ERROR either through a NON-ITERATIVE LINEAR ALGORITHM , or two alternative ITERATIVE ALGORITHMS . it is shown by experiments with SYNTHETIC DATA that the ITERATIVE METHODS , though minimizing ALGEBRAIC , rather than more correctly geometric error measured in the image , give almost optimal results . \n",
            "this paper addresses the problem of PROJECTIVE SCENE RECONSTRUCTION in PROJECTIVE SCENE RECONSTRUCTION . in this paper , we propose a new NON-ITERATIVE LINEAR ALGORITHM based on the NON-ITERATIVE LINEAR ALGORITHM . the proposed NON-ITERATIVE LINEAR ALGORITHM is based on the NON-ITERATIVE LINEAR ALGORITHM and the NON-ITERATIVE LINEAR ALGORITHM . experimental results on SYNTHETIC DATA show that the proposed method achieves better performance than the conventional ITERATIVE ALGORITHMS .\n",
            "\n",
            "336 1000\n",
            "in this paper , we propose CO-PRIME ARRAYS for effective DIRECTION-OF-ARRIVAL ESTIMATION . to fully utilize the VIRTUAL APERTURE achieved in the difference <unk> constructed from a CO-PRIME ARRAY STRUCTURE , SPARSITY-BASED SPATIAL SPECTRUM ESTIMATION TECHNIQUE is exploited . compared to existing techniques , the proposed technique achieves better utilization of the CO-ARRAY APERTURE and thus results in increased DEGREES-OF-FREEDOM as well as improved DOA ESTIMATION PERFORMANCE . \n",
            "this paper addresses the problem of DIRECTION-OF-ARRIVAL ESTIMATION in CO-PRIME ARRAYS . we propose a method for DIRECTION-OF-ARRIVAL ESTIMATION based on CO-PRIME ARRAYS . the proposed method is based on the use of a VIRTUAL APERTURE and a VIRTUAL APERTURE . the proposed method is based on the SPARSITY-BASED SPATIAL SPECTRUM ESTIMATION TECHNIQUE . the experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "337 1000\n",
            "naive bayes -lrb- nb -rrb- is well-known to be a simple but effective CLASSIFIER , especially when combined with FEATURE SELECTION . unfortunately , FEATURE SELECTION METHODS are often greedy and thus can not guarantee an optimal feature set is selected . an alternative to FEATURE SELECTION is to use BAYESIAN MODEL averaging -lrb- BMA -rrb- , which computes a weighted average over multiple predictors ; when the different PREDICTOR MODELS correspond to different feature sets , BMA has the advantage over FEATURE SELECTION that its predictions tend to have lower variance on average in comparison to any single model . in this paper , we show for the first time that BMA-NB CLASSIFIER is possible to exactly evaluate BMA over the <unk> <unk> of NB FEATURE MODELS in linear-time in the number of FEATURES ; this yields an algorithm about as expensive to train as a single NB MODEL with all FEATURES , but yet provably converges to the GLOBALLY OPTIMAL FEATURE SUBSET in the ASYMPTOTIC LIMIT OF DATA . we evaluate this novel BMA-NB CLASSIFIER on a range of datasets showing that BMA-NB CLASSIFIER never <unk> nb -lrb- as expected -rrb- and sometimes offers performance competitive -lrb- or superior -rrb- to CLASSIFIERS such as SVMS and logistic regression while taking a fraction of the time to train . \n",
            "this paper addresses the problem of FEATURE SELECTION for ASYMPTOTIC LIMIT OF DATA . we propose a new BAYESIAN MODEL , called NAIVE BAYES , that is based on the ASYMPTOTIC LIMIT OF DATA . the proposed BAYESIAN MODEL is based on the ASYMPTOTIC LIMIT OF DATA . the proposed BAYESIAN MODEL is based on the ASYMPTOTIC LIMIT OF DATA , and is based on the ASYMPTOTIC LIMIT OF DATA . the proposed BAYESIAN MODEL is compared with other FEATURE SELECTION METHODS such as SVMS and SVMS . the experimental results show that the proposed BAYESIAN MODEL outperforms the conventional SVMS .\n",
            "\n",
            "338 1000\n",
            "voice morphing is a technique for modifying a source speaker 's speech to sound as if VOICE MORPHING was spoken by some designated target speaker . most of the recent approaches to VOICE MORPHING apply a LINEAR TRANSFORMATION to the SPECTRAL ENVELOPE and PITCH SCALING to modify the PROSODY . whilst these methods are effective , they also introduce artifacts arising from the effects of GLOTTAL COUPLING , PHASE INCOHERENCE , UNNATURAL PHASE DISPERSION and the HIGH SPECTRAL VARIANCE OF UNVOICED SOUNDS . a practical VOICE MORPHING SYSTEM must account for these if high AUDIO QUALITY is to be preserved . this paper describes a complete VOICE MORPHING SYSTEM and the enhancements needed for dealing with the various artifacts , including a novel method for synthesising NATURAL PHASE DISPERSION . each technique is assessed individually and the overall performance of the VOICE MORPHING SYSTEM evaluated using LISTENING TESTS . overall VOICE MORPHING is found that the enhancements significantly improve SPEAKER IDENTIFICATION SCORES and PERCEIVED AUDIO QUALITY . \n",
            "this paper presents a new method for HIGH SPECTRAL VARIANCE OF UNVOICED SOUNDS in the VOICE MORPHING SYSTEM . the proposed method is based on the use of a LINEAR TRANSFORMATION and a LINEAR TRANSFORMATION . the proposed method is based on a LINEAR TRANSFORMATION and a LINEAR TRANSFORMATION . the proposed VOICE MORPHING SYSTEM is based on the UNNATURAL PHASE DISPERSION and the GLOTTAL COUPLING . the proposed VOICE MORPHING SYSTEM is evaluated on LISTENING TESTS and LISTENING TESTS . the results show that the proposed method is robust and robust to PERCEIVED AUDIO QUALITY and AUDIO QUALITY .\n",
            "\n",
            "339 1000\n",
            "occlusions provide critical cues about the 3D STRUCTURE OF MAN-MADE AND NATURAL SCENES . we present a MATHEMATICAL FRAMEWORK and algorithm to detect and localize OCCLUSIONS in image sequences of scenes that include DEFORMING OBJECTS . our OCCLUSION DETECTOR works under far weaker assumptions than other detectors . we prove that OCCLU-SIONS in DEFORMING SCENES occur when certain WELL-DEFINED LOCAL TOPOLOGI-CAL INVARIANTS are not preserved . our framework employs these invariants to detect OCCLUSIONS with a ZERO FALSE POSITIVE RATE under assumptions of BOUNDED DEFORMATIONS and COLOR VARIATION . the novelty and strength of this methodology is that it does not rely on SPATIO-TEMPORAL DERIVATIVES or MATCHING , which can be problematic in scenes including DEFORMING OBJECTS , but is instead based on a MATHEMATICAL REPRESENTATION of the underlying cause of OCCLUSIONS in a DEFORMING 3D SCENE . we demonstrate the effectiveness of the OCCLUSION DETECTOR using IMAGE SEQUENCES OF NATURAL SCENES , including DEFORMING CLOTH AND HAND MOTIONS . \n",
            "this paper presents a novel MATHEMATICAL FRAMEWORK for IMAGE SEQUENCES OF NATURAL SCENES . the MATHEMATICAL FRAMEWORK is based on a MATHEMATICAL FRAMEWORK and a MATHEMATICAL FRAMEWORK . the proposed MATHEMATICAL FRAMEWORK is based on the MATHEMATICAL REPRESENTATION and the MATHEMATICAL FRAMEWORK . the proposed MATHEMATICAL FRAMEWORK is based on the MATHEMATICAL FRAMEWORK and the MATHEMATICAL FRAMEWORK . the proposed MATHEMATICAL FRAMEWORK is based on the MATHEMATICAL REPRESENTATION and the MATHEMATICAL FRAMEWORK . the proposed method is based on the MATHEMATICAL FRAMEWORK and the MATHEMATICAL FRAMEWORK . the proposed method is based on the MATHEMATICAL FRAMEWORK and the MATHEMATICAL FRAMEWORK . the proposed method is tested on IMAGE SEQUENCES OF NATURAL SCENES and on IMAGE SEQUENCES OF NATURAL SCENES .\n",
            "\n",
            "340 1000\n",
            "in our paper , we divide the corpus into 8 domains through TEXT CLASSIFICATION using K-MEANS ALGORITHM , and calculate the TRIGRAM LMS for each one . but the experiment shows the performance in some ones becomes worse . in order to solve this problem , we try to do the LM ADAPTATION based on the DOMAIN LMS . the adaptation is done by mixing the DOMAIN LMS with the BACKGROUND LM by a LINEAR INTERPOLATION . RELATIVE WORD ERROR RATE REDUCTIONS of between 5 and 10 % over the PRUNED BACKGROUND LM are achieved . \n",
            "this paper proposes a new K-MEANS ALGORITHM for TEXT CLASSIFICATION . the proposed K-MEANS ALGORITHM is based on the K-MEANS ALGORITHM and the K-MEANS ALGORITHM . the proposed K-MEANS ALGORITHM is based on the K-MEANS ALGORITHM and the K-MEANS ALGORITHM . the proposed K-MEANS ALGORITHM is based on the K-MEANS ALGORITHM and the K-MEANS ALGORITHM . experimental results show that the proposed K-MEANS ALGORITHM can improve the performance of the K-MEANS ALGORITHM .\n",
            "\n",
            "341 1000\n",
            "we consider the problem of LEARNING CONTROL POLICIES via TRAJECTORY PREFERENCE QUERIES to an expert . in particular , the LEARNING CONTROL POLICIES presents an expert with short runs of a pair of POLICIES originating from the same state and the expert indicates which trajectory is preferred . the LEARNING CONTROL POLICIES 's goal is to elicit a LATENT TARGET POLICY from the expert with as few queries as possible . to tackle this problem we propose a novel BAYESIAN MODEL of the QUERYING PROCESS and introduce two methods that exploit this BAYESIAN MODEL to actively select expert queries . experimental results on four benchmark problems indicate that our BAYESIAN MODEL can effectively learn POLICIES from TRAJECTORY PREFERENCE QUERIES and that ACTIVE QUERY SELECTION can be substantially more efficient than RANDOM SELECTION . \n",
            "this paper addresses the problem of ACTIVE QUERY SELECTION for ACTIVE QUERY SELECTION . the BAYESIAN MODEL is based on the use of a BAYESIAN MODEL . the proposed BAYESIAN MODEL is based on the BAYESIAN MODEL . the BAYESIAN MODEL is shown to outperform the conventional POLICIES in terms of ACTIVE QUERY SELECTION .\n",
            "\n",
            "342 1000\n",
            "we derive a recursive <unk> pruned <unk> fast fourier transform -lrb- FFT -rrb- algorithm in KRONECKER PRODUCT NOTATION . the algorithm is compatible with VECTORIZATION and parallelization required on state-of-the-art MULTICORE CPUS . we include the PRUNED FFT ALGORITHM into the PROGRAM GENERATION SYSTEM SPIRAL , and automatically generate optimized implementations of the pruned FFT for the INTEL CORE2DUO MULTICORE PROCESSOR . experimental results show that using the pruned FFT can indeed speed up the fastest available FFT IMPLEMENTATIONS by up to 30 % when the PROBLEM SIZE and the pattern of UNUSED INPUTS and outputs are known in advance . \n",
            "this paper presents a new method for PROGRAM GENERATION SYSTEM SPIRAL based on KRONECKER PRODUCT NOTATION . the proposed method is based on the PRUNED FFT ALGORITHM and the PRUNED FFT ALGORITHM . the proposed method is based on the PRUNED FFT ALGORITHM and the PRUNED FFT ALGORITHM . the proposed method is based on the PRUNED FFT ALGORITHM and the PRUNED FFT ALGORITHM . the experimental results show that the proposed method outperforms the conventional method in terms of PROBLEM SIZE and VECTORIZATION .\n",
            "\n",
            "343 1000\n",
            "we investigate the utility of SUPERTAG INFORMATION for guiding an existing DEPENDENCY PARSER OF GERMAN . using WEIGHTED CONSTRAINTS to integrate the additionally available information , the DECISION PROCESS of the DEPENDENCY PARSER OF GERMAN is influenced by changing its preferences , without excluding alternative structural interpretations from being considered . the paper reports on a series of experiments using varying MODELS OF SU-PERTAGS that significantly increase the PARSING ACCURACY . in addition , an upper bound on the ACCURACY that can be achieved with perfect SUPERTAGS is estimated . \n",
            "this paper presents a method for DEPENDENCY PARSER OF GERMAN based on WEIGHTED CONSTRAINTS . the proposed method is based on the use of WEIGHTED CONSTRAINTS and WEIGHTED CONSTRAINTS . the proposed method is based on the MODELS OF SU-PERTAGS . the proposed method is based on the MODELS OF SU-PERTAGS of the DECISION PROCESS . the ACCURACY of the proposed method is compared with the conventional SUPERTAGS .\n",
            "\n",
            "344 1000\n",
            "we investigate the incorporation of context into the spoken language understanding -lrb- slu -rrb- sub-tasks of INTENT PREDICTION and SLOT DETECTION . using a corpus that contains information about whole sessions rather than just single utterances , we experiment with the incorporation of information from previous INTRA-SESSION UTTERANCES into the SLU TASKS on a given utterance . for SLOT DETECTION , we find no significant increase using CRF FEATURES indicating slots in previous utterances . for INTENT PREDICTION , we achieve ERROR RATE REDUCTIONS of upto 8.7 % by incorporating the intent of the previous utterance as an SVM FEATURE , and similar gains when treating INTENT PREDICTION as a SEQUENTIAL TAGGING PROBLEM with SVM-HMMS . u 1 get clip show me the -lsb- <unk> -rsb- content − name -lsb- <unk> -rsb- type show me the -lsb- <unk> -rsb- content − name -lsb- <unk> -rsb- type u 2 find <unk> who directed -lsb- it -rsb- content − name − <unk> who directed -lsb- it -rsb- content − name − <unk> u 3 find content what else has -lsb- he -rsb- director − <unk> done what else has -lsb- he -rsb- director − <unk> done u 4 play content play -lsb- the <unk> -rsb- content − name plane -lsb- <unk> -rsb- content − name traditionally , both intents and -lsb- slots -rsb- are PREDICTED PER-UTTERANCE , while ignoring previous utterances within the session . however , the data is gathered not one utterance at a time but one session at a time ; each utterance occurs in the context of a larger DISCOURSE . we examine the effect of incorporating information from previous INTRA-SESSION UTTERANCES -lrb- <unk> <unk> , context -rrb- . CONTEXT can serve as an additional source of information and help get around other errors such as those introduced during the ASR PROCESS . \n",
            "this paper presents a new method for SLOT DETECTION based on CRF FEATURES . the proposed method is based on a SEQUENTIAL TAGGING PROBLEM , which is based on a SEQUENTIAL TAGGING PROBLEM . the proposed method is based on a SEQUENTIAL TAGGING PROBLEM , which is based on a SEQUENTIAL TAGGING PROBLEM . the proposed method is based on a SEQUENTIAL TAGGING PROBLEM , which is based on a SEQUENTIAL TAGGING PROBLEM . the proposed method is based on a SEQUENTIAL TAGGING PROBLEM and a SEQUENTIAL TAGGING PROBLEM . the proposed method is based on a SEQUENTIAL TAGGING PROBLEM , which is based on a SEQUENTIAL TAGGING PROBLEM . the proposed method is evaluated on SLU TASKS and SLU TASKS .\n",
            "\n",
            "345 1000\n",
            "we present a new approach to INTRINSIC SUMMARY EVALUATION , based on initial experiments in van <unk> and <unk> -lrb- 2003 -rrb- , which combines two novel aspects : comparison of information content -lrb- rather than STRING SIMILARITY -rrb- in gold standard and SYSTEM SUMMARY , measured in SHARED ATOMIC INFORMATION UNITS which we call <unk> , and comparison to more than one gold standard summary -lrb- in our data : 20 and 50 summaries respectively -rrb- . in this paper , we show that FACTOID ANNOTATION is highly reproducible , introduce a WEIGHTED FACTOID SCORE , estimate how many summaries are required for STABLE SYSTEM RANKINGS , and show that the FAC-TOID SCORES can not be sufficiently approximated by UNIGRAMS and the DUC INFORMATION OVERLAP MEASURE . \n",
            "this paper presents a new method for FACTOID ANNOTATION based on SHARED ATOMIC INFORMATION UNITS . the proposed method is based on the DUC INFORMATION OVERLAP MEASURE of the SHARED ATOMIC INFORMATION UNITS and the STRING SIMILARITY . the proposed method is based on the DUC INFORMATION OVERLAP MEASURE of the SHARED ATOMIC INFORMATION UNITS and the STRING SIMILARITY . the proposed method is based on the DUC INFORMATION OVERLAP MEASURE of the SYSTEM SUMMARY and the DUC INFORMATION OVERLAP MEASURE of the SYSTEM SUMMARY .\n",
            "\n",
            "346 1000\n",
            "this paper describes an empirical study of the '' INFORMATION SYNTHESIS '' TASK , defined as the process of -lrb- given a complex information need -rrb- extracting , organizing and <unk> the pieces of information contained in a set of relevant documents , in order to obtain a comprehensive , non redundant report that satisfies the information need . two main results are presented : a -rrb- the creation of an INFORMATION SYNTHESIS TESTBED with 72 reports manually generated by nine subjects for eight complex topics with 100 relevant documents each ; and b -rrb- an empirical comparison of SIMILARITY METRICS between reports , under the hypothesis that the best metric is the one that best distinguishes between manual and automatically generated reports . a metric based on key concepts overlap gives better results than metrics based on N-GRAM OVERLAP -lrb- such as rouge -rrb- or SENTENCE OVERLAP . \n",
            "this paper presents a new method for the INFORMATION SYNTHESIS '' TASK . the proposed method is based on the use of SIMILARITY METRICS as the basis for the INFORMATION SYNTHESIS TESTBED . the proposed method is based on the use of a INFORMATION SYNTHESIS TESTBED . the proposed method is compared with the conventional SIMILARITY METRICS .\n",
            "\n",
            "347 1000\n",
            "we introduce a new approach for recognizing and reconstructing 3d objects in images . our approach is based on an analysis by SYNTHESIS STRATEGY . a FORWARD SYNTHESIS MODEL constructs possible GEOMETRIC INTERPRETATIONS OF THE WORLD , and then selects the interpretation that best agrees with the MEASURED VISUAL EVIDENCE . the FORWARD SYNTHESIS MODEL synthesizes VISUAL TEMPLATES defined on INVARIANT FEATURES . these VISUAL TEMPLATES are discriminatively trained to be accurate for INVERSE ESTIMATION . we introduce an efficient '' BRUTE-FORCE '' APPROACH to INFERENCE that searches through a large number of CANDIDATE RECONSTRUCTIONS , returning the optimal one . one benefit of such an approach is that RECOGNITION is inherently -lrb- re -rrb- constructive . we show state of the art performance for DETECTION and reconstruction on two challenging 3D OBJECT RECOGNITION DATASETS of cars and <unk> . \n",
            "this paper presents a new SYNTHESIS STRATEGY for INVERSE ESTIMATION . the '' BRUTE-FORCE '' APPROACH is based on the '' BRUTE-FORCE '' APPROACH . the '' BRUTE-FORCE '' APPROACH is based on the '' BRUTE-FORCE '' APPROACH . the proposed '' BRUTE-FORCE '' APPROACH is based on the '' BRUTE-FORCE '' APPROACH . the proposed '' BRUTE-FORCE '' APPROACH is based on the '' BRUTE-FORCE '' APPROACH . the proposed '' BRUTE-FORCE '' APPROACH is based on the '' BRUTE-FORCE '' APPROACH . the proposed '' BRUTE-FORCE '' APPROACH is based on the '' BRUTE-FORCE '' APPROACH . the proposed '' BRUTE-FORCE '' APPROACH is based on the '' BRUTE-FORCE '' APPROACH . the proposed '' BRUTE-FORCE '' APPROACH is applied to the 3D OBJECT RECOGNITION DATASETS and is shown to outperform the conventional '' BRUTE-FORCE '' APPROACH in terms of DETECTION .\n",
            "\n",
            "348 1000\n",
            "this work addresses the problem of developing a DOMAIN-INDEPENDENT BINARY CLASSIFIER for a TEST DOMAIN given labeled data from several training domains where the TEST DOMAIN is not necessarily present in TRAINING DATA . the DOMAIN-INDEPENDENT BINARY CLASSIFIER accepts or rejects the ASR HYPOTHESIS based on the confidence generated by the ASR HYPOTHESIS . in the proposed approach , TRAINING DATA is grouped into ACROSS-DOMAIN CLUSTERS and separate CLUSTER-SPECIFIC CLASSIFIERS are trained . one of the main findings is that the CLUSTER PURITY and the NORMALIZED MUTUAL INFORMATION of the clusters are not very high which suggests that the domains might not necessarily be NATURAL CLUSTERS . the performance of these CLUSTER-SPECIFIC CLASSIFIERS is better than that of : -lrb- a -rrb- a single DOMAIN-INDEPENDENT BINARY CLASSIFIER trained on data from all the domains , and -lrb- b -rrb- a set of CLASSIFIERS trained separately for each of the training domains . at an operating point corresponding to low false accept , the correct accept of the proposed technique is on an average 2.3 % higher than that obtained by the SINGLE-CLASSIFIER or the individual <unk> CLASSIFIERS . \n",
            "this paper presents a new DOMAIN-INDEPENDENT BINARY CLASSIFIER for TRAINING DATA . the DOMAIN-INDEPENDENT BINARY CLASSIFIER is based on the NORMALIZED MUTUAL INFORMATION and the NORMALIZED MUTUAL INFORMATION . the proposed DOMAIN-INDEPENDENT BINARY CLASSIFIER is based on the NORMALIZED MUTUAL INFORMATION and the NORMALIZED MUTUAL INFORMATION . the proposed DOMAIN-INDEPENDENT BINARY CLASSIFIER is based on the NORMALIZED MUTUAL INFORMATION and NORMALIZED MUTUAL INFORMATION . the proposed DOMAIN-INDEPENDENT BINARY CLASSIFIER is compared with a DOMAIN-INDEPENDENT BINARY CLASSIFIER and a DOMAIN-INDEPENDENT BINARY CLASSIFIER based on the NORMALIZED MUTUAL INFORMATION .\n",
            "\n",
            "349 1000\n",
            "a common approach in VISUAL SPEECH SYNTHESIS is the use of VISEMES as ATOMIC UNITS OF SPEECH . in this paper , PHONEME-BASED AND VISEME-BASED AUDIOVISUAL SPEECH SYNTHESIS TECHNIQUES are compared in order to explore the balancing between data availability and an improved AUDIOVISUAL COHERENCE for SYNTHESIS OPTIMIZATION . a technique for AUTOMATIC VISEME CLUSTERING is described and it is compared to the STANDARDIZED VISEME SET described in MPEG-4 . both objective and subjective testing indicated that a <unk> approach leads to better synthesis results . in addition , the test results improve when more different VISEMES are defined . this raises some questions on the widely applied <unk> approach . it appears that a MANY-TO-ONE PHONEME-TO-VISEME MAPPING is not capable of describing all subtle details of the VISUAL SPEECH INFORMATION . in addition , with VISEME-BASED SYNTHESIS the PERCEIVED SYNTHESIS QUALITY is affected by the loss of AUDIOVISUAL COHERENCE in the SYNTHETIC SPEECH . \n",
            "this paper addresses the problem of VISUAL SPEECH SYNTHESIS in VISUAL SPEECH SYNTHESIS . in this paper , we propose a new method for SYNTHESIS OPTIMIZATION in VISUAL SPEECH SYNTHESIS . the proposed method is based on the MANY-TO-ONE PHONEME-TO-VISEME MAPPING of the ATOMIC UNITS OF SPEECH . the proposed method is based on the MANY-TO-ONE PHONEME-TO-VISEME MAPPING . the proposed method is based on the MANY-TO-ONE PHONEME-TO-VISEME MAPPING . the proposed method is tested on SYNTHETIC SPEECH with SYNTHETIC SPEECH . the results show that the proposed method is effective for VISUAL SPEECH SYNTHESIS in VISUAL SPEECH SYNTHESIS .\n",
            "\n",
            "350 1000\n",
            "stress and <unk> are essential attributes for several components in TEXT-TO SPEECH SYSTEMS . STRESS are responsible for improving GRAPHEME-TO-PHONEME CONVERSION RULES and for enhancing the SYNTHETIC INTELLIGIBILITY , since STRESS and SYLLABLE are key units in PROSODY PREDICTION . this paper presents three LINGUISTICALLY RULE-BASED AUTOMATIC ALGORITHMS for CATALAN TEXT-TO-SPEECH CONVERSION : a WORD STRESS MARKER , an ORTHOGRAPHIC SYLLABIFICATION ALGORITHM and a PHONOLOGICAL SYLLABIFICATION ALGORITHM . the LINGUISTICALLY RULE-BASED AUTOMATIC ALGORITHMS were implemented and tested . the results gave rise to the following WORD ACCURACY RATES : 100 % for the STRESS MARKER ALGORITHM , <unk> % for the ORTHOGRAPHIC SYLLABIFICATION ALGORITHM and <unk> % for the PHONOLOGICAL SYLLABIFICATION ALGORITHM . \n",
            "this paper addresses the problem of CATALAN TEXT-TO-SPEECH CONVERSION in TEXT-TO SPEECH SYSTEMS . in this paper , we propose a new PHONOLOGICAL SYLLABIFICATION ALGORITHM , called WORD STRESS MARKER , which combines the advantages of both LINGUISTICALLY RULE-BASED AUTOMATIC ALGORITHMS and LINGUISTICALLY RULE-BASED AUTOMATIC ALGORITHMS . the proposed PHONOLOGICAL SYLLABIFICATION ALGORITHM is based on the STRESS MARKER ALGORITHM and the STRESS MARKER ALGORITHM . the proposed PHONOLOGICAL SYLLABIFICATION ALGORITHM is compared with state-of-the-art LINGUISTICALLY RULE-BASED AUTOMATIC ALGORITHMS , including STRESS , STRESS , and STRESS .\n",
            "\n",
            "351 1000\n",
            "in this research we investigated USER 'S BEHAVIOR while facing a system coping with COMMON KNOWLEDGE about KEYWORDS and compared it with not only classic WORD-SPOTTING METHOD but also with RANDOM TEXT-MINING . we show how even a simple implementation of our idea can enrich the conversation and increase the naturalness of computer 's utterances . our results show that even very COM-MONSENSICAL UTTERANCES are more natural than classic approaches and also methods we developed to make a conversation more interesting . for <unk> opinion exchange during the session , we will also briefly introduce our idea of combining latest NLP ACHIEVEMENTS into one HOLISTIC SYSTEM where the main engine we want to base on COMMONSENSE PROCESSING and AFFECTIVE COMPUTING . \n",
            "this paper addresses the problem of AFFECTIVE COMPUTING in AFFECTIVE COMPUTING . we propose a method for AFFECTIVE COMPUTING based on the WORD-SPOTTING METHOD . the proposed method is based on the WORD-SPOTTING METHOD and the WORD-SPOTTING METHOD . the proposed method is based on the WORD-SPOTTING METHOD and the WORD-SPOTTING METHOD . experimental results show the effectiveness of the proposed method in terms of USER 'S BEHAVIOR and USER 'S BEHAVIOR .\n",
            "\n",
            "352 1000\n",
            "we propose a combined LINE SEGMENT and ELLIPTICAL ARC DETECTOR , which formally guarantees the control of the number of false positives and requires no PARAMETER TUNING . the ACCURACY of the DETECTED ELLIPTICAL FEATURES is improved by using a novel NON-ITERATIVE ELLIPSE FITTING TECHNIQUE , which merges the ALGEBRAIC DISTANCE with the GRADIENT ORIENTATION . the performance of the ELLIPTICAL ARC DETECTOR is evaluated on COMPUTER-GENERATED IMAGES and on NATURAL IMAGES . \n",
            "this paper presents a novel NON-ITERATIVE ELLIPSE FITTING TECHNIQUE for COMPUTER-GENERATED IMAGES . the proposed NON-ITERATIVE ELLIPSE FITTING TECHNIQUE is based on a NON-ITERATIVE ELLIPSE FITTING TECHNIQUE and a NON-ITERATIVE ELLIPSE FITTING TECHNIQUE . the proposed NON-ITERATIVE ELLIPSE FITTING TECHNIQUE is based on a NON-ITERATIVE ELLIPSE FITTING TECHNIQUE and a NON-ITERATIVE ELLIPSE FITTING TECHNIQUE . the proposed NON-ITERATIVE ELLIPSE FITTING TECHNIQUE is evaluated on NATURAL IMAGES and NATURAL IMAGES . the proposed NON-ITERATIVE ELLIPSE FITTING TECHNIQUE is evaluated on COMPUTER-GENERATED IMAGES and NATURAL IMAGES .\n",
            "\n",
            "353 1000\n",
            "a fundamental issue in DIFFERENTIAL MOTION ANALYSIS is the compromise between the flexibility of the MATCHING CRITERION for IMAGE REGIONS and the ability of recovering the motion . LOCALIZED MATCHING CRITERIA , e.g. , PIXEL-BASED SSD , may enable the RECOVERY OF ALL MOTION PARAMETERS , but it does not tolerate much APPEARANCE CHANGES . on the other hand , GLOBAL CRITERIA , e.g. , MATCHING HISTOGRAMS , can accommodate DRAMATIC APPEARANCE CHANGES , but may be blind to some MOTION PARAMETERS , e.g. , SCALING and ROTATION . this paper presents a novel CLOSED FORM SOLUTION that integrates the advantages of both in a principled way based on a SPATIAL-APPEARANCE MODEL that combines LOCAL APPEARANCES VARIATIONS and GLOBAL SPATIAL STRUCTURES . this CLOSED FORM SOLUTION can capture a large variety of APPEARANCE VARIATIONS that are attributed to the LOCAL NON-RIGIDITY . at the same time , this CLOSED FORM SOLUTION enables efficient RECOVERY OF ALL MOTION PARAMETERS . a MAXIMUM LIKELIHOOD MATCHING CRITERION is defined and rigorous analytical results are obtained that lead to a CLOSED FORM SOLUTION to MOTION TRACKING . very encouraging results demonstrate the effectiveness and efficiency of the proposed CLOSED FORM SOLUTION for TRACKING NON-RIGID OBJECTS that exhibit DRAMATIC APPEARANCE DEFORMATIONS , LARGE OBJECT SCALE CHANGES and PARTIAL OCCLUSIONS . \n",
            "this paper presents a new SPATIAL-APPEARANCE MODEL for TRACKING NON-RIGID OBJECTS . the SPATIAL-APPEARANCE MODEL is based on the MAXIMUM LIKELIHOOD MATCHING CRITERION and the RECOVERY OF ALL MOTION PARAMETERS . the proposed SPATIAL-APPEARANCE MODEL is based on the MAXIMUM LIKELIHOOD MATCHING CRITERION and the MATCHING CRITERION . the proposed SPATIAL-APPEARANCE MODEL is based on the MAXIMUM LIKELIHOOD MATCHING CRITERION and the RECOVERY OF ALL MOTION PARAMETERS . the proposed SPATIAL-APPEARANCE MODEL is based on the MAXIMUM LIKELIHOOD MATCHING CRITERION and the RECOVERY OF ALL MOTION PARAMETERS . the proposed SPATIAL-APPEARANCE MODEL is based on the MAXIMUM LIKELIHOOD MATCHING CRITERION and the RECOVERY OF ALL MOTION PARAMETERS . the proposed SPATIAL-APPEARANCE MODEL is based on a MAXIMUM LIKELIHOOD MATCHING CRITERION , and is shown to be robust to PARTIAL OCCLUSIONS and PARTIAL OCCLUSIONS .\n",
            "\n",
            "354 1000\n",
            "the BARK COHERENCE FUNCTION -lsb- 1 -rsb- defines a COHERENCE FUNCTION with LOUDNESS SPEECH as a new COGNITION MODULE , robust to LINEAR DISTORTIONS due to the analog interface of DIGITAL MOBILE SYSTEM . preliminary experiments have shown the superiority of BARK COHERENCE FUNCTION over current measures . in this paper , a new BARK COHERENCE FUNCTION suitable for VOIP is developed . the new BARK COHERENCE FUNCTION is based on the WAVELET SERIES EXPANSION that provides good frequency resolution while keeping good time locality . the proposed WAVELET BASED BARK COHERENCE FUNCTION is robust to VARIABLE DELAY often observed in INTERNET TELEPHONY such as VOIP . we also show that the refinement of TIME SYNCHRONIZATION after SIGNAL DECOMPOSITION can improve the performance of the WAVELET BASED BARK COHERENCE FUNCTION . the TIME SYNCHRONIZATION was performed with VOIP SPEECH DATA . the CORRELATION COEFFICIENTS and the standard error of estimates computed using the WAVELET BASED BARK COHERENCE FUNCTION showed noticeable improvement over the PSQM that is recommended by WAVELET BASED BARK COHERENCE FUNCTION . \n",
            "this paper proposes a DIGITAL MOBILE SYSTEM for VOIP SPEECH DATA . the BARK COHERENCE FUNCTION is based on the BARK COHERENCE FUNCTION , which is based on the BARK COHERENCE FUNCTION . the BARK COHERENCE FUNCTION is based on a WAVELET BASED BARK COHERENCE FUNCTION , which is based on the BARK COHERENCE FUNCTION . the proposed WAVELET BASED BARK COHERENCE FUNCTION is based on a WAVELET BASED BARK COHERENCE FUNCTION , which is based on the BARK COHERENCE FUNCTION . the proposed WAVELET BASED BARK COHERENCE FUNCTION is compared with the conventional PSQM and the COGNITION MODULE .\n",
            "\n",
            "355 1000\n",
            "in this paper we examine the construction of LONG-RANGE LANGUAGE MODELS using LOG-LINEAR INTERPOLATION and how this can be achieved effectively . particular attention is paid to the efficient computation of the <unk> in the LONG-RANGE LANGUAGE MODELS . using the PENN TREEBANK for experiments we argue that the perplexity performance demonstrated recently in the literature using GRAMMAR-BASED APPROACHES can actually be achieved with an appropriately SMOOTHED 4-GRAM LANGUAGE MODEL . using such a model as the baseline , we demonstrate how further improvements can be obtained using LOG-LINEAR INTERPOLATION to combine DISTANCE WORD AND CLASS MODELS . we also examine the performance of similar MODEL COMBINATIONS for RESCORING WORD LATTICES on a MEDIUM-SIZED VOCABULARY WALL STREET JOURNAL TASK . \n",
            "this paper addresses the problem of LOG-LINEAR INTERPOLATION for RESCORING WORD LATTICES . in this paper , we propose a SMOOTHED 4-GRAM LANGUAGE MODEL based on the SMOOTHED 4-GRAM LANGUAGE MODEL . the proposed method is based on the DISTANCE WORD AND CLASS MODELS . the proposed method is based on the DISTANCE WORD AND CLASS MODELS . the proposed method is based on the DISTANCE WORD AND CLASS MODELS . the experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "356 1000\n",
            "this paper examines the issues in extending a LARGE VOCABULARY SPEECH RECOGNITION SYSTEM designed for CLEAN AND NOISY READ SPEECH TASKS to handle BROADCAST NEWS TRANSCRIPTION . results using the 1995 darpa h 4 e v <unk> data set are presented for DIERENT FRONT-END ANALYSES and use of UNSUPERVISED MODEL ADAPTATION using MAXIMUM LIKELIHOOD LINEAR REGRESSION . the LARGE VOCABULARY SPEECH RECOGNITION SYSTEM for the 1996 H4 EVALUATION is then described . LARGE VOCABULARY SPEECH RECOGNITION SYSTEM includes a number of new FEATURES over previous HTK LARGE VOCABULARY SYSTEMS including DECODER-GUIDED SEGMENTATION , SEGMENT CLUSTERING , CACHE-BASED LANGUAGE MODELLING , and combined map and mllr adaptation . the LARGE VOCABULARY SPEECH RECOGNITION SYSTEM runs in multiple passes through the data and the detailed results of each pass are given . \n",
            "this paper presents a novel LARGE VOCABULARY SPEECH RECOGNITION SYSTEM for BROADCAST NEWS TRANSCRIPTION . the LARGE VOCABULARY SPEECH RECOGNITION SYSTEM is based on a LARGE VOCABULARY SPEECH RECOGNITION SYSTEM and a LARGE VOCABULARY SPEECH RECOGNITION SYSTEM . the proposed DIERENT FRONT-END ANALYSES is based on the MAXIMUM LIKELIHOOD LINEAR REGRESSION and the DIERENT FRONT-END ANALYSES . the proposed UNSUPERVISED MODEL ADAPTATION is based on a LARGE VOCABULARY SPEECH RECOGNITION SYSTEM and a LARGE VOCABULARY SPEECH RECOGNITION SYSTEM . the UNSUPERVISED MODEL ADAPTATION is evaluated on the CLEAN AND NOISY READ SPEECH TASKS . the results show the effectiveness of the proposed LARGE VOCABULARY SPEECH RECOGNITION SYSTEM .\n",
            "\n",
            "357 1000\n",
            "in this work , we consider a CDMA CELL with multiple terminals transmitting video signals . we minimize the sum of SIGNAL PROCESSING and TRANSMITTER POWER while the received QUALITY at each terminal is guaranteed . the system parameters to be adjusted include VIDEO CODING BIT RATE , VIDEO COMPRESSION COMPLEXITY and TRANSMITTER POWER . instead of FULL SEARCH in the space of bit rate , COMPLEXITY , TRANSMITTER POWER ¡ for all users , we design a TWO-STEP FAST ALGORITHM to reduce the COMPUTATION BURDEN in the base station . in our TWO-STEP FAST ALGORITHM , the search in the base station is over the space of COMPLEXITY only . our results indicate that for the same class of video users , the one who is closer to the base station compresses at less COMPLEXITY . this is used to further reduce the computation required by our TWO-STEP FAST ALGORITHM . \n",
            "this paper presents a method for SIGNAL PROCESSING in CDMA CELL . the method is based on the TWO-STEP FAST ALGORITHM and the TWO-STEP FAST ALGORITHM . the proposed method is based on the TWO-STEP FAST ALGORITHM and the TWO-STEP FAST ALGORITHM . the proposed method is based on the TWO-STEP FAST ALGORITHM and the TWO-STEP FAST ALGORITHM . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "358 1000\n",
            "when multiple views of data are available for a set of subjects , CO-CLUSTERING aims to identify subject CLUSTERS that agree across the different views . we explore the problem of CO-CLUSTERING when the underlying CLUSTERS exist in different sub-spaces of each view . we propose a PROXIMAL ALTERNATING LINEARIZED MINIMIZATION ALGORITHM that simultaneously decomposes multiple data matrices into SPARSE ROW and columns vectors . this PROXIMAL ALTERNATING LINEARIZED MINIMIZATION ALGORITHM is able to group subjects consistently across the views and simultaneously identify the subset of FEATURES in each view that are associated with the CLUSTERS . the proposed PROXIMAL ALTERNATING LINEARIZED MINIMIZATION ALGORITHM can globally converge to a critical point of the problem . a simulation study validates that the proposed PROXIMAL ALTERNATING LINEARIZED MINIMIZATION ALGORITHM can identify the HYPOTHESIZED CLUSTERS and their associated FEATURES . comparison with several latest MULTI-VIEW CO-CLUSTERING METHODS on BENCHMARK DATASETS demonstrates the superior performance of the proposed PROXIMAL ALTERNATING LINEARIZED MINIMIZATION ALGORITHM . \n",
            "this paper proposes a new PROXIMAL ALTERNATING LINEARIZED MINIMIZATION ALGORITHM based on the PROXIMAL ALTERNATING LINEARIZED MINIMIZATION ALGORITHM . the proposed PROXIMAL ALTERNATING LINEARIZED MINIMIZATION ALGORITHM is based on the PROXIMAL ALTERNATING LINEARIZED MINIMIZATION ALGORITHM . the proposed PROXIMAL ALTERNATING LINEARIZED MINIMIZATION ALGORITHM is based on the PROXIMAL ALTERNATING LINEARIZED MINIMIZATION ALGORITHM . the proposed PROXIMAL ALTERNATING LINEARIZED MINIMIZATION ALGORITHM is evaluated on the BENCHMARK DATASETS and on the BENCHMARK DATASETS . experimental results show that the proposed PROXIMAL ALTERNATING LINEARIZED MINIMIZATION ALGORITHM outperforms the conventional MULTI-VIEW CO-CLUSTERING METHODS in terms of CO-CLUSTERING .\n",
            "\n",
            "359 1000\n",
            "we present an approach for MODEL-FREE MARKERLESS MOTION CAPTURE OF ARTICULATED KINEMATIC STRUCTURES . this approach is centered on our method for generating underlying NONLINEAR AXES -lrb- or a SKELETON CURVE -rrb- of a volume of genus zero -lrb- i.e. , without holes -rrb- . we describe the use of SKELETON CURVES for deriving a KINEMATIC MODEL and motion -lrb- in the form of JOINT ANGLES over time -rrb- from a CAPTURED VOLUME SEQUENCE . our motion capture method uses a SKELETON CURVE , found in each frame of a VOLUME SEQUENCE , to automatically determine KINEMATIC POSTURES . these SKELETON CURVES are aligned to determine a common KINEMATIC MODEL for the VOLUME SEQUENCE . the derived KINEMATIC MODEL is then <unk> to each frame in the VOLUME SEQUENCE to find the MOTION SEQUENCE suited to this KINEMATIC MODEL . we demonstrate our method on several types of motion , from SYNTHETICALLY GENERATED VOLUME SEQUENCES with an ARBITRARY KINEMATIC TOPOLOGY , to HUMAN VOLUME SEQUENCES captured from a set of multiple calibrated cameras . \n",
            "this paper presents a new KINEMATIC MODEL for MODEL-FREE MARKERLESS MOTION CAPTURE OF ARTICULATED KINEMATIC STRUCTURES . the KINEMATIC MODEL is based on the KINEMATIC MODEL of the CAPTURED VOLUME SEQUENCE . the KINEMATIC MODEL is based on the KINEMATIC MODEL of the CAPTURED VOLUME SEQUENCE . the KINEMATIC MODEL is based on the KINEMATIC MODEL . the KINEMATIC MODEL is applied to the CAPTURED VOLUME SEQUENCE . the proposed KINEMATIC MODEL is based on the KINEMATIC MODEL . the proposed KINEMATIC MODEL is based on the KINEMATIC MODEL . the proposed KINEMATIC MODEL is based on a KINEMATIC MODEL . the proposed KINEMATIC MODEL is based on a KINEMATIC MODEL . the proposed KINEMATIC MODEL is based on a SKELETON CURVE , and is shown to be robust to KINEMATIC POSTURES .\n",
            "\n",
            "360 1000\n",
            "1 efficient NATURAL LANGUAGE GENERATION has been successfully demonstrated using highly compiled knowledge about SPEECH ACTS and their related social actions . a design and prototype implementation of a PARSER which utilizes this same PRAGMATIC KNOWLEDGE to efficiently guide parsing is presented . such guidance is shown to prune the SEARCH SPACE and thus avoid <unk> processing of <unk> unlikely constituent structures . INTRODUCTION the use of purely SYNTACTIC KNOWLEDGE during the PARSE phase of NATURAL LANGUAGE UNDERSTANDING yields considerable local ambiguity -lrb- consideration of impossible <unk> -rrb- as well global ambiguity -lrb- construction of syntactically valid parses not applicable to the SOCIO-PRAGMATIC CONTEXT -rrb- . this research investigates bringing SOCIO-PRAGMATIC KNOWLEDGE to bear during the PARSE , while maintaining a DOMAIN INDEPENDENT GRAMMAR and PARSER . the particular technique explored uses knowledge about the PRAGMATIC CONTEXT to order the consideration of proposed PARSE constituents , thus guiding the PARSER to consider the best -lrb- wrt the expectations -rrb- \n",
            "this paper presents a new method for NATURAL LANGUAGE GENERATION from SPEECH ACTS . the method is based on a PARSER and a PARSER based on PRAGMATIC KNOWLEDGE . the proposed method is based on a PARSER , which is based on a PARSER . the proposed method is based on a PARSER and a PARSER . the proposed method is based on a PARSER and a PARSER . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "361 1000\n",
            "a BAYESIAN-KULLBACK LEARNING SCHEME , called YING-YANG MACHINE , is proposed based on the two complement but equivalent BAYESIAN REPRESENTATIONS for JOINT DENSITY and their KULLBACK DIVERGENCE . not only the BAYESIAN-KULLBACK LEARNING SCHEME unifies existing MAJOR SUPERVISED AND UNSU-PERVISED LEARNINGS , including the classical maximum likelihood or LEAST SQUARE LEARNING , the MAXIMUM INFORMATION PRESERVATION , the EM & EM ALGORITHM and INFORMATION GEOMETRY , the recent popular HELMHOLTZ MACHINE , as well as other LEARNING METHODS with new variants and new results ; but also the BAYESIAN-KULLBACK LEARNING SCHEME provides a number of new LEARNING METHODS . \n",
            "this paper presents a new BAYESIAN-KULLBACK LEARNING SCHEME , called YING-YANG MACHINE , which combines the advantages of both MAJOR SUPERVISED AND UNSU-PERVISED LEARNINGS and LEAST SQUARE LEARNING . the BAYESIAN-KULLBACK LEARNING SCHEME is based on a BAYESIAN-KULLBACK LEARNING SCHEME and a BAYESIAN-KULLBACK LEARNING SCHEME . the EM & EM ALGORITHM is based on the EM & EM ALGORITHM and the EM & EM ALGORITHM . the BAYESIAN-KULLBACK LEARNING SCHEME is a BAYESIAN-KULLBACK LEARNING SCHEME and a BAYESIAN-KULLBACK LEARNING SCHEME . the EM & EM ALGORITHM is a BAYESIAN-KULLBACK LEARNING SCHEME and a BAYESIAN-KULLBACK LEARNING SCHEME .\n",
            "\n",
            "362 1000\n",
            "-- a novel SUBOPTIMAL HIDING ALGORITHM for BINARY DATA based on WEIGHT APPROXIMATION EMBEDDING , WAE , is proposed . given a specified EMBEDDING rate , this SUBOPTIMAL HIDING ALGORITHM exhibits an advantage of efficient BINARY EMBEDDING with reduced EMBEDDING COMPLEXITY . the SUBOPTIMAL HIDING ALGORITHM performs an EMBEDDING PROCEDURE through a PARITY CHECK MATRIX . the optimal EMBEDDING based on MAXIMAL LIKELIHOOD ALGORITHM aims to locate the COSET LEADER to minimize the EMBEDDING DISTORTION . on the contrary , the SUBOPTIMAL HIDING ALGORITHM looks for a TARGET VECTOR close to the COSET LEADER in an efficiently iterative manner . given an LINEAR EMBEDDING CODE c -lrb- n , k -rrb- , the EMBEDDING COMPLEXITY using the optimal SUBOPTIMAL HIDING ALGORITHM is o -lrb- 2 k -rrb- , while the COMPLEXITY in the SUBOPTIMAL WAE is reduced to o -lrb- <unk> -rrb- where s is the average iterations . \n",
            "this paper proposes a SUBOPTIMAL HIDING ALGORITHM for BINARY EMBEDDING . the EMBEDDING PROCEDURE is based on the MAXIMAL LIKELIHOOD ALGORITHM . the EMBEDDING PROCEDURE is based on the MAXIMAL LIKELIHOOD ALGORITHM . the EMBEDDING PROCEDURE is based on the MAXIMAL LIKELIHOOD ALGORITHM . the proposed EMBEDDING PROCEDURE is based on the MAXIMAL LIKELIHOOD ALGORITHM . the proposed EMBEDDING PROCEDURE is based on the MAXIMAL LIKELIHOOD ALGORITHM . the proposed EMBEDDING PROCEDURE is based on the MAXIMAL LIKELIHOOD ALGORITHM . the proposed SUBOPTIMAL HIDING ALGORITHM is based on the MAXIMAL LIKELIHOOD ALGORITHM . the proposed SUBOPTIMAL HIDING ALGORITHM is based on the MAXIMAL LIKELIHOOD ALGORITHM . the proposed EMBEDDING PROCEDURE is based on the MAXIMAL LIKELIHOOD ALGORITHM . the proposed SUBOPTIMAL HIDING ALGORITHM is based on the MAXIMAL LIKELIHOOD ALGORITHM . the proposed SUBOPTIMAL HIDING ALGORITHM is based on the MAXIMAL LIKELIHOOD ALGORITHM . the proposed SUBOPTIMAL HIDING ALGORITHM is based on the MAXIMAL LIKELIHOOD ALGORITHM . the proposed SUBOPTIMAL HIDING ALGORITHM is based on the MAXIMAL LIKELIHOOD ALGORITHM .\n",
            "\n",
            "363 1000\n",
            "this paper proposes a DATA-DRIVEN METHOD for CONCEPT-TO-TEXT GENERATION , the task of automatically producing textual output from NON-LINGUISTIC INPUT . a key insight in our DATA-DRIVEN METHOD is to reduce the tasks of CONTENT SELECTION -lrb- '' what to say '' -rrb- and surface realization -lrb- '' how to say '' -rrb- into a common PARSING PROBLEM . we define a PROBABILISTIC CONTEXT-FREE GRAMMAR that describes the structure of the input -lrb- a CORPUS OF DATABASE RECORDS and text describing some of them -rrb- and represent PROBABILISTIC CONTEXT-FREE GRAMMAR compactly as a WEIGHTED HYPERGRAPH . the HYPER-GRAPH STRUCTURE encodes exponentially many derivations , which we rerank discriminatively using LOCAL AND GLOBAL FEATURES . we propose a novel DECODING ALGORITHM for finding the best SCORING DERIVATION and generating in this setting . experimental evaluation on the ATIS DOMAIN shows that our DATA-DRIVEN METHOD outperforms a competitive DISCRIMINATIVE SYSTEM both using BLEU and in a JUDGMENT ELICITATION STUDY . \n",
            "this paper presents a DATA-DRIVEN METHOD for CONCEPT-TO-TEXT GENERATION . the proposed DATA-DRIVEN METHOD is based on a CORPUS OF DATABASE RECORDS and a WEIGHTED HYPERGRAPH . the proposed DATA-DRIVEN METHOD is based on a CORPUS OF DATABASE RECORDS and a WEIGHTED HYPERGRAPH . the proposed DATA-DRIVEN METHOD is based on a CORPUS OF DATABASE RECORDS and a WEIGHTED HYPERGRAPH . the proposed DATA-DRIVEN METHOD is evaluated on a CORPUS OF DATABASE RECORDS and on a CORPUS OF DATABASE RECORDS . the experimental results show that the proposed DATA-DRIVEN METHOD outperforms the conventional DATA-DRIVEN METHOD in terms of BLEU and BLEU .\n",
            "\n",
            "364 1000\n",
            "in this paper , an approach of continuous speech recognition based on LAYERED SELF-ADJUSTING DECODING GRAPH is described . it utilizes a SCAOLDING LAYER to support FAST NETWORK EXPANSION and RELEASING . a TWO LEVEL HASHING STRUCTURE is also described . it introduces SELF-ADJUSTING CAPABILITY i n DYNAMIC DECODING on GENERAL RE-ENTRANT DECODING NETWORK . in STACK DECODING , the SCAOLDING LAYER in the proposed approach enables the DECODER to look several layers into the future so that long span <unk> context dependency can be exactly preserved . experimental results indicate that highly ECIENT DECODING can be achieved with a signicant savings on RECOGNITION RESOURCES . \n",
            "this paper addresses the problem of DYNAMIC DECODING for STACK DECODING . the DECODER is based on a GENERAL RE-ENTRANT DECODING NETWORK of the SCAOLDING LAYER and the DECODER . the DECODER is based on a FAST NETWORK EXPANSION . the DECODER is based on a GENERAL RE-ENTRANT DECODING NETWORK . the DECODER is applied to the DECODER . experimental results show the effectiveness of the proposed DECODER .\n",
            "\n",
            "365 1000\n",
            "<unk> -lrb- th -rrb- circuits in the front end of HIGH-SPEED HIGH-RESOLUTION ANALOG-TO-DIGITAL CONVERTERS typically limit ADC performance at high INPUT SIGNAL frequencies . this paper develops MATHEMATICAL MODELS for ADC implemented in both BIPOLAR AND MOS TECHNOLOGIES . the MATHEMATICAL MODELS are derived by analyzing the SAMPLING INSTANT ERROR and reveal that the NONLINEAR BEHAVIOR is dependent on the INPUT SIGNAL and DIGITAL POST COMPENSATION METHOD 's derivatives . a DIGITAL POST COMPENSATION METHOD is then presented with DIGITAL POST COMPENSATION METHOD 's coefficients estimated using an ENERGY-FREE METHOD in a BACKGROUND CALIBRATION CONFIGURATION . simulation results on a NONLINEAR TH MODEL show that the proposed DIGITAL POST COMPENSATION METHOD achieves a significant improvement in the SPURIOUS FREE DYNAMIC RANGE . the DIGITAL POST COMPENSATION METHOD is also applied to a commercially available ADC to demonstrate DIGITAL POST COMPENSATION METHOD 's effectiveness . \n",
            "this paper presents a DIGITAL POST COMPENSATION METHOD for HIGH-SPEED HIGH-RESOLUTION ANALOG-TO-DIGITAL CONVERTERS . the proposed DIGITAL POST COMPENSATION METHOD is based on the NONLINEAR TH MODEL . the proposed DIGITAL POST COMPENSATION METHOD is based on the NONLINEAR TH MODEL . the proposed DIGITAL POST COMPENSATION METHOD is based on the NONLINEAR TH MODEL . the proposed DIGITAL POST COMPENSATION METHOD is based on the ENERGY-FREE METHOD . the proposed DIGITAL POST COMPENSATION METHOD is evaluated on the INPUT SIGNAL . the proposed DIGITAL POST COMPENSATION METHOD is compared with the conventional DIGITAL POST COMPENSATION METHOD .\n",
            "\n",
            "366 1000\n",
            "belief revision is a ubiquitous process underlying many forms of INTELLIGENT BEHAVIOUR . the AGM PARADIGM is a powerful framework for modeling and implementing BELIEF REVISION SYSTEMS based on the principle of MINIMAL CHANGE ; AGM PARADIGM provides a rich and rigorous foundation for COMPUTER-BASED BELIEF REVISION ARCHITECTURES . MAXI-ADJUSTMENT is a BELIEF REVISION STRATEGY for THEORY BASES that can be implemented using a standard THEOREM PROVER , and one that has been used successfully for several applications . in this paper we provide an ANYTIME DECISION PROCEDURE for MAXI-ADJUSTMENTS , and study its COMPLEXITY . furthermore , we outline a set of GUIDELINES that serve as a PROTOMETHODOLOGY for building BELIEF REVISION SYSTEMS employing a MAXI-ADJUSTMENT . the ANYTIME DECISION PROCEDURE is under development in the BELIEF REVISION MODULE of the CIN PROJECT . \n",
            "this paper presents a ANYTIME DECISION PROCEDURE for INTELLIGENT BEHAVIOUR , which is based on a CIN PROJECT . the AGM PARADIGM is based on a BELIEF REVISION STRATEGY , which is a CIN PROJECT and a CIN PROJECT . the THEOREM PROVER is based on the THEOREM PROVER and the BELIEF REVISION STRATEGY . the proposed BELIEF REVISION STRATEGY is based on the THEOREM PROVER and the BELIEF REVISION STRATEGY . the proposed BELIEF REVISION STRATEGY is based on the THEOREM PROVER and the BELIEF REVISION STRATEGY . the proposed BELIEF REVISION STRATEGY is based on the THEOREM PROVER and the BELIEF REVISION STRATEGY . the proposed BELIEF REVISION STRATEGY is based on a BELIEF REVISION STRATEGY , which is a BELIEF REVISION STRATEGY . the proposed BELIEF REVISION STRATEGY is based on a BELIEF REVISION STRATEGY , and is shown to be robust to COMPLEXITY and COMPLEXITY .\n",
            "\n",
            "367 1000\n",
            "we describe a method of using a LAGRANGE MULTIPLIER to make a locally optimal trade off between RATE and DISTORTION in the MOTION SEARCH for VIDEO SEQUENCES , while maintaining a CONSTANT BIT RATE CHANNEL . simulation of this method shows that it gives up to 3.5 db psnr improvement in a high motion sequence . a locally rate-distortion -lrb- r-d -rrb- OPTIMAL MODE SELECTION mechanism is also described . this method also gives significant quality benefit over the NOMINAL METHOD . though the benefit of these techniques is significant when used separately , when the OPTIMAL MODE SELECTION is combined with the R-D OPTIMAL MOTION SEARCH , it does not perform much better than the codec does with only the R-D OPTIMAL MOTION SEARCH . \n",
            "this paper presents a method for OPTIMAL MODE SELECTION based on OPTIMAL MODE SELECTION . the proposed method is based on a NOMINAL METHOD of the LAGRANGE MULTIPLIER and the DISTORTION . the proposed method is based on the R-D OPTIMAL MOTION SEARCH . the proposed method is based on the R-D OPTIMAL MOTION SEARCH . the proposed method is based on the R-D OPTIMAL MOTION SEARCH . the proposed method is based on the NOMINAL METHOD . the proposed method is compared with the conventional NOMINAL METHOD .\n",
            "\n",
            "368 1000\n",
            "in a broad range of NATURAL LANGUAGE PROCESSING TASKS , LARGE-SCALE KNOWLEDGE-BASE OF PARAPHRASES is anticipated to improve their performance . the key issue in creating such a resource is to establish a practical method of COMPUTING SEMANTIC EQUIVALENCE and SYNTACTIC SUBSTITUTABILITY , i.e. , PARAPHRASABILITY , between given pair of expressions . this paper addresses the issues of COMPUTING PARAPHRASABILITY , focusing on SYNTACTIC VARIANTS OF PREDICATE PHRASES . our model estimates PARAPHRASABILITY based on traditional DISTRIBUTIONAL SIMILARITY MEASURES , where the WEB SNIPPETS are used to overcome the DATA SPARSENESS PROBLEM in handling PREDICATE PHRASES . several feature sets are evaluated through empirical experiments . \n",
            "this paper presents a method for COMPUTING SEMANTIC EQUIVALENCE in NATURAL LANGUAGE PROCESSING TASKS . the proposed method is based on the SYNTACTIC VARIANTS OF PREDICATE PHRASES and the DATA SPARSENESS PROBLEM . the proposed method is based on the SYNTACTIC VARIANTS OF PREDICATE PHRASES and the DATA SPARSENESS PROBLEM . the proposed method is based on DISTRIBUTIONAL SIMILARITY MEASURES and DISTRIBUTIONAL SIMILARITY MEASURES . the proposed method is based on the SYNTACTIC VARIANTS OF PREDICATE PHRASES and the DATA SPARSENESS PROBLEM .\n",
            "\n",
            "369 1000\n",
            "an ASSOCIATIVE MEMORY is a STRUCTURE learned from a dataset m of vectors -lrb- signals -rrb- in a way such that , given a noisy version of one of the vectors as input , the nearest valid VECTOR from m -lrb- NEAREST NEIGHBOR -rrb- is provided as output , preferably via a fast ITERATIVE ALGORITHM . traditionally , BINARY -LRB- OR Q-ARY -RRB- HOPFIELD NEURAL NETWORKS are used to model the above STRUCTURE . in this paper , for the first time , we propose a model of ASSOCIATIVE MEMORY based on SPARSE RECOVERY OF SIGNALS . our basic premise is simple . for a dataset , we learn a set of LINEAR CONSTRAINTS that every VECTOR in the dataset must satisfy . provided these LINEAR CONSTRAINTS possess some special properties , it is possible to cast the task of finding NEAREST NEIGHBOR as a SPARSE RECOVERY PROBLEM . assuming GENERIC RANDOM MODELS for the dataset , we show that it is possible to store SUPER-POLYNOMIAL OR EXPONENTIAL NUMBER OF N-LENGTH VECTORS in a neural network of size o -lrb- n -rrb- . furthermore , given a noisy version of one of the STORED VECTORS corrupted in NEAR-LINEAR NUMBER OF COORDINATES , the VECTOR can be correctly <unk> using a NEURALLY FEASIBLE ALGORITHM . \n",
            "this paper addresses the problem of SPARSE RECOVERY OF SIGNALS in BINARY -LRB- OR Q-ARY -RRB- HOPFIELD NEURAL NETWORKS . we propose a NEURALLY FEASIBLE ALGORITHM based on the SUPER-POLYNOMIAL OR EXPONENTIAL NUMBER OF N-LENGTH VECTORS of the STORED VECTORS . the proposed ITERATIVE ALGORITHM is based on the NEAR-LINEAR NUMBER OF COORDINATES of the STORED VECTORS . the proposed ITERATIVE ALGORITHM is based on the ITERATIVE ALGORITHM . the proposed ITERATIVE ALGORITHM is based on the NEURALLY FEASIBLE ALGORITHM . the ITERATIVE ALGORITHM is applied to the SPARSE RECOVERY PROBLEM . experimental results show the effectiveness of the proposed ITERATIVE ALGORITHM .\n",
            "\n",
            "370 1000\n",
            "in this paper , we present a CLASS-BASED VARIABLE MEMORY LENGTH MARKOV MODEL and its LEARNING ALGORITHM . this is an extension of a VARIABLE MEMORY LENGTH MARKOV MODEL . our CLASS-BASED VARIABLE MEMORY LENGTH MARKOV MODEL is based on a CLASS-BASED PROBABILISTIC SUFFIX TREE , whose NODES have an automatically acquired WORD-CLASS RELATION . we experimentally compared our new CLASS-BASED VARIABLE MEMORY LENGTH MARKOV MODEL with a WORD-BASED BI-GRAM MODEL , a WORD-BASED TRI-GRAM MODEL , a CLASS-BASED BI-GRAM MODEL , and a WORD-BASED VARIABLE MEMORY LENGTH MARKOV MODEL . the results show that a CLASS-BASED VARIABLE MEMORY LENGTH MARKOV MODEL outperforms the other models in perplexity and MODEL SIZE . \n",
            "this paper proposes a new method for CLASS-BASED VARIABLE MEMORY LENGTH MARKOV MODEL based on a WORD-BASED VARIABLE MEMORY LENGTH MARKOV MODEL . the proposed LEARNING ALGORITHM is based on the CLASS-BASED VARIABLE MEMORY LENGTH MARKOV MODEL . the proposed CLASS-BASED VARIABLE MEMORY LENGTH MARKOV MODEL is based on the CLASS-BASED VARIABLE MEMORY LENGTH MARKOV MODEL . the proposed CLASS-BASED VARIABLE MEMORY LENGTH MARKOV MODEL is based on the CLASS-BASED VARIABLE MEMORY LENGTH MARKOV MODEL . the proposed CLASS-BASED VARIABLE MEMORY LENGTH MARKOV MODEL is compared with the conventional WORD-BASED BI-GRAM MODEL . the proposed CLASS-BASED VARIABLE MEMORY LENGTH MARKOV MODEL is compared with the conventional WORD-BASED BI-GRAM MODEL . the proposed CLASS-BASED VARIABLE MEMORY LENGTH MARKOV MODEL is compared with the conventional WORD-BASED BI-GRAM MODEL .\n",
            "\n",
            "371 1000\n",
            "this paper argues for using AMBIGUITY PLANE FEATURES within DYNAMIC STATISTICAL MODELS for CLASSIFICATION PROBLEMS . the relative contribution of the two DYNAMIC STATISTICAL MODELS are investigated in the context of ACOUSTICALLY MONITORING CUTTER WEAR during <unk> of TITANIUM , an application where it is known that standard STATIC CLASSIFICATION TECHNIQUES work poorly . experiments show that EXPLICIT MODELING OF LONG-TERM CONTEXT via a HIDDEN MARKOV MODEL STATE improves performance , but mainly by using this to augment SPARSELY LABELED TRAINING DATA . an additional performance gain is achieved by using the SHORTER-TERM CONTEXT OF AMBIGUITY PLANE FEATURES . \n",
            "this paper addresses the problem of EXPLICIT MODELING OF LONG-TERM CONTEXT from SPARSELY LABELED TRAINING DATA . we propose a method for EXPLICIT MODELING OF LONG-TERM CONTEXT based on DYNAMIC STATISTICAL MODELS . the proposed method is based on the use of DYNAMIC STATISTICAL MODELS for EXPLICIT MODELING OF LONG-TERM CONTEXT . the proposed method is based on the use of DYNAMIC STATISTICAL MODELS for the EXPLICIT MODELING OF LONG-TERM CONTEXT . experimental results show that the proposed method is effective for CLASSIFICATION PROBLEMS .\n",
            "\n",
            "372 1000\n",
            "the best voices in TEXT-TO-SPEECH SYNTHESIS are currently obtained via ACOUSTIC UNITS CONCATENATION-BASED SYSTEMS . in such ACOUSTIC UNITS CONCATENATION-BASED SYSTEMS , the choice of units whose <unk> will produce an ACOUSTIC MESSAGE is a crucial stage . moreover , it can be observed that current TTS SYSTEMS use ACOUSTIC UNITS which most often correspond to VARIABLE-LENGTH PHONETIC DESCRIPTIONS . in this article , an original framework is proposed which allows the automatic determination of an optimum set of VARIABLE-LENGTH ACOUSTIC UNITS . \n",
            "this paper addresses the problem of TEXT-TO-SPEECH SYNTHESIS in TTS SYSTEMS . we propose a method for TEXT-TO-SPEECH SYNTHESIS based on VARIABLE-LENGTH PHONETIC DESCRIPTIONS . the proposed method is based on the use of VARIABLE-LENGTH PHONETIC DESCRIPTIONS , and is based on the ACOUSTIC MESSAGE . the proposed method is based on the ACOUSTIC MESSAGE . the experimental results show that the proposed method is robust and robust to VARIABLE-LENGTH PHONETIC DESCRIPTIONS .\n",
            "\n",
            "373 1000\n",
            "1 this paper presents a low-power structure of DIGITAL MATCHED FILTERS , which is proposed for DIRECT SEQUENCE SPREAD SPECTRUM SYSTEMS . traditionally , LOW-POWER APPROACHES for DMFS are based on either the TRANSPOSED-FORM STRUCTURE or the <unk> one . a new HYBRID STRUCTURE that employs the DIRECT-FORM STRUCTURE for local addition and the TRANSPOSED-FORM STRUCTURE for global addition is used to take advantages of both structures . for a 128-TAP DMF , the proposed DMFS that processes 32 <unk> a cycle consumes 46 % less power at the expense of 6 % AREA OVERHEAD as compared to the state-of-the-art low-power DMFS -lsb- 7 -rsb- . \n",
            "this paper presents a new method for DIRECT SEQUENCE SPREAD SPECTRUM SYSTEMS based on DIGITAL MATCHED FILTERS . the proposed method is based on the use of DIGITAL MATCHED FILTERS and DIGITAL MATCHED FILTERS . the proposed method is based on the use of DIGITAL MATCHED FILTERS and DIGITAL MATCHED FILTERS . the proposed method is based on the use of DIGITAL MATCHED FILTERS and DIGITAL MATCHED FILTERS . experimental results show the effectiveness of the proposed method in terms of AREA OVERHEAD and AREA OVERHEAD .\n",
            "\n",
            "374 1000\n",
            "we present a CNF to <unk> -lrb- CNF -rrb- <unk> with COMPLEXITY at most exponential in the TREE WIDTH . we then present algorithms for interesting queries on CNF . although some of the presented query algorithms are in the worst case exponential in the TREE WIDTH , our experiments show that CNF can answer non-trivial queries like CLAUSAL ENTAILMENT in reasonable time for several realistic instances . while our <unk> compiles all the used 91 instances , D-DNNF COMPILATION failed for 12 or 8 of them based on the DECOMPOSITION HEURIS-TIC used . also , on the succeeded instances , a D-DNNF COMPILATION is up to 1000 times larger than the MATCHING TOB . the TOB COMPILATIONS are often an order of magnitude faster than the D-DNNF COMPILATION . this makes CNF a quite interesting KNOWLEDGE COMPILATION FORM . \n",
            "this paper presents a new method for D-DNNF COMPILATION . the proposed method is based on the use of a DECOMPOSITION HEURIS-TIC and a TREE WIDTH . the proposed method is compared with the conventional CNF and compared with the conventional CNF . the proposed method is compared with the conventional CNF .\n",
            "\n",
            "375 1000\n",
            "the explosive growth of the internet and the <unk> have attracted a great deal of attention to the implementation and performance of NETWORKED MULTIMEDIA SERVICES . which involve the transport of REAL-TIME MULTIMEDIA DATA STREAMS over NON-GUARANTEED QUALITY OF SERVICE NETWORKS based on the INTERNET PROTOCOL . in this paper , i present an overview of the existing ARCHITECTURAL ELEMENTS supporting REAL-TIME DATA TRANSMISSION over the internet . effective implementations of such systems require a thorough understanding of both the NETWORK PROTOCOLS and the CODING SYSTEMS used for compressing the signals to be transmitted in real-time . the paper includes a section discussing the issues to be considered in designing SIGNAL COMPRESSION APPLICATIONS suitable for network use . \n",
            "this paper addresses the problem of REAL-TIME DATA TRANSMISSION in NETWORKED MULTIMEDIA SERVICES . we propose a method for REAL-TIME DATA TRANSMISSION based on NETWORK PROTOCOLS . the proposed method is based on the use of NETWORK PROTOCOLS for REAL-TIME DATA TRANSMISSION . the proposed method is based on the use of a INTERNET PROTOCOL and a INTERNET PROTOCOL . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "376 1000\n",
            "the goal of this study is to investigate whether learners ' WRITTEN DATA in highly INFLECTIONAL CZECH can suggest a consistent set of clues for AUTOMATIC IDENTIFICATION OF THE LEARNERS ' L1 BACKGROUND . for our experiments , we use texts written by learners of CZECH , which have been automatically and manually annotated for errors . we define two classes of learners : SPEAKERS OF INDO-EUROPEAN LANGUAGES and speakers of <unk> languages . we use an SVM CLASSIFIER to perform the BINARY CLASSIFICATION . we show that NON-CONTENT BASED FEATURES perform well on HIGHLY INFLECTIONAL DATA . in particular , FEATURES reflecting errors in ORTHOGRAPHY are the most useful , yielding about 89 % PRECISION and the same RECALL . a detailed discussion of the best performing FEATURES is provided . \n",
            "this paper presents a new method for AUTOMATIC IDENTIFICATION OF THE LEARNERS ' L1 BACKGROUND based on HIGHLY INFLECTIONAL DATA . the proposed method is based on a SVM CLASSIFIER and a SVM CLASSIFIER based on NON-CONTENT BASED FEATURES . the proposed method is based on a SVM CLASSIFIER and a SVM CLASSIFIER . the proposed method is based on a SVM CLASSIFIER , which is based on the SVM CLASSIFIER . the proposed method is based on a SVM CLASSIFIER and a SVM CLASSIFIER . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "377 1000\n",
            "this paper proposes a new class of LIFTING WAVELET TRANSFORM which can guarantee LOSSLESSNESS OF SPECIFIC SIGNALS , e.g. WHITE BALANCE . the <unk> WAVELET TRANSFORM composed of two LIFTING STEPS can reconstruct an input signal without any loss and has been utilized for LOSSLESS CODING . the 9/7 WAVELET contains two more LIFTING STEPS and two SCALING PAIRS for effective LOSSY CODING . however the LOSSLESSNESS is not guaranteed due to ROUNDING OF SIGNAL VALUES and SCALING COEFFICIENT VALUES . this paper analyzes condition on word length -lrb- wl -rrb- and bit depth -lrb- <unk> -rrb- for the LOSSLESSNESS and proposes a new class of WAVELET TRANSFORM with '' dc lossless '' property which is a kind of specific LOSSLESSNESS . this can be utilized as a standard condition for algorithms or LSI PROCESSORS to guarantee no error from the WAVELET TRANSFORM for WHITE BALANCE SIGNALS . \n",
            "this paper addresses the problem of LOSSLESS CODING for WHITE BALANCE SIGNALS . the LIFTING WAVELET TRANSFORM is based on the LIFTING WAVELET TRANSFORM and the LIFTING WAVELET TRANSFORM . the LIFTING WAVELET TRANSFORM is based on the LIFTING WAVELET TRANSFORM and the LIFTING WAVELET TRANSFORM . the proposed LIFTING WAVELET TRANSFORM is based on the LIFTING WAVELET TRANSFORM and the LIFTING WAVELET TRANSFORM . the proposed method is based on the LIFTING WAVELET TRANSFORM of the LIFTING WAVELET TRANSFORM . the proposed method is based on the LIFTING WAVELET TRANSFORM and the LIFTING WAVELET TRANSFORM . the proposed method is based on the LIFTING WAVELET TRANSFORM and the LIFTING WAVELET TRANSFORM .\n",
            "\n",
            "378 1000\n",
            "we describe an ENERGY MINIMIZATION ALGORITHM for FUNCTIONS defined on 4-CONNECTED LATTICES , of the type usually encountered in problems involving IMAGES . such FUNCTIONS are often minimized using GRAPH-CUTS/MAX-FLOW , but this ENERGY MINIMIZATION ALGORITHM is only applicable to SUBMODULAR PROBLEMS . in this paper , we describe an ENERGY MINIMIZATION ALGORITHM that will solve any BINARY PROBLEM , irrespective of whether it is SUBMODULAR or not , and for MULTILABEL PROBLEMS we use ALPHA-EXPANSION . the ENERGY MINIMIZATION ALGORITHM is based on the ELIMINATION ALGORITHM , which eliminates NODES from the GRAPH until the remaining function is SUBMODULAR . it can then be solved using MAX-FLOW . values of ELIMINATED VARIABLES are recovered using BACK-SUBSTITUTION . we compare the ENERGY MINIMIZATION ALGORITHM 's performance against alternative methods for solving SUBMODULAR PROBLEMS , with favourable results . \n",
            "this paper presents a new ELIMINATION ALGORITHM for MULTILABEL PROBLEMS . the ENERGY MINIMIZATION ALGORITHM is based on the ENERGY MINIMIZATION ALGORITHM of the GRAPH and the GRAPH . the ENERGY MINIMIZATION ALGORITHM is based on the ENERGY MINIMIZATION ALGORITHM of the GRAPH . the proposed ELIMINATION ALGORITHM is based on the ENERGY MINIMIZATION ALGORITHM and uses a ELIMINATION ALGORITHM to estimate the NODES from the GRAPH . the proposed ELIMINATION ALGORITHM is applied to the BINARY PROBLEM , and is shown to be useful for MULTILABEL PROBLEMS .\n",
            "\n",
            "379 1000\n",
            "in this paper , we propose a SPARSE RANDOM FEATURES ALGORITHM , which learns a SPARSE NON-LINEAR PREDICTOR by minimizing an ℓ 1-REGULARIZED OBJECTIVE FUNCTION over the HILBERT SPACE induced from a KERNEL FUNCTION . by interpreting the SPARSE RANDOM FEATURES ALGORITHM as RANDOMIZED COORDINATE DESCENT in an INFINITE-DIMENSIONAL SPACE , we show the proposed SPARSE RANDOM FEATURES ALGORITHM converges to a solution within <unk> of that using an exact KERNEL METHOD , by drawing o -lrb- 1 / <unk> -rrb- random features , in contrast to the O -LRB- 1 / Ε 2 -RRB- CONVERGENCE achieved by current monte-carlo analyses of RANDOM FEATURES . in our experiments , the SPARSE RANDOM FEATURES ALGORITHM obtains a sparse solution that requires less MEMORY AND PREDICTION TIME , while maintaining comparable performance on REGRESSION AND CLASSIFICATION TASKS . moreover , as an APPROXIMATE SOLVER for the INFINITE-DIMENSIONAL ℓ 1-REGULARIZED PROBLEM , the SPARSE RANDOM FEATURES ALGORITHM also enjoys better convergence guarantees than a BOOSTING APPROACH in the setting where the GREEDY BOOSTING STEP can not be performed exactly . \n",
            "this paper proposes a new BOOSTING APPROACH for RANDOMIZED COORDINATE DESCENT . the proposed KERNEL METHOD is based on the SPARSE RANDOM FEATURES ALGORITHM . the proposed SPARSE RANDOM FEATURES ALGORITHM is based on the SPARSE RANDOM FEATURES ALGORITHM . the proposed KERNEL METHOD is based on the SPARSE RANDOM FEATURES ALGORITHM . the proposed KERNEL METHOD is based on the SPARSE RANDOM FEATURES ALGORITHM . the proposed KERNEL METHOD is compared with the conventional KERNEL METHOD and the BOOSTING APPROACH . the proposed BOOSTING APPROACH is compared with the conventional KERNEL METHOD and the KERNEL METHOD .\n",
            "\n",
            "380 1000\n",
            "in this paper we address the issue of output instability of DEEP NEURAL NETWORKS : SMALL PERTURBATIONS in the VISUAL INPUT can significantly distort the FEATURE EMBEDDINGS and output of a NEURAL NETWORK . such instability affects many DEEP ARCHITECTURES with state-of-the-art performance on a wide range of COMPUTER VISION TASKS . we present a GENERAL STABILITY TRAINING METHOD to stabilize DEEP NEURAL NETWORKS against SMALL INPUT DISTORTIONS that result from various types of COMMON IMAGE PROCESSING , such as COMPRESSION , RESCALING , and CROPPING . we validate our method by <unk> the state-of-the-art INCEPTION ARCHITECTURE -lsb- 11 -rsb- against these types of distortions . in addition , we demonstrate that our <unk> model gives robust state-of-the-art performance on LARGE-SCALE NEAR-DUPLICATE DETECTION , SIMILAR-IMAGE RANKING , and CLASSIFICATION on NOISY DATASETS . \n",
            "this paper addresses the problem of LARGE-SCALE NEAR-DUPLICATE DETECTION in COMPUTER VISION TASKS such as CLASSIFICATION , CLASSIFICATION , and CLASSIFICATION . in this paper , we propose a new method for LARGE-SCALE NEAR-DUPLICATE DETECTION , which is based on the INCEPTION ARCHITECTURE . the proposed method is based on the use of DEEP NEURAL NETWORKS and DEEP NEURAL NETWORKS . the proposed method is based on the use of a NEURAL NETWORK and a GENERAL STABILITY TRAINING METHOD . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "381 1000\n",
            "one of the main challenges in learning FINE-GRAINED VISUAL CATEGORIES is gathering training images . recent work in ZERO-SHOT LEARNING circumvents this challenge by describing categories via attributes or text . however , not all VISUAL CONCEPTS , e.g. , two people <unk> , are easily amenable to such descriptions . in this paper , we propose a new MODALITY for ZERO-SHOT LEARNING using VISUAL ABSTRACTION to learn DIFFICULT-TO-DESCRIBE CONCEPTS . specifically , we explore CONCEPTS related to people and their interactions with others . our proposed MODALITY allows one to provide TRAINING DATA by manipulating ABSTRACT VISUALIZATIONS , e.g. , one can illustrate interactions between two <unk> people by manipulating each person 's pose , expression , GAZE , and gender . the feasibility of our MODALITY is shown on a HUMAN POSE DATASET and a new dataset containing complex interactions between two people , where we outperform several baselines . to better match across the two domains , we learn an explicit mapping between the abstract and real worlds . \n",
            "this paper presents a new method for ZERO-SHOT LEARNING based on DIFFICULT-TO-DESCRIBE CONCEPTS . the proposed method is based on the use of a MODALITY and a MODALITY for ZERO-SHOT LEARNING . the proposed method is based on the use of a MODALITY and a MODALITY . the proposed method is based on a MODALITY , which is based on the MODALITY . the proposed method is applied to the HUMAN POSE DATASET .\n",
            "\n",
            "382 1000\n",
            "we present a generalization of SIMILARITY-BASED RETRIEVAL in RECOMMENDER SYSTEMS which ensures that for any case that is acceptable to the user , the RETRIEVAL SET contains a case that is at least as good in an objective sense and so also likely to be acceptable . our approach recognizes that similarity to the target query is only one of several possible criteria according to which a given case might be considered at least as good as another . \n",
            "this paper addresses the problem of SIMILARITY-BASED RETRIEVAL in RECOMMENDER SYSTEMS . in this paper , we present a method for SIMILARITY-BASED RETRIEVAL , which is based on a RETRIEVAL SET .\n",
            "\n",
            "383 1000\n",
            "many INVERSE PROBLEMS require to minimize a CRITERION being the sum of a NON NECESSARILY SMOOTH FUNCTION and a LIPSCHITZ DIFFEREN-TIABLE FUNCTION . such an OPTIMIZATION PROBLEM can be solved with the FORWARD-BACKWARD ALGORITHM which can be accelerated thanks to the use of VARIABLE METRICS derived from the MAJORIZE-MINIMIZE PRINCIPLE . the convergence of this approach is guaranteed provided that the CRITERION satisfies some additional technical conditions . combining this method with an ALTERNATING MINIMIZATION STRATEGY will be shown to allow us to address a broad class of OPTIMIZATION PROBLEMS involving LARGE-SIZE SIGNALS . an application example to a NONCONVEX SPECTRAL UNMIXING PROBLEM will be presented . \n",
            "this paper presents a new method for OPTIMIZATION PROBLEMS in LARGE-SIZE SIGNALS . the proposed method is based on a ALTERNATING MINIMIZATION STRATEGY , which is based on the FORWARD-BACKWARD ALGORITHM . the proposed method is based on the ALTERNATING MINIMIZATION STRATEGY . the proposed method is based on the ALTERNATING MINIMIZATION STRATEGY . the proposed method is based on the ALTERNATING MINIMIZATION STRATEGY . the proposed method is based on the ALTERNATING MINIMIZATION STRATEGY . the proposed method is based on the ALTERNATING MINIMIZATION STRATEGY . the proposed method is based on the ALTERNATING MINIMIZATION STRATEGY . the experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "384 1000\n",
            "in this paper we present a novel MODEL BASED SPARSE PRINCIPAL COMPONENT ANALYSIS METHOD based on the L 0 PENALTY . we develop an ESTIMATION METHOD based on the GENERALIZED EM ALGORITHM and ITERATIVE HARD THRESHOLDING and an ASSOCIATED MODEL SELECTION METHOD based on BAYESIAN INFORMATION CRITERION . the ESTIMATION METHOD is compared to a previous SPARSE PCA METHOD using both SIMULATED DATA and DNA MICROARRAY DATA . \n",
            "this paper proposes a new MODEL BASED SPARSE PRINCIPAL COMPONENT ANALYSIS METHOD for DNA MICROARRAY DATA . the proposed MODEL BASED SPARSE PRINCIPAL COMPONENT ANALYSIS METHOD is based on the BAYESIAN INFORMATION CRITERION and the BAYESIAN INFORMATION CRITERION . the proposed MODEL BASED SPARSE PRINCIPAL COMPONENT ANALYSIS METHOD is based on the BAYESIAN INFORMATION CRITERION and the BAYESIAN INFORMATION CRITERION . the proposed MODEL BASED SPARSE PRINCIPAL COMPONENT ANALYSIS METHOD is compared with the conventional ESTIMATION METHOD and the MODEL BASED SPARSE PRINCIPAL COMPONENT ANALYSIS METHOD .\n",
            "\n",
            "385 1000\n",
            "in recent years , DEEP ARCHITECTURES have gained a lot of prominence for LEARNING complex AI TASKS because of their capability to incorporate complex variations in data within the model . however , these models often need to be trained for a long time in order to obtain good results . in this paper , we propose a technique , called ` <unk> ' , that allows the same models to perform considerably better with very little training . we show that LEARNING can be done tractably , even when the WEIGHT MATRIX is <unk> to infinity , for some specific models . we also study TRACTABLE ALGORITHMS for implementing STRETCHING in DEEP CONVOLUTIONAL ARCHITECTURES in an ITERATIVE MANNER and derive bounds for its convergence . our experimental results suggest that the proposed STRETCHED DEEP CONVOLUTIONAL NETWORKS are capable of achieving good performance for many OBJECT RECOGNITION TASKS . more importantly , for a FIXED NETWORK ARCHITECTURE , one can achieve much better ACCURACY using STRETCHING rather than LEARNING the weights using BACKPROPAGATION . \n",
            "this paper addresses the problem of LEARNING in AI TASKS . in this paper , we propose a new method for LEARNING based on STRETCHED DEEP CONVOLUTIONAL NETWORKS . the proposed method is based on the use of STRETCHED DEEP CONVOLUTIONAL NETWORKS in the WEIGHT MATRIX . the proposed method is based on the use of STRETCHED DEEP CONVOLUTIONAL NETWORKS with BACKPROPAGATION . the proposed method is based on the use of STRETCHED DEEP CONVOLUTIONAL NETWORKS with BACKPROPAGATION . experimental results show the effectiveness of the proposed method in terms of ACCURACY and ACCURACY .\n",
            "\n",
            "386 1000\n",
            "a novel method for determining the set of parameters for a PHASE SPACE REPRESENTATION of a time series is proposed . based upon the DIFFERENTIAL ENTROPY , both the OPTIMAL EMBEDDING DIMENSION , and TIME LAG , are simultaneously determined . the choice of these parameters is closely related to the length of the optimal tap input delay line of an ADAPTIVE FILTER or TIME-DELAY NEURAL NETWORK . the method employs a single criterion -- the '' entropy ratio '' between the PHASE SPACE REPRESENTATION of a signal and an ensemble of its <unk> -- and is first systematically tested on SYNTHETIC TIME SERIES for which the optimal EMBEDDING PARAMETERS are known , after which it is verified on a number of benchmark real-world time series . the proposed entropy ratio method is shown to consistently outperform some well-established methods . \n",
            "this paper addresses the problem of OPTIMAL EMBEDDING DIMENSION in the presence of PHASE SPACE REPRESENTATION and DIFFERENTIAL ENTROPY . we propose a method to estimate the EMBEDDING PARAMETERS and the EMBEDDING PARAMETERS . the proposed method is based on a TIME-DELAY NEURAL NETWORK and a TIME-DELAY NEURAL NETWORK . the proposed method is compared with the conventional ADAPTIVE FILTER and the ADAPTIVE FILTER .\n",
            "\n",
            "387 1000\n",
            "empirical mode decomposition -lrb- EMPIRICAL MODE DECOMPOSITION -rrb- is an ADAPTIVE METHOD for NONLINEAR AND NONSTATIONARY SIGNAL PROCESSING . although the ADAPTIVE METHOD is easy to implement and widely deployed , its THEORETICAL BACKGROUND and limitations remain uncertain . this paper investigates the performance of EMPIRICAL MODE DECOMPOSITION in two TONE SEPARATION PROBLEM , especially for the TRANSITION REGION between perfect separation and failure , with emphasis on the effect of the INITIAL PHASE . relationships between AMPLITUDE RATIO , FREQUENCY RATIO , INITIAL PHASE and performance are derived . \n",
            "this paper proposes a ADAPTIVE METHOD for NONLINEAR AND NONSTATIONARY SIGNAL PROCESSING . the ADAPTIVE METHOD is based on the EMPIRICAL MODE DECOMPOSITION . the ADAPTIVE METHOD is based on the EMPIRICAL MODE DECOMPOSITION and the ADAPTIVE METHOD . the proposed ADAPTIVE METHOD is based on the EMPIRICAL MODE DECOMPOSITION and the ADAPTIVE METHOD . the FREQUENCY RATIO of the proposed ADAPTIVE METHOD is compared with the conventional ADAPTIVE METHOD and the ADAPTIVE METHOD .\n",
            "\n",
            "388 1000\n",
            "most SENTIMENT ANALYSIS approaches use as baseline a SUPPORT VECTOR MACHINES CLASSIFIER with BINARY UNIGRAM WEIGHTS . in this paper , we explore whether more sophisticated FEATURE WEIGHTING SCHEMES from INFORMATION RETRIEVAL can enhance CLASSIFICATION ACCURACY . we show that variants of the classic TF.IDF SCHEME adapted to SENTIMENT ANALYSIS provide significant increases in ACCURACY , especially when using a SUBLINEAR FUNCTION for TERM FREQUENCY WEIGHTS and DOCUMENT FREQUENCY SMOOTHING . the techniques are tested on a wide selection of DATA SETS and produce the best ACCURACY to our knowledge . \n",
            "this paper addresses the problem of INFORMATION RETRIEVAL in DATA SETS . we propose a TF.IDF SCHEME based on a SUPPORT VECTOR MACHINES CLASSIFIER and a SUPPORT VECTOR MACHINES CLASSIFIER . the proposed TF.IDF SCHEME is based on a SUPPORT VECTOR MACHINES CLASSIFIER with BINARY UNIGRAM WEIGHTS and TERM FREQUENCY WEIGHTS . the CLASSIFICATION ACCURACY of the proposed TF.IDF SCHEME is evaluated using DATA SETS and DATA SETS . experimental results show that the proposed method is robust and robust to CLASSIFICATION ACCURACY and CLASSIFICATION ACCURACY .\n",
            "\n",
            "389 1000\n",
            "while HUMAN LISTENING is robust in complex AUDITORY SCENES , current SPEECH SEGREGATION ALGORITHMS do not perform well in NOISY AND REVERBERANT ENVIRONMENTS . this paper addresses the ROBUSTNESS in BINAURAL SPEECH SEGREGATION by employing BINARY CLASSIFICATION based on DEEP NEURAL NETWORKS -lrb- dnns -rrb- . we systematically examine DNN BASED BINAURAL CLASSIFICATION to UNTRAINED CONFIGURATIONS . evaluations and comparisons show that DNN BASED BINAURAL CLASSIFICATION produces superior segregation performance in a variety of MULTISOURCE AND REVERBERANT CONDITIONS . \n",
            "this paper presents a method for BINAURAL SPEECH SEGREGATION in AUDITORY SCENES . the proposed method is based on the use of DEEP NEURAL NETWORKS for BINAURAL SPEECH SEGREGATION . the proposed method is based on the use of DEEP NEURAL NETWORKS for BINAURAL SPEECH SEGREGATION . the ROBUSTNESS of the proposed method is demonstrated by the use of DEEP NEURAL NETWORKS for BINAURAL SPEECH SEGREGATION .\n",
            "\n",
            "390 1000\n",
            "in this paper we provide the first , to the best of our knowledge , BAYESIAN FORMULATION of one of the most successful and well-studied STATISTICAL MODELS OF SHAPE AND TEXTURE , i.e. ACTIVE APPEARANCE MODELS . to this end , we use a simple PROBABILISTIC MODEL for TEXTURE GENERATION assuming both GAUSSIAN NOISE and a GAUSSIAN PRIOR over a LATENT TEXTURE SPACE . we retrieve the SHAPE PARAMETERS by formulating a novel COST FUNCTION obtained by marginalizing out the LATENT TEXTURE SPACE . this results in a fast implementation when compared to other SIMULTANEOUS ALGORITHMS for fitting ACTIVE APPEARANCE MODELS , mainly due to the removal of the CALCULATION OF TEXTURE PARAMETERS . we demonstrate that , contrary to what is believed regarding the performance of ACTIVE APPEARANCE MODELS in GENERIC FITTING SCENARIOS , optimization of the proposed COST FUNCTION produces results that outperform discriminatively trained state-of-the-art methods in the problem of facial alignment '' in the wild '' . \n",
            "this paper presents a new BAYESIAN FORMULATION for TEXTURE GENERATION . the PROBABILISTIC MODEL is based on a BAYESIAN FORMULATION and a GAUSSIAN PRIOR . the PROBABILISTIC MODEL is based on the CALCULATION OF TEXTURE PARAMETERS and the CALCULATION OF TEXTURE PARAMETERS . the proposed BAYESIAN FORMULATION is based on a BAYESIAN FORMULATION and a GAUSSIAN PRIOR . the proposed BAYESIAN FORMULATION is based on a BAYESIAN FORMULATION and a BAYESIAN FORMULATION . the proposed BAYESIAN FORMULATION is based on a BAYESIAN FORMULATION and is shown to be robust to GAUSSIAN NOISE .\n",
            "\n",
            "391 1000\n",
            "we introduce an ERROR MINING TECHNIQUE for AUTOMATICALLY DETECTING ERRORS in resources that are used in PARSING SYSTEMS . we applied this ERROR MINING TECHNIQUE on PARSING results produced on several million words by two distinct PARSING SYSTEMS , which share the SYNTACTIC LEXICON and the PRE-PARSING PROCESSING CHAIN . we were thus able to identify MISSING AND ERRONEOUS INFORMATION in these resources . \n",
            "this paper presents a ERROR MINING TECHNIQUE for AUTOMATICALLY DETECTING ERRORS . the proposed ERROR MINING TECHNIQUE is based on a PRE-PARSING PROCESSING CHAIN and a PRE-PARSING PROCESSING CHAIN . the proposed ERROR MINING TECHNIQUE is based on the ERROR MINING TECHNIQUE and the ERROR MINING TECHNIQUE . the proposed ERROR MINING TECHNIQUE is based on the ERROR MINING TECHNIQUE and the ERROR MINING TECHNIQUE . experimental results show that the proposed ERROR MINING TECHNIQUE is effective for AUTOMATICALLY DETECTING ERRORS .\n",
            "\n",
            "392 1000\n",
            "differential privacy preserving regression models guarantee protection against attempts to infer whether a subject was included in the training set used to derive a model . it is not designed to protect ATTRIBUTE PRIVACY of a target individual when MODEL INVERSION ATTACKS are <unk> . in MODEL INVERSION ATTACKS , an adversary uses the RELEASED MODEL to make predictions of sensitive attributes -lrb- used as input to the model -rrb- of a target individual when some background information about the target individual is available . previous research showed that existing DIFFERENTIAL PRIVACY MECHANISMS can not effectively prevent MODEL INVERSION ATTACKS while retaining MODEL EFFICACY . in this paper , we develop a novel approach which leverages the FUNCTIONAL MECHANISM to perturb coefficients of the POLYNOMIAL REPRESENTATION OF THE OBJECTIVE FUNCTION but effectively balances the PRIVACY BUDGET for SENSITIVE AND NON-SENSITIVE ATTRIBUTES in learning the DIFFERENTIAL PRIVACY PRESERVING REGRESSION MODEL . THEORETICAL ANALYSIS and empirical evaluations demonstrate our approach can effectively prevent MODEL INVERSION ATTACKS and retain MODEL UTILITY . \n",
            "this paper proposes a new method for MODEL INVERSION ATTACKS based on the FUNCTIONAL MECHANISM . the proposed method is based on the FUNCTIONAL MECHANISM . the proposed method is based on the FUNCTIONAL MECHANISM . the proposed method is based on the use of a POLYNOMIAL REPRESENTATION OF THE OBJECTIVE FUNCTION . the proposed method is based on the FUNCTIONAL MECHANISM . the proposed method is based on the FUNCTIONAL MECHANISM . the proposed method is based on the DIFFERENTIAL PRIVACY PRESERVING REGRESSION MODEL . the proposed method is based on the DIFFERENTIAL PRIVACY PRESERVING REGRESSION MODEL . the proposed method is based on the DIFFERENTIAL PRIVACY PRESERVING REGRESSION MODEL . the proposed method is based on the THEORETICAL ANALYSIS . the proposed method is based on the DIFFERENTIAL PRIVACY PRESERVING REGRESSION MODEL . the proposed method is based on the THEORETICAL ANALYSIS .\n",
            "\n",
            "393 1000\n",
            "in this paper we propose a novel method for IMAGE SEMANTIC SEGMENTATION using multiple graphs . the MULTI-VIEW AFFINITY GRAPH is constructed by leveraging the consistency between SEMANTIC SPACE and multiple VISUAL SPACES . with BLOCK-DIAGONAL CONSTRAINTS , we enforce the AFFINITY MATRIX to be sparse such that the PAIRWISE POTENTIAL for DISSIMILAR SUPERPIXELS is close to zero . by a DIVIDE-AND-CONQUER STRATEGY , the OPTIMIZATION for LEARNING AFFINITY MATRIX is decomposed into several subproblems that can be solved in parallel . using the neighborhood relationship between superpixels and the consistency between AFFINITY MATRIX and LABEL-CONFIDENCE MATRIX , we infer the SEMANTIC LABEL for each superpixel of UNLABELED IMAGES by minimizing an objective whose CLOSED FORM SOLUTION can be easily obtained . experimental results on two REAL-WORLD IMAGE DATASETS demonstrate the effectiveness of our method . \n",
            "this paper presents a new method for IMAGE SEMANTIC SEGMENTATION based on BLOCK-DIAGONAL CONSTRAINTS . the proposed method is based on a DIVIDE-AND-CONQUER STRATEGY , which is based on the LEARNING AFFINITY MATRIX . the proposed method is based on a DIVIDE-AND-CONQUER STRATEGY and a DIVIDE-AND-CONQUER STRATEGY based on BLOCK-DIAGONAL CONSTRAINTS . the proposed method is based on a DIVIDE-AND-CONQUER STRATEGY , which is based on the LEARNING AFFINITY MATRIX . the proposed method is based on a DIVIDE-AND-CONQUER STRATEGY and a DIVIDE-AND-CONQUER STRATEGY . the proposed method is based on a DIVIDE-AND-CONQUER STRATEGY , which is based on the LEARNING AFFINITY MATRIX . the proposed method is based on a DIVIDE-AND-CONQUER STRATEGY and a CLOSED FORM SOLUTION .\n",
            "\n",
            "394 1000\n",
            "this paper presents an IMAGE PROCESSING SYSTEM for DETECTING MOUSE PRETERM LABOR using SECOND HARMONIC GENERATION MICROSCOPY IMAGES . two classes of SHG IMAGES are considered : NORMAL PREGNANT CERVIX and PREMATURE CERVICAL REMODELING induced by MIFEPRISTONE . among the commonly used TEXTURE FEATURES in IMAGE PROCESSING , WAVELET-BASED TEXTURE FEATURES together with previously utilized IMAGE FEATURES for SHG MICROSCOPY of ARTIFICIAL COLLAGEN GELS are identified to form an effective set of FEATURES for distinguishing the two classes of IMAGES . the results obtained indicate that correct DETECTION RATES above 98 % are achievable . \n",
            "this paper addresses the problem of DETECTING MOUSE PRETERM LABOR in IMAGES . we propose a method for DETECTING MOUSE PRETERM LABOR based on WAVELET-BASED TEXTURE FEATURES . the proposed method is based on a NORMAL PREGNANT CERVIX , which is based on the TEXTURE FEATURES . the proposed method is based on the use of WAVELET-BASED TEXTURE FEATURES , which are used for DETECTING MOUSE PRETERM LABOR . the proposed method is based on a NORMAL PREGNANT CERVIX , which is based on the NORMAL PREGNANT CERVIX . the proposed method is based on the use of WAVELET-BASED TEXTURE FEATURES , which are used for DETECTING MOUSE PRETERM LABOR . experimental results show the effectiveness of the proposed IMAGE PROCESSING SYSTEM .\n",
            "\n",
            "395 1000\n",
            "in this paper we relate the PARTITION FUNCTION to the MAX-STATISTICS OF RANDOM VARIABLES . in particular , we provide a novel framework for approximating and bounding the PARTITION FUNCTION using MAP INFERENCE on RANDOMLY PERTURBED MODELS . as a result , we can use efficient MAP SOLVERS such as GRAPH-CUTS to evaluate the corresponding PARTITION FUNCTION . we show that our method excels in the typical '' high <unk> coupling '' regime that results in RAGGED ENERGY LANDSCAPES difficult for alternative approaches . \n",
            "this paper addresses the problem of RAGGED ENERGY LANDSCAPES in RANDOMLY PERTURBED MODELS . we propose a method for MAP INFERENCE based on the MAX-STATISTICS OF RANDOM VARIABLES . the proposed method is based on a PARTITION FUNCTION , which is based on the PARTITION FUNCTION . the proposed method is based on a PARTITION FUNCTION , and is shown to be more robust to GRAPH-CUTS than the conventional MAP SOLVERS .\n",
            "\n",
            "396 1000\n",
            "most methods for DECISION-THEORETIC ONLINE LEARNING are based on the HEDGE ALGORITHM , which takes a PARAMETER called the LEARNING RATE . in most previous analyses the LEARNING RATE was carefully tuned to obtain optimal worst-case performance , leading to suboptimal performance on easy instances , for example when there exists an action that is significantly better than all others . we propose a new way of setting the LEARNING RATE , which adapts to the difficulty of the DECISION-THEORETIC ONLINE LEARNING : in the worst case our procedure still guarantees optimal performance , but on easy instances it achieves much smaller regret . in particular , our adaptive method achieves constant regret in a PROBABILISTIC SETTING , when there exists an action that on average obtains strictly smaller loss than all other actions . we also provide a SIMULATION STUDY comparing our approach to existing methods . \n",
            "this paper presents a SIMULATION STUDY for DECISION-THEORETIC ONLINE LEARNING . the HEDGE ALGORITHM is based on a SIMULATION STUDY . the HEDGE ALGORITHM is based on a SIMULATION STUDY . the proposed HEDGE ALGORITHM is based on a SIMULATION STUDY , which is a generalization of the HEDGE ALGORITHM .\n",
            "\n",
            "397 1000\n",
            "this paper presents a new approach for estimating voice source and vocal tract filter characteristics of VOICED SPEECH . when it is required to know the TRANSFER FUNCTION of a system in SIGNAL PROCESSING , the input and output of the system are experimentally observed and used to calculate the function . however , in the case of SOURCE-FILTER SEPARATION we deal with in this paper , only the output -lrb- speech -rrb- is observed and the characteristics of the system -lrb- vocal tract -rrb- and the input -lrb- voice source -rrb- must simultaneously be estimated . hence the estimate becomes extremely difficult , and it is usually solved approximately using OVERSIMPLIFIED MODELS . we demonstrate that these characteristics are separable under the assumption that they are independently controlled by different factors . the separation is realised using an ITERATIVE APPROXIMATION along with the MULTI-FRAME ANALYSIS METHOD , which we have proposed to find spectral envelopes of VOICED SPEECH with minimum interference of the HARMONIC STRUCTURE . \n",
            "this paper presents a new method for SOURCE-FILTER SEPARATION in VOICED SPEECH . the method is based on a MULTI-FRAME ANALYSIS METHOD , which is based on a MULTI-FRAME ANALYSIS METHOD . the proposed method is based on a MULTI-FRAME ANALYSIS METHOD . the proposed method is based on the MULTI-FRAME ANALYSIS METHOD . the proposed method is based on the MULTI-FRAME ANALYSIS METHOD . the proposed method is based on the MULTI-FRAME ANALYSIS METHOD and the MULTI-FRAME ANALYSIS METHOD .\n",
            "\n",
            "398 1000\n",
            "target TARGET CLASSIFICATION using DISTRIBUTED SENSOR ARRAYS remains a challenging problem due to the NON-STATIONARITY OF TARGET SIGNATURES , large geographical area coverage of sensor arrays , and the requirements of <unk> and reliable information delivery . in this paper , we develop an algorithm to derive effective and stable features from both the frequency and the time-frequency domains of the acoustic signals . a modified DATA FUSION ALGORITHM for DISTRIBUTED SENSOR ARRAYS is also developed in order to integrate the TARGET CLASSIFICATION results from different sensors and provide <unk> . by using DATA FUSION , the ACCURACY of the TARGET CLASSIFICATION can be increased by as many as 50 % . \n",
            "this paper presents a novel DATA FUSION ALGORITHM for TARGET CLASSIFICATION . the proposed DATA FUSION ALGORITHM is based on the NON-STATIONARITY OF TARGET SIGNATURES and the NON-STATIONARITY OF TARGET SIGNATURES . the proposed DATA FUSION ALGORITHM is based on the NON-STATIONARITY OF TARGET SIGNATURES and the NON-STATIONARITY OF TARGET SIGNATURES . experimental results show that the proposed DATA FUSION ALGORITHM can improve the performance of DISTRIBUTED SENSOR ARRAYS .\n",
            "\n",
            "399 1000\n",
            "the recent availability of large corpora for training n-gram language models has shown the utility of models of higher order than just <unk> . in this paper , we investigate methods to control the increase in MODEL SIZE resulting from applying standard methods at higher orders . we introduce SIGNIFICANCE-BASED N-GRAM SELECTION , which not only reduces MODEL SIZE , but also improves PERPLEXITY for several SMOOTHING METHODS , including KATZ BACK-OFF and ABSOLUTE DISCOUNTING . we also show that , when combined with a new SMOOTHING METHOD and a novel variant of WEIGHTED-DIFFERENCE PRUNING , our SELECTION METHOD performs better in the trade-off between MODEL SIZE and PERPLEXITY than the best PRUNING METHOD we found for MODIFIED KNESER-NEY SMOOTHING . \n",
            "this paper presents a new SELECTION METHOD for SIGNIFICANCE-BASED N-GRAM SELECTION . the proposed SELECTION METHOD is based on the SMOOTHING METHOD and the SMOOTHING METHOD . the proposed SELECTION METHOD is based on the SMOOTHING METHOD and the SELECTION METHOD . the proposed SELECTION METHOD is compared with the conventional SMOOTHING METHOD and the PRUNING METHOD .\n",
            "\n",
            "400 1000\n",
            "<unk> is the most common GAME TREE SEARCH ALGORITHM , due to its high-performance and straightforward implementation . in practice one must find the best trade-off between HEURISTIC EVALUATION TIME and bringing the subset of NODES explored closer to a MINIMUM PROOF GRAPH . in this paper we present a series of structural properties of MINIMUM PROOF GRAPHS that help us to prove that finding such GRAPHS is np-hard for ARBITRARY DAG INPUTS , but can be done in LINEAR TIME for trees . we then introduce the class of FASTEST-CUT-FIRST SEARCH HEURISTICS that aim to approximate MINIMUM PROOF GRAPHS by sorting moves based on APPROXIMATIONS OF SUB-DAG VALUES and sizes . to explore how various aspects of the GAME TREE -lrb- such as BRANCHING FACTOR and distribution of move values -rrb- affect the performance of ALPHA-BETA we introduce the class of '' prefix value game trees '' that allows us to label INTERIOR NODES with true MINIMAX VALUES on the fly without search . using these trees we show that by explicitly attempting to approximate a MINIMUM GAME TREE we are able to achieve performance gains over ALPHA-BETA with common extensions . \n",
            "this paper proposes a GAME TREE SEARCH ALGORITHM , called MINIMUM PROOF GRAPHS , which is based on MINIMUM PROOF GRAPHS . the GAME TREE SEARCH ALGORITHM is based on a MINIMUM GAME TREE , which is a GAME TREE . the proposed GAME TREE SEARCH ALGORITHM is a GAME TREE SEARCH ALGORITHM , which is a BRANCHING FACTOR . the proposed GAME TREE SEARCH ALGORITHM is a GAME TREE SEARCH ALGORITHM , called ALPHA-BETA , for ARBITRARY DAG INPUTS . the proposed GAME TREE SEARCH ALGORITHM is a GAME TREE SEARCH ALGORITHM , which is a MINIMUM PROOF GRAPH . the proposed GAME TREE SEARCH ALGORITHM is based on a MINIMUM PROOF GRAPH , which is a BRANCHING FACTOR . the proposed GAME TREE SEARCH ALGORITHM is based on a GAME TREE SEARCH ALGORITHM , which is a GAME TREE SEARCH ALGORITHM . the proposed GAME TREE SEARCH ALGORITHM is compared with state-of-the-art FASTEST-CUT-FIRST SEARCH HEURISTICS .\n",
            "\n",
            "401 1000\n",
            "in the WRAPPER APPROACH for FEATURE SELECTION , a popular criterion used is the LEAVE-ONE-OUT ESTIMATE of the CLASSIFICATION ERROR . while being relatively unbiased , the LEAVE-ONE-OUT ERROR ESTIMATE is nonetheless known to exhibit a large variance , which can be detrimental especially for small samples . we propose reducing its variance -lrb- i.e. smoothing -rrb- at two levels . at the first level , we smooth the ERROR COUNT using ESTIMATES OF POSTERIOR PROBABILITIES ; while at the second level , we smooth the POSTERIOR PROBABILITY ESTIMATES themselves using BAYESIAN ESTIMATION with CONJUGATE PRIORS . furthermore , we propose using the JACKKNIFE to reduce the BIAS inherent in BAYESIAN ESTIMATORS . we then show empirically that smoothing the ERROR ESTIMATE gives improved performance in FEATURE SELECTION . \n",
            "this paper proposes a new WRAPPER APPROACH for BAYESIAN ESTIMATION with CONJUGATE PRIORS . the proposed WRAPPER APPROACH is based on the ESTIMATES OF POSTERIOR PROBABILITIES of the LEAVE-ONE-OUT ESTIMATE . the proposed WRAPPER APPROACH is based on the ESTIMATES OF POSTERIOR PROBABILITIES of the LEAVE-ONE-OUT ESTIMATE . the proposed WRAPPER APPROACH is based on the ESTIMATES OF POSTERIOR PROBABILITIES of the LEAVE-ONE-OUT ESTIMATE . the proposed WRAPPER APPROACH is applied to BAYESIAN ESTIMATION with CONJUGATE PRIORS and is shown to outperform the conventional BAYESIAN ESTIMATORS .\n",
            "\n",
            "402 1000\n",
            "in this paper , an efficient GLOBAL ALGORITHM for VECTORIZING LINE DRAWINGS is presented . GLOBAL ALGORITHM first extracts a SEED SEGMENT of a GRAPHIC ENTITY from a RASTER IMAGE to obtain its direction and width , then tracks the pixels under the guidance of the direction so that the TRACKING can track through junctions and is not affected by NOISE and DEGRADATION OF IMAGE QUALITY . thus , an ENTITY will be <unk> in one step without POSTPROCESSING . the relations among lines are also used to realize the CONTINUOUS VECTORIZATION OF A LINE NET . the speed and quality of VECTORIZATION are greatly improved with this GLOBAL ALGORITHM . the performance evaluation is carried out both by THEORETICAL ANALYSIS and by experiments . comparisons with other VECTORIZATION ALGORITHMS are also made . \n",
            "this paper presents a new method for TRACKING based on the CONTINUOUS VECTORIZATION OF A LINE NET . the proposed method is based on the CONTINUOUS VECTORIZATION OF A LINE NET and the GLOBAL ALGORITHM . the proposed method is based on the CONTINUOUS VECTORIZATION OF A LINE NET and the VECTORIZATION . the proposed method is based on the CONTINUOUS VECTORIZATION OF A LINE NET and the VECTORIZATION . the proposed method is based on the CONTINUOUS VECTORIZATION OF A LINE NET and the VECTORIZATION . the proposed method is based on the CONTINUOUS VECTORIZATION OF A LINE NET and the GLOBAL ALGORITHM . the proposed method is evaluated on the RASTER IMAGE and the results show the effectiveness of the proposed method .\n",
            "\n",
            "403 1000\n",
            "gaussian mixture models -lrb- gmms -rrb- and ERGODIC HIDDEN MARKOV MODELS have been successfully applied to model SHORT-TERM ACOUSTIC VECTORS for SPEAKER RECOGNITION SYSTEMS . PROSODIC FEATURES are known to carry information concerning the SPEAKER 'S IDENTITY and PROSODIC FEATURES can be combined with the SHORT-TERM ACOUSTIC VECTORS in order to increase the performance of the SPEAKER RECOGNITION SYSTEMS . in this paper , a STATISTICAL APPROACH using PITCH-DEPENDENT GMMS for modeling speakers is presented . this new STATISTICAL APPROACH is capable of simultaneously modeling the statistical distributions of the SHORT-TERM ACOUSTIC VECTORS and LONG-TERM PROSODIC FEATURES . \n",
            "this paper addresses the problem of SPEAKER 'S IDENTITY in SPEAKER RECOGNITION SYSTEMS . in this paper , we propose a new STATISTICAL APPROACH based on ERGODIC HIDDEN MARKOV MODELS . the proposed method is based on the use of LONG-TERM PROSODIC FEATURES and PITCH-DEPENDENT GMMS . the proposed method is based on the use of LONG-TERM PROSODIC FEATURES and LONG-TERM PROSODIC FEATURES . the proposed method is compared with other state-of-the-art methods .\n",
            "\n",
            "404 1000\n",
            "in MACHINE LEARNING PROBLEMS with tens of thousands of FEATURES and only dozens or hundreds of independent training examples , DIMENSIONALITY REDUCTION is essential for good learning performance . in previous work , many researchers have treated the MACHINE LEARNING PROBLEMS in two separate phases : first use an algorithm such as SINGULAR VALUE DECOMPOSITION to reduce the dimensionality of the data set , and then use a CLASSIFICATION ALGORITHM such as na <unk> ve bayes or support vector machines to learn a CLASSIFIER . we demonstrate that it is possible to combine the two goals of DIMENSIONALITY REDUCTION and classification into a single learning objective , and present a novel and efficient algorithm which optimizes this objective directly . we present experimental results in FMRI ANALYSIS which show that we can achieve better learning performance and LOWER-DIMENSIONAL REPRESENTATIONS than TWO-PHASE APPROACHES can . \n",
            "this paper addresses the problem of DIMENSIONALITY REDUCTION for MACHINE LEARNING PROBLEMS in MACHINE LEARNING PROBLEMS . in this paper , we propose a new CLASSIFICATION ALGORITHM based on the SINGULAR VALUE DECOMPOSITION . the proposed CLASSIFICATION ALGORITHM is based on the use of the SINGULAR VALUE DECOMPOSITION and the CLASSIFICATION ALGORITHM . experimental results show that the proposed CLASSIFICATION ALGORITHM outperforms existing TWO-PHASE APPROACHES in terms of DIMENSIONALITY REDUCTION .\n",
            "\n",
            "405 1000\n",
            "this paper presents a novel 3D FACE RECOGNITION METHOD by means of the EVOLUTION OF ISO-GEODESIC DISTANCE CURVES . specifically , the proposed 3D FACE RECOGNITION METHOD compares two neighboring ISO-GEODESIC DISTANCE CURVES , and formalizes the evolution between them as a ONE-DIMENSIONAL FUNCTION , named EVOLUTION ANGLE FUNCTION , which is EUCLIDEAN INVARIANT . the novelty of this paper consists in formalizing 3D FACE by an EVOLUTION ANGLE FUNCTIONS , and in computing the distance between two faces by that of two functions . experiments on face recognition grand challenge -lrb- <unk> -rrb- <unk> .0 shows that our 3D FACE RECOGNITION METHOD works very well on both NEUTRAL FACES and NON-NEUTRAL FACES . by introducing a WEIGHT FUNCTION , we also show a very promising result on NON-NEUTRAL FACE DATABASE . \n",
            "this paper presents a new method for EVOLUTION OF ISO-GEODESIC DISTANCE CURVES based on the EVOLUTION OF ISO-GEODESIC DISTANCE CURVES . the proposed method is based on the EVOLUTION OF ISO-GEODESIC DISTANCE CURVES . the proposed method is based on the EVOLUTION OF ISO-GEODESIC DISTANCE CURVES , which is based on the EVOLUTION OF ISO-GEODESIC DISTANCE CURVES . the proposed method is based on the EVOLUTION OF ISO-GEODESIC DISTANCE CURVES . the proposed method is based on the EVOLUTION OF ISO-GEODESIC DISTANCE CURVES . the proposed method is based on the EVOLUTION OF ISO-GEODESIC DISTANCE CURVES . the proposed method is based on the EVOLUTION OF ISO-GEODESIC DISTANCE CURVES . the proposed method is compared with the conventional 3D FACE RECOGNITION METHOD and the 3D FACE RECOGNITION METHOD .\n",
            "\n",
            "406 1000\n",
            "in this paper , we introduce a new approach to TWO-DIMENSIONAL PROCESSING of the ONE-DIMENSIONAL SPEECH SIGNAL in the TIME-FREQUENCY PLANE . specifically , we obtain the SHORT-SPACE 2-D FOURIER TRANSFORM MAGNITUDE of a NARROWBAND SPECTROGRAM of the signal and show that this 2-D TRANSFORMATION MAPS HARMONICALLY-RELATED SIGNAL COMPONENTS to a concentrated entity in the new 2-D PLANE . we refer to this series of operations as the '' GRATING COMPRESSION TRANSFORM '' , consistent with SINE-WAVE GRATING PATTERNS in the SPECTROGRAM reduced to SMEARED IMPULSES . the GRATING COMPRESSION TRANSFORM '' forms the basis of a SPEECH PITCH ESTIMATOR that uses the RADIAL DISTANCE to the largest peak in the GCT PLANE . using an average magnitude difference between <unk> estimates , the GCT-BASED PITCH ESTIMATOR is shown to compare favorably to a SINE-WAVE-BASED PITCH ESTIMATOR for ALL-VOICED SPEECH in ADDITIVE WHITE NOISE . an extension to a basis for TWO-SPEAKER PITCH ESTIMATION is also proposed . \n",
            "this paper proposes a GCT-BASED PITCH ESTIMATOR for TWO-SPEAKER PITCH ESTIMATION . the proposed SPEECH PITCH ESTIMATOR is based on the GRATING COMPRESSION TRANSFORM '' and the GRATING COMPRESSION TRANSFORM '' . the proposed GRATING COMPRESSION TRANSFORM '' is based on the GRATING COMPRESSION TRANSFORM '' and the GCT-BASED PITCH ESTIMATOR . the proposed GCT-BASED PITCH ESTIMATOR is based on the GRATING COMPRESSION TRANSFORM '' . the proposed SINE-WAVE-BASED PITCH ESTIMATOR is based on the GRATING COMPRESSION TRANSFORM '' . the proposed SINE-WAVE-BASED PITCH ESTIMATOR is compared with the conventional SINE-WAVE-BASED PITCH ESTIMATOR and the GCT-BASED PITCH ESTIMATOR . the proposed SINE-WAVE-BASED PITCH ESTIMATOR is compared with the conventional GCT-BASED PITCH ESTIMATOR and the SINE-WAVE-BASED PITCH ESTIMATOR .\n",
            "\n",
            "407 1000\n",
            "for MULTIUSER MISO SYSTEMS with BOUNDED UNCERTAINTIES in the CHANNEL STATE INFORMATION , we consider two CLASSICAL ROBUST DESIGN PROBLEMS : maximizing the minimum rate subject to a TRANSMIT POWER CONSTRAINT , and POWER MINIMIZATION under a RATE CONSTRAINT . contrary to conventional strategies , we propose a RATE-SPLITTING STRATEGY where each message is divided into two parts , a common part and a private part . all common parts are packed into one super common message encoded using a shared codebook and decoded by all users , while private parts are independently encoded and retrieved by their corresponding users . we prove that RS-BASED DESIGNS achieve higher MAX-MIN DEGREES OF FREEDOM compared to conventional designs -lrb- MAX-MIN DEGREES OF FREEDOM -rrb- for UNCERTAINTY REGIONS that scale with SNR . for the special case of NON-SCALING UNCERTAINTY REGIONS , MAX-MIN DEGREES OF FREEDOM contrasts with MAX-MIN DEGREES OF FREEDOM and achieves a NON-SATURATING MAX-MIN RATE . in the POWER MINIMIZATION PROBLEM , MAX-MIN DEGREES OF FREEDOM is shown to combat the FEASIBILITY PROBLEM arising from MULTIUSER INTERFERENCE in MAX-MIN DEGREES OF FREEDOM . a robust design of PRECODERS for MAX-MIN DEGREES OF FREEDOM is proposed , and performance gains over MAX-MIN DEGREES OF FREEDOM are demonstrated through simulations . \n",
            "this paper presents a new method for MAX-MIN DEGREES OF FREEDOM in MULTIUSER MISO SYSTEMS . the proposed method is based on a RATE-SPLITTING STRATEGY and a RATE-SPLITTING STRATEGY based on CHANNEL STATE INFORMATION . the proposed RATE-SPLITTING STRATEGY is based on the RATE-SPLITTING STRATEGY and the RATE-SPLITTING STRATEGY . the proposed RATE-SPLITTING STRATEGY is based on the MAX-MIN DEGREES OF FREEDOM and the TRANSMIT POWER CONSTRAINT . the proposed RATE-SPLITTING STRATEGY is compared with conventional RS-BASED DESIGNS and RS-BASED DESIGNS . the proposed RATE-SPLITTING STRATEGY is compared with conventional RS-BASED DESIGNS and RS-BASED DESIGNS .\n",
            "\n",
            "408 1000\n",
            "current diagnostic methods for MENTAL PATHOLOGIES , including POST-TRAUMATIC STRESS DISORDER , involve a CLINICIAN-CODED INTERVIEW , which can be subjective . HEART RATE and skin <unk> , as well as other PERIPHERAL PHYSIOLOGY MEASURES , have previously shown utility in PREDICTING BINARY DIAGNOSTIC DECISIONS . the BINARY DIAGNOSTIC DECISIONS is easier , but misses important information on the severity of the patients condition . this work utilizes a novel experimental setup that exploits VIRTUAL REALITY VIDEOS and PERIPHERAL PHYSIOLOGY for PTSD DIAGNOSIS . in pursuit of an AUTOMATED PHYSIOLOGY-BASED OBJECTIVE DIAGNOSTIC METHOD , we propose a LEARNING FORMULATION that integrates the description of the experimental data and expert knowledge on desirable properties of a PHYSIOLOGICAL DIAGNOSTIC SCORE . from a list of desired criteria , we derive a new COST FUNCTION that combines regression and CLASSIFICATION while learning the salient features for PREDICTING PHYSIOLOGICAL SCORE . the PHYSIOLOGICAL SCORE produced by SPARSE COMBINED REGRESSION-CLASSIFICATION is assessed with respect to three sets of criteria chosen to reflect design goals for an objective , PHYSIOLOGICAL PTSD SCORE : <unk> and context of selected features , DIAGNOSTIC SCORE VALIDITY , and LEARNING GENERALIZABILITY . for these criteria , we demonstrate that SPARSE COMBINED REGRESSION-CLASSIFICATION performs better than more GENERIC LEARNING APPROACHES . \n",
            "this paper addresses the problem of PREDICTING BINARY DIAGNOSTIC DECISIONS in VIRTUAL REALITY VIDEOS . in this paper , we propose a LEARNING FORMULATION based on a LEARNING FORMULATION . the proposed AUTOMATED PHYSIOLOGY-BASED OBJECTIVE DIAGNOSTIC METHOD is based on a LEARNING FORMULATION and a LEARNING FORMULATION . the proposed method is based on the LEARNING FORMULATION and the LEARNING FORMULATION . the proposed method is based on the LEARNING FORMULATION and the LEARNING FORMULATION . the proposed AUTOMATED PHYSIOLOGY-BASED OBJECTIVE DIAGNOSTIC METHOD is based on a LEARNING FORMULATION . the proposed AUTOMATED PHYSIOLOGY-BASED OBJECTIVE DIAGNOSTIC METHOD is compared with conventional GENERIC LEARNING APPROACHES and GENERIC LEARNING APPROACHES . the proposed AUTOMATED PHYSIOLOGY-BASED OBJECTIVE DIAGNOSTIC METHOD is compared with conventional GENERIC LEARNING APPROACHES and GENERIC LEARNING APPROACHES .\n",
            "\n",
            "409 1000\n",
            "this paper addresses the problem of VIDEO ANOMALY RECOVERY from a sequence of SPECTRALLY COMPRESSED VIDEO FRAMES . ANALYSIS OF ANOMALIES occurring in both time and spectrum is important in VIDEO SURVEILLANCE APPLICATIONS . we present a methodology for the RECOVERY OF ANOMALIES such as moving objects and their SPECTRAL SIGNATURES from SPECTRALLY COMPRESSED VIDEO . the SPECTRALLY COMPRESSED VIDEO FRAMES are obtained by using a coded aperture snapshot spectral imaging -lrb- <unk> -rrb- system . the CASSI SYSTEM encodes a 3-D DATA CUBE containing both 2-D SPATIAL INFORMATION and SPECTRAL INFORMATION in a single 2-D MEASUREMENT . in the proposed methodology , we use the SPECTRALLY COMPRESSED VIDEO as columns of a large data matrix g g g. PRINCIPAL COMPONENT PURSUIT is then used to decompose G G G into the STATIONARY BACKGROUND and a SPARSE MATRIX capturing the anomalies in the foreground . the SPARSE MATRIX is then used jointly with G G G to recover the SPECTRAL INFORMATION of the objects of interest . an example for the RECOVERY OF VIDEO ANOMALIES in a 3-CHANNEL SPECTRAL VIDEO SYSTEM -lrb- rgb -rrb- is presented . \n",
            "this paper addresses the problem of RECOVERY OF VIDEO ANOMALIES in VIDEO SURVEILLANCE APPLICATIONS . we propose a method for VIDEO ANOMALY RECOVERY based on PRINCIPAL COMPONENT PURSUIT . the proposed method is based on the PRINCIPAL COMPONENT PURSUIT , which is based on PRINCIPAL COMPONENT PURSUIT and PRINCIPAL COMPONENT PURSUIT . the proposed method is based on PRINCIPAL COMPONENT PURSUIT and PRINCIPAL COMPONENT PURSUIT . the proposed method is based on PRINCIPAL COMPONENT PURSUIT and PRINCIPAL COMPONENT PURSUIT . the proposed method is based on PRINCIPAL COMPONENT PURSUIT and PRINCIPAL COMPONENT PURSUIT . the proposed method is based on PRINCIPAL COMPONENT PURSUIT and PRINCIPAL COMPONENT PURSUIT . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "410 1000\n",
            "we consider the problem of IMAGE SEGMENTATION using ACTIVE CONTOURS through the minimization of an ENERGY CRITERION involving both REGION AND BOUNDARY FUNCTIONALS . these IMAGE SEGMENTATION are derived through a SHAPE DERIVATIVE APPROACH instead of CLASSICAL CALCULUS OF VARIATION . the IMAGE SEGMENTATION can be elegantly derived without converting the REGION INTEGRALS into boundary integrals . from the DERIVATIVE , we deduce the EVOLUTION EQUATION of an ACTIVE CONTOUR that makes it evolve towards a minimum of the criterion . we focus more particularly on STATISTICAL FEATURES globally attached to the region and especially to the probability density functions of IMAGE FEATURES such as the COLOR HISTOGRAM of a region . a THEORETICAL FRAMEWORK is set for the MINIMIZATION OF THE DISTANCE between two HISTOGRAMS for matching or tracking purposes . an application of this THEORETICAL FRAMEWORK to the SEGMENTATION OF COLOR HISTOGRAMS in VIDEO SEQUENCES is then proposed . we briefly describe our NUMERICAL SCHEME and show some experimental results . \n",
            "this paper presents a novel THEORETICAL FRAMEWORK for IMAGE SEGMENTATION . the SHAPE DERIVATIVE APPROACH is based on the ENERGY CRITERION of a COLOR HISTOGRAM and a MINIMIZATION OF THE DISTANCE . the proposed SHAPE DERIVATIVE APPROACH is based on a THEORETICAL FRAMEWORK , which is based on the SHAPE DERIVATIVE APPROACH . the proposed THEORETICAL FRAMEWORK is based on the SEGMENTATION OF COLOR HISTOGRAMS . the proposed SHAPE DERIVATIVE APPROACH is based on a THEORETICAL FRAMEWORK . the proposed THEORETICAL FRAMEWORK is based on the SEGMENTATION OF COLOR HISTOGRAMS . the proposed THEORETICAL FRAMEWORK is based on a THEORETICAL FRAMEWORK . the proposed THEORETICAL FRAMEWORK is based on a THEORETICAL FRAMEWORK . the proposed THEORETICAL FRAMEWORK is based on a THEORETICAL FRAMEWORK . the proposed THEORETICAL FRAMEWORK is based on a THEORETICAL FRAMEWORK . the proposed THEORETICAL FRAMEWORK is based on a THEORETICAL FRAMEWORK and is shown to be more robust to IMAGE SEGMENTATION .\n",
            "\n",
            "411 1000\n",
            "despite its success , UNIT SELECTION BASED TEXT-TO-SPEECH SYNTHESIS has has some disadvantages such as sudden discontinuities in SPEECH that <unk> the listeners . the HMM-BASED TTS APPROACH has been increasingly getting more attention from the tts research community . one of the advantage is the lack of spurious errors that are observed in the UNIT SELECTION SCHEME . another advantage of the HMM-BASED TTS APPROACH is the small MEMORY FOOTPRINT requirement which makes HMM-BASED TTS APPROACH attractive for EMBEDDED DEVICES . here , we propose a novel HYBRID STATISTICAL UNIT SELECTION TTS SYSTEM for AGGLUTINATIVE LANGUAGES that aims at improving the quality of the BASELINE HTS SYSTEM while keeping the MEMORY FOOTPRINT small . the INTELLIGI-BILITY AND QUALITY SCORES of the BASELINE HTS SYSTEM are comparable to the mos scores of english reported in the BLIZZARD CHALLENGE TESTS . listeners preferred the BASELINE HTS SYSTEM over the BASELINE HTS SYSTEM in the A/B PREFERENCE TESTS . \n",
            "this paper presents a HYBRID STATISTICAL UNIT SELECTION TTS SYSTEM for UNIT SELECTION BASED TEXT-TO-SPEECH SYNTHESIS . the proposed HYBRID STATISTICAL UNIT SELECTION TTS SYSTEM is based on the UNIT SELECTION SCHEME . the proposed UNIT SELECTION SCHEME is based on the UNIT SELECTION SCHEME . the proposed UNIT SELECTION SCHEME is based on the UNIT SELECTION SCHEME . the proposed UNIT SELECTION SCHEME is evaluated on the BASELINE HTS SYSTEM and compared with the conventional BASELINE HTS SYSTEM . the proposed HMM-BASED TTS APPROACH is compared with the conventional BASELINE HTS SYSTEM and the BASELINE HTS SYSTEM .\n",
            "\n",
            "412 1000\n",
            "we propose an ACTIVE LEARNING APPROACH to training a SEGMENTATION CLASSIFIER that exploits GEOMETRIC PRIORS to <unk> the ANNOTATION PROCESS in 3D IMAGE VOLUMES . to this end , we use these priors not only to select voxels most in need of ANNOTATION but to guarantee that they lie on 2D PLANAR PATCH , which makes it much easier to annotate than if they were randomly distributed in the volume . a simplified version of this ACTIVE LEARNING APPROACH is effective in NATURAL 2D IMAGES . we evaluated our ACTIVE LEARNING APPROACH on ELECTRON MICROSCOPY AND MAGNETIC RESONANCE IMAGE VOLUMES , as well as on NATURAL 2D IMAGES . comparing our ACTIVE LEARNING APPROACH against several accepted baselines demonstrates a marked performance increase . \n",
            "this paper proposes a new ACTIVE LEARNING APPROACH for ELECTRON MICROSCOPY AND MAGNETIC RESONANCE IMAGE VOLUMES . the ACTIVE LEARNING APPROACH is based on the ACTIVE LEARNING APPROACH . the ACTIVE LEARNING APPROACH is based on the ACTIVE LEARNING APPROACH . the proposed ACTIVE LEARNING APPROACH is based on the ACTIVE LEARNING APPROACH . the proposed ACTIVE LEARNING APPROACH is based on the ACTIVE LEARNING APPROACH . the proposed ACTIVE LEARNING APPROACH is based on the ACTIVE LEARNING APPROACH . the proposed ACTIVE LEARNING APPROACH is evaluated on the ELECTRON MICROSCOPY AND MAGNETIC RESONANCE IMAGE VOLUMES .\n",
            "\n",
            "413 1000\n",
            "template protection is an issue of paramount importance in the design of BIOMETRIC RECOGNITION SYSTEMS . in this paper we present a BIOMETRIC CRYPTOSYSTEM applied to IRIS BIOMETRICS , where TEMPLATE SECURITY is guaranteed by means of a framework inspired by the DIGITAL MODULATION PARADIGM . specifically , the properties of MODULATION CONSTELLATIONS and TURBO CODES with SOFT-DECODING are exploited to design a BIOMETRIC CRYPTOSYSTEM with high performance in terms of both VERIFICATION RATES and SECURITY , even while dealing with a IRIS BIOMETRICS characterized by a high INTRA-CLASS VARIABILITY such as the IRIS . the effectiveness of the proposed BIOMETRIC CRYPTOSYSTEM is evaluated by performing tests on the interval subset of the CASIA-IRISV4 DATABASE . \n",
            "this paper presents a new method for BIOMETRIC RECOGNITION SYSTEMS in BIOMETRIC RECOGNITION SYSTEMS . the DIGITAL MODULATION PARADIGM is based on the DIGITAL MODULATION PARADIGM and the DIGITAL MODULATION PARADIGM . the proposed method is based on the use of TURBO CODES and TEMPLATE PROTECTION . the proposed method is based on the use of TURBO CODES and TEMPLATE PROTECTION . the proposed method is evaluated on the CASIA-IRISV4 DATABASE and on the CASIA-IRISV4 DATABASE . experimental results show the effectiveness of the proposed DIGITAL MODULATION PARADIGM .\n",
            "\n",
            "414 1000\n",
            "in this work we empirically study the MULTI-SCALE BOUNDARY DETECTION PROBLEM in NATURAL IMAGES . we utilize LOCAL BOUNDARY CUES including CONTRAST , LOCALIZATION and RELATIVE CONTRAST , and train a CLASSIFIER to integrate them across scales . our approach successfully combines strengths from both LARGE-SCALE DETECTION -lrb- robust but POOR LOCALIZATION -rrb- and SMALL-SCALE DETECTION -lrb- <unk> but sensitive to CLUTTER -rrb- . we carry out quantitative evaluations on a variety of BOUNDARY AND OBJECT DATASETS with HUMAN-MARKED GROUNDTRUTH . we show that MULTI-SCALE BOUNDARY DETECTION PROBLEM offers large improvements , ranging from 20 % to 50 % , over SINGLE-SCALE APPROACHES . this is the first time that multi-scale is demonstrated to improve BOUNDARY DETECTION on large datasets of NATURAL IMAGES . \n",
            "this paper presents a new method for LARGE-SCALE DETECTION in NATURAL IMAGES . the proposed method is based on the use of LOCAL BOUNDARY CUES , such as CONTRAST , CONTRAST , and RELATIVE CONTRAST . the proposed method is based on the MULTI-SCALE BOUNDARY DETECTION PROBLEM , which is based on the MULTI-SCALE BOUNDARY DETECTION PROBLEM . the proposed method is based on the use of LOCAL BOUNDARY CUES , CONTRAST , CONTRAST , and CONTRAST . experimental results show that the proposed method outperforms SINGLE-SCALE APPROACHES and other SINGLE-SCALE APPROACHES such as LOCALIZATION , LOCALIZATION , and LOCALIZATION .\n",
            "\n",
            "415 1000\n",
            "a key challenge in MULTIAGENT ENVIRONMENTS is the CONSTRUCTION OF AGENTS that are able to learn while acting in the presence of other agents that are simultaneously learning and adapting . these domains require ON-LINE LEARNING METHODS without the benefit of repeated training examples , as well as the ability to adapt to the evolving behavior of other agents in the environment . the difficulty is further exacerbated when the agents are in an ADVERSARIAL RELATIONSHIP , demanding that a robust -lrb- i.e. winning -rrb- NON-STATIONARY POLICY be rapidly learned and adapted . we propose an ON-LINE SEQUENCE LEARNING ALGORITHM , ELPH , based on a straightforward ENTROPY PRUNING TECHNIQUE that is able to rapidly learn and adapt to NON-STATIONARY POLICIES . we demonstrate the performance of this ON-LINE SEQUENCE LEARNING ALGORITHM in a non-stationary learning environment of ADVERSARIAL ZERO-SUM MATRIX GAMES . \n",
            "this paper addresses the problem of CONSTRUCTION OF AGENTS in MULTIAGENT ENVIRONMENTS . in this paper , we propose a new ENTROPY PRUNING TECHNIQUE , called ELPH , which is based on a ENTROPY PRUNING TECHNIQUE . the proposed ON-LINE SEQUENCE LEARNING ALGORITHM is a ENTROPY PRUNING TECHNIQUE , which is a ENTROPY PRUNING TECHNIQUE . the proposed ENTROPY PRUNING TECHNIQUE is a ENTROPY PRUNING TECHNIQUE , called ELPH . the proposed method is compared with state-of-the-art ON-LINE LEARNING METHODS .\n",
            "\n",
            "416 1000\n",
            "linear discriminant or <unk> eve transforms are established techniques for MAPPING FEATURES into a LOWER DIMENSIONAL SUBSPACE . this paper introduces a UNIFORM STATISTICAL FRAMEWORK , where the computation of the OPTIMAL FEATURE REDUCTION is formalized as a MAXIMUM-LIKELIHOOD ESTIMATION PROBLEM . the experimental evaluation of this suggested extension of LINEAR SELECTION METHODS shows a slight improvement of the RECOGNITION ACCURACY . \n",
            "this paper presents a new method for OPTIMAL FEATURE REDUCTION based on a UNIFORM STATISTICAL FRAMEWORK . the proposed method is based on the use of a UNIFORM STATISTICAL FRAMEWORK and a UNIFORM STATISTICAL FRAMEWORK . the proposed UNIFORM STATISTICAL FRAMEWORK is evaluated on the MAXIMUM-LIKELIHOOD ESTIMATION PROBLEM . the experimental results show that the proposed method outperforms the conventional LINEAR SELECTION METHODS in terms of both RECOGNITION ACCURACY and RECOGNITION ACCURACY .\n",
            "\n",
            "417 1000\n",
            "a scheme for BINAURAL PRE-PROCESSING OF SPEECH SIGNALS for input to a standard LINEAR HEARING AID has been investigated . the system is based on that of <unk> & <unk> -lsb- l -rsb- who applied the LEAST MEAN SQUARES ALGORITHM in SUB-BANDS to SPEECH SIGNALS from various ACOUSTIC ENVIRONMENTS and signal to NOISE RATIOS . the processing scheme attempts to take advantage of the multiple inputs to perform NOISE CANCELLATION . the use of SUB-BANDS enables a diverse PROCESSING MECHANISM to be employed , where the WIDE-BAND SIGNAL is split into smaller frequency limited SUB-BANDS , which can subsequently he processed according to their SIGNAL CHARACTERISTICS . the results of a large scale series of intelligibility tests are presented from experiments in which ACOUSTIC SPEECH AND NOISE DATA , generated using SIMULATED AND REAL-ROOM ACOUSTICS was tested on HEARING IMPAIRED VOLUNTEERS . \n",
            "this paper presents a novel PROCESSING MECHANISM for BINAURAL PRE-PROCESSING OF SPEECH SIGNALS . the PROCESSING MECHANISM is based on the LEAST MEAN SQUARES ALGORITHM . the PROCESSING MECHANISM is based on the LEAST MEAN SQUARES ALGORITHM . the PROCESSING MECHANISM is based on the LEAST MEAN SQUARES ALGORITHM of the LINEAR HEARING AID . the PROCESSING MECHANISM is applied to the BINAURAL PRE-PROCESSING OF SPEECH SIGNALS . the PROCESSING MECHANISM is applied to the ACOUSTIC SPEECH AND NOISE DATA . the proposed PROCESSING MECHANISM is shown to outperform the conventional PROCESSING MECHANISM in terms of NOISE RATIOS and NOISE RATIOS .\n",
            "\n",
            "418 1000\n",
            "the ON-CHIP LINE BUFFER dominates the total area and power of LINE-BASED 2-D DWT . therefore , the LINE BUFFER WORDLENGTH has to be carefully designed to maintain the quality level due to the DYNAMIC RANGE growing and the ROUND-OFF ERRORS . in this paper , a complete ANALYSIS METHODOLOGY is proposed to derive the required WORDLENGTH OF LINE BUFFER given the desired quality level of RECONSTRUCTED IMAGE . the proposed ANALYSIS METHODOLOGY can guarantee to avoid OVERFLOW OF COEFFICIENTS , and the difference between predicted and experimental quality level is <unk> 0.06 db in terms of PSNR . \n",
            "this paper presents a ANALYSIS METHODOLOGY for LINE-BASED 2-D DWT . the proposed ANALYSIS METHODOLOGY is based on the WORDLENGTH OF LINE BUFFER of the RECONSTRUCTED IMAGE . the proposed ANALYSIS METHODOLOGY is based on the WORDLENGTH OF LINE BUFFER of the RECONSTRUCTED IMAGE . the proposed ANALYSIS METHODOLOGY is based on the WORDLENGTH OF LINE BUFFER of the RECONSTRUCTED IMAGE . the proposed ANALYSIS METHODOLOGY is based on the WORDLENGTH OF LINE BUFFER . the proposed ANALYSIS METHODOLOGY is based on a WORDLENGTH OF LINE BUFFER and is shown to be more robust to ROUND-OFF ERRORS than the conventional ANALYSIS METHODOLOGY .\n",
            "\n",
            "419 1000\n",
            "our understanding of the INPUT-OUTPUT FUNCTION OF SINGLE CELLS has been substantially advanced by BIOPHYSICALLY ACCURATE MULTI-COMPARTMENTAL MODELS . the large number of parameters needing HAND TUNING in these BIOPHYSICALLY ACCURATE MULTI-COMPARTMENTAL MODELS has , however , somewhat hampered their applicability and <unk> . here we propose a simple and well-founded method for AUTOMATIC ESTIMATION of many of these key parameters : 1 -rrb- the SPATIAL DISTRIBUTION OF CHANNEL DENSITIES on the cell 's <unk> ; 2 -rrb- the SPATIOTEMPORAL PATTERN OF SYNAPTIC INPUT ; 3 -rrb- the channels ' REVERSAL POTENTIALS ; 4 -rrb- the IN-TERCOMPARTMENTAL CONDUCTANCES ; and 5 -rrb- the NOISE LEVEL in each <unk> . we assume experimental access to : a -rrb- the SPATIOTEMPORAL VOLTAGE SIGNAL in the <unk> -lrb- or some contiguous <unk> thereof , e.g. via VOLTAGE SENSITIVE IMAGING TECHNIQUES -rrb- , b -rrb- an approximate <unk> description of the channels and synapses present in each <unk> , and c -rrb- the morphology of the part of the neuron under investigation . the key observation is that , given data a -rrb- - c -rrb- , all of the parameters 1 -rrb- -4 -rrb- may be simultaneously inferred by a version of CONSTRAINED LINEAR REGRESSION ; this CONSTRAINED LINEAR REGRESSION , in turn , is efficiently solved using standard algorithms , without any '' local minima '' problems despite the large number of parameters and complex dynamics . the NOISE LEVEL 5 -RRB- may also be estimated by standard techniques . we demonstrate the method 's ACCURACY on several MODEL DATASETS , and describe techniques for quantifying the uncertainty in our estimates . \n",
            "this paper addresses the problem of AUTOMATIC ESTIMATION in the presence of NOISE LEVEL . we propose a method for AUTOMATIC ESTIMATION based on CONSTRAINED LINEAR REGRESSION . the method is based on the SPATIOTEMPORAL PATTERN OF SYNAPTIC INPUT and the SPATIOTEMPORAL PATTERN OF SYNAPTIC INPUT . the proposed method is based on the SPATIOTEMPORAL PATTERN OF SYNAPTIC INPUT and the SPATIOTEMPORAL PATTERN OF SYNAPTIC INPUT . the proposed method is based on the SPATIOTEMPORAL PATTERN OF SYNAPTIC INPUT and the SPATIOTEMPORAL PATTERN OF SYNAPTIC INPUT . the proposed method is evaluated on the MODEL DATASETS and the MODEL DATASETS .\n",
            "\n",
            "420 1000\n",
            "we present a novel DIRECTIONALLY ADAPTIVE IMAGE INTERPOLATION based on a MULTIPLE-DIRECTION WAVELET TRANSFORM , called DIRECTIONLETS . the DIRECTIONALLY ADAPTIVE IMAGE INTERPOLATION uses DIRECTIONLETS to efficiently capture DIRECTIONAL FEATURES and to extract EDGE INFORMATION along different directions from the LOW-RESOLUTION IMAGE . then , the HIGH-RESOLUTION IMAGE is generated using this information to preserve SHARPNESS OF DETAILS . our DIRECTIONALLY ADAPTIVE IMAGE INTERPOLATION outperforms the state-of-the-art methods in terms of both NUMERIC AND VISUAL QUALITY of the INTERPOLATED IMAGE . \n",
            "this paper proposes a new MULTIPLE-DIRECTION WAVELET TRANSFORM for DIRECTIONALLY ADAPTIVE IMAGE INTERPOLATION . the MULTIPLE-DIRECTION WAVELET TRANSFORM is based on the MULTIPLE-DIRECTION WAVELET TRANSFORM of the LOW-RESOLUTION IMAGE . the MULTIPLE-DIRECTION WAVELET TRANSFORM is estimated by the MULTIPLE-DIRECTION WAVELET TRANSFORM . the MULTIPLE-DIRECTION WAVELET TRANSFORM is estimated using the MULTIPLE-DIRECTION WAVELET TRANSFORM . the proposed MULTIPLE-DIRECTION WAVELET TRANSFORM is shown to be robust to NUMERIC AND VISUAL QUALITY and SHARPNESS OF DETAILS .\n",
            "\n",
            "421 1000\n",
            "we propose two approaches for improving the OBJECTIVE FUNCTION for the DEEP NEURAL NETWORK FRAME-LEVEL TRAINING in LARGE VOCABULARY CONTINUOUS SPEECH RECOGNITION . the LARGE VOCABULARY CONTINUOUS SPEECH RECOGNITION used in LARGE VOCABULARY CONTINUOUS SPEECH RECOGNITION are often constructed with an OUTPUT LAYER with SOFTMAX ACTIVATION and the CROSS-ENTROPY OBJECTIVE FUNCTION is always employed in the <unk> training of LARGE VOCABULARY CONTINUOUS SPEECH RECOGNITION . the pairing of SOFTMAX ACTIVATION and CROSS-ENTROPY OBJECTIVE FUNCTION contributes much in the success of LARGE VOCABULARY CONTINUOUS SPEECH RECOGNITION . the first approach developed in this paper improves the CROSS-ENTROPY OBJECTIVE FUNCTION by boosting the importance of the frames for which the LARGE VOCABULARY CONTINUOUS SPEECH RECOGNITION has low target predictions -lrb- low target posterior probabilities -rrb- and the second one considers jointly minimizing the <unk> and maximizing the LOG POSTERIOR RATIO between the target <unk> -lrb- <unk> states -rrb- and the most competing one . experiments on SWITCHBOARD TASK demonstrate that the two proposed methods can provide 3.1 % and 1.5 % RELATIVE WORD ERROR RATE REDUCTION , respectively , against the already very strong conventional CROSS-ENTROPY TRAINED DNN SYSTEM . \n",
            "this paper presents a CROSS-ENTROPY TRAINED DNN SYSTEM for LARGE VOCABULARY CONTINUOUS SPEECH RECOGNITION . the proposed DEEP NEURAL NETWORK FRAME-LEVEL TRAINING is based on a CROSS-ENTROPY TRAINED DNN SYSTEM and a DEEP NEURAL NETWORK FRAME-LEVEL TRAINING . the proposed DEEP NEURAL NETWORK FRAME-LEVEL TRAINING is based on the CROSS-ENTROPY OBJECTIVE FUNCTION and the LOG POSTERIOR RATIO of the CROSS-ENTROPY TRAINED DNN SYSTEM . the RELATIVE WORD ERROR RATE REDUCTION of the proposed CROSS-ENTROPY TRAINED DNN SYSTEM is compared with the conventional CROSS-ENTROPY TRAINED DNN SYSTEM and the CROSS-ENTROPY TRAINED DNN SYSTEM .\n",
            "\n",
            "422 1000\n",
            "in this paper , we compare and validate different PROBABILISTIC MODELS of HUMAN HEART BEAT INTERVALS for assessment of the ELECTROCARDIOGRAM DATA recorded with varying conditions in <unk> and <unk> <unk> <unk> . the models are validated using the ADAPTIVE POINT PROCESS FILTERING PARADIGM and KOLMOGOROV-SMIRNOV TEST . the INVERSE GAUSSIAN MODEL was found to achieve the overall best performance in the ANALYSIS OF AUTONOMIC CONTROL . we further improve the INVERSE GAUSSIAN MODEL by incorporating the RESPIRATORY COVARIATE MEASUREMENTS and present DYNAMIC RESPIRATORY SINUS ARRHYTHMIA ANALYSIS . our results suggest the INSTANTANEOUS RSA GAIN computed from our proposed INVERSE GAUSSIAN MODEL as a potential INDEX OF VAGAL CONTROL DYNAMICS . \n",
            "this paper presents a new method for ANALYSIS OF AUTONOMIC CONTROL . the ADAPTIVE POINT PROCESS FILTERING PARADIGM is based on the INDEX OF VAGAL CONTROL DYNAMICS and the INDEX OF VAGAL CONTROL DYNAMICS . the proposed ADAPTIVE POINT PROCESS FILTERING PARADIGM is based on the INDEX OF VAGAL CONTROL DYNAMICS and the INDEX OF VAGAL CONTROL DYNAMICS . the proposed ADAPTIVE POINT PROCESS FILTERING PARADIGM is based on the INDEX OF VAGAL CONTROL DYNAMICS and the INDEX OF VAGAL CONTROL DYNAMICS . the proposed ADAPTIVE POINT PROCESS FILTERING PARADIGM is based on the INDEX OF VAGAL CONTROL DYNAMICS and the INDEX OF VAGAL CONTROL DYNAMICS .\n",
            "\n",
            "423 1000\n",
            "we examine the use of HIDDEN MARKOV AND HIDDEN SEMI-MARKOV MODELS for automatically segmenting an ELECTROCARDIOGRAM WAVEFORM into its CONSTITUENT WAVEFORM FEATURES . an UNDECIMATED WAVELET TRANSFORM is used to generate an OVERCOMPLETE REPRESENTATION OF THE SIGNAL that is more appropriate for subsequent modelling . we show that the STATE DURATIONS implicit in a standard HIDDEN MARKOV MODEL are ill-suited to those of REAL ECG FEATURES , and we investigate the use of HIDDEN SEMI-MARKOV MODELS for improved STATE DURATION MODELLING . \n",
            "this paper presents a new method for STATE DURATION MODELLING based on HIDDEN MARKOV AND HIDDEN SEMI-MARKOV MODELS . the proposed method is based on the UNDECIMATED WAVELET TRANSFORM of the UNDECIMATED WAVELET TRANSFORM . the proposed method is based on the UNDECIMATED WAVELET TRANSFORM of the UNDECIMATED WAVELET TRANSFORM . the proposed method is based on the UNDECIMATED WAVELET TRANSFORM . the proposed method is based on the UNDECIMATED WAVELET TRANSFORM . the proposed method is based on a HIDDEN MARKOV MODEL .\n",
            "\n",
            "424 1000\n",
            "in the present study we explore the implication of HIGH AND LOW LEVEL MECHANISMS in DEGRADED SPEECH COMPREHENSION in normal hearing subjects . in experiment 1 we compared the loss of intelligibility due to the increasing size of REVERSION WINDOWS in both words and PSEUDOWORDS . results showed that words are generally reconstructed better than PSEUDOWORDS , suggesting the existence of a LEXICAL BENEFIT in DEGRADED SPEECH RESTORATION . moreover , there was greater variability between individuals when reconstructing PSEUDOWORDS than words . in experiment 2 , we demonstrated that this INTERINDIVIDUAL VARIABILITY correlated with the subjects ' MEDIAL OLIVOCOCHLEAR BUNDLE FUNCTIONALITY , as measured by CONTRALATERAL SUPPRESSION OF OTOACOUSTIC EMISSIONS . together these experiments highlight the importance of LOW-LEVEL AUDITORY MECHANISMS in DEGRADED SPEECH RESTORATION . moreover they put forward the existence of major INTERINDIVIDUAL VARIABILITY in the capacity to reconstruct DEGRADED SPEECH , which correlates with the PHYSIOLOGICAL PROPERTIES of the AUDITORY SYSTEM -lrb- low-level property -rrb- . in addition , our results also suggest the existence of multiple HIGHER-LEVEL STRATEGIES that can compensate on-line for the lack of information caused by SPEECH DEGRADATION . \n",
            "this paper presents a method for CONTRALATERAL SUPPRESSION OF OTOACOUSTIC EMISSIONS based on CONTRALATERAL SUPPRESSION OF OTOACOUSTIC EMISSIONS . the proposed method is based on the CONTRALATERAL SUPPRESSION OF OTOACOUSTIC EMISSIONS of the AUDITORY SYSTEM . the proposed method is based on the CONTRALATERAL SUPPRESSION OF OTOACOUSTIC EMISSIONS . the proposed method is based on the CONTRALATERAL SUPPRESSION OF OTOACOUSTIC EMISSIONS . the proposed method is based on the CONTRALATERAL SUPPRESSION OF OTOACOUSTIC EMISSIONS . the proposed method is based on the CONTRALATERAL SUPPRESSION OF OTOACOUSTIC EMISSIONS . the proposed method is based on the CONTRALATERAL SUPPRESSION OF OTOACOUSTIC EMISSIONS . the proposed method is based on the CONTRALATERAL SUPPRESSION OF OTOACOUSTIC EMISSIONS . the proposed method is based on the CONTRALATERAL SUPPRESSION OF OTOACOUSTIC EMISSIONS of the AUDITORY SYSTEM . experimental results show that the proposed method can improve the performance of DEGRADED SPEECH RESTORATION .\n",
            "\n",
            "425 1000\n",
            "in this paper , an accurate and robust positioning system based on STREET VIEW RECOGNITION is introduced . VISION-BASED TECHNIQUE is employed for DYNAMICALLY RECOGNIZING SHOP OR BUILDING SIGNS on the GPS MAP . two mechanisms including VIEW-ANGLE INVARIANT DISTANCE ESTIMATION and PATH REFINEMENT are proposed for ROBUST AND ACCURATE POSITION ESTIMATION . through the combination of VISUAL RECOGNITION TECHNIQUE and GPS SCALE DATA , the REAL USER LOCATION can be accurately inferred . experimental results demonstrate that the proposed system is reliable and feasible . compared with <unk> error of position estimation provided by the gps , our system only has 0.97 M ERROR ESTIMATION . \n",
            "this paper addresses the problem of DYNAMICALLY RECOGNIZING SHOP OR BUILDING SIGNS in STREET VIEW RECOGNITION . in this paper , we propose a novel VISION-BASED TECHNIQUE based on VIEW-ANGLE INVARIANT DISTANCE ESTIMATION and VIEW-ANGLE INVARIANT DISTANCE ESTIMATION . the proposed method is based on the VISION-BASED TECHNIQUE and the VISION-BASED TECHNIQUE . the proposed VISUAL RECOGNITION TECHNIQUE is based on the VISION-BASED TECHNIQUE and the VISION-BASED TECHNIQUE . experimental results demonstrate the effectiveness of the proposed VISUAL RECOGNITION TECHNIQUE .\n",
            "\n",
            "426 1000\n",
            "we present a new approach to QUASI TEXT-INDEPENDENT SPEAKER VERIFICATION based on PATTERN MATCHING . our method first seeks PHONETICALLY MATCHED SEGMENTS in two SPEECH SIGNALS . for all aligned frame pairs of these segments we compute the probability that they were uttered by the same speaker . based on these FRAME-LEVEL PROBABILITIES we take the decision whether the two signals were spoken by the same speaker or not . our method to find PHONETICALLY MATCHED SEGMENTS does not depend on a SPEECH RECOGNIZER . we show that our system performs better than a baseline speaker verification system based on GAUSSIAN MIXTURE MODELS when the signals are long enough . especially interesting is the fact that a combination of the devised system with the baseline system performs much better than either of the systems alone . \n",
            "this paper addresses the problem of QUASI TEXT-INDEPENDENT SPEAKER VERIFICATION in SPEECH SIGNALS . we propose a method for QUASI TEXT-INDEPENDENT SPEAKER VERIFICATION in SPEECH SIGNALS . the method is based on the idea of PATTERN MATCHING . the method is based on a SPEECH RECOGNIZER . the proposed method is tested on a SPEECH RECOGNIZER .\n",
            "\n",
            "427 1000\n",
            "understanding common sense reasoning about the PHYSICAL WORLD is one of the goals of QUALITATIVE REASONING RESEARCH . this paper describes how we combine QUALITATIVE MECHANICS and ANALOGY to solve EVERYDAY PHYSICAL REASONING PROBLEMS posed as sketches . the problems are drawn from the BENNETT MECHANICAL COMPREHENSION TEST , which is used to evaluate <unk> candidates . we discuss SKETCH ANNOTATIONS , which define CONCEPTUAL QUANTITIES in terms of visual measurements , how MODELING DECISIONS are made by ANALOGY , and how ANALOGY can be used to frame COMPARATIVE ANALYSIS PROBLEMS . experimental results support the plausibility of this approach . \n",
            "this paper presents a new method for UNDERSTANDING COMMON SENSE REASONING in UNDERSTANDING COMMON SENSE REASONING . the proposed method is based on the use of QUALITATIVE MECHANICS and QUALITATIVE MECHANICS . the proposed method is based on the use of SKETCH ANNOTATIONS and ANALOGY in the PHYSICAL WORLD . the proposed method is based on a BENNETT MECHANICAL COMPREHENSION TEST , which is based on the BENNETT MECHANICAL COMPREHENSION TEST . the proposed method is evaluated on the BENNETT MECHANICAL COMPREHENSION TEST . the results show that the proposed method is robust and robust to CONCEPTUAL QUANTITIES and is robust to CONCEPTUAL QUANTITIES .\n",
            "\n",
            "428 1000\n",
            "in this paper we present new research in TRANSLATION ASSISTANCE . we describe a system capable of translating NATIVE LANGUAGE FRAGMENTS to FOREIGN LANGUAGE FRAGMENTS in an L2 CONTEXT . practical applications of this research can be framed in the context of SECOND LANGUAGE LEARNING . the type of TRANSLATION ASSISTANCE SYSTEM under investigation here encourages LANGUAGE LEARNERS to write in their target language while allowing them to fall back to their native language in case the correct word or expression is not known . these CODE SWITCHES are subsequently translated to L2 given the L2 CONTEXT . we study the feasibility of exploiting CROSS-LINGUAL CONTEXT to obtain HIGH-QUALITY TRANSLATION SUGGESTIONS that improve over STATISTICAL LANGUAGE MODELLING and WORD-SENSE DIS-AMBIGUATION BASELINES . a CLASSIFICATION-BASED APPROACH is presented that is indeed found to improve significantly over these baselines by making use of a CONTEX-TUAL WINDOW spanning a small number of neighbouring words . \n",
            "this paper presents a new method for STATISTICAL LANGUAGE MODELLING based on SECOND LANGUAGE LEARNING . the proposed method is based on the CLASSIFICATION-BASED APPROACH and the CLASSIFICATION-BASED APPROACH . the proposed method is based on the use of CODE SWITCHES and CODE SWITCHES . the proposed method is based on the CLASSIFICATION-BASED APPROACH and the CLASSIFICATION-BASED APPROACH . the proposed method is based on the CLASSIFICATION-BASED APPROACH and the CLASSIFICATION-BASED APPROACH . the proposed method is based on the CLASSIFICATION-BASED APPROACH and the CLASSIFICATION-BASED APPROACH . the proposed method is compared with other WORD-SENSE DIS-AMBIGUATION BASELINES and WORD-SENSE DIS-AMBIGUATION BASELINES .\n",
            "\n",
            "429 1000\n",
            "this paper describes the realization of a CORPUS-BASED CHINESE SPEECH SYNTHESIS SYSTEM , including the CORPUS DESIGN and UNIT SELECTION PROCEDURE . the CORPUS-BASED CHINESE SPEECH SYNTHESIS SYSTEM selects the SYNTHESIS UNIT according to CONTEXT SIMILARITY between target unit and candidate unit . neither PROSODY PARAMETER PREDICTION nor PROSODY FEATURE MODIFICATION is needed . the informal test shows that the SYNTHESIZED SPEECH is quite natural , and the speaking style of original speaker is preserved because units are all from the SPEAKER 'S UTTERANCES . \n",
            "this paper presents a CORPUS-BASED CHINESE SPEECH SYNTHESIS SYSTEM for PROSODY PARAMETER PREDICTION . the CORPUS-BASED CHINESE SPEECH SYNTHESIS SYSTEM is based on a UNIT SELECTION PROCEDURE and a UNIT SELECTION PROCEDURE . the CORPUS-BASED CHINESE SPEECH SYNTHESIS SYSTEM is based on a CORPUS DESIGN and a UNIT SELECTION PROCEDURE . the CORPUS-BASED CHINESE SPEECH SYNTHESIS SYSTEM is based on the CORPUS DESIGN and the UNIT SELECTION PROCEDURE . the CORPUS-BASED CHINESE SPEECH SYNTHESIS SYSTEM is evaluated on a CORPUS-BASED CHINESE SPEECH SYNTHESIS SYSTEM and a CORPUS DESIGN .\n",
            "\n",
            "430 1000\n",
            "the success of an IMAGE CLASSIFICATION ALGORITHM largely depends on how it incorporates LOCAL INFORMATION in the GLOBAL DECISION . popular approaches such as AVERAGE-POOLING and MAX-POOLING are suboptimal in many situations . in this paper we propose region ranking svm -lrb- <unk> -rrb- , a novel method for POOLING LOCAL INFORMATION from multiple regions . <unk> exploits the correlation of LOCAL REGIONS in an IMAGE , and it jointly learns a REGION EVALUATION FUNCTION and a scheme for integrating multiple regions . experiments on pascal voc 2007 , voc 2012 , and ILSVRC2014 DATASETS show that <unk> outperforms the methods that use the same FEATURE TYPE and extract features from the same set of LOCAL REGIONS . <unk> achieves similar to or better than the state-of-the-art performance on all datasets . \n",
            "this paper presents a new method for GLOBAL DECISION in the IMAGE . the proposed method is based on the POOLING LOCAL INFORMATION in the IMAGE . the proposed method is based on the LOCAL INFORMATION and the POOLING LOCAL INFORMATION . the proposed method is based on the LOCAL INFORMATION and the POOLING LOCAL INFORMATION . the proposed method is based on the LOCAL INFORMATION and the POOLING LOCAL INFORMATION . the proposed method is evaluated on the ILSVRC2014 DATASETS .\n",
            "\n",
            "431 1000\n",
            "ordinal regression is an important research topic in MACHINE LEARNING . it aims to automatically determine the IMPLIED RATING of a DATA ITEM on a FIXED , DISCRETE RATING SCALE . in this paper , we present a novel ORDINAL REGRESSION APPROACH via MANIFOLD LEARNING , which is capable of uncovering the EMBEDDED NONLINEAR STRUCTURE of the data set according to the observations in the HIGH-DIMENSIONAL FEATURE SPACE . by optimizing the order information of the observations and preserving the INTRINSIC GEOMETRY of the data set simultaneously , the proposed ORDINAL REGRESSION APPROACH provides the faithful ORDINAL REGRESSION to the new coming data points . to offer more general solution to the data with NATURAL TENSOR STRUCTURE , we further introduce the multilinear extension of the proposed ORDINAL REGRESSION APPROACH , which can support the ORDINAL REGRESSION OF HIGH ORDER DATA like IMAGES . experiments on various DATA SETS validate the effectiveness of the proposed ORDINAL REGRESSION APPROACH as well as its extension . \n",
            "this paper presents a ORDINAL REGRESSION APPROACH for ORDINAL REGRESSION OF HIGH ORDER DATA . the ORDINAL REGRESSION APPROACH is based on the ORDINAL REGRESSION APPROACH of the DATA ITEM . the ORDINAL REGRESSION APPROACH is based on a ORDINAL REGRESSION APPROACH . the ORDINAL REGRESSION APPROACH is based on the ORDINAL REGRESSION APPROACH . the proposed ORDINAL REGRESSION APPROACH is based on the ORDINAL REGRESSION APPROACH . the proposed ORDINAL REGRESSION APPROACH is based on the ORDINAL REGRESSION APPROACH . the proposed ORDINAL REGRESSION APPROACH is based on the ORDINAL REGRESSION APPROACH . the proposed ORDINAL REGRESSION APPROACH is based on a ORDINAL REGRESSION APPROACH . the proposed ORDINAL REGRESSION APPROACH is based on a ORDINAL REGRESSION APPROACH . the proposed ORDINAL REGRESSION APPROACH is based on a ORDINAL REGRESSION APPROACH . the proposed ORDINAL REGRESSION APPROACH is evaluated on the DATA SETS .\n",
            "\n",
            "432 1000\n",
            "causal structure learning from TIME SERIES DATA is a major scientific challenge . EXTANT ALGORITHMS assume that measurements occur sufficiently quickly ; more precisely , they assume approximately equal system and measurement timescales . in many domains , however , measurements occur at a significantly slower rate than the underlying system changes , but the size of the TIMESCALE MISMATCH is often unknown . this paper develops three CAUSAL STRUCTURE LEARNING ALGORITHMS , each of which discovers all DYNAMIC CAUSAL GRAPHS that explain the OBSERVED MEASUREMENT DATA , perhaps given UNDERSAMPLING . that is , these CAUSAL STRUCTURE LEARNING ALGORITHMS all learn CAUSAL STRUCTURE in a `` <unk> '' manner : they do not assume any particular relation between the measurement and system timescales . we apply these CAUSAL STRUCTURE LEARNING ALGORITHMS to data from simulations to gain insight into the challenge of UNDERSAMPLING . \n",
            "this paper addresses the problem of CAUSAL STRUCTURE LEARNING for DYNAMIC CAUSAL GRAPHS . the CAUSAL STRUCTURE LEARNING ALGORITHMS is based on the OBSERVED MEASUREMENT DATA . the CAUSAL STRUCTURE LEARNING ALGORITHMS is based on the OBSERVED MEASUREMENT DATA . the proposed CAUSAL STRUCTURE LEARNING ALGORITHMS is based on the use of TIME SERIES DATA , and is based on the OBSERVED MEASUREMENT DATA . the proposed method is based on the use of TIME SERIES DATA , and the CAUSAL STRUCTURE LEARNING ALGORITHMS is applied to the OBSERVED MEASUREMENT DATA .\n",
            "\n",
            "433 1000\n",
            "drawing a sample from a DISCRETE DISTRIBUTION is one of the building components for MONTE CARLO METHODS . like other SAMPLING ALGORITHMS , DISCRETE DISTRIBUTION also suffers from high computational burden in LARGE-SCALE INFERENCE PROBLEMS . we study the problem of sampling a DISCRETE RANDOM VARIABLE with a high degree of dependency that is typical in LARGE-SCALE BAYESIAN INFERENCE and GRAPHICAL MODELS , and propose an efficient APPROXIMATE SOLUTION with a SUBSAMPLING APPROACH . we make a novel connection between the DISCRETE DISTRIBUTION and MULTI-ARMED BANDITS PROBLEMS with a FINITE REWARD POPULATION and provide three algorithms with THEORETICAL GUARANTEES . empirical evaluations show the ROBUSTNESS and efficiency of the APPROXIMATE ALGORITHMS in both SYNTHETIC AND REAL-WORLD LARGE-SCALE PROBLEMS . \n",
            "this paper addresses the problem of LARGE-SCALE BAYESIAN INFERENCE in GRAPHICAL MODELS . in this paper , we propose a new SUBSAMPLING APPROACH based on LARGE-SCALE BAYESIAN INFERENCE and LARGE-SCALE BAYESIAN INFERENCE . the proposed SUBSAMPLING APPROACH is based on a DISCRETE RANDOM VARIABLE and a FINITE REWARD POPULATION . the proposed SUBSAMPLING APPROACH is based on a DISCRETE RANDOM VARIABLE , which is a FINITE REWARD POPULATION . the proposed SUBSAMPLING APPROACH is evaluated on SYNTHETIC AND REAL-WORLD LARGE-SCALE PROBLEMS and SYNTHETIC AND REAL-WORLD LARGE-SCALE PROBLEMS . experimental results on SYNTHETIC AND REAL-WORLD LARGE-SCALE PROBLEMS show that the proposed SUBSAMPLING APPROACH outperforms the state-of-the-art APPROXIMATE ALGORITHMS in terms of ROBUSTNESS and ROBUSTNESS .\n",
            "\n",
            "434 1000\n",
            "in this paper , a system for OVERLAPPING ACOUSTIC EVENT DETECTION is proposed , which models the TEMPORAL EVOLUTION OF SOUND EVENTS . the system is based on PROBABILISTIC LATENT COMPONENT ANALYSIS , supporting the use of a SOUND EVENT DICTIONARY where each exemplar consists of a SUCCESSION OF SPECTRAL TEMPLATES . the TEMPORAL SUCCESSION OF THE TEMPLATES is controlled through EVENT CLASS-WISE HIDDEN MARKOV MODELS . as INPUT TIME/FREQUENCY REPRESENTATION , the EQUIVALENT RECTANGULAR BANDWIDTH SPECTROGRAM is used . experiments are carried out on POLYPHONIC DATASETS OF OFFICE SOUNDS generated using an ACOUSTIC SCENE SIMULATOR , as well as REAL AND SYNTHESIZED MONOPHONIC DATASETS for comparative purposes . results show that the proposed system outperforms several state-of-the-art methods for OVERLAPPING ACOUSTIC EVENT DETECTION on the same task , using both FRAME-BASED AND EVENT-BASED METRICS , and is robust to varying event density and noise levels . \n",
            "this paper presents a novel method for OVERLAPPING ACOUSTIC EVENT DETECTION from POLYPHONIC DATASETS OF OFFICE SOUNDS . the proposed method is based on a PROBABILISTIC LATENT COMPONENT ANALYSIS , which is based on PROBABILISTIC LATENT COMPONENT ANALYSIS . the proposed method is based on a PROBABILISTIC LATENT COMPONENT ANALYSIS . the proposed method is based on a PROBABILISTIC LATENT COMPONENT ANALYSIS . the proposed method is based on a PROBABILISTIC LATENT COMPONENT ANALYSIS . the proposed method is based on a PROBABILISTIC LATENT COMPONENT ANALYSIS . the proposed method is based on the SUCCESSION OF SPECTRAL TEMPLATES . the proposed method is based on a PROBABILISTIC LATENT COMPONENT ANALYSIS . the proposed method is based on the SUCCESSION OF SPECTRAL TEMPLATES . the proposed method is based on a PROBABILISTIC LATENT COMPONENT ANALYSIS . the proposed method is based on a PROBABILISTIC LATENT COMPONENT ANALYSIS . the proposed method is compared with conventional FRAME-BASED AND EVENT-BASED METRICS .\n",
            "\n",
            "435 1000\n",
            "here we show that reproducing the FUNCTIONAL PROPERTIES OF MT CELLS with various center -- surround interactions enriches MOTION REPRESENTATION and improves the ACTION RECOGNITION performance . to do so , we propose a simplified <unk> -- inspired model of the MOTION PATHWAY in primates : it is a FEEDFORWARD MODEL restricted to V1-MT CORTICAL LAYERS , CORTICAL CELLS cover the VISUAL SPACE with a FOVEATED STRUCTURE and , more importantly , we reproduce some of the richness of CENTER-SURROUND INTERACTIONS OF MT CELLS . interestingly , as observed in NEUROPHYSIOLOGY , our MT CELLS not only behave like simple VELOCITY DETECTORS , but also respond to several kinds of MOTION CONTRASTS . results show that this diversity of MOTION REPRESENTATION at the MT LEVEL is a major advantage for an ACTION RECOGNITION TASK . DEFINING MOTION MAPS as our FEATURE VECTORS , we used a standard CLASSIFICATION METHOD on the WEIZMANN DATABASE : we obtained an AVERAGE RECOGNITION RATE of <unk> % , which is superior to the recent results by <unk> et al. -lrb- 2007 -rrb- . these promising results encourage us to further develop BIO -- INSPIRED MODELS incorporating other BRAIN MECHANISMS and cortical layers in order to deal with more COMPLEX VIDEOS . \n",
            "this paper presents a CLASSIFICATION METHOD for ACTION RECOGNITION . the CLASSIFICATION METHOD is based on a FEEDFORWARD MODEL of the MOTION PATHWAY . the CLASSIFICATION METHOD is based on the FUNCTIONAL PROPERTIES OF MT CELLS of the VISUAL SPACE . the proposed CLASSIFICATION METHOD is based on the FUNCTIONAL PROPERTIES OF MT CELLS of the MOTION PATHWAY . the proposed CLASSIFICATION METHOD is based on a FEEDFORWARD MODEL . the proposed CLASSIFICATION METHOD is based on a FEEDFORWARD MODEL . the proposed CLASSIFICATION METHOD is evaluated on a ACTION RECOGNITION TASK . the results show that the proposed CLASSIFICATION METHOD improves the AVERAGE RECOGNITION RATE of the VISUAL SPACE and the AVERAGE RECOGNITION RATE of the proposed CLASSIFICATION METHOD .\n",
            "\n",
            "436 1000\n",
            "in GAME-PLAYING PROGRAMS relying on the MINIMAX PRINCIPLE , DEEPER SEARCHES generally produce better evaluations . theoretical analyses , however , suggest that in many cases <unk> amplifies the NOISE introduced by the HEURISTIC FUNCTION used to evaluate the leaves of the GAME TREE , leading to what is known as PATHOLOGICAL BEHAVIOR , where DEEPER SEARCHES produce worse evaluations . in most of the previous research , positions were evaluated as losses or wins . dependence between the values of positions close to each other was identified as the property of realistic GAME TREES that eliminates the PATHOLOGY and explains why MINIMAX is successful in practice . in this paper we present an alternative explanation that does not rely on VALUE DEPENDENCE . we show that if real numbers are used for POSITION VALUES , POSITION VALUES tend to be further apart at lower levels of the GAME TREE , which leads to a larger proportion of more extreme positions , where error is less probable . decreased probability of error in searches to greater depths is sufficient to eliminate the PATHOLOGY and no additional properties of GAME TREES are required . \n",
            "this paper presents a new method for GAME-PLAYING PROGRAMS based on the MINIMAX PRINCIPLE . the proposed method is based on the MINIMAX PRINCIPLE of the MINIMAX PRINCIPLE . the proposed method is based on the MINIMAX PRINCIPLE of the MINIMAX PRINCIPLE . the proposed method is based on the MINIMAX PRINCIPLE and the MINIMAX PRINCIPLE . the proposed method is based on the MINIMAX PRINCIPLE . the proposed method is based on the MINIMAX PRINCIPLE . the proposed method is based on the MINIMAX PRINCIPLE . the proposed method is based on the MINIMAX PRINCIPLE . the proposed method is based on the MINIMAX PRINCIPLE .\n",
            "\n",
            "437 1000\n",
            "we address the problem of <unk> i.e. deciding whether there exists a CONSISTENT ESTIMATOR of a given relation q , when data are missing not at random . we employ a FORMAL REPRESENTATION called ` MISSINGNESS GRAPHS ' to explicitly <unk> the CAUSAL MECHANISMS responsible for MISSINGNESS and to encode dependencies between these CAUSAL MECHANISMS and the variables being measured . using this FORMAL REPRESENTATION , we derive conditions that the GRAPH should satisfy to ensure <unk> and devise algorithms to detect the presence of these conditions in the GRAPH . \n",
            "this paper presents a new CONSISTENT ESTIMATOR for ` MISSINGNESS GRAPHS . the CONSISTENT ESTIMATOR is based on a CONSISTENT ESTIMATOR , which is a CONSISTENT ESTIMATOR . the CONSISTENT ESTIMATOR is based on a CONSISTENT ESTIMATOR , and is a CONSISTENT ESTIMATOR for ` MISSINGNESS GRAPHS . the CONSISTENT ESTIMATOR is based on a CONSISTENT ESTIMATOR .\n",
            "\n",
            "438 1000\n",
            "in this paper we propose a class of efficient GENERALIZED METHOD-OF-MOMENTS ALGORITHMS for computing parameters of the PLACKETT-LUCE MODEL , where the data consists of FULL RANKINGS over alternatives . our GENERALIZED METHOD-OF-MOMENTS ALGORITHMS is based on breaking the FULL RANKINGS into PAIRWISE COMPARISONS , and then computing parameters that satisfy a set of GENERALIZED MOMENT CONDITIONS . we identify conditions for the output of GENERALIZED METHOD-OF-MOMENTS ALGORITHMS to be unique , and identify a general class of CONSISTENT AND INCONSISTENT BREAKINGS . we then show by theory and experiments that our GENERALIZED METHOD-OF-MOMENTS ALGORITHMS run significantly faster than the CLASSICAL MINORIZE-MAXIMIZATION ALGORITHM , while achieving competitive STATISTICAL EFFICIENCY . \n",
            "this paper addresses the problem of STATISTICAL EFFICIENCY for PAIRWISE COMPARISONS . we propose a new CLASSICAL MINORIZE-MAXIMIZATION ALGORITHM based on the CLASSICAL MINORIZE-MAXIMIZATION ALGORITHM . the proposed GENERALIZED METHOD-OF-MOMENTS ALGORITHMS is based on the CLASSICAL MINORIZE-MAXIMIZATION ALGORITHM . the proposed CLASSICAL MINORIZE-MAXIMIZATION ALGORITHM is compared with the conventional CLASSICAL MINORIZE-MAXIMIZATION ALGORITHM and the CLASSICAL MINORIZE-MAXIMIZATION ALGORITHM . the proposed CLASSICAL MINORIZE-MAXIMIZATION ALGORITHM is compared with the conventional CLASSICAL MINORIZE-MAXIMIZATION ALGORITHM and the CLASSICAL MINORIZE-MAXIMIZATION ALGORITHM .\n",
            "\n",
            "439 1000\n",
            "<unk> measures for the HETEROGENEITY OF CLUSTERS have been used for a long time . this paper studies the ENTROPY-BASED CRITERION in CLUSTERING CATEGORICAL DATA . it first shows that the ENTROPY-BASED CRITERION can be derived in the formal framework of PROBABILISTIC CLUSTERING MODELS and establishes the connection between the criterion and the approach based on DISSIMILARITY CO-EFFICIENTS . an iterative monte-carlo procedure is then presented to search for the PARTITIONS minimizing the criterion . experiments are conducted to show the effectiveness of the proposed procedure . \n",
            "this paper addresses the problem of CLUSTERING CATEGORICAL DATA in CLUSTERING CATEGORICAL DATA . we propose a method for CLUSTERING CATEGORICAL DATA based on the ENTROPY-BASED CRITERION . the proposed method is based on the use of ENTROPY-TYPE MEASURES as a ENTROPY-BASED CRITERION . the proposed method is based on the use of ENTROPY-TYPE MEASURES as a ENTROPY-BASED CRITERION .\n",
            "\n",
            "440 1000\n",
            "the majority of the existing algorithms for LEARNING DECISION TREES are greedy -- a TREE is induced top-down , making locally optimal decisions at each node . in most cases , however , the CONSTRUCTED TREE is not globally optimal . furthermore , the GREEDY ALGORITHMS require a fixed amount of time and are not able to generate a better TREE if additional time is available . to overcome this problem , we present two LOOKAHEAD-BASED ALGORITHMS for ANYTIME INDUCTION OF DECISION TREES , thus allowing tradeoff between TREE QUALITY and LEARNING TIME . the first one is DEPTH-K LOOKAHEAD , where a larger TIME ALLOCATION permits larger k . the second algorithm uses a novel strategy for evaluating candidate splits ; a STOCHASTIC VERSION of ID3 is repeatedly invoked to estimate the size of the TREE in which each split results , and the one that minimizes the expected size is preferred . experimental results indicate that for several hard concepts , our proposed approach exhibits good anytime behavior and yields significantly better DECISION TREES when more time is available . \n",
            "this paper addresses the problem of ANYTIME INDUCTION OF DECISION TREES in a TREE . we propose a method for ANYTIME INDUCTION OF DECISION TREES based on the CONSTRUCTED TREE . the proposed method is based on the use of DECISION TREES and DEPTH-K LOOKAHEAD . the proposed method is based on the use of DECISION TREES and DEPTH-K LOOKAHEAD . the proposed method is based on the use of a STOCHASTIC VERSION and a STOCHASTIC VERSION . the proposed method is compared with conventional LOOKAHEAD-BASED ALGORITHMS and GREEDY ALGORITHMS .\n",
            "\n",
            "441 1000\n",
            "the <unk> <unk> -lrb- 0 < p < 1 -rrb- is usually used to replace the standard NUCLEAR NORM in order to approximate the RANK FUNCTION more accurately . however , existing SCHATTEN-P QUASI-NORM MINIMIZATION ALGORITHMS involve SINGULAR VALUE DECOMPOSITION or EIGENVALUE DECOMPOSITION in each iteration , and thus may become very slow and impractical for LARGE-SCALE PROBLEMS . in this paper , we first define two TRACTABLE SCHATTEN QUASI-NORMS , i.e. , the FROBENIUS/NUCLEAR HYBRID AND BI-NUCLEAR QUASI-NORMS , and then prove that TRACTABLE SCHATTEN QUASI-NORMS are in essence the <unk> / 3 and 1/2 <unk> , respectively , which lead to the design of very efficient algorithms that only need to update two much smaller FACTOR MATRICES . we also design two efficient proximal alternating linearized minimization algorithms for solving REPRESENTATIVE MATRIX COMPLETION PROBLEMS . finally , we provide the GLOBAL CONVERGENCE and performance guarantees for our algorithms , which have better convergence properties than existing algorithms . experimental results on SYNTHETIC AND REAL-WORLD DATA show that our algorithms are more accurate than the state-of-the-art methods , and are orders of magnitude faster . \n",
            "this paper addresses the problem of SINGULAR VALUE DECOMPOSITION in LARGE-SCALE PROBLEMS . in this paper , we propose a new method for SINGULAR VALUE DECOMPOSITION based on SINGULAR VALUE DECOMPOSITION . the proposed method is based on the SINGULAR VALUE DECOMPOSITION and the NUCLEAR NORM . the proposed method is based on the EIGENVALUE DECOMPOSITION and the NUCLEAR NORM . the proposed method is based on the EIGENVALUE DECOMPOSITION and the NUCLEAR NORM . the proposed method is compared with conventional SCHATTEN-P QUASI-NORM MINIMIZATION ALGORITHMS and SCHATTEN-P QUASI-NORM MINIMIZATION ALGORITHMS .\n",
            "\n",
            "442 1000\n",
            "data-driven grapheme-to-phoneme conversion involves either -LRB- TOP-DOWN -RRB- INDUCTIVE LEARNING or -lrb- bottom-up -rrb- pronunciation by analogy . as both approaches rely on LOCAL CONTEXT INFORMATION , they typically require some EXTERNAL LINGUISTIC KNOWLEDGE , e.g. , individual <unk> correspondences . to avoid such SUPERVISION , this paper proposes an alternative solution , dubbed pronunciation by LATENT ANALOGY , which adopts a more GLOBAL DEFINITION OF ANALOGOUS EVENTS . for each OUT-OF-VOCABULARY WORD , a neighborhood of GLOBALLY RELEVANT PRONUNCIATIONS is constructed through an appropriate DATA-DRIVEN MAPPING of its GRAPHEMIC FORM . PHONEME TRANSCRIPTION then proceeds via LOCALLY OPTIMAL SEQUENCE ALIGNMENT and MAXIMUM LIKELIHOOD POSITION SCORING . this method was successfully applied to the SYNTHESIS OF PROPER NAMES with a large diversity of origin . \n",
            "this paper addresses the problem of LOCALLY OPTIMAL SEQUENCE ALIGNMENT for DATA-DRIVEN GRAPHEME-TO-PHONEME CONVERSION . we propose a method for LOCALLY OPTIMAL SEQUENCE ALIGNMENT based on LOCALLY OPTIMAL SEQUENCE ALIGNMENT . the proposed method is based on a DATA-DRIVEN MAPPING and a DATA-DRIVEN MAPPING . the proposed method is based on the LOCAL CONTEXT INFORMATION and the LOCAL CONTEXT INFORMATION . the proposed method is based on the GLOBAL DEFINITION OF ANALOGOUS EVENTS and the LOCAL CONTEXT INFORMATION . the proposed method is based on the GLOBAL DEFINITION OF ANALOGOUS EVENTS and the GLOBAL DEFINITION OF ANALOGOUS EVENTS . the proposed method is compared with the conventional DATA-DRIVEN GRAPHEME-TO-PHONEME CONVERSION .\n",
            "\n",
            "443 1000\n",
            "we propose a feature space maximum a POSTERIORI LINEAR REGRESSION FRAMEWORK to adapt PARAMETERS for context dependent deep neural network hidden markov models -lrb- <unk> -rrb- . due to the huge amount of PARAMETERS used in DNN ACOUSTIC MODELS in LARGE VOCABULARY CONTINUOUS SPEECH RECOGNITION , the problem of OVER-FITTING can be severe in DNN ADAPTATION , thus often impair the ROBUSTNESS of the adapted DNN ADAPTATION . LINEAR INPUT NETWORK as a STRAIGHTFORWARD FEATURE SPACE ADAPTATION METHOD for DNN ADAPTATION , similar to feature space maximum likelihood linear regression -lrb- fmllr -rrb- , can potentially suffer from the same ROBUSTNESS SITUATION . the proposed STRAIGHTFORWARD FEATURE SPACE ADAPTATION METHOD is built based on map estimation of the LIN PARAMETERS by incorporating PRIOR KNOWLEDGE into the ADAPTATION PROCESS . experimental results on the SWITCHBOARD TASK show that against the SPEAKER INDEPENDENT CD-DNN-HMM SYSTEMS , LINEAR INPUT NETWORK provides <unk> % RELATIVE WORD ERROR RATE REDUCTION and the proposed FMAPLIN METHOD is able to provide further <unk> % -lrb- totally <unk> % -rrb- RELATIVE WORD ERROR RATE REDUCTION on top of LINEAR INPUT NETWORK . \n",
            "this paper presents a new STRAIGHTFORWARD FEATURE SPACE ADAPTATION METHOD for LARGE VOCABULARY CONTINUOUS SPEECH RECOGNITION . the proposed STRAIGHTFORWARD FEATURE SPACE ADAPTATION METHOD is based on a POSTERIORI LINEAR REGRESSION FRAMEWORK . the proposed STRAIGHTFORWARD FEATURE SPACE ADAPTATION METHOD is based on a POSTERIORI LINEAR REGRESSION FRAMEWORK . the proposed STRAIGHTFORWARD FEATURE SPACE ADAPTATION METHOD is based on the FMAPLIN METHOD . the proposed STRAIGHTFORWARD FEATURE SPACE ADAPTATION METHOD is evaluated on the SWITCHBOARD TASK . the proposed FEATURE SPACE ADAPTATION METHOD is compared with the conventional FMAPLIN METHOD and the STRAIGHTFORWARD FEATURE SPACE ADAPTATION METHOD . the proposed STRAIGHTFORWARD FEATURE SPACE ADAPTATION METHOD is compared with the conventional FMAPLIN METHOD and the STRAIGHTFORWARD FEATURE SPACE ADAPTATION METHOD .\n",
            "\n",
            "444 1000\n",
            "this paper presents several novel CODEC STRUCTURES for NOISE FEEDBACK CODING incorporating both LONG-TERM AND SHORT-TERM NOISE SPECTRAL SHAPING , as well as LONG-TERM AND SHORT-TERM PREDICTION . in addition , the paper generalizes the conventional SCALAR-QUANTIZATION-BASED NFC to VECTOR-QUANTIZATION-BASED NFC , and it lays the foundation for the associated efficient VQ CODEBOOK SEARCH and CLOSED-LOOP VQ CODEBOOK DESIGN . BROADVOICE ® 16 , a PACKETCABLE 1.5 MANDATORY NARROWBAND SPEECH CODEC standardized by <unk> ® for voice over <unk> in north america , is based on one of such novel NFC CODEC STRUCTURES . \n",
            "this paper addresses the problem of LONG-TERM AND SHORT-TERM PREDICTION in BROADVOICE ® 16 . we propose a novel PACKETCABLE 1.5 MANDATORY NARROWBAND SPEECH CODEC , called VQ CODEBOOK SEARCH , which is based on a PACKETCABLE 1.5 MANDATORY NARROWBAND SPEECH CODEC . the proposed PACKETCABLE 1.5 MANDATORY NARROWBAND SPEECH CODEC is based on a PACKETCABLE 1.5 MANDATORY NARROWBAND SPEECH CODEC and a VQ CODEBOOK SEARCH . the proposed PACKETCABLE 1.5 MANDATORY NARROWBAND SPEECH CODEC is based on a PACKETCABLE 1.5 MANDATORY NARROWBAND SPEECH CODEC , which is robust to NOISE FEEDBACK CODING and NOISE FEEDBACK CODING . experimental results demonstrate the effectiveness of the proposed method .\n",
            "\n",
            "445 1000\n",
            "previous work in SOCIAL NETWORK ANALYSIS has modeled the existence of links from one entity to another , but not the LANGUAGE CONTENT or topics on those links . we present the AUTHOR-RECIPIENT-TOPIC MODEL for SOCIAL NETWORK ANALYSIS , which learns TOPIC DISTRIBUTIONS based on the DIRECTION-SENSITIVE MESSAGES sent between entities . the AUTHOR-RECIPIENT-TOPIC MODEL builds on LATENT DIRICHLET ALLOCATION and the AUTHOR-TOPIC MODEL , adding the key attribute that distribution over topics is conditioned distinctly on both the <unk> and <unk> -- steering the discovery of topics according to the relationships between people . we give results on both the ENRON EMAIL CORPUS and a RE-SEARCHER 'S EMAIL ARCHIVE , providing evidence not only that clearly relevant topics are discovered , but that the AUTHOR-RECIPIENT-TOPIC MODEL better predicts people 's roles . \n",
            "this paper presents a AUTHOR-RECIPIENT-TOPIC MODEL for SOCIAL NETWORK ANALYSIS . the AUTHOR-TOPIC MODEL is based on the AUTHOR-RECIPIENT-TOPIC MODEL and the AUTHOR-RECIPIENT-TOPIC MODEL . the AUTHOR-RECIPIENT-TOPIC MODEL is based on the AUTHOR-RECIPIENT-TOPIC MODEL and the AUTHOR-RECIPIENT-TOPIC MODEL . the AUTHOR-TOPIC MODEL is based on the AUTHOR-RECIPIENT-TOPIC MODEL and the AUTHOR-RECIPIENT-TOPIC MODEL . the AUTHOR-RECIPIENT-TOPIC MODEL is based on the AUTHOR-RECIPIENT-TOPIC MODEL and the AUTHOR-RECIPIENT-TOPIC MODEL . the AUTHOR-RECIPIENT-TOPIC MODEL is applied to the ENRON EMAIL CORPUS .\n",
            "\n",
            "446 1000\n",
            "we present an approach to PARALLEL VARIATIONAL OPTICAL FLOW COMPUTATION on standard hardware by DOMAIN DECOMPOSITION . using an arbitrary partition of the IMAGE PLANE into RECTANGULAR SUBDOMAINS , the GLOBAL SOLUTION to the VARIATIONAL APPROACH is obtained by iteratively combining LOCAL SOLUTIONS which can be efficiently computed in parallel by separate MULTI-GRID ITERATIONS for each <unk> . the approach is particularly suited for implementations on PC-CLUSTERS because INTER-PROCESS COMMUNICATION between subdomains -lrb- i.e. processors -rrb- is minimized by restricting the exchange of data to a LOWER-DIMENSIONAL INTERFACE . by applying a DEDICATED INTERFACE PRECONDITIONER , the necessary number of ITERATIONS between subdomains to achieve a fixed error is bounded independently of the number of subdomains . our approach provides a major step towards REAL-TIME 2D IMAGE PROCESSING using off-the-shelf PC-HARDWARE and facilitates the efficient application of VARIATIONAL APPROACHES to LARGE-SCALE IMAGE PROCESSING PROBLEMS . \n",
            "this paper presents a new method for REAL-TIME 2D IMAGE PROCESSING based on PARALLEL VARIATIONAL OPTICAL FLOW COMPUTATION . the proposed method is based on the VARIATIONAL APPROACH , which is based on the VARIATIONAL APPROACH . the proposed method is based on the VARIATIONAL APPROACH . the proposed method is based on the VARIATIONAL APPROACH of the IMAGE PLANE . the proposed method is based on the VARIATIONAL APPROACH . the proposed method is based on the VARIATIONAL APPROACH . the proposed method is based on the VARIATIONAL APPROACH . the proposed method is based on the VARIATIONAL APPROACH . the proposed method is based on the VARIATIONAL APPROACH . the proposed method is based on the VARIATIONAL APPROACH . the proposed method is based on the VARIATIONAL APPROACH . the proposed method is based on the VARIATIONAL APPROACH . the proposed method is based on the VARIATIONAL APPROACH . the proposed method is based on the VARIATIONAL APPROACH . the proposed method is based on the VARIATIONAL APPROACH . the proposed method is based on the VARIATIONAL APPROACH .\n",
            "\n",
            "447 1000\n",
            "in this paper , we describe systems that were developed for the open performance sub-challenge of the interspeech 2009 emotion challenge . we participate in both TWO-CLASS AND FIVE-CLASS EMOTION DETECTION . for the TWO-CLASS AND FIVE-CLASS EMOTION DETECTION , the best performance is obtained by LOGISTIC REGRESSION FUSION of three systems . these systems use SHORT-AND LONG-TERM SPEECH FEATURES . FUSION allowed to an absolute improvement of 2.6 % on the UNWEIGHTED RECALL VALUE compared with -lsb- 1 -rsb- . for the TWO-CLASS AND FIVE-CLASS EMOTION DETECTION , we submitted two individual systems : CEPSTRAL GMM vs. long-term gmm-ubm . the best result comes from a CEPSTRAL GMM and produces an absolute improvement of 3.5 % compared to -lsb- 6 -rsb- . \n",
            "this paper proposes a new method for LOGISTIC REGRESSION FUSION based on LOGISTIC REGRESSION FUSION . the proposed method is based on the CEPSTRAL GMM . the proposed method is based on the CEPSTRAL GMM . the experimental results show that the proposed method outperforms the conventional CEPSTRAL GMM in terms of UNWEIGHTED RECALL VALUE and UNWEIGHTED RECALL VALUE .\n",
            "\n",
            "448 1000\n",
            "this paper applies a DYNAMIC SINUSOIDAL SYNTHESIS MODEL to STATISTICAL PARAMETRIC SPEECH SYNTHESIS . for this , we utilise REGULARISED CEPSTRAL COEFFICIENTS to represent both the STATIC AMPLITUDE and dynamic slope of selected sinusoids for STATISTICAL MODELLING . during synthesis , a DYNAMIC SINUSOIDAL SYNTHESIS MODEL is used to reconstruct SPEECH . a PREFERENCE TEST is conducted to compare the selection of different sinusoids for CEPSTRAL REPRESENTATION . our results show that when integrated with STATISTICAL PARAMETRIC SPEECH SYNTHESIS , a relatively small number of sinusoids selected according to a PERCEPTUAL CRITERION can produce quality comparable to using all harmonics . a MEAN OPINION SCORE TEST shows that our proposed DYNAMIC SINUSOIDAL SYNTHESIS MODEL is preferred to one using MEL-CEPSTRA from PITCH SYNCHRONOUS SPECTRAL ANALYSIS . \n",
            "this paper proposes a DYNAMIC SINUSOIDAL SYNTHESIS MODEL for STATISTICAL PARAMETRIC SPEECH SYNTHESIS . the DYNAMIC SINUSOIDAL SYNTHESIS MODEL is based on the PERCEPTUAL CRITERION and the PERCEPTUAL CRITERION . the proposed DYNAMIC SINUSOIDAL SYNTHESIS MODEL is based on the PERCEPTUAL CRITERION and the PERCEPTUAL CRITERION . the proposed DYNAMIC SINUSOIDAL SYNTHESIS MODEL is based on the PERCEPTUAL CRITERION and the PERCEPTUAL CRITERION . the proposed DYNAMIC SINUSOIDAL SYNTHESIS MODEL is evaluated on the PREFERENCE TEST of the PREFERENCE TEST .\n",
            "\n",
            "449 1000\n",
            "we present a new approach to model and classify BREAST PARENCHYMAL TISSUE . given a MAMMOGRAM , first , we will discover the distribution of the different TISSUE DENSITIES in an UNSUPERVISED MANNER , and second , we will use this TISSUE DISTRIBUTION to perform the CLASSIFICATION . we achieve this using a CLASSIFIER based on LOCAL DESCRIPTORS and PROBABILIS-TIC LATENT SEMANTIC ANALYSIS , a GENERATIVE MODEL from the STATISTICAL TEXT LITERATURE . we studied the influence of different DESCRIPTORS like TEXTURE and SIFT FEATURES at the CLASSIFICATION STAGE showing that textons outperform PROBABILIS-TIC LATENT SEMANTIC ANALYSIS in all cases . moreover we demonstrate that PROBABILIS-TIC LATENT SEMANTIC ANALYSIS automatically extracts meaningful latent aspects generating a COMPACT TISSUE REPRESENTATION based on their densities , useful for discriminating on MAM-MOGRAM CLASSIFICATION . we show the results of CLASSIFICATION STAGE over the MIAS AND DDSM DATASETS . we compare our method with approaches that classified these same PROBABILIS-TIC LATENT SEMANTIC ANALYSIS showing a better performance of our proposal . \n",
            "this paper presents a new GENERATIVE MODEL for MAM-MOGRAM CLASSIFICATION . the GENERATIVE MODEL is based on a GENERATIVE MODEL and a GENERATIVE MODEL . the proposed GENERATIVE MODEL is based on a GENERATIVE MODEL and a GENERATIVE MODEL . the proposed GENERATIVE MODEL is based on the GENERATIVE MODEL and the GENERATIVE MODEL . the proposed GENERATIVE MODEL is based on a GENERATIVE MODEL and a COMPACT TISSUE REPRESENTATION . the proposed GENERATIVE MODEL is compared with the conventional GENERATIVE MODEL and the GENERATIVE MODEL . the proposed GENERATIVE MODEL is compared with the conventional GENERATIVE MODEL and the GENERATIVE MODEL .\n",
            "\n",
            "450 1000\n",
            "with the growing popularity of SHORT-FORM VIDEO SHARING PLATFORMS such as INSTAGRAM and <unk> , there has been an increasing need for techniques that automatically extract highlights from video . whereas prior works have approached this problem with HEURISTIC RULES or SUPERVISED LEARNING , we present an UNSUPERVISED LEARNING APPROACH that takes advantage of the abundance of USER-EDITED VIDEOS on SOCIAL MEDIA WEBSITES such as YOUTUBE . based on the idea that the most significant sub-events within a VIDEO CLASS are commonly present among EDITED VIDEOS while less interesting ones appear less frequently , we identify the significant sub-events via a ROBUST RECURRENT AUTO-ENCODER trained on a collection of USER-EDITED VIDEOS queried for each particular class of interest . the ROBUST RECURRENT AUTO-ENCODER is trained using a proposed SHRINKING EXPONENTIAL LOSS FUNCTION that makes ROBUST RECURRENT AUTO-ENCODER robust to NOISE in the WEB-CRAWLED TRAINING DATA , and is configured with bidirectional long short term memory -lrb- lstm -rrb- -lsb- 5 -rsb- cells to better model the TEMPORAL STRUCTURE OF HIGHLIGHT SEGMENTS . different from SUPERVISED TECHNIQUES , our UNSUPERVISED LEARNING APPROACH can infer highlights using only a set of DOWN-LOADED EDITED VIDEOS , without also needing their <unk> counterparts which are rarely available online . extensive experiments indicate the promise of our proposed UNSUPERVISED LEARNING APPROACH in this challenging UNSUPERVISED SETTING . \n",
            "this paper presents a novel UNSUPERVISED LEARNING APPROACH for SOCIAL MEDIA WEBSITES . the UNSUPERVISED LEARNING APPROACH is based on the ROBUST RECURRENT AUTO-ENCODER , which is a ROBUST RECURRENT AUTO-ENCODER . the UNSUPERVISED LEARNING APPROACH is based on a ROBUST RECURRENT AUTO-ENCODER and a ROBUST RECURRENT AUTO-ENCODER . the UNSUPERVISED LEARNING APPROACH is based on a ROBUST RECURRENT AUTO-ENCODER and a ROBUST RECURRENT AUTO-ENCODER . the proposed UNSUPERVISED LEARNING APPROACH is based on a ROBUST RECURRENT AUTO-ENCODER and a ROBUST RECURRENT AUTO-ENCODER . the proposed UNSUPERVISED LEARNING APPROACH is based on a ROBUST RECURRENT AUTO-ENCODER , which is based on the ROBUST RECURRENT AUTO-ENCODER . the proposed UNSUPERVISED LEARNING APPROACH is based on a ROBUST RECURRENT AUTO-ENCODER and a ROBUST RECURRENT AUTO-ENCODER .\n",
            "\n",
            "451 1000\n",
            "given a set of LOW RESOLUTION CAMERA IMAGES , it is possible to reconstruct HIGH RESOLUTION LUMINANCE AND DEPTH INFORMATION , specially i f the RELATIVE DISPLACEMENTS of the IMAGE FRAMES are known . we have proposed ITERATIVE ALGORITHMS for RECOVERING HIGH RESOLUTION ALBEDO AND DEPTH MAPS that require no a PRIORI KNOWLEDGE OF THE SCENE , and therefore do not depend on other methods , as regards boundary and initial conditions . the problem of SURFACE RECONSTRUCTION has been formulated as one of EXPECTATION MAXIMIZATION and has been tackled in a PROBABILISTIC FRAMEWORK <unk> MARKOV RANDOM FIELDS -lsb- 1 -rsb- -lsb- 3 -rsb- . as for the DEPTH MAP , our ITERATIVE ALGORITHMS is directly recovering surface heights without <unk> to SURFACE ORIENTATIONS , <unk> increasing the resolution by camera <unk> -lsb- 2 -rsb- . conventional STATISTICAL MODELS have been coupled with GEOMETRICAL TECHNIQUES to construct a general model of <unk> world and the IMAGING PROCESS . \n",
            "this paper addresses the problem of RECOVERING HIGH RESOLUTION ALBEDO AND DEPTH MAPS in IMAGE FRAMES . we propose a method for RECOVERING HIGH RESOLUTION ALBEDO AND DEPTH MAPS based on MARKOV RANDOM FIELDS . the proposed method is based on a PRIORI KNOWLEDGE OF THE SCENE , which is based on the EXPECTATION MAXIMIZATION . the proposed method is based on the EXPECTATION MAXIMIZATION . the proposed method is based on the EXPECTATION MAXIMIZATION . the proposed method is based on the EXPECTATION MAXIMIZATION . the proposed method is based on the use of EXPECTATION MAXIMIZATION for RECOVERING HIGH RESOLUTION ALBEDO AND DEPTH MAPS . the proposed method is based on the EXPECTATION MAXIMIZATION . the proposed method is based on the EXPECTATION MAXIMIZATION . the proposed method is based on a PRIORI KNOWLEDGE OF THE SCENE .\n",
            "\n",
            "452 1000\n",
            "we study the EVENT DETECTION PROBLEM using CONVOLUTIONAL NEURAL NETWORKS that overcome the two fundamental limitations of the traditional FEATURE-BASED APPROACHES to this EVENT DETECTION PROBLEM : complicated FEATURE ENGINEERING for RICH FEATURE SETS and ERROR PROPAGATION from the preceding stages which generate these FEATURES . the experimental results show that the CONVOLUTIONAL NEURAL NETWORKS outper-form the best reported FEATURE-BASED SYSTEMS in the general setting as well as the DOMAIN ADAPTATION SETTING without resorting to extensive EXTERNAL RESOURCES . \n",
            "this paper addresses the problem of EVENT DETECTION PROBLEM in FEATURE-BASED SYSTEMS . in this paper , we propose a new EVENT DETECTION PROBLEM based on CONVOLUTIONAL NEURAL NETWORKS . the proposed CONVOLUTIONAL NEURAL NETWORKS is based on the use of CONVOLUTIONAL NEURAL NETWORKS and CONVOLUTIONAL NEURAL NETWORKS . the proposed CONVOLUTIONAL NEURAL NETWORKS is compared with state-of-the-art FEATURE-BASED APPROACHES and FEATURE-BASED APPROACHES . the experimental results show that the proposed method outperforms the conventional FEATURE-BASED APPROACHES and the FEATURE-BASED APPROACHES .\n",
            "\n",
            "453 1000\n",
            "prediction , estimation , and SMOOTHING are fundamental to SIGNAL PROCESSING . to perform these interrelated tasks given NOISY DATA , we form a TIME SERIES MODEL of the process that generates the data . taking noise in the system explicitly into account , maximum-likelihood and KALMAN FRAMEWORKS are discussed which involve the dual process of estimating both the MODEL PARAMETERS and the underlying state of the system . we review several established methods in the LINEAR CASE , and propose SEVERA ! extensions utilizing dual kalman filters -lrb- <unk> -rrb- and FORWARD-BACKWARD FILTERS that are applicable to NEURAL NETWORKS . methods are compared on several SIMULATIONS OF NOISY TIME SERIES . we also include an example of NONLINEAR NOISE REDUCTION in SPEECH . \n",
            "this paper addresses the problem of NONLINEAR NOISE REDUCTION in SPEECH . we propose a method for PREDICTION based on FORWARD-BACKWARD FILTERS . the proposed method is based on the use of FORWARD-BACKWARD FILTERS , which is based on the TIME SERIES MODEL . the proposed method is based on the use of FORWARD-BACKWARD FILTERS to estimate the MODEL PARAMETERS in the LINEAR CASE . the proposed method is based on the use of FORWARD-BACKWARD FILTERS , which is based on the TIME SERIES MODEL . the proposed method is evaluated on the LINEAR CASE . the results show that the proposed method is robust to SPEECH and can be used to improve the performance of NEURAL NETWORKS .\n",
            "\n",
            "454 1000\n",
            "the fixed point implementation of IIR DIGITAL FILTERS usually leads to the appearance of ZERO-INPUT LIMIT CYCLES , which degrade the performance of the system . in this paper , we develop an efficient MONTE CARLO ALGORITHM to detect and characterize LIMIT CYCLES in FIXED-POINT IIR DIGITAL FILTERS . the proposed MONTE CARLO ALGORITHM considers FILTERS formulated in the STATE SPACE and is valid for any FIXED POINT REPRESENTATION and QUANTIZA-TION FUNCTION . NUMERICAL SIMULATIONS on several HIGH-ORDER FILTERS , where an exhaustive search is unfeasible , show the effectiveness of the proposed MONTE CARLO ALGORITHM . \n",
            "this paper proposes a new MONTE CARLO ALGORITHM for IIR DIGITAL FILTERS . the proposed MONTE CARLO ALGORITHM is based on the MONTE CARLO ALGORITHM and the MONTE CARLO ALGORITHM . the proposed MONTE CARLO ALGORITHM is based on the MONTE CARLO ALGORITHM and the MONTE CARLO ALGORITHM . the proposed MONTE CARLO ALGORITHM is based on the MONTE CARLO ALGORITHM and the MONTE CARLO ALGORITHM . the proposed MONTE CARLO ALGORITHM is based on the MONTE CARLO ALGORITHM and the MONTE CARLO ALGORITHM . the proposed MONTE CARLO ALGORITHM is based on the MONTE CARLO ALGORITHM and the MONTE CARLO ALGORITHM .\n",
            "\n",
            "455 1000\n",
            "this paper relates experiences of ALGORITHMIC TRANSFORMATIONS in HIGH LEVEL SYNTHESIS , in the area of ACOUSTIC ECHO CANCELLATION . the PROCESSING AND MEMORY UNITS are automatically designed for various equivalent LMS ALGORITHMS , in the FIR CASE , with important COMPUTATIONAL LOAD . the results obtained with DIERENT LTER LENGTHS , give an accurate prototyping of new fast versions of the LMS ALGORITHMS . it also show that a THEORETICAL ARITHMETIC REDUCTION must be correlated to the associated increase of MEMORY REQUIREMENTS . \n",
            "this paper addresses the problem of HIGH LEVEL SYNTHESIS for ACOUSTIC ECHO CANCELLATION . we propose a method for ACOUSTIC ECHO CANCELLATION based on the PROCESSING AND MEMORY UNITS . the proposed method is based on the use of ALGORITHMIC TRANSFORMATIONS in the FIR CASE . the proposed method is based on the use of ALGORITHMIC TRANSFORMATIONS . the proposed method is based on the use of ALGORITHMIC TRANSFORMATIONS . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "456 1000\n",
            "in this paper , we propose a novel general framework for TEN-SOR BASED NULL SPACE AFFINE INVARIANTS , namely , TENSOR NULL SPACE INVARIANTS with a LINEAR CLASSIFIER for HIGH ORDER DATA CLASSIFICATION and RETRIEVAL . we first derive TENSOR NULL SPACE INVARIANTS , which is perfectly invariant to MULTIDIMENSIONAL AFFINE TRANSFORMATIONS due to CAMERA MOTIONS for multiple motion trajectories in CONSECUTIVE MOTION EVENTS . we subsequently propose an efficient CLASSIFICATION AND RETRIEVAL SYSTEM relying on TENSOR NULL SPACE INVARIANTS for ARCHIV-ING AND SEARCHING MOTION EVENTS consisting of multiple motion trajectories . the simulation results demonstrate superior performance of the proposed CLASSIFICATION AND RETRIEVAL SYSTEM . \n",
            "this paper presents a new method for HIGH ORDER DATA CLASSIFICATION based on TEN-SOR BASED NULL SPACE AFFINE INVARIANTS . the proposed method is based on a LINEAR CLASSIFIER . the proposed method is based on a LINEAR CLASSIFIER , which is based on the TENSOR NULL SPACE INVARIANTS . the proposed method is based on the use of MULTIDIMENSIONAL AFFINE TRANSFORMATIONS in the LINEAR CLASSIFIER . the proposed method is based on the TENSOR NULL SPACE INVARIANTS . the CLASSIFICATION AND RETRIEVAL SYSTEM is applied to the CLASSIFICATION AND RETRIEVAL SYSTEM . the experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "457 1000\n",
            "noise <unk> present serious complications to accurate DATA ANALYSIS in FUNCTIONAL MAGNETIC RESONANCE IMAGING . simply relying on CONTEXTUAL IMAGE INFORMATION often results in unsatisfactory segmentation of ACTIVE BRAIN REGIONS . to remedy this , we propose a novel GROUP MARKOV RANDOM FIELD -LRB- GROUP MRF -RRB- that extends the NEIGHBORHOOD SYSTEM to other subjects to incorporate GROUP INFORMATION in modeling each subject 's BRAIN ACTIVATION . our GROUP MARKOV RANDOM FIELD -LRB- GROUP MRF -RRB- has the distinct advantage of being able to regularize the states of both INTRA-AND INTER-SUBJECT NEIGHBORS without having to create a STRINGENT ONE-TO-ONE VOXEL CORRESPONDENCE as in standard FMRI GROUP ANALYSIS . also , our GROUP MARKOV RANDOM FIELD -LRB- GROUP MRF -RRB- can be efficiently implemented as a single MRF , hence enabling ACTIVATION MAPS of a group of subjects to be simultaneously and collaboratively segmented . we validate on both SYNTHETIC AND REAL FMRI DATA and demonstrate superior performance over standard ANALYSIS TECHNIQUES . \n",
            "this paper addresses the problem of STRINGENT ONE-TO-ONE VOXEL CORRESPONDENCE in FUNCTIONAL MAGNETIC RESONANCE IMAGING . in this paper , we propose a new method for DATA ANALYSIS based on the GROUP MARKOV RANDOM FIELD -LRB- GROUP MRF -RRB- . the proposed method is based on the GROUP MARKOV RANDOM FIELD -LRB- GROUP MRF -RRB- . the proposed method is based on the GROUP MARKOV RANDOM FIELD -LRB- GROUP MRF -RRB- . the proposed method is based on the GROUP MARKOV RANDOM FIELD -LRB- GROUP MRF -RRB- . the proposed method is based on the GROUP MARKOV RANDOM FIELD -LRB- GROUP MRF -RRB- . the proposed method is based on the GROUP MARKOV RANDOM FIELD -LRB- GROUP MRF -RRB- . the proposed method is evaluated on the SYNTHETIC AND REAL FMRI DATA and on SYNTHETIC AND REAL FMRI DATA .\n",
            "\n",
            "458 1000\n",
            "we show that a recently developed UNIVERSAL DEMOSAICKER by the present authors greatly outperforms existing DEMOSAICKERS when tested with a realistic OPTICAL PIPELINE . we present SPEED AND QUALITY OPTIMIZATIONS of this DEMOSAICKERS for the case of regular pattern color filter arrays . we implement and extensively test optimized versions for several common CFAS including BAYER , CMY and several RGBW PATTERNS . these tests show that the proposed algorithms outperform other DEMOSAICKERS by a substantial margin while being faster than most of them . HIGH SENSITIVITY RGBW CFAS are shown to have better performance than BAYER <unk> with previous algorithms . the proposed UNIVERSAL DEMOSAICKER is a set of FINITE IMPULSE RESPONSE FILTERS , which allows a single , efficient , IMAGE SIGNAL PROCESSOR DESIGN to support different CFAS by changing its FILTER WEIGHTS . being linear , the DEMOSAICKERS is free of NOISE INDUCED ARTI-FACTS and outputs IMAGES with NEAR POISSONIAN NOISE which is NOISE REDUCTION FRIENDLY . \n",
            "this paper presents a new method for IMAGE SIGNAL PROCESSOR DESIGN based on FINITE IMPULSE RESPONSE FILTERS . the proposed method is based on the use of FINITE IMPULSE RESPONSE FILTERS , a UNIVERSAL DEMOSAICKER and a UNIVERSAL DEMOSAICKER . the proposed method is based on a UNIVERSAL DEMOSAICKER , which is based on the UNIVERSAL DEMOSAICKER . the proposed method is based on the FILTER WEIGHTS and the FILTER WEIGHTS . the proposed method is based on the UNIVERSAL DEMOSAICKER . the proposed method is based on the FILTER WEIGHTS and the FILTER WEIGHTS . the proposed method is compared with the conventional DEMOSAICKERS and the SPEED AND QUALITY OPTIMIZATIONS .\n",
            "\n",
            "459 1000\n",
            "long short-term memory -lrb- lstm -rrb- is a specific RECURRENT NEURAL NETWORK ARCHITECTURE that is designed to model TEMPORAL SEQUENCES and their LONG-RANGE DEPENDENCIES more accurately than conventional RNNS . in this paper , we propose to use DEEP BIDIREC-TIONAL LSTM for AUDIO/VISUAL MODELING in our PHOTO-REAL TALKING HEAD SYSTEM . an AUDIO/VISUAL DATABASE of a subject 's talking is firstly recorded as our training data . the AUDIO/VISUAL STEREO DATA are converted into two PARALLEL TEMPORAL SEQUENCES , i.e. , CON-TEXTUAL LABEL SEQUENCES obtained by forced aligning audio against text , and VISUAL FEATURE SEQUENCES by applying ACTIVE-APPEARANCE-MODEL on the LOWER FACE REGION among all the training image samples . the deep BLSTM is then trained to learn the REGRESSION MODEL by minimizing the sum of SQUARE ERROR of PREDICTING VISUAL SEQUENCE from LABEL SEQUENCE . after testing different NETWORK TOPOLOGIES , we interestingly found the best network is two BLSTM LAYERS sitting on top of one FEED-FORWARD LAYER on our datasets . compared with our previous HMM-BASED SYSTEM , the newly proposed DEEP BIDIREC-TIONAL LSTM is better on both OBJECTIVE MEASUREMENT and SUBJECTIVE A/B TEST . \n",
            "this paper presents a RECURRENT NEURAL NETWORK ARCHITECTURE for PREDICTING VISUAL SEQUENCE . the RECURRENT NEURAL NETWORK ARCHITECTURE is based on a RECURRENT NEURAL NETWORK ARCHITECTURE of the LABEL SEQUENCE . the RECURRENT NEURAL NETWORK ARCHITECTURE is based on a RECURRENT NEURAL NETWORK ARCHITECTURE , which is based on the DEEP BIDIREC-TIONAL LSTM . the proposed RECURRENT NEURAL NETWORK ARCHITECTURE is based on the RECURRENT NEURAL NETWORK ARCHITECTURE and the RECURRENT NEURAL NETWORK ARCHITECTURE . the proposed RECURRENT NEURAL NETWORK ARCHITECTURE is based on the RECURRENT NEURAL NETWORK ARCHITECTURE and the RECURRENT NEURAL NETWORK ARCHITECTURE . the proposed RECURRENT NEURAL NETWORK ARCHITECTURE is based on a RECURRENT NEURAL NETWORK ARCHITECTURE . the proposed RECURRENT NEURAL NETWORK ARCHITECTURE is based on a RECURRENT NEURAL NETWORK ARCHITECTURE and is shown to be robust to LONG-RANGE DEPENDENCIES and LONG-RANGE DEPENDENCIES . the proposed RECURRENT NEURAL NETWORK ARCHITECTURE is based on a RECURRENT NEURAL NETWORK ARCHITECTURE and is shown to be robust to LONG-RANGE DEPENDENCIES and RNNS .\n",
            "\n",
            "460 1000\n",
            "recently , ESPRIT-BASED PARAMETER ESTIMATION ALGORITHMS have been developed to exploit the structure of signals from strictly second-order -lrb- so -rrb- non-circular -lrb- nc -rrb- sources . they achieve a higher ESTIMATION ACCURACY and can resolve up to twice as many sources . however , these NC METHODS assume that all the received signals are strictly non-circular . in this paper , we present the <unk> standard <unk> and the C-NC UNITARY ESPRIT ALGORITHMS designed for the more practical scenario of a received mixture of CIRCULAR AND STRICTLY NON-CIRCULAR SIGNALS . assuming that the number of CIRCULAR AND STRICTLY NON-CIRCULAR SIGNALS is known , the two proposed ESPRIT-BASED PARAMETER ESTIMATION ALGORITHMS yield CLOSED-FORM ESTIMATES and C-NC UNITARY ESPRIT ALGORITHMS also enables an entirely real-valued implementation . as a main result , it is shown that the ESTIMATION ACCURACY of the presented ESPRIT-BASED PARAMETER ESTIMATION ALGORITHMS improves with an increasing number of strictly non-circular signals among a fixed number of sources . thereby , not only the ESTIMATION ACCURACY of the strictly non-circular signals themselves is improved , but also the ESTIMATION ACCURACY of the circular signals . these results are validated by simulations . \n",
            "this paper presents a new method for CIRCULAR AND STRICTLY NON-CIRCULAR SIGNALS in CIRCULAR AND STRICTLY NON-CIRCULAR SIGNALS . the proposed method is based on the use of C-NC UNITARY ESPRIT ALGORITHMS and C-NC UNITARY ESPRIT ALGORITHMS . experimental results on CIRCULAR AND STRICTLY NON-CIRCULAR SIGNALS show that the proposed method achieves better performance than the conventional C-NC UNITARY ESPRIT ALGORITHMS .\n",
            "\n",
            "461 1000\n",
            "one of the main challenges in ZERO-SHOT LEARNING OF VISUAL CATEGORIES is gathering SEMANTIC ATTRIBUTES to accompany images . recent work has shown that learning from TEXTUAL DESCRIPTIONS , such as WIKIPEDIA ARTICLES , avoids the problem of having to explicitly define these attributes . we present a new model that can classify UNSEEN CATEGORIES from their TEXTUAL DESCRIPTION . specifically , we use TEXT FEATURES to predict the output weights of both the convolutional and the fully connected layers in a DEEP CONVOLUTIONAL NEU-RAL NETWORK . we take advantage of the architecture of CNNS and learn FEATURES at different layers , rather than just learning an EMBEDDING SPACE for both MODALITIES , as is common with existing approaches . the proposed model also allows us to automatically generate a list of <unk> for each VISUAL CATEGORY consisting of words from WIKIPEDIA ARTICLES . we train our models end-to-end using the CALTECH-UCSD BIRD AND FLOWER DATASETS and evaluate both ROC AND PRECISION-RECALL CURVES . our empirical results show that the proposed model significantly outper-forms previous methods . \n",
            "this paper addresses the problem of ZERO-SHOT LEARNING OF VISUAL CATEGORIES in WIKIPEDIA ARTICLES such as CNNS . we propose a method for ZERO-SHOT LEARNING OF VISUAL CATEGORIES based on CNNS . the method is based on a DEEP CONVOLUTIONAL NEU-RAL NETWORK , which is based on a DEEP CONVOLUTIONAL NEU-RAL NETWORK . the proposed method is based on a DEEP CONVOLUTIONAL NEU-RAL NETWORK , which is based on a DEEP CONVOLUTIONAL NEU-RAL NETWORK . the method is based on a DEEP CONVOLUTIONAL NEU-RAL NETWORK . the proposed method is based on a DEEP CONVOLUTIONAL NEU-RAL NETWORK . the method is based on a DEEP CONVOLUTIONAL NEU-RAL NETWORK . the proposed method is based on a DEEP CONVOLUTIONAL NEU-RAL NETWORK , which is based on a DEEP CONVOLUTIONAL NEU-RAL NETWORK .\n",
            "\n",
            "462 1000\n",
            "in this paper , we investigate the performance of ACOUSTIC EQUALIZATION in REVERBERANT ENVIRONMENTS . we first highlight an efficient general ACOUSTIC EQUALIZATION of a SOUND FIELD using SPHERICAL HARMONICS . we then use this ACOUSTIC EQUALIZATION to develop a CONCISE CLOSED-FORM EXPRESSION for robustness of equalization to SENSOR MOVEMENT . this CONCISE CLOSED-FORM EXPRESSION is used -lrb- i -rrb- to characterize equalization performance for a general class of NON-ISOTROPIC SOUND FIELDS and -lrb- ii -rrb- to quantify the improvements to EQUALIZER ROBUSTNESS that can be obtained by using a DIRECTIONAL MICROPHONE . the CONCISE CLOSED-FORM EXPRESSION used here does not use any of the assumptions of STATISTICAL ACOUSTICS , but instead exploits the inherent properties of a SOUND FIELD as described by the WAVE EQUATION . \n",
            "this paper presents a method for ACOUSTIC EQUALIZATION in REVERBERANT ENVIRONMENTS . the proposed method is based on a CONCISE CLOSED-FORM EXPRESSION , which is based on the WAVE EQUATION of the SOUND FIELD . the proposed method is based on the WAVE EQUATION of the WAVE EQUATION and the WAVE EQUATION . the proposed method is based on the WAVE EQUATION and the EQUALIZER ROBUSTNESS of the SOUND FIELD .\n",
            "\n",
            "463 1000\n",
            "the EIGENVALUE PROBLEM is solved on the FINITE ELEMENT MODEL of the EXTERNAL OUTER EAR CANAL . the <unk> of the CANAL WALLS and the interaction between EXTERNAL EAR CAVITY SUBSYSTEM and the ELASTIC TYMPANIC MEMBRANE is considered . the results of the FINITE ELEMENT MODEL are compared with experimental measurements on HUMAN DISSECTIONS . the calculations support hypothesis of possible influence of EXTERNAL EAR CANAL on the ENHANCEMENT OF HEARING SENSITIVITY in <unk> khz frequency range . \n",
            "this paper presents a FINITE ELEMENT MODEL for ENHANCEMENT OF HEARING SENSITIVITY . the proposed FINITE ELEMENT MODEL is based on a FINITE ELEMENT MODEL and a FINITE ELEMENT MODEL . the proposed FINITE ELEMENT MODEL is based on a FINITE ELEMENT MODEL and a FINITE ELEMENT MODEL . the proposed FINITE ELEMENT MODEL is based on a FINITE ELEMENT MODEL and is shown to be robust to HUMAN DISSECTIONS and ENHANCEMENT OF HEARING SENSITIVITY .\n",
            "\n",
            "464 1000\n",
            "in this paper , we present a new LEARNING FRAMEWORK for IMAGE STYLE TRANSFORMS . considering that the images in different style representations constitute different VECTOR SPACES , we propose a novel framework called COUPLED GAUSSIAN MIXTURE MODEL to learn the relations between different spaces and use COUPLED GAUSSIAN MIXTURE MODEL to infer the images from one style to another style . observing that for each style , only the components correlated to the space of the target style are useful for INFERENCE , we first develop the COUPLED GAUSSIAN MIXTURE MODEL to pursue the EMBEDDED HIDDEN SUBSPACES that best preserve the INTER-SPACE CORRELATION INFORMATION . then we develop the coupled bidirectional transform algorithm to estimate the transforms between the two EMBEDDED SPACES , where the COUPLING between the FORWARD TRANSFORM and the BACKWARD TRANSFORM is explicitly taken into account . to enhance the capability of modelling complex data , we further develop the COUPLED GAUSSIAN MIXTURE MODEL to generalize our framework to a MIXTURE-MODEL ARCHITECTURE . the effectiveness of the framework is demonstrated in the applications including FACE SUPER-RESOLUTION and BIDIRECTIONAL PORTRAIT STYLE TRANSFORMS . \n",
            "this paper presents a new LEARNING FRAMEWORK for FACE SUPER-RESOLUTION . the LEARNING FRAMEWORK is based on a COUPLED GAUSSIAN MIXTURE MODEL and a COUPLED GAUSSIAN MIXTURE MODEL for FACE SUPER-RESOLUTION . the proposed LEARNING FRAMEWORK is based on a COUPLED GAUSSIAN MIXTURE MODEL and a COUPLED GAUSSIAN MIXTURE MODEL . the proposed LEARNING FRAMEWORK is based on a COUPLED GAUSSIAN MIXTURE MODEL and a COUPLED GAUSSIAN MIXTURE MODEL . the proposed LEARNING FRAMEWORK is based on a COUPLED GAUSSIAN MIXTURE MODEL and a COUPLED GAUSSIAN MIXTURE MODEL for FACE SUPER-RESOLUTION .\n",
            "\n",
            "465 1000\n",
            "<unk> analysis <unk> in applications of BINARY CLASSIFICATION where true negatives do not add value and hence should not affect assessment of the CLASSIFIER 's performance . perhaps inspired by the many advantages of RECEIVER OPERATING CHARACTERISTIC CURVES and the area under such curves for ACCURACY-BASED PERFORMANCE ASSESSMENT , many researchers have taken to report PRECISION-RECALL CURVES and associated areas as PERFORMANCE METRIC . we demonstrate in this paper that this practice is <unk> with difficulties , mainly because of INCOHERENT SCALE ASSUMPTIONS -- e.g. , the area under a PR CURVE takes the ARITHMETIC MEAN OF PRECISION VALUES whereas the F Β SCORE applies the harmonic mean . we show how to fix this by plotting PR CURVES in a different COORDINATE SYSTEM , and demonstrate that the new PRECISION-RECALL-GAIN CURVES inherit all key advantages of ROC CURVES . in particular , the area under PRECISION-RECALL-GAIN CURVES conveys an expected f 1 score on a HARMONIC SCALE , and the CONVEX HULL of a PRECISION-RECALL-GAIN CURVE allows us to calibrate the CLASSIFIER 's scores so as to determine , for each operating point on the CONVEX HULL , the INTERVAL OF Β VALUES for which the point optimises F Β . we demonstrate experimentally that the area under traditional PR CURVES can easily favour models with lower expected f 1 score than others , and so the use of PRECISION-RECALL-GAIN CURVES will result in better MODEL SELECTION . \n",
            "this paper proposes a new method for MODEL SELECTION based on the INTERVAL OF Β VALUES . the proposed method is based on the INTERVAL OF Β VALUES of the PR CURVE and the INTERVAL OF Β VALUES . the proposed method is based on the INTERVAL OF Β VALUES of the PR CURVE and the INTERVAL OF Β VALUES . the proposed method is based on the INTERVAL OF Β VALUES of the PR CURVE . the proposed method is based on the PERFORMANCE METRIC of the COORDINATE SYSTEM . the proposed method is based on the INTERVAL OF Β VALUES and the F Β SCORE of the COORDINATE SYSTEM .\n",
            "\n",
            "466 1000\n",
            "sparse FEATURE SELECTION has been demonstrated to be effective in handling HIGH-DIMENSIONAL DATA . while promising , most of the existing works use CONVEX METHODS , which may be suboptimal in terms of the ACCURACY of FEATURE SELECTION and PARAMETER ESTIMATION . in this paper , we expand a NONCONVEX PARADIGM to SPARSE GROUP FEATURE SELECTION , which is motivated by applications that require identifying the underlying group structure and performing FEATURE SELECTION simultaneously . the main contributions of this article are twofold : -lrb- 1 -rrb- statistically , we introduce a NONCONVEX SPARSE GROUP FEATURE SELECTION MODEL which can reconstruct the ORACLE ESTIMATOR . therefore , consistent FEATURE SELECTION and PARAMETER ESTIMATION can be achieved ; -lrb- 2 -rrb- computationally , we propose an efficient algorithm that is applicable to LARGE-SCALE PROBLEMS . numerical results suggest that the proposed NONCONVEX SPARSE GROUP FEATURE SELECTION MODEL compares favorably against its competitors on SYNTHETIC DATA and REAL-WORLD APPLICATIONS , thus achieving desired goal of delivering high performance . \n",
            "this paper presents a NONCONVEX SPARSE GROUP FEATURE SELECTION MODEL for SPARSE GROUP FEATURE SELECTION . the proposed NONCONVEX SPARSE GROUP FEATURE SELECTION MODEL is based on the NONCONVEX SPARSE GROUP FEATURE SELECTION MODEL and the ORACLE ESTIMATOR . the proposed NONCONVEX SPARSE GROUP FEATURE SELECTION MODEL is based on the ORACLE ESTIMATOR and the ORACLE ESTIMATOR . the proposed NONCONVEX SPARSE GROUP FEATURE SELECTION MODEL is based on the ORACLE ESTIMATOR and the ORACLE ESTIMATOR . the proposed NONCONVEX SPARSE GROUP FEATURE SELECTION MODEL is evaluated on the SYNTHETIC DATA and on the SYNTHETIC DATA . the proposed NONCONVEX SPARSE GROUP FEATURE SELECTION MODEL is shown to outperform the conventional ORACLE ESTIMATOR in terms of ACCURACY and ACCURACY .\n",
            "\n",
            "467 1000\n",
            "1 this paper presents an analysis of LOMBARD SPEECH produced under different types and levels of NOISE . the speech used for the analysis forms a part of the UT-SCOPE DATABASE and consists of sentences from the well-known TIMIT CORPUS , spoken in the presence of <unk> , large crowd and <unk> NOISE . differences are shown to exist in the SPEECH CHARACTERISTICS under these varying NOISE TYPES . the deterioration of the EER of an INSET SPEAKER IDENTIFICATION SYSTEM trained on neutral and tested with LOMBARD SPEECH is also illustrated . a clear <unk> between the effect of NOISE and lombard effect on NOISE is also given by testing with NOISY LOMBARD SPEECH . the effect of TEST-TOKEN DURATION on system performance under the LOMBARD CONDITION is addressed . it is seen that test duration has no effect on the EER under lombard effect . the average EER for <unk> test duration is 14 . \n",
            "this paper presents a INSET SPEAKER IDENTIFICATION SYSTEM for NOISY LOMBARD SPEECH . the INSET SPEAKER IDENTIFICATION SYSTEM is based on a INSET SPEAKER IDENTIFICATION SYSTEM of the UT-SCOPE DATABASE . the INSET SPEAKER IDENTIFICATION SYSTEM is based on the LOMBARD CONDITION and the LOMBARD CONDITION . the INSET SPEAKER IDENTIFICATION SYSTEM is applied to the TIMIT CORPUS . the INSET SPEAKER IDENTIFICATION SYSTEM is evaluated on the TIMIT CORPUS . the results show that the proposed method is robust and robust to NOISE and is robust to NOISE .\n",
            "\n",
            "468 1000\n",
            "<unk> speech prosody is a primary characteristic of AUTISM SPECTRUM DISORDERS , yet ATYPICAL SPEECH PROSODY is often excluded from DIAGNOSTIC INSTRUMENT ALGORITHMS due to poor subjective reliability . robust , objective prosodic cues can enhance our understanding of those aspects which are <unk> in autism . in this work , we connect objective <unk> descriptors of prosody to SUBJECTIVE PERCEPTIONS OF PROSODIC AWKWARDNESS . subjectively , more AWKWARD SPEECH is less expressive -lrb- more monotone -rrb- and more often has PERCEIVED AWKWARD RATE/RHYTHM , VOLUME , and INTONATION . we also find EXPRESSIVITY can be quantified through OBJECTIVE INTONATION VARIABILITY FEATURES , and that SPEAKING RATE and RHYTHM CUES are highly predictive of PERCEIVED AWKWARDNESS . ACOUSTIC-PROSODIC FEATURES are also able to significantly differentiate subjects with AUTISM SPECTRUM DISORDERS from typically developing -lrb- td -rrb- subjects in a CLASSIFICATION TASK , emphasizing the potential of AUTOMATED METHODS for DIAGNOSTIC EFFICIENCY and CLARITY . \n",
            "this paper addresses the problem of ATYPICAL SPEECH PROSODY for AWKWARD SPEECH . we propose a method for ATYPICAL SPEECH PROSODY based on the SUBJECTIVE PERCEPTIONS OF PROSODIC AWKWARDNESS . the proposed method is based on the use of ACOUSTIC-PROSODIC FEATURES and RHYTHM CUES . the proposed method is based on the SUBJECTIVE PERCEPTIONS OF PROSODIC AWKWARDNESS and the OBJECTIVE INTONATION VARIABILITY FEATURES . the proposed method is compared with conventional DIAGNOSTIC INSTRUMENT ALGORITHMS and AUTOMATED METHODS . the results show that the proposed method outperforms the conventional AUTOMATED METHODS and AUTOMATED METHODS .\n",
            "\n",
            "469 1000\n",
            "we present a UNIFIED MODEL for FACE DETECTION , POSE ESTIMATION , and LANDMARK ESTIMATION in REAL-WORLD , CLUTTERED IMAGES . our UNIFIED MODEL is based on a MIXTURES OF TREES with a shared pool of parts ; we UNIFIED MODEL every FACIAL LANDMARK as a part and use GLOBAL MIXTURES to capture TOPOLOGICAL CHANGES due to VIEWPOINT . we show that TREE-STRUCTURED MODELS are surprisingly effective at capturing GLOBAL ELASTIC DEFORMATION , while being easy to optimize unlike DENSE GRAPH STRUCTURES . we present extensive results on standard FACE BENCHMARKS , as well as a new '' in the WILD '' ANNOTATED DATASET , that suggests our UNIFIED MODEL advances the state-of-the-art , sometimes considerably , for all three tasks . though our UNIFIED MODEL is <unk> trained with hundreds of faces , UNIFIED MODEL compares favorably to COMMERCIAL SYSTEMS trained with billions of examples -lrb- such as GOOGLE PICASA and <unk> -rrb- . \n",
            "this paper presents a novel UNIFIED MODEL for FACE DETECTION . the UNIFIED MODEL is based on a UNIFIED MODEL and a UNIFIED MODEL . the proposed UNIFIED MODEL is based on a UNIFIED MODEL and a UNIFIED MODEL . the proposed UNIFIED MODEL is based on a UNIFIED MODEL and a UNIFIED MODEL . the proposed UNIFIED MODEL is based on a UNIFIED MODEL and a UNIFIED MODEL . the proposed UNIFIED MODEL is based on a UNIFIED MODEL and a UNIFIED MODEL for FACE DETECTION . experimental results show that the proposed UNIFIED MODEL performs better than the conventional TREE-STRUCTURED MODELS .\n",
            "\n",
            "470 1000\n",
            "we design a new LEARNING ALGORITHM for the SET COVERING MACHINE from a PAC-BAYES PERSPECTIVE and propose a <unk> risk bound which is minimized for CLASSIFIERS achieving a NON TRIVIAL MARGIN-SPARSITY TRADE-OFF . \n",
            "this paper presents a new LEARNING ALGORITHM for SET COVERING MACHINE . the LEARNING ALGORITHM is based on the LEARNING ALGORITHM . the LEARNING ALGORITHM is based on the LEARNING ALGORITHM . the LEARNING ALGORITHM is based on the LEARNING ALGORITHM . the LEARNING ALGORITHM is applied to the SET COVERING MACHINE .\n",
            "\n",
            "471 1000\n",
            "an approach to CLUSTERING is presented that adapts the basic TOP-DOWN INDUCTION OF DECISION TREES METHOD towards CLUSTERING . to this aim , it employs the principles of INSTANCE BASED LEARNING . the resulting methodology is implemented in the TOP-DOWN INDUCTION OF DECISION TREES METHOD -lrb- top down induction of clustering trees -rrb- system for FIRST ORDER CLUSTERING . the TOP-DOWN INDUCTION OF DECISION TREES METHOD employs the first ORDER LOGICAL DECISION TREE REPRESENTATION of the INDUCTIVE LOGIC PROGRAMMING SYSTEM <unk> . various experiments with TOP-DOWN INDUCTION OF DECISION TREES METHOD are presented , in both PROPOSITIONAL AND RE-LATIONAL DOMAINS . \n",
            "this paper presents a novel TOP-DOWN INDUCTION OF DECISION TREES METHOD for CLUSTERING . the TOP-DOWN INDUCTION OF DECISION TREES METHOD is based on the TOP-DOWN INDUCTION OF DECISION TREES METHOD . the proposed TOP-DOWN INDUCTION OF DECISION TREES METHOD is based on the TOP-DOWN INDUCTION OF DECISION TREES METHOD . the proposed TOP-DOWN INDUCTION OF DECISION TREES METHOD is based on the TOP-DOWN INDUCTION OF DECISION TREES METHOD . the proposed TOP-DOWN INDUCTION OF DECISION TREES METHOD is based on the TOP-DOWN INDUCTION OF DECISION TREES METHOD . the proposed TOP-DOWN INDUCTION OF DECISION TREES METHOD is based on the TOP-DOWN INDUCTION OF DECISION TREES METHOD .\n",
            "\n",
            "472 1000\n",
            "in this paper , we present our CROSSWORD PUZZLE RESOLUTION SYSTEM , which exploits SYNTACTIC STRUCTURES for CLUE RERANKING and ANSWER EXTRACTION . CROSSWORD PUZZLE RESOLUTION SYSTEM uses a database -lrb- db -rrb- containing previously solved <unk> in order to generate the list of candidate answers . additionally , CROSSWORD PUZZLE RESOLUTION SYSTEM uses innovative FEATURES , such as the ANSWER POSITION in the RANK and AGGREGATED INFORMATION such as the MIN , max and AVERAGE CLUE RERANKING SCORES . our CROSSWORD PUZZLE RESOLUTION SYSTEM is based on WEBCROW , one of the most advanced systems for AUTOMATIC CROSSWORD PUZZLE RESOLUTION . our extensive experiments over our two million CLUE DATASET show that our CROSSWORD PUZZLE RESOLUTION SYSTEM highly improves the quality of the ANSWER LIST , enabling the achievement of unprecedented results on the complete CP RESOLUTION TASKS , i.e. , ACCURACY of <unk> % . \n",
            "this paper presents a novel CROSSWORD PUZZLE RESOLUTION SYSTEM for ANSWER EXTRACTION . the CROSSWORD PUZZLE RESOLUTION SYSTEM is based on a CROSSWORD PUZZLE RESOLUTION SYSTEM , which is based on a CROSSWORD PUZZLE RESOLUTION SYSTEM . the proposed CROSSWORD PUZZLE RESOLUTION SYSTEM is based on a CROSSWORD PUZZLE RESOLUTION SYSTEM , which is based on a CROSSWORD PUZZLE RESOLUTION SYSTEM . the proposed CROSSWORD PUZZLE RESOLUTION SYSTEM is based on a CROSSWORD PUZZLE RESOLUTION SYSTEM , which is based on the AGGREGATED INFORMATION . the proposed CROSSWORD PUZZLE RESOLUTION SYSTEM is evaluated on CP RESOLUTION TASKS and CP RESOLUTION TASKS . the proposed CROSSWORD PUZZLE RESOLUTION SYSTEM is compared with other FEATURES such as ANSWER EXTRACTION and ANSWER EXTRACTION .\n",
            "\n",
            "473 1000\n",
            "incomplete preferences are likely to arise in REAL-WORLD PREFERENCE AGGREGATION and VOTING SYSTEMS . this paper deals with determining whether an INCOMPLETE PREFERENCE PROFILE is single-peaked . this is essential information since many intractable voting problems become tractable for SINGLE-PEAKED PROFILES . we prove that for incomplete profiles the problem of DETERMINING SINGLE-PEAKEDNESS is np-complete . despite this computational hardness result , we find two POLYNOMIAL-TIME ALGORITHMS for reasonably restricted settings . \n",
            "this paper addresses the problem of DETERMINING SINGLE-PEAKEDNESS in VOTING SYSTEMS . we propose a method for REAL-WORLD PREFERENCE AGGREGATION based on the INCOMPLETE PREFERENCE PROFILE . the proposed method is based on the use of a INCOMPLETE PREFERENCE PROFILE and a INCOMPLETE PREFERENCE PROFILE . experimental results show the effectiveness of the proposed method in terms of INCOMPLETE PREFERENCES and INCOMPLETE PREFERENCES .\n",
            "\n",
            "474 1000\n",
            "we investigate the problem of reducing the COMPLEXITY of a GRAPHICAL MODEL -lrb- G , p G -rrb- by finding a SUBGRAPH H OF G , chosen from a class of SUBGRAPHS H , such that h is optimal with respect to KL-DIVERGENCE . we do this by first defining a DECOMPOSITION TREE REPRESENTATION for G , which is closely related to the JUNCTION-TREE REPRESENTATION for G . we then give an algorithm which uses this DECOMPOSITION TREE REPRESENTATION to compute the optimal h ∈ h. <unk> -lsb- 2 -rsb- and <unk> -lsb- 3 -rsb- have used GRAPH SEPARATION PROPERTIES to solve several COMBINATORIAL OPTIMIZATION PROBLEMS when the size of the MINIMAL SEPARATORS in the G is bounded . we present an extension of this technique which applies to some important choices of h even when the size of the MINIMAL SEPARATORS of G are arbitrarily large . in particular , this applies to problems such as finding an optimal SUBGRAPHICAL MODEL over a -lrb- k − 1 -rrb- - TREE of a GRAPHICAL MODEL over a <unk> -lrb- for arbitrary k -rrb- and selecting an optimal SUBGRAPHICAL MODEL with -lrb- a constant -rrb- d fewer edges with respect to KL-DIVERGENCE can be solved in time polynomial in | v -lrb- G -rrb- | using this formulation . \n",
            "this paper presents a new method for COMBINATORIAL OPTIMIZATION PROBLEMS based on the SUBGRAPH H OF G . the proposed method is based on the SUBGRAPH H OF G of the TREE and the SUBGRAPH H OF G . the proposed method is based on the SUBGRAPH H OF G of the TREE . the proposed method is based on the SUBGRAPH H OF G of the TREE . the proposed method is based on the SUBGRAPH H OF G of the TREE . the proposed method is based on the DECOMPOSITION TREE REPRESENTATION . the proposed method is based on the DECOMPOSITION TREE REPRESENTATION .\n",
            "\n",
            "475 1000\n",
            "multilingual applications frequently involve dealing with proper names , but names are often missing in BILINGUAL LEXICONS . this MULTILINGUAL APPLICATIONS is exacerbated for applications involving translation between LATIN-SCRIPTED LANGUAGES and ASIAN LANGUAGES such as CHINESE , JAPANESE and korean -lrb- <unk> -rrb- where simple STRING COPYING is not a solution . we present a novel approach for generating the IDEOGRAPHIC REPRESENTATIONS of a CJK NAME written in a LATIN SCRIPT . the proposed approach involves first identifying the origin of the name , and then <unk> the name to all possible CHINESE CHARACTERS using LANGUAGE-SPECIFIC MAPPINGS . to reduce the massive number of possibilities for COMPUTATION , we apply a THREE-TIER FILTERING PROCESS by filtering first through a set of ATTESTED BIGRAMS , then through a set of attested terms , and lastly through the WWW for a final validation . we illustrate the approach with ENGLISH-TO-JAPANESE BACK-TRANSLITERATION . against test sets of JAPANESE given names and <unk> , we have achieved AVERAGE PRECISIONS of 73 % and 90 % , respectively . \n",
            "this paper presents a method for MULTILINGUAL APPLICATIONS from ASIAN LANGUAGES . the method is based on the use of BILINGUAL LEXICONS and a LATIN SCRIPT . the method is based on the use of IDEOGRAPHIC REPRESENTATIONS and BILINGUAL LEXICONS . the proposed method is based on the use of IDEOGRAPHIC REPRESENTATIONS and BILINGUAL LEXICONS . the proposed method is based on the use of IDEOGRAPHIC REPRESENTATIONS and BILINGUAL LEXICONS . it is shown that the proposed method is robust and robust to CHINESE CHARACTERS such as COMPUTATION , COMPUTATION , and COMPUTATION .\n",
            "\n",
            "476 1000\n",
            "our study deals with a SILENT SPEECH INTERFACE based on MAPPING SURFACE ELECTROMYOGRAPHIC SIGNALS to SPEECH WAVEFORMS . ELECTROMYOGRAPHIC SIGNALS recorded from the FACIAL MUSCLES capture the activity of the HUMAN ARTICULATORY APPARATUS and therefore allow to <unk> speech , even when no AUDIBLE SIGNAL is produced . the MAPPING OF EMG SIGNALS to speech is done via a GAUSSIAN MIXTURE MODEL - based conversion technique . in this paper , we follow the lead of EMG-BASED SPEECH-TO-TEXT SYSTEMS and apply two major recent technological advances to our system , namely , we consider EMG-BASED SPEECH-TO-TEXT SYSTEMS , which are robust against ELECTRODE REPOSITIONING , and we show that MAPPING the EMG SIGNAL to WHISPERED SPEECH creates a better SPEECH SIGNAL than a MAPPING to normally SPOKEN SPEECH . we objectively evaluate the performance of our systems using a SPECTRAL DISTORTION MEASURE . \n",
            "this paper presents a method for MAPPING SURFACE ELECTROMYOGRAPHIC SIGNALS from SPOKEN SPEECH . the proposed method is based on a GAUSSIAN MIXTURE MODEL for the MAPPING OF EMG SIGNALS . the proposed method is based on a GAUSSIAN MIXTURE MODEL of the AUDIBLE SIGNAL . the proposed method is based on a GAUSSIAN MIXTURE MODEL of the AUDIBLE SIGNAL . the proposed method is based on the MAPPING OF EMG SIGNALS . the proposed method is based on a GAUSSIAN MIXTURE MODEL . the proposed method is based on the MAPPING OF EMG SIGNALS . the proposed method is based on a GAUSSIAN MIXTURE MODEL and is applied to the MAPPING OF EMG SIGNALS .\n",
            "\n",
            "477 1000\n",
            "large-scale clustering has found wide applications in many fields and received much attention in recent years . however , most existing LARGE-SCALE CLUSTERING METHODS can only achieve <unk> performance , because LARGE-SCALE CLUSTERING METHODS are sensitive to the unavoidable presence of NOISE in the LARGE-SCALE DATA . to address this challenging problem , we thus propose a LARGE-SCALE SPARSE CLUSTERING ALGORITHM . in this paper , we choose a TWO-STEP OPTIMIZATION STRATEGY for LARGE-SCALE SPARSE CLUSTERING : 1 -rrb- k-means clustering over the LARGE-SCALE DATA to obtain the initial clustering results ; 2 -rrb- CLUSTERING REFINEMENT over the initial results by developing a SPARE CODING ALGORITHM . to guarantee the <unk> of the second step for LARGE-SCALE DATA , we also utilize NONLINEAR APPROXIMATION and DIMENSION REDUCTION TECHNIQUES to speed up the SPARE CODING ALGORITHM . experimental results on both SYNTHETIC AND REAL-WORLD DATASETS demonstrate the promising performance of our LARGE-SCALE SPARSE CLUSTERING ALGORITHM . \n",
            "this paper proposes a new TWO-STEP OPTIMIZATION STRATEGY for LARGE-SCALE SPARSE CLUSTERING . the proposed SPARE CODING ALGORITHM is based on the TWO-STEP OPTIMIZATION STRATEGY and the SPARE CODING ALGORITHM . the proposed SPARE CODING ALGORITHM is based on the TWO-STEP OPTIMIZATION STRATEGY and the SPARE CODING ALGORITHM . the proposed TWO-STEP OPTIMIZATION STRATEGY is based on the TWO-STEP OPTIMIZATION STRATEGY and the SPARE CODING ALGORITHM . the proposed SPARE CODING ALGORITHM is evaluated on the SYNTHETIC AND REAL-WORLD DATASETS and the SYNTHETIC AND REAL-WORLD DATASETS .\n",
            "\n",
            "478 1000\n",
            "we present a NONLINEAR FORWARD-SEARCH METHOD suitable for planning the reactions of an agent operating in a highly unpredictable environment . we show that this NONLINEAR FORWARD-SEARCH METHOD is more eecient than existing LINEAR METHODS . we then introduce the notion of SAFETY AND LIVENESS RULES . this makes possible a sharper exploitation of the information retrieved when exploring the future of the agent . \n",
            "this paper introduces a new NONLINEAR FORWARD-SEARCH METHOD for SAFETY AND LIVENESS RULES . the proposed NONLINEAR FORWARD-SEARCH METHOD is based on the use of SAFETY AND LIVENESS RULES , and is shown to outperform existing LINEAR METHODS .\n",
            "\n",
            "479 1000\n",
            "in this paper , we develop a new method for WEIGHTED LEAST SQUARES 2D LINEAR-PHASE FIR FILTER DESIGN . it poses the problem of FILTER DESIGN as the problem of projecting the DESIRED FREQUENCY RESPONSE onto the SUBSPACE spanned by an appropriate ORTHONORMAL BASIS . we show how to compute the ORTHONORMAL BASIS efficiently in the cases of QUADRANTALLY-SYMMETRIC FILTER DESIGN and CENTRO-SYMMETRIC FILTER DESIGN . the design examples show that the proposed method is faster than a conventional WEIGHTED LEAST SQUARES FILTER DESIGN METHOD . also , the amount of STORAGE required to compute the FILTER COEFFICIENTS is greatly reduced . \n",
            "this paper presents a new method for QUADRANTALLY-SYMMETRIC FILTER DESIGN . the proposed method is based on the WEIGHTED LEAST SQUARES FILTER DESIGN METHOD and the WEIGHTED LEAST SQUARES FILTER DESIGN METHOD . the proposed method is based on the WEIGHTED LEAST SQUARES FILTER DESIGN METHOD and the WEIGHTED LEAST SQUARES FILTER DESIGN METHOD . the proposed method is based on the WEIGHTED LEAST SQUARES FILTER DESIGN METHOD and the WEIGHTED LEAST SQUARES FILTER DESIGN METHOD . the proposed method is based on the WEIGHTED LEAST SQUARES FILTER DESIGN METHOD and the WEIGHTED LEAST SQUARES FILTER DESIGN METHOD . the experimental results show that the proposed method outperforms the conventional method in terms of STORAGE and STORAGE .\n",
            "\n",
            "480 1000\n",
            "<unk> and STOCHASTIC EQUIVALENCE are two primary features of interest in SOCIAL NETWORKS . recently , the MULTIPLICATIVE LATENT FACTOR MODEL is proposed to model SOCIAL NETWORKS with DIRECTED LINKS . although MULTIPLICATIVE LATENT FACTOR MODEL can capture STOCHASTIC EQUIVALENCE , it can not model well <unk> in NETWORK STRUCTURE . however , many REAL-WORLD NETWORKS exhibit HOMOPHILY or both HOMOPHILY and STOCHASTIC EQUIVALENCE , and hence the NETWORK STRUCTURE of these NETWORK STRUCTURE can not be mod-eled well by MULTIPLICATIVE LATENT FACTOR MODEL . in this paper , we propose a novel model , called generalized latent factor model -lrb- <unk> -rrb- , for SOCIAL NETWORK ANALYSIS by enhancing HOMOPHILY MODELING in MULTIPLICATIVE LATENT FACTOR MODEL . we devise a MINORIZATION-MAXIMIZATION ALGORITHM with LINEAR-TIME COMPLEXITY and CONVERGENCE GUARANTEE to learn the MODEL PARAMETERS . extensive experiments on some REAL-WORLD NETWORKS show that <unk> can effectively model HOMOPHILY to dramatically outperform state-of-the-art methods . \n",
            "this paper presents a MULTIPLICATIVE LATENT FACTOR MODEL for SOCIAL NETWORK ANALYSIS . the MINORIZATION-MAXIMIZATION ALGORITHM is based on a MULTIPLICATIVE LATENT FACTOR MODEL and a MULTIPLICATIVE LATENT FACTOR MODEL . the MINORIZATION-MAXIMIZATION ALGORITHM is based on a MULTIPLICATIVE LATENT FACTOR MODEL and a MULTIPLICATIVE LATENT FACTOR MODEL . the proposed MINORIZATION-MAXIMIZATION ALGORITHM is based on a MULTIPLICATIVE LATENT FACTOR MODEL and a MULTIPLICATIVE LATENT FACTOR MODEL . the proposed MINORIZATION-MAXIMIZATION ALGORITHM is based on a MULTIPLICATIVE LATENT FACTOR MODEL and a MULTIPLICATIVE LATENT FACTOR MODEL . the proposed MINORIZATION-MAXIMIZATION ALGORITHM is based on a MULTIPLICATIVE LATENT FACTOR MODEL and a MULTIPLICATIVE LATENT FACTOR MODEL .\n",
            "\n",
            "481 1000\n",
            "in WIRELESS SYSTEMS where half <unk> transceivers are employed , most existing practical COOPERATIVE PROTOCOLS achieve a SPECTRAL EFFICIENCY of 0.5 symbols per channel use -lrb- <unk> -rrb- . recently a DECODE-AND-FORWARD PROTOCOL was developed to achieve a SPECTRAL EFFICIENCY of 2/3 symbols <unk> . but there is no practical AMPLIFY-AND-FORWARD PROTOCOL that can achieve a SPECTRAL EFFICIENCY higher than 0.5 symbols <unk> . in this paper , we develop a NONORTHOGONAL AF PROTOCOL which achieves a SPECTRAL EFFICIENCY of 2/3 symbols <unk> and provides almost the same bit ERROR RATE as the traditional ORTHOGONAL AF which has a SPECTRAL EFFICIENCY of 0.5 symbols <unk> . \n",
            "this paper presents a novel AMPLIFY-AND-FORWARD PROTOCOL for WIRELESS SYSTEMS . the proposed AMPLIFY-AND-FORWARD PROTOCOL is based on the NONORTHOGONAL AF PROTOCOL . the proposed AMPLIFY-AND-FORWARD PROTOCOL is based on the NONORTHOGONAL AF PROTOCOL . the proposed AMPLIFY-AND-FORWARD PROTOCOL is evaluated on the ORTHOGONAL AF and on the ERROR RATE of the NONORTHOGONAL AF PROTOCOL .\n",
            "\n",
            "482 1000\n",
            "it is a common practice to approximate '' complicated '' functions with more friendly ones . in LARGE-SCALE MACHINE LEARNING APPLICATIONS , NONSMOOTH LOSSES/REGULARIZERS that entail great COMPUTATIONAL CHALLENGES are usually approximated by SMOOTH FUNCTIONS . we reexamine this powerful methodology and point out a NONSMOOTH APPROXIMATION which simply <unk> the linearity of the PROXI-MAL MAP . the new approximation is justified using a recent CONVEX ANALYSIS TOOL -- proximal average , and yields a novel PROXIMAL GRADIENT ALGORITHM that is strictly better than the one based on SMOOTHING , without incurring any extra OVERHEAD . numerical experiments conducted on two important applications , OVERLAPPING GROUP LASSO and <unk> fused lasso , corroborate the theoretical claims . \n",
            "this paper presents a new method for LARGE-SCALE MACHINE LEARNING APPLICATIONS based on the PROXIMAL GRADIENT ALGORITHM . the proposed method is based on the use of SMOOTH FUNCTIONS and the PROXIMAL GRADIENT ALGORITHM . the proposed method is based on the PROXIMAL GRADIENT ALGORITHM . the proposed method is based on the PROXIMAL GRADIENT ALGORITHM . the proposed method is based on the PROXIMAL GRADIENT ALGORITHM . the proposed method is based on the PROXIMAL GRADIENT ALGORITHM . the proposed method is based on the PROXIMAL GRADIENT ALGORITHM . the proposed method is based on the PROXIMAL GRADIENT ALGORITHM .\n",
            "\n",
            "483 1000\n",
            "this work is a contribution to the analysis of the procedure , based on WAVELET COEECIENT PARTITION FUNCTIONS , commonly used to estimate the LEGENDRE MULTIFRACTAL SPECTRUM . the procedure is applied to two examples , a FRACTIONAL BROWNIAN MOTION in MULTIFRACTAL TIME and a SELF-SIMILAR-STABLE PROCESS , whose SAMPLE PATHS exhibit irregularities that by eye appear very close . we observe that , for the second example , this analysis results in a qualitatively inaccurate estimation of its MULTIFRACTAL SPECTRUM , and a related masking of <unk> nature of the process . we explain the origin of this error through a detailed analysis of the PARTITION FUNCTIONS of the SELF-SIMILAR-STABLE PROCESS . such a study is made possible by the speciic properties of the WAVELET CO-EECIENTS of such processes . we indicate how the ESTIMATION PROCEDURE might be modiied to avoid such errors . \n",
            "this paper addresses the problem of FRACTIONAL BROWNIAN MOTION in the presence of FRACTIONAL BROWNIAN MOTION . we propose a method to estimate the SAMPLE PATHS of the MULTIFRACTAL SPECTRUM in the SELF-SIMILAR-STABLE PROCESS . the proposed method is based on the WAVELET COEECIENT PARTITION FUNCTIONS and the WAVELET COEECIENT PARTITION FUNCTIONS . the proposed method is based on the WAVELET COEECIENT PARTITION FUNCTIONS and the ESTIMATION PROCEDURE . the proposed method is compared with the conventional ESTIMATION PROCEDURE and the ESTIMATION PROCEDURE .\n",
            "\n",
            "484 1000\n",
            "joint object recognition and POSE ESTIMATION solely from RANGE IMAGES is an important task e.g. in ROBOTICS APPLICATIONS and in AUTOMATED MANUFACTURING ENVIRONMENTS . the lack of COLOR INFORMATION and limitations of current COMMODITY DEPTH SENSORS make this task a challenging COMPUTER VISION PROBLEM , and a standard RANDOM SAMPLING BASED APPROACH is prohibitively time-consuming . we propose to address this difficult problem by generating promising inlier sets for POSE ESTIMATION by early rejection of CLEAR OUTLIERS with the help of LOCAL BELIEF PROPAGATION -lrb- or DYNAMIC PROGRAMMING -rrb- . by exploiting <unk> our method is fast , and we also do not rely on a COMPUTATIONALLY EXPENSIVE TRAINING PHASE . we demonstrate state-of-the art performance on a standard dataset and illustrate our approach on challenging REAL SEQUENCES . \n",
            "this paper addresses the problem of JOINT OBJECT RECOGNITION in AUTOMATED MANUFACTURING ENVIRONMENTS . in this paper , we propose a method for JOINT OBJECT RECOGNITION based on LOCAL BELIEF PROPAGATION . the proposed method is based on LOCAL BELIEF PROPAGATION and LOCAL BELIEF PROPAGATION . the proposed method is based on a RANDOM SAMPLING BASED APPROACH , which is based on LOCAL BELIEF PROPAGATION and DYNAMIC PROGRAMMING . experimental results show that the proposed method is robust and robust to CLEAR OUTLIERS and CLEAR OUTLIERS .\n",
            "\n",
            "485 1000\n",
            "enlarging or reducing the TEMPLATE SIZE by adding new parts , or removing parts of the template , according to their suitability for TRACKING , requires the ability to deal with the variation of the TEMPLATE SIZE . for instance , REAL-TIME TEMPLATE TRACKING using LINEAR PREDICTORS , although fast and reliable , requires using templates of fixed size and does not allow ON-LINE MODIFICATION of the predictor . to solve this problem we propose the ADAPTIVE LINEAR PREDICTORS which enable fast online modifications of PRE-LEARNED LINEAR PREDICTORS . instead of applying a FULL MATRIX INVERSION for every modification of the TEMPLATE SHAPE as standard approaches to learning LINEAR PREDICTORS do , we just perform a fast update of this inverse . this allows us to learn the ADAPTIVE LINEAR PREDICTORS in a much shorter time than standard LEARNING APPROACHES while performing equally well . we performed exhaustive evaluation of our ADAPTIVE LINEAR PREDICTORS and compared ADAPTIVE LINEAR PREDICTORS to standard LINEAR PREDICTORS and other state of the art approaches . \n",
            "this paper proposes a new method for REAL-TIME TEMPLATE TRACKING based on PRE-LEARNED LINEAR PREDICTORS . the proposed method is based on the use of PRE-LEARNED LINEAR PREDICTORS . the proposed method is based on the use of PRE-LEARNED LINEAR PREDICTORS , and is based on the use of PRE-LEARNED LINEAR PREDICTORS . the experimental results show that the proposed LEARNING APPROACHES outperform conventional LEARNING APPROACHES in terms of TEMPLATE SIZE and TEMPLATE SIZE .\n",
            "\n",
            "486 1000\n",
            "we introduce a novel approach to the CEREBRAL WHITE MATTER CONNECTIVITY MAPPING from DIFFUSION TENSOR MRI . DT-MRI is the unique NON-INVASIVE TECHNIQUE capable of probing and quantifying the ANISOTROPIC DIFFUSION OF WATER MOLECULES in BIOLOGICAL TISSUES . we address the problem of CONSISTENT NEURAL FIBERS RECONSTRUCTION in areas of COMPLEX DIFFUSION PROFILES with potentially multiple fibers orientations . our method relies on a GLOBAL MODELIZATION of the acquired mri volume as a RIEMANNIAN MANIFOLD M and proceeds in 4 <unk> steps : first , we establish the link between BROWNIAN MOTION and diffusion mri by using the LAPLACE-BELTRAMI OPERATOR on m . we then expose how the sole knowledge of the diffusion properties of water molecules on m is sufficient to infer its geometry . there exists a DIRECT MAPPING between the DIFFUSION TENSOR and the metric of m. next , having access to that metric , we propose a novel LEVEL SET FORMULATION SCHEME to approximate the DISTANCE FUNCTION related to a RADIAL BROWNIAN MOTION on m. finally , a rigorous NUMERICAL SCHEME using the EXPONENTIAL MAP is derived to estimate the GEODESICS OF M , seen as the DIFFUSION PATHS OF WATER MOLECULES . numerical experimentations conducted on SYNTHETIC AND REAL DIFFUSION MRI DATASETS illustrate the potentialities of this LEVEL SET FORMULATION SCHEME . \n",
            "this paper proposes a new NON-INVASIVE TECHNIQUE for ANISOTROPIC DIFFUSION OF WATER MOLECULES in DIFFUSION TENSOR MRI . the proposed NON-INVASIVE TECHNIQUE is based on a LEVEL SET FORMULATION SCHEME , which employs a CEREBRAL WHITE MATTER CONNECTIVITY MAPPING . the proposed LEVEL SET FORMULATION SCHEME is based on a NON-INVASIVE TECHNIQUE , which is based on the RIEMANNIAN MANIFOLD M . the proposed LEVEL SET FORMULATION SCHEME is based on the GEODESICS OF M of the LAPLACE-BELTRAMI OPERATOR . the proposed LEVEL SET FORMULATION SCHEME is based on a LEVEL SET FORMULATION SCHEME . the proposed LEVEL SET FORMULATION SCHEME is based on a LEVEL SET FORMULATION SCHEME . the proposed LEVEL SET FORMULATION SCHEME is based on a LEVEL SET FORMULATION SCHEME , which is based on a LEVEL SET FORMULATION SCHEME . the proposed LEVEL SET FORMULATION SCHEME is based on a LEVEL SET FORMULATION SCHEME and is shown to be robust to BROWNIAN MOTION . the proposed LEVEL SET FORMULATION SCHEME is compared with the conventional LEVEL SET FORMULATION SCHEME and the LEVEL SET FORMULATION SCHEME .\n",
            "\n",
            "487 1000\n",
            "<unk> detection for MIXED-TYPE DATA is an important problem that has not been well addressed in the MACHINE LEARNING FIELD . there are two challenging issues for MIXED-TYPE DATASETS , namely modeling mutual correlations between MIXED-TYPE ATTRIBUTES and capturing large variations due to anomalies . this paper presents BUFFDETECT , a robust ERROR BUFFERING APPROACH for ANOMALY DETECTION in MIXED-TYPE DATASETS . a new variant of the GENERALIZED LINEAR MODEL is proposed to GENERALIZED LINEAR MODEL the dependency between MIXED-TYPE ATTRIBUTES . the GENERALIZED LINEAR MODEL incorporates an ERROR BUFFERING COMPONENT based on STUDENT-T DISTRIBUTION to absorb the variations caused by anomalies . however , because of the NON-GAUSSIAN DESIGN , the problem becomes analytically intractable . we propose a novel BAYESIAN INFERENCE APPROACH , which integrates LAPLACE APPROXIMATION and several COMPUTATIONAL OPTIMIZATIONS , and is able to efficiently approximate the POSTERIOR OF HIGH DIMENSIONAL LATENT VARIABLES by iteratively updating the LATENT VARIABLES in groups . extensive experimental evaluations based on 13 benchmark datasets demonstrate the effectiveness and efficiency of BUFFDETECT . \n",
            "this paper presents a BAYESIAN INFERENCE APPROACH for ANOMALY DETECTION . the BAYESIAN INFERENCE APPROACH is based on a GENERALIZED LINEAR MODEL and a GENERALIZED LINEAR MODEL . the BAYESIAN INFERENCE APPROACH is based on a GENERALIZED LINEAR MODEL and a GENERALIZED LINEAR MODEL . the proposed BAYESIAN INFERENCE APPROACH is based on a GENERALIZED LINEAR MODEL and a GENERALIZED LINEAR MODEL . the proposed BAYESIAN INFERENCE APPROACH is based on a GENERALIZED LINEAR MODEL and a GENERALIZED LINEAR MODEL . the proposed BAYESIAN INFERENCE APPROACH is based on a GENERALIZED LINEAR MODEL and a GENERALIZED LINEAR MODEL . the proposed BAYESIAN INFERENCE APPROACH is based on a GENERALIZED LINEAR MODEL and a GENERALIZED LINEAR MODEL .\n",
            "\n",
            "488 1000\n",
            "a blending of PHONOLOGICAL CONCEPTS and TECHNICAL ANALYSIS is proposed to yield a better MODELING AND UNDERSTANDING OF PHONOLOGICAL PROCESSES . based on the manual segmentation and labeling of the ITALIAN CLIPS CORPUS we automatically derive a probabilistic set of PHONOLOGICAL PRONUNCIATION RULES : a new ALIGNMENT TECHNIQUE is used to map the PHONOLOGICAL FORM OF SPONTANEOUS SENTENCES onto the PHONETIC SURFACE FORM . a MACHINE-LEARNING ALGORITHM then calculates a set of PHONOLOGI-CAL REPLACEMENT RULES together with their CONDITIONAL PROBABILITIES . a critical analysis of the resulting PROBABILISTIC RULE SET is presented and discussed with regard to REGIONAL ITALIAN ACCENTS . the RULE set presented here is also applied in the newly published <unk> <unk> that allows a user to segment and phonetically label ITALIAN SPEECH via a simple WEB-INTERFACE . \n",
            "this paper presents a ALIGNMENT TECHNIQUE for MODELING AND UNDERSTANDING OF PHONOLOGICAL PROCESSES . the proposed ALIGNMENT TECHNIQUE is based on the ALIGNMENT TECHNIQUE and the ALIGNMENT TECHNIQUE . the proposed ALIGNMENT TECHNIQUE is based on the ALIGNMENT TECHNIQUE and the ALIGNMENT TECHNIQUE . the proposed ALIGNMENT TECHNIQUE is based on the ALIGNMENT TECHNIQUE and the MACHINE-LEARNING ALGORITHM . the proposed ALIGNMENT TECHNIQUE is based on the ALIGNMENT TECHNIQUE and the ALIGNMENT TECHNIQUE . the proposed ALIGNMENT TECHNIQUE is based on the ALIGNMENT TECHNIQUE and the ALIGNMENT TECHNIQUE . the proposed ALIGNMENT TECHNIQUE is based on the ALIGNMENT TECHNIQUE and the ALIGNMENT TECHNIQUE .\n",
            "\n",
            "489 1000\n",
            "this paper provides an analysis of the STEADY-STATE BEHAVIOR of the FILTERED-X AFFINE PROJECTION ALGORITHM . this efficient AFFINE PROJECTION ALGORITHM for ACTIVE NOISE CONTROL APPLICATIONS is based on the FILTERED-X SCHEME , unlike most AP ALGORITHMS based on the more COMPUTATIONALLY DEMANDING MODIFIED FILTERED-X SCHEME . this study depends on ENERGY CONSERVATION ARGUMENTS and does not require an specific SIGNAL DISTRIBUTION . the THEORETICAL EXPRESSIONS derived for the MEAN SQUARE ERROR allowed to accurately predict the steady-state performance of the AFFINE PROJECTION ALGORITHM for meaningful practical cases . simulation results of a SINGLE-CHANNEL ANC SYSTEM validate the analysis and the THEORETICAL EXPRESSIONS derived . \n",
            "this paper proposes a new COMPUTATIONALLY DEMANDING MODIFIED FILTERED-X SCHEME for ACTIVE NOISE CONTROL APPLICATIONS . the proposed FILTERED-X AFFINE PROJECTION ALGORITHM is based on a COMPUTATIONALLY DEMANDING MODIFIED FILTERED-X SCHEME . the proposed FILTERED-X AFFINE PROJECTION ALGORITHM is based on a COMPUTATIONALLY DEMANDING MODIFIED FILTERED-X SCHEME . the proposed AFFINE PROJECTION ALGORITHM is based on a COMPUTATIONALLY DEMANDING MODIFIED FILTERED-X SCHEME . the proposed COMPUTATIONALLY DEMANDING MODIFIED FILTERED-X SCHEME is based on a COMPUTATIONALLY DEMANDING MODIFIED FILTERED-X SCHEME . the proposed COMPUTATIONALLY DEMANDING MODIFIED FILTERED-X SCHEME is based on a COMPUTATIONALLY DEMANDING MODIFIED FILTERED-X SCHEME and is shown to be robust to STEADY-STATE BEHAVIOR .\n",
            "\n",
            "490 1000\n",
            "examples of FIGURATIVE LANGUAGE can range from the explicit and the obvious to the implicit and <unk> <unk> . some simpler forms , like SIMILE , often wear their meanings on their <unk> , while more challenging forms , like METAPHOR , can make CRYPTIC ALLUSIONS more akin to those of <unk> or CROSSWORD PUZZLES . in this paper we argue that because the same concepts and properties are described in either case , a COMPUTATIONAL AGENT can learn from the easy cases -lrb- explicit <unk> -rrb- how to comprehend and generate the HARD CASES -LRB- NON-EXPLICIT METAPHORS -rrb- . we demonstrate that the MARKED-NESS OF SIMILES allows for a large CASE-BASE of ILLUSTRATIVE EXAMPLES to be easily acquired from the web , and present a system , called <unk> , that uses this CASE-BASE both to understand PROPERTY-ATTRIBUTION METAPHORS and to generate <unk> <unk> for a given target on demand . in each case , we show how the text of the web is used as a source of TACIT KNOWLEDGE about what CATE-GORIZATIONS are allowable and what properties are most contextually appropriate . overall , we demonstrate that by using the web as a primary knowledge source , a system can achieve a robust and scalable competence with METAPHOR while minimizing the need for HAND-CRAFTED RESOURCES like WORDNET . \n",
            "this paper addresses the problem of MARKED-NESS OF SIMILES in FIGURATIVE LANGUAGE such as WORDNET . we propose a method for estimating the MARKED-NESS OF SIMILES from a FIGURATIVE LANGUAGE . the method is based on the MARKED-NESS OF SIMILES of the FIGURATIVE LANGUAGE . the method is based on the use of PROPERTY-ATTRIBUTION METAPHORS , such as WORDNET , and a COMPUTATIONAL AGENT . the proposed method is based on a COMPUTATIONAL AGENT of the FIGURATIVE LANGUAGE . the proposed method is compared with other state-of-the-art methods in terms of SIMILE such as WORDNET and WORDNET .\n",
            "\n",
            "491 1000\n",
            "in this paper , we tackle the problem of CO-LOCALIZATION in REAL-WORLD IMAGES . CO-LOCALIZATION is the problem of simultaneously localizing -lrb- with bounding boxes -rrb- objects of the same class across a set of distinct images . although similar problems such as CO-SEGMENTATION and WEAKLY SUPERVISED LOCALIZATION have been previously studied , we focus on being able to perform CO-LOCALIZATION in REAL-WORLD SETTINGS , which are typically characterized by large amounts of INTRA-CLASS VARIATION , INTER-CLASS DIVERSITY , and ANNOTATION NOISE . to address these issues , we present a JOINT IMAGE-BOX FORMULATION for solving the CO-LOCALIZATION PROBLEM , and show how JOINT IMAGE-BOX FORMULATION can be relaxed to a CONVEX QUADRATIC PROGRAM which can be efficiently solved . we perform an extensive evaluation of our JOINT IMAGE-BOX FORMULATION compared to previous state-of-the-art approaches on the challenging pascal voc 2007 and OBJECT DISCOVERY DATASETS . in addition , we also present a large-scale study of CO-LOCALIZATION on IMAGENET , involving GROUND-TRUTH ANNOTATIONS for <unk> classes and approximately 1 million images . \n",
            "this paper presents a new JOINT IMAGE-BOX FORMULATION for WEAKLY SUPERVISED LOCALIZATION . the JOINT IMAGE-BOX FORMULATION is based on the JOINT IMAGE-BOX FORMULATION and the JOINT IMAGE-BOX FORMULATION . the proposed JOINT IMAGE-BOX FORMULATION is based on the JOINT IMAGE-BOX FORMULATION and the JOINT IMAGE-BOX FORMULATION . the proposed JOINT IMAGE-BOX FORMULATION is based on the JOINT IMAGE-BOX FORMULATION and the JOINT IMAGE-BOX FORMULATION . the proposed JOINT IMAGE-BOX FORMULATION is based on the JOINT IMAGE-BOX FORMULATION and the JOINT IMAGE-BOX FORMULATION . the proposed JOINT IMAGE-BOX FORMULATION is based on the JOINT IMAGE-BOX FORMULATION and the JOINT IMAGE-BOX FORMULATION . experimental results show that the proposed JOINT IMAGE-BOX FORMULATION is robust and robust to ANNOTATION NOISE and ANNOTATION NOISE .\n",
            "\n",
            "492 1000\n",
            "vector taylor series -lrb- vts -rrb- model based compensation is a powerful approach for NOISE ROBUST SPEECH RECOGNITION . an important extension to this approach is VTS ADAPTIVE TRAINING , which allows CANONICAL MODELS to be estimated on DIVERSE NOISE-DEGRADED TRAINING DATA . these CANONICAL MODELS can be estimated using EM-BASED APPROACHES , allowing simple extensions to DISCRIMINATIVE VAT . however to ensure a DIAGONAL CORRUPTED SPEECH COVARIANCE MATRIX the JACO-BIAN -LRB- LOADING MATRIX -rrb- relating the NOISE and CLEAN SPEECH is <unk> . in this work an approach for yielding optimal DIAGONAL LOADING MATRICES based on minimising the expected KL-DIVERGENCE between the DIAGONAL LOADING MATRIX and '' correct '' distributions is proposed . the performance of DISCRIMINATIVE VAT using the standard and OPTIMAL DIAGONALISATION was evaluated on both IN-CAR COLLECTED DATA and the AURORA4 TASK . \n",
            "this paper addresses the problem of NOISE ROBUST SPEECH RECOGNITION in CLEAN SPEECH . we propose a method for NOISE ROBUST SPEECH RECOGNITION based on CANONICAL MODELS . the proposed method is based on the JACO-BIAN -LRB- LOADING MATRIX and the JACO-BIAN -LRB- LOADING MATRIX . the proposed method is based on the JACO-BIAN -LRB- LOADING MATRIX and the DIAGONAL LOADING MATRIX . the proposed method is based on the DIAGONAL LOADING MATRIX and the JACO-BIAN -LRB- LOADING MATRIX . the proposed method is evaluated on the AURORA4 TASK and on the AURORA4 TASK . the experimental results show that the proposed method outperforms the conventional EM-BASED APPROACHES in terms of NOISE and NOISE .\n",
            "\n",
            "493 1000\n",
            "we propose a STATISTICAL FORMULATION for 2-D HUMAN POSE ESTIMATION from SINGLE IMAGES . the HUMAN BODY CONFIGURATION is modeled by a MARKOV NETWORK and the ESTIMATION PROBLEM is to infer pose parameters from IMAGE CUES such as APPEARANCE , SHAPE , EDGE , and color . from a set of HAND LABELED IMAGES , we accumulate prior knowledge of 2-D BODY SHAPES by learning their LOW-DIMENSIONAL REPRESENTATIONS for inference of pose parameters . a DATA DRIVEN BELIEF PROPAGATION MONTE CARLO ALGORITHM , utilizing IMPORTANCE SAMPLING FUNCTIONS built from BOTTOM-UP VISUAL CUES , is proposed for efficient PROBABILISTIC INFERENCE . contrasted to the few sequential statistical formulations in the literature , our STATISTICAL FORMULATION integrates both top-down as well as BOTTOM-UP REASONING MECHANISMS , and can carry out the INFERENCE TASKS in parallel . experimental results demonstrate the <unk> and effectiveness of the proposed STATISTICAL FORMULATION in ESTIMATING 2-D HUMAN POSE from SINGLE IMAGES . \n",
            "this paper presents a novel STATISTICAL FORMULATION for ESTIMATING 2-D HUMAN POSE . the DATA DRIVEN BELIEF PROPAGATION MONTE CARLO ALGORITHM is based on a STATISTICAL FORMULATION and a STATISTICAL FORMULATION . the ESTIMATION PROBLEM is formulated as a MARKOV NETWORK and a MARKOV NETWORK . the ESTIMATION PROBLEM is formulated as a MARKOV NETWORK and a MARKOV NETWORK . the ESTIMATION PROBLEM is formulated as a ESTIMATION PROBLEM . the ESTIMATION PROBLEM is formulated as a ESTIMATION PROBLEM . the ESTIMATION PROBLEM is formulated as a MARKOV NETWORK . the proposed DATA DRIVEN BELIEF PROPAGATION MONTE CARLO ALGORITHM is applied to the ESTIMATION PROBLEM . the experimental results show that the proposed DATA DRIVEN BELIEF PROPAGATION MONTE CARLO ALGORITHM is effective for ESTIMATING 2-D HUMAN POSE .\n",
            "\n",
            "494 1000\n",
            "several paradigms for HIGH-LEVEL MUSIC DESCRIPTIONS have been proposed to develop effective system for browsing and retrieving MUSICAL CONTENT in large repositories . such paradigms are based on either CATEGORICAL OR DIMENSIONAL MODELS . the interest in DIMENSIONAL MODELS has recently grown a great deal , as they define a SEMANTIC RELATION between concepts through GRADED DESCRIPTIONS . one problem that affects SEMANTIC DESCRIPTIONS is the ambiguity that often arises from using the same descriptor in different contexts . in order to overcome this difficulty , it is important to DIMENSIONAL CONTEXTUAL SEMANTIC MODEL and address POLYSEMY , which is the property of words to take on different meanings depending on the <unk> . in this paper we propose a DIMENSIONAL CONTEXTUAL SEMANTIC MODEL for defining SEMANTIC RELATIONS among descriptors in a CONTEXT-AWARE FASHION . this DIMENSIONAL CONTEXTUAL SEMANTIC MODEL is here used for developing a SEMANTIC MUSIC SEARCH ENGINE . in order to evaluate the effectiveness of our DIMENSIONAL CONTEXTUAL SEMANTIC MODEL , we compare this DIMENSIONAL CONTEXTUAL SEMANTIC MODEL with two systems that are based on different DESCRIPTION MODELS . \n",
            "this paper presents a novel DIMENSIONAL CONTEXTUAL SEMANTIC MODEL for HIGH-LEVEL MUSIC DESCRIPTIONS . the DIMENSIONAL CONTEXTUAL SEMANTIC MODEL is based on the DIMENSIONAL CONTEXTUAL SEMANTIC MODEL . the DIMENSIONAL CONTEXTUAL SEMANTIC MODEL is based on the DIMENSIONAL CONTEXTUAL SEMANTIC MODEL . the DIMENSIONAL CONTEXTUAL SEMANTIC MODEL is based on the DIMENSIONAL CONTEXTUAL SEMANTIC MODEL . the proposed DIMENSIONAL CONTEXTUAL SEMANTIC MODEL is based on the DIMENSIONAL CONTEXTUAL SEMANTIC MODEL . the proposed DIMENSIONAL CONTEXTUAL SEMANTIC MODEL is based on the DIMENSIONAL CONTEXTUAL SEMANTIC MODEL . the proposed DIMENSIONAL CONTEXTUAL SEMANTIC MODEL is based on the DIMENSIONAL CONTEXTUAL SEMANTIC MODEL . the proposed DIMENSIONAL CONTEXTUAL SEMANTIC MODEL is based on the DIMENSIONAL CONTEXTUAL SEMANTIC MODEL . the proposed DIMENSIONAL CONTEXTUAL SEMANTIC MODEL is based on a DIMENSIONAL CONTEXTUAL SEMANTIC MODEL and is shown to be useful for HIGH-LEVEL MUSIC DESCRIPTIONS .\n",
            "\n",
            "495 1000\n",
            "the paper addresses LANGUAGE MODEL ADAPTATION for AUTOMATIC LECTURE TRANSCRIPTION by fully exploiting PRESENTATION SLIDE INFORMATION used in the lecture . as the text in the presentation slides is small in its size and <unk> in its content , a ROBUST ADAPTATION SCHEME is addressed by focusing on the KEYWORD AND TOPIC INFORMATION . several methods are investigated and combined ; first , GLOBAL TOPIC ADAPTATION is conducted based on PLSA -LRB- PROBABILISTIC LATENT SEMANTIC ANALYSIS -rrb- using KEYWORDS appearing in all slides . WEB TEXT is also retrieved to enhance the relevant text . then , LOCAL PREFERENCE of the KEYWORDS are reflected with a CACHE MODEL by referring to the slide used during each utterance . experimental evaluations on REAL LECTURES show that the proposed method combining the GLOBAL AND LOCAL SLIDE INFORMATION achieves a significant improvement of RECOGNITION ACCURACY , especially in the DETECTION RATE OF CONTENT KEYWORDS . \n",
            "this paper addresses the problem of AUTOMATIC LECTURE TRANSCRIPTION in WEB TEXT . we propose a ROBUST ADAPTATION SCHEME based on a CACHE MODEL . the proposed method is based on the DETECTION RATE OF CONTENT KEYWORDS of the WEB TEXT . the proposed method is based on the DETECTION RATE OF CONTENT KEYWORDS of the WEB TEXT . the proposed method is based on the PLSA -LRB- PROBABILISTIC LATENT SEMANTIC ANALYSIS . the proposed method is based on the PLSA -LRB- PROBABILISTIC LATENT SEMANTIC ANALYSIS . the proposed method is based on the PLSA -LRB- PROBABILISTIC LATENT SEMANTIC ANALYSIS . the proposed method is based on a ROBUST ADAPTATION SCHEME . the proposed method is based on the DETECTION RATE OF CONTENT KEYWORDS of the WEB TEXT .\n",
            "\n",
            "496 1000\n",
            "we present a new DISCRIMINATIVE FEATURE transform approach to LARGE VOCABULARY CONTINUOUS SPEECH RECOGNITION using gaussian mixture density hidden markov models -lrb- GMM-HMMS -rrb- for ACOUSTIC MODELING . the feature transform is formulated with a set of CONTEXT-EXPANDED REGION-DEPENDENT LINEAR TRANSFORMS utilizing both LONG-SPAN FEATURES and CONTEXTUAL WEIGHT EXPANSION . the CONTEXT-EXPANDED REGION-DEPENDENT LINEAR TRANSFORMS are estimated by LATTICE-FREE , TIED-STATE BASED DISCRIMINATIVE TRAINING using MAXIMUM MUTUAL INFORMATION CRITERION , while the GMM-HMMS are trained by conventional LATTICE-BASED , BOOSTED MMI TRAINING . compared with two baseline systems , which use CONTEXT-EXPANDED REGION-DEPENDENT LINEAR TRANSFORMS with either LONG-SPAN FEATURES or WEIGHT EXPANSION only and are trained using the conventional LATTICE-BASED DISCRIMINATIVE TRAINING for both CONTEXT-EXPANDED REGION-DEPENDENT LINEAR TRANSFORMS and HMMS , the proposed approach achieves a RELATIVE WORD ERROR RATE REDUCTION of 10 % and 6 % respectively on SWITCHBOARD-1 CONVERSATIONAL TELEPHONE SPEECH TRANSCRIPTION TASK . \n",
            "this paper addresses the problem of LARGE VOCABULARY CONTINUOUS SPEECH RECOGNITION in LARGE VOCABULARY CONTINUOUS SPEECH RECOGNITION . in this paper , we propose a new DISCRIMINATIVE FEATURE based on the MAXIMUM MUTUAL INFORMATION CRITERION and the MAXIMUM MUTUAL INFORMATION CRITERION . the proposed DISCRIMINATIVE FEATURE is based on the MAXIMUM MUTUAL INFORMATION CRITERION and the MAXIMUM MUTUAL INFORMATION CRITERION . the proposed DISCRIMINATIVE FEATURE is based on the MAXIMUM MUTUAL INFORMATION CRITERION and the MAXIMUM MUTUAL INFORMATION CRITERION . experimental results show that the proposed method outperforms the conventional LATTICE-FREE , TIED-STATE BASED DISCRIMINATIVE TRAINING and the LATTICE-FREE , TIED-STATE BASED DISCRIMINATIVE TRAINING .\n",
            "\n",
            "497 1000\n",
            "this paper presents a CLOSED FORM RECURSIVE SOLUTION for training ADAPTIVE FILTERS using the MAXIMUM CORRENTROPY CRITERION . CORRENTROPY has been recently proposed as a robust similarity measure between two RANDOM VARIABLES or signals , when the PDF S involved are heavy <unk> and NON-GAUSSIAN . maximizing the CROSS-CORRENTROPY between the output of an ADAPTIVE FILTERS and the desired response leads to the MAXIMUM CORRENTROPY CRITERION for ADAPTIVE SYSTEMS TRAINING . we show that a CLOSED FORM , CLOSED FORM RECURSIVE SOLUTION of the FILTER WEIGHTS using this CLOSED FORM yields a simple WEIGHTED LEAST SQUARES like formulation . our simulations show that training the FILTER WEIGHTS using this CLOSED FORM RECURSIVE SOLUTION is much faster than GRADIENT BASED TRAINING , and more accurate than the RLS ALGORITHM in cases where the ERROR PDF is NON-GAUSSIAN and heavy <unk> . \n",
            "this paper proposes a new RLS ALGORITHM for ADAPTIVE SYSTEMS TRAINING . the proposed RLS ALGORITHM is based on the MAXIMUM CORRENTROPY CRITERION . the proposed RLS ALGORITHM is based on the MAXIMUM CORRENTROPY CRITERION . the proposed RLS ALGORITHM is based on the MAXIMUM CORRENTROPY CRITERION and the RLS ALGORITHM . experimental results show that the proposed RLS ALGORITHM outperforms the conventional RLS ALGORITHM in terms of ERROR PDF .\n",
            "\n",
            "498 1000\n",
            "we resolve a <unk> old open question in VISIBILITY-BASED PURSUIT EVASION : how many pursuers are needed to capture an evader in an ARBITRARY POLYGONAL ENVIRONMENT with obstacles ? the evader is assumed to be adversarial , moves with the same MAXIMUM SPEED as pursuers , and is '' sensed '' by a <unk> only when it lies in <unk> of that <unk> . the players move in DISCRETE TIME STEPS , and the capture occurs when a <unk> reaches the position of the evader on its move . our main result is that o -lrb- √ h + log n -rrb- pursuers can always win the game with a DETERMINISTIC SEARCH STRATEGY in any <unk> with n vertices and h obstacles -lrb- holes -rrb- . in order to achieve this bound , however , we argue that the environment must satisfy a MINIMUM FEATURE SIZE PROPERTY , which essentially requires the MINIMUM DISTANCE between any two vertices to be of the same order as the speed of the players . without the MINIMUM FEATURE SIZE ASSUMPTION , we show that ω -lrb- n / log n -rrb- pursuers are needed in the worst-case even for SIMPLY-CONNECTED POLYGONS OF N VERTICES ! this reveals an unexpected <unk> that seems to have been overlooked in previous work <unk> that O -LRB- LOG N -RRB- PURSUERS can always win in SIMPLY-CONNECTED N-GONS . our lower bound also shows that capturing an evader is inherently more difficult than just '' seeing '' it because O -LRB- LOG N -RRB- PURSUERS are prov-ably sufficient for LINE-OF-SIGHT DETECTION even against an arbitrarily fast evader in simple <unk> . \n",
            "this paper presents a new method for LINE-OF-SIGHT DETECTION based on a DETERMINISTIC SEARCH STRATEGY . the proposed method is based on a DETERMINISTIC SEARCH STRATEGY , which is based on the MINIMUM FEATURE SIZE ASSUMPTION . the proposed method is based on a DETERMINISTIC SEARCH STRATEGY . the proposed method is based on a DETERMINISTIC SEARCH STRATEGY . the proposed method is based on a DETERMINISTIC SEARCH STRATEGY . the proposed method is based on a DETERMINISTIC SEARCH STRATEGY . the proposed method is compared with the conventional DETERMINISTIC SEARCH STRATEGY .\n",
            "\n",
            "499 1000\n",
            "this work introduces a modified <unk> multiple to multiple EM-DRIVEN ALIGNMENT ALGORITHM for GRAPHEME-TO-PHONEME CONVERSION , and preliminary experimental results applying a RECURRENT NEURAL NETWORK LANGUAGE MODEL as an N-BEST RESCORING MECHANISM for G2P CONVERSION . the ALIGNMENT ALGORITHM leverages the WFST-BASED G2P FRAMEWORK and introduces several simple STRUCTURAL CONSTRAINTS which yield a small but consistent improvement in WORD ACCURACY on a selection of standard base-lines . the RECURRENT NEURAL NETWORK LANGUAGE MODEL further extends these gains and achieves state-of-the-art performance on four standard G2P DATASETS . the system is also shown to be significantly faster than existing solutions . finally , the complete WFST-BASED G2P FRAMEWORK is provided as an OPEN-SOURCE TOOLKIT . \n",
            "this paper proposes a RECURRENT NEURAL NETWORK LANGUAGE MODEL for GRAPHEME-TO-PHONEME CONVERSION . the RECURRENT NEURAL NETWORK LANGUAGE MODEL is based on a RECURRENT NEURAL NETWORK LANGUAGE MODEL . the proposed RECURRENT NEURAL NETWORK LANGUAGE MODEL is based on a RECURRENT NEURAL NETWORK LANGUAGE MODEL , which is based on the OPEN-SOURCE TOOLKIT . the proposed RECURRENT NEURAL NETWORK LANGUAGE MODEL is based on the OPEN-SOURCE TOOLKIT . the proposed EM-DRIVEN ALIGNMENT ALGORITHM is based on the OPEN-SOURCE TOOLKIT . the proposed RECURRENT NEURAL NETWORK LANGUAGE MODEL is evaluated on the G2P DATASETS . the proposed RECURRENT NEURAL NETWORK LANGUAGE MODEL is compared with the conventional OPEN-SOURCE TOOLKIT .\n",
            "\n",
            "500 1000\n",
            "the INTERVAL ALGEBRA and a subset of the REGION CONNECTION CALCULUS , namely RCC-8 , are the dominant ARTIFICIAL INTELLIGENCE APPROACHES for representing and reasoning about QUALITATIVE TEMPORAL AND TOPOLOGICAL RELATIONS respectively . such QUALITATIVE TEMPORAL AND TOPOLOGICAL RELATIONS can be formulated as a QUALITATIVE CONSTRAINT NETWORK . in this paper , we focus on the MINIMAL LABELING PROBLEM and we propose an algorithm to efficiently derive all the feasible base relations of a QUALITATIVE CONSTRAINT NETWORK . our algorithm considers CHORDAL QCNS and a new form of PARTIAL CONSISTENCY which we define as ◆ G-CONSISTENCY . further , the proposed algorithm uses tractable subclasses of relations having a specific PATCHWORK PROPERTY for <unk> implies the consistency of the input QUALITATIVE CONSTRAINT NETWORK . <unk> with QUALITATIVE CONSTRAINT NETWORK of INTERVAL ALGEBRA and RCC-8 show the importance and efficiency of this new approach . \n",
            "this paper presents a new method for QUALITATIVE TEMPORAL AND TOPOLOGICAL RELATIONS , such as RCC-8 , a QUALITATIVE CONSTRAINT NETWORK and a QUALITATIVE CONSTRAINT NETWORK . the proposed method is based on a QUALITATIVE CONSTRAINT NETWORK and a QUALITATIVE CONSTRAINT NETWORK . the proposed method is based on the use of a QUALITATIVE CONSTRAINT NETWORK and a QUALITATIVE CONSTRAINT NETWORK . the proposed method is compared with other ARTIFICIAL INTELLIGENCE APPROACHES such as ◆ G-CONSISTENCY , RCC-8 , and ◆ G-CONSISTENCY .\n",
            "\n",
            "501 1000\n",
            "many state-of-the-art OPTICAL FLOW ESTIMATION ALGORITHMS optimize the DATA AND REGULARIZATION TERMS to solve ILL-POSED PROBLEMS . in this paper , in contrast to the conventional OPTICAL FLOW FRAMEWORK that uses a SINGLE OR FIXED DATA MODEL , we study a novel framework that employs LOCALLY VARYING DATA TERM that adaptively combines different multiple types of DATA MODELS . the locally adaptive data term greatly reduces the MATCHING AMBIGUITY due to the complementary nature of the MULTIPLE DATA MODELS . the optimal number of COMPLEMENTARY DATA MODELS is learnt by minimizing the redundancy among them under the MINIMUM DESCRIPTION LENGTH CONSTRAINT . from these chosen DATA MODELS , a new OPTICAL FLOW ESTIMATION ENERGY MODEL is designed with the weighted sum of the MULTIPLE DATA MODELS , and a convex <unk> highly effective and practical solution that finds the OPTICAL FLOW , as well as the weights is proposed . comparative experimental results on the MIDDLEBURY OPTICAL FLOW BENCHMARK show that the proposed method using the COMPLEMENTARY DATA MODELS outperforms the STATE-OF-THE ART METHODS . \n",
            "this paper addresses the problem of ILL-POSED PROBLEMS in ILL-POSED PROBLEMS . we propose a novel OPTICAL FLOW ESTIMATION ENERGY MODEL based on the MINIMUM DESCRIPTION LENGTH CONSTRAINT . the proposed OPTICAL FLOW ESTIMATION ALGORITHMS is based on the MINIMUM DESCRIPTION LENGTH CONSTRAINT . the proposed OPTICAL FLOW ESTIMATION ENERGY MODEL is based on the LOCALLY VARYING DATA TERM and the MINIMUM DESCRIPTION LENGTH CONSTRAINT . the proposed OPTICAL FLOW ESTIMATION ALGORITHMS is compared with other STATE-OF-THE ART METHODS . the proposed OPTICAL FLOW ESTIMATION ALGORITHMS is compared with other STATE-OF-THE ART METHODS . the proposed OPTICAL FLOW ESTIMATION ALGORITHMS is compared with other STATE-OF-THE ART METHODS .\n",
            "\n",
            "502 1000\n",
            "we propose a method for extracting SEMANTIC ORIENTATIONS OF WORDS : desirable or undesirable . regarding SEMANTIC ORI-ENTATIONS as SPINS OF ELECTRONS , we use the MEAN FIELD APPROXIMATION to compute the APPROXIMATE PROBABILITY FUNCTION of the system instead of the INTRACTABLE ACTUAL PROBABILITY FUNCTION . we also propose a criterion for PARAMETER SELECTION on the basis of MAGNETIZATION . given only a small number of SEED WORDS , the proposed method extracts SEMANTIC ORIENTA-TIONS with high ACCURACY in the experiments on ENGLISH LEXICON . the result is comparable to the best value ever reported . \n",
            "this paper presents a method for PARAMETER SELECTION from a ENGLISH LEXICON . the method is based on a SPINS OF ELECTRONS , which is a SPINS OF ELECTRONS . the proposed method is based on a SPINS OF ELECTRONS , which is a SPINS OF ELECTRONS . the proposed method is based on a SPINS OF ELECTRONS , which is based on the MEAN FIELD APPROXIMATION . the proposed method is based on a SPINS OF ELECTRONS , which is based on the MEAN FIELD APPROXIMATION . the method is based on the MEAN FIELD APPROXIMATION . the proposed method is compared with the conventional MEAN FIELD APPROXIMATION .\n",
            "\n",
            "503 1000\n",
            "in this paper we consider the problem of CLASSIFICATION in the presence of PAIRWISE CONSTRAINTS , which consist of pairs of examples as well as a BINARY VARIABLE indicating whether they belong to the same class or not . we propose a method which can effectively utilize PAIRWISE CONSTRAINTS to construct an estimator of the DECISION BOUNDARY , and we show that the resulting estimator is <unk> consistent with respect to the OPTIMAL LINEAR DECISION BOUNDARY . we also study the ASYMPTOTIC VARIANCE of the estimator and extend the method to handle both LABELED AND PAIRWISE EXAMPLES in a natural way . several experiments on SIMULATED DATASETS and REAL WORLD CLASSIFICATION DATASETS are conducted . the results not only verify the theoretical properties of the proposed method but also demonstrate its practical value in applications . \n",
            "this paper addresses the problem of CLASSIFICATION in the presence of CLASSIFICATION and CLASSIFICATION . we propose a method for CLASSIFICATION based on PAIRWISE CONSTRAINTS and PAIRWISE CONSTRAINTS . the proposed method is based on the ASYMPTOTIC VARIANCE and the ASYMPTOTIC VARIANCE . experimental results show the effectiveness of the proposed method in terms of CLASSIFICATION and CLASSIFICATION .\n",
            "\n",
            "504 1000\n",
            "we propose a simple GENERATIVE , SYNTACTIC LANGUAGE MODEL that conditions on OVERLAPPING WINDOWS OF TREE CONTEXT -lrb- or <unk> -rrb- in the same way that N-GRAM LANGUAGE MODELS condition on OVERLAPPING WINDOWS OF LINEAR CONTEXT . we estimate the parameters of our GENERATIVE , SYNTACTIC LANGUAGE MODEL by collecting counts from AUTOMATICALLY PARSED TEXT using standard N-GRAM LANGUAGE MODEL ESTIMATION TECHNIQUES , allowing us to train a GENERATIVE , SYNTACTIC LANGUAGE MODEL on over one billion tokens of data using a single machine in a matter of hours . we evaluate on perplexity and a range of GRAMMATICALITY TASKS , and find that we perform as well or better than N-GRAM MODELS and other GENERATIVE BASELINES . our GENERATIVE , SYNTACTIC LANGUAGE MODEL even competes with state-of-the-art DISCRIMINATIVE MODELS <unk> for the GRAMMATICALITY TASKS , despite training on POSITIVE DATA alone . we also show fluency improvements in a preliminary machine translation experiment . \n",
            "this paper presents a novel GENERATIVE , SYNTACTIC LANGUAGE MODEL for AUTOMATICALLY PARSED TEXT . the GENERATIVE , SYNTACTIC LANGUAGE MODEL is based on a GENERATIVE , SYNTACTIC LANGUAGE MODEL and a GENERATIVE , SYNTACTIC LANGUAGE MODEL for AUTOMATICALLY PARSED TEXT . the proposed GENERATIVE , SYNTACTIC LANGUAGE MODEL is based on a GENERATIVE , SYNTACTIC LANGUAGE MODEL and a GENERATIVE , SYNTACTIC LANGUAGE MODEL for AUTOMATICALLY PARSED TEXT . the proposed GENERATIVE , SYNTACTIC LANGUAGE MODEL is compared with other N-GRAM LANGUAGE MODEL ESTIMATION TECHNIQUES and N-GRAM LANGUAGE MODEL ESTIMATION TECHNIQUES .\n",
            "\n",
            "505 1000\n",
            "continuous speech input for ASR PROCESSING is usually <unk> into speech <unk> by pauses . in this paper , we propose that smaller , PROSODICALLY DEFINED UNITS can be identified by tackling the problem on IMBALANCED PROSODIC UNIT BOUNDARY DETECTION using five MACHINE LEARNING TECHNIQUES . a parsimonious set of LINGUISTICALLY MOTIVATED PROSODIC FEATURES has been proven to be useful to characterize PROSODIC BOUNDARY INFORMATION . furthermore , BMPM is prone to have true positive rate on the MINORITY CLASS , i.e. the DEFINED PROSODIC UNITS . as a whole , the DECISION TREE CLASSIFIER , C4 .5 , reaches a more stable performance than the other algorithms . \n",
            "this paper addresses the problem of IMBALANCED PROSODIC UNIT BOUNDARY DETECTION in ASR PROCESSING . in this paper , we propose a new DECISION TREE CLASSIFIER based on LINGUISTICALLY MOTIVATED PROSODIC FEATURES . the proposed MACHINE LEARNING TECHNIQUES is based on the PROSODIC BOUNDARY INFORMATION of the DEFINED PROSODIC UNITS . the proposed MACHINE LEARNING TECHNIQUES is based on a DECISION TREE CLASSIFIER of the MINORITY CLASS . the proposed method is based on a DECISION TREE CLASSIFIER , which is based on a DECISION TREE CLASSIFIER .\n",
            "\n",
            "506 1000\n",
            "we investigate the problem of learning the STRUCTURE of an ARTICULATED OBJECT , i.e. its KINEMATIC CHAIN , from FEATURE TRA-JECTORIES under AFFINE PROJECTIONS . we demonstrate this possibility by proposing an algorithm which first segments the trajectories by LOCAL SAMPLING and SPECTRAL CLUSTERING , then builds the KINEMATIC CHAIN as a MINIMUM SPANNING TREE of a GRAPH constructed from the SEGMENTED MOTION SUBSPACES . we test our method in challenging DATA SETS and demonstrate the ability to automatically build the KINEMATIC CHAIN of an ARTICULATED OBJECT from FEATURE TRAJECTORIES . the algorithm also works when there are multiple ARTICULATED OBJECTS in the scene . furthermore , we take into account NON-RIGID ARTICULATED PARTS that exist in HUMAN MOTIONS . we believe this advance will have impact on ARTICULATED OBJECT TRACKING and dynamical STRUCTURE from motion . \n",
            "this paper presents a method for ARTICULATED OBJECT TRACKING from DATA SETS . the method is based on the use of AFFINE PROJECTIONS and LOCAL SAMPLING . the method is based on LOCAL SAMPLING and LOCAL SAMPLING . the method is based on LOCAL SAMPLING and LOCAL SAMPLING . the method is based on LOCAL SAMPLING and LOCAL SAMPLING . the method is based on LOCAL SAMPLING and LOCAL SAMPLING . the method is based on LOCAL SAMPLING and LOCAL SAMPLING . experimental results show that the proposed method outperforms the conventional method in terms of HUMAN MOTIONS and HUMAN MOTIONS .\n",
            "\n",
            "507 1000\n",
            "in this paper we introduce CONTEXT-SENSITIVE DECISION FORESTS-A NEW PERSPECTIVE to exploit CONTEXTUAL INFORMATION in the popular DECISION FOREST FRAMEWORK for the OBJECT DETECTION PROBLEM . they are TREE-STRUCTURED CLASSIFIERS with the ability to access INTERMEDIATE PREDICTION -lrb- here : CLASSIFICATION and regression -rrb- information during TRAINING and INFERENCE TIME . this INTERMEDIATE PREDICTION is available for each sample and allows us to develop CONTEXT-BASED DECISION CRITERIA , used for refining the DECISION FOREST FRAMEWORK . in addition , we introduce a novel SPLIT CRITERION which in combination with a priority based way of constructing the TREES , allows more accurate REGRESSION MODE SELECTION and hence improves the current context information . in our experiments , we demonstrate improved results for the task of PEDESTRIAN DETECTION on the challenging TUD DATA SET when compared to state-of-the-art methods . \n",
            "this paper presents a novel DECISION FOREST FRAMEWORK for PEDESTRIAN DETECTION . the DECISION FOREST FRAMEWORK is based on the SPLIT CRITERION of the TUD DATA SET . the DECISION FOREST FRAMEWORK is based on the SPLIT CRITERION . the proposed DECISION FOREST FRAMEWORK is based on the SPLIT CRITERION . the proposed DECISION FOREST FRAMEWORK is based on the SPLIT CRITERION of the TUD DATA SET . the proposed DECISION FOREST FRAMEWORK is based on the SPLIT CRITERION . the proposed DECISION FOREST FRAMEWORK is applied to the OBJECT DETECTION PROBLEM . the experimental results show the effectiveness of the proposed DECISION FOREST FRAMEWORK for PEDESTRIAN DETECTION .\n",
            "\n",
            "508 1000\n",
            "learning semantic representations and TREE STRUCTURES of BILINGUAL PHRASES is beneficial for STATISTICAL MACHINE TRANSLATION . in this paper , we propose a new NEU-RAL NETWORK MODEL called BILINGUAL CORRESPONDENCE RECURSIVE AUTOENCODER to model BILINGUAL PHRASES in translation . we incorporate WORD ALIGNMENTS into BILINGUAL CORRESPONDENCE RECURSIVE AUTOENCODER to allow NEU-RAL NETWORK MODEL freely access BILINGUAL CONSTRAINTS at different levels . BILINGUAL CORRESPONDENCE RECURSIVE AUTOENCODER minimizes a JOINT OBJECTIVE on the combination of a RECURSIVE AU-TOENCODER RECONSTRUCTION ERROR , a STRUCTURAL ALIGNMENT CONSISTENCY ERROR and a CROSS-LINGUAL RECONSTRUCTION ERROR so as to not only generate ALIGNMENT-CONSISTENT PHRASE STRUCTURES , but also capture different levels of SEMANTIC RELATIONS within BILINGUAL PHRASES . in order to examine the effectiveness of BILINGUAL CORRESPONDENCE RECURSIVE AUTOENCODER , we incorporate both SEMANTIC AND STRUCTURAL SIMILARITY FEATURES built on BILINGUAL PHRASE REPRESENTATIONS and TREE STRUCTURES learned by BILINGUAL CORRESPONDENCE RECURSIVE AUTOENCODER into a state-of-the-art SMT SYSTEM . experiments on NIST CHINESE-ENGLISH TEST SETS show that our NEU-RAL NETWORK MODEL achieves a substantial improvement of up to <unk> bleu points over the baseline . \n",
            "this paper presents a BILINGUAL CORRESPONDENCE RECURSIVE AUTOENCODER for STATISTICAL MACHINE TRANSLATION . the NEU-RAL NETWORK MODEL is based on the BILINGUAL CORRESPONDENCE RECURSIVE AUTOENCODER and the BILINGUAL CORRESPONDENCE RECURSIVE AUTOENCODER . the BILINGUAL CORRESPONDENCE RECURSIVE AUTOENCODER is based on the BILINGUAL CORRESPONDENCE RECURSIVE AUTOENCODER and the BILINGUAL CORRESPONDENCE RECURSIVE AUTOENCODER . the proposed BILINGUAL CORRESPONDENCE RECURSIVE AUTOENCODER is based on the BILINGUAL CORRESPONDENCE RECURSIVE AUTOENCODER and the BILINGUAL CORRESPONDENCE RECURSIVE AUTOENCODER . the proposed NEU-RAL NETWORK MODEL is based on the BILINGUAL CORRESPONDENCE RECURSIVE AUTOENCODER and the BILINGUAL CORRESPONDENCE RECURSIVE AUTOENCODER . the proposed BILINGUAL CORRESPONDENCE RECURSIVE AUTOENCODER is evaluated on NIST CHINESE-ENGLISH TEST SETS and NIST CHINESE-ENGLISH TEST SETS .\n",
            "\n",
            "509 1000\n",
            "in this paper , we present a novel approach for DIALOG MODELING , which extends the idea underlying the partially observable markov decision processes -lrb- pomdps -rrb- , i. e. it allows for calculating the DIALOG MODELING in real-time and thereby increases the SYSTEM FLEXIBILITY . the use of STATISTICAL DIALOG MODELS is particularly advantageous to react adequately to common errors of SPEECH RECOGNITION SYSTEMS . comparing our results to the REFERENCE SYSTEM , we achieve a relative reduction of <unk> % of the AVERAGE DIALOG LENGTH . furthermore , the proposed system shows a relative enhancement of <unk> % of the SENSITIVITY RATE in the ERROR RECOGNITION CAPABILITIES using the same SPECIFITY RATE in both systems . the achieved results are based on the AIR TRAVELLING INFORMATION SYSTEM with 21 <unk> USER UTTERANCES in 1 <unk> NATURAL SPOKEN DIALOGS . \n",
            "this paper presents a new method for DIALOG MODELING in NATURAL SPOKEN DIALOGS . the proposed method is based on the AIR TRAVELLING INFORMATION SYSTEM . the proposed method is based on the AIR TRAVELLING INFORMATION SYSTEM . the proposed method is based on the AVERAGE DIALOG LENGTH of the AIR TRAVELLING INFORMATION SYSTEM and the SENSITIVITY RATE of the REFERENCE SYSTEM . the proposed method is compared with the conventional REFERENCE SYSTEM in terms of SENSITIVITY RATE and SENSITIVITY RATE .\n",
            "\n",
            "510 1000\n",
            "this paper describes a DISCRIMINATIVE APPROACH that further advances the framework for WEIGHTED FINITE STATE TRANSDUCER based decoding . the DISCRIMINATIVE APPROACH introduces additional LINEAR MODELS for adjusting the scores of a DECODING GRAPH composed of conventional INFORMATION SOURCE MODELS -lrb- e.g. , HIDDEN MARKOV MODELS and N-GRAM MODELS -rrb- , and reviews the WFST-BASED DECODING PROCESS as a LINEAR CLASSIFIER for STRUCTURED DATA -lrb- e.g. , SEQUENTIAL MULTICLASS DATA -rrb- . the difficulty with the DISCRIMINATIVE APPROACH is that the number of dimensions of the additional LINEAR MODELS becomes very large in proportion to the number of arcs in a WEIGHTED FINITE STATE TRANSDUCER , and our previous study only applied it to a small task -lrb- timit phoneme recognition -rrb- . this paper proposes a TRAINING METHOD for a LARGE-SCALE LINEAR CLASSIFIER employed in WFST-BASED DECODING by using a DISTRIBUTED PERCEPTRON ALGORITHM . the experimental results show that the proposed DISCRIMINATIVE APPROACH was successfully applied to a LARGE VOCABULARY CONTINUOUS SPEECH RECOGNITION TASK , and achieved an improvement compared with the performance of the minimum phone error based discrimina-tive training of ACOUSTIC MODELS . \n",
            "this paper presents a DISCRIMINATIVE APPROACH for WFST-BASED DECODING . the DISTRIBUTED PERCEPTRON ALGORITHM is based on a LARGE-SCALE LINEAR CLASSIFIER , which is a LINEAR CLASSIFIER for WFST-BASED DECODING . the TRAINING METHOD is based on the DISTRIBUTED PERCEPTRON ALGORITHM and the TRAINING METHOD . the TRAINING METHOD is based on the TRAINING METHOD and the TRAINING METHOD . the TRAINING METHOD is based on the DISTRIBUTED PERCEPTRON ALGORITHM and the TRAINING METHOD . the TRAINING METHOD is based on the DISTRIBUTED PERCEPTRON ALGORITHM and the TRAINING METHOD . the TRAINING METHOD is applied to a LARGE VOCABULARY CONTINUOUS SPEECH RECOGNITION TASK . the experimental results show that the proposed TRAINING METHOD is effective for WFST-BASED DECODING , including WFST-BASED DECODING and WFST-BASED DECODING .\n",
            "\n",
            "511 1000\n",
            "we address the problem of DETECTING BATCHES OF EMAILS that have been created according to the same template . this problem is motivated by the desire to filter SPAM more effectively by exploiting COLLECTIVE INFORMATION about entire batches of JOINTLY GENERATED MESSAGES . the application matches the problem setting of SUPERVISED CLUSTERING , because examples of correct clusterings can be collected . known DECODING PROCEDURES for SUPERVISED CLUSTERING are cubic in the number of instances . when decisions can not be <unk> once they have been made -- owing to the STREAMING NATURE of the data -- then the DECODING PROBLEM can be solved in LINEAR TIME . we devise a SEQUENTIAL DECODING PROCEDURE and derive the corresponding optimization problem of SUPERVISED CLUSTERING . we study the impact of COLLECTIVE ATTRIBUTES of EMAIL BATCHES on the effectiveness of recognizing SPAM EMAILS . \n",
            "this paper presents a method for DETECTING BATCHES OF EMAILS in JOINTLY GENERATED MESSAGES . the proposed method is based on a SEQUENTIAL DECODING PROCEDURE of the COLLECTIVE INFORMATION and the COLLECTIVE INFORMATION . the proposed method is based on a SEQUENTIAL DECODING PROCEDURE of the JOINTLY GENERATED MESSAGES . the proposed method is based on a SEQUENTIAL DECODING PROCEDURE of the JOINTLY GENERATED MESSAGES . the proposed method is based on a SEQUENTIAL DECODING PROCEDURE . the proposed method is based on a SEQUENTIAL DECODING PROCEDURE of the JOINTLY GENERATED MESSAGES . the proposed method is compared with conventional DECODING PROCEDURES .\n",
            "\n",
            "512 1000\n",
            "one of the most compelling issues in the design of WIRELESS COMMUNICATION COMPONENTS is to keep POWER DISSIPATION between bounds . while LOW-POWER SOLUTIONS are readily achieved in an APPLICATION-SPECIFIC APPROACH , doing so in a PROGRAMMABLE ENVIRONMENT is a substantially harder problem . this paper presents an approach to LOW-POWER PROGRAMMABLE DSP that is based on the DYNAMIC RECONFIGURA-TION OF HARDWARE MODULES . this technique has shown to yield at least an order of magnitude of POWER REDUCTION compared to traditional INSTRUCTION-BASED ENGINES for problems in the area of WIRELESS COMMUNICATION . \n",
            "this paper presents a new APPLICATION-SPECIFIC APPROACH for WIRELESS COMMUNICATION . the APPLICATION-SPECIFIC APPROACH is based on the DYNAMIC RECONFIGURA-TION OF HARDWARE MODULES . the proposed APPLICATION-SPECIFIC APPROACH is based on the DYNAMIC RECONFIGURA-TION OF HARDWARE MODULES . the proposed APPLICATION-SPECIFIC APPROACH is based on the DYNAMIC RECONFIGURA-TION OF HARDWARE MODULES . the proposed APPLICATION-SPECIFIC APPROACH is based on the DYNAMIC RECONFIGURA-TION OF HARDWARE MODULES . the proposed APPLICATION-SPECIFIC APPROACH is based on the DYNAMIC RECONFIGURA-TION OF HARDWARE MODULES . the proposed APPLICATION-SPECIFIC APPROACH is based on the DYNAMIC RECONFIGURA-TION OF HARDWARE MODULES .\n",
            "\n",
            "513 1000\n",
            "frequency warping using ALLPASS STRUCTURES or LAGUERRE FILTERS has found increasingly applications in AUDIO SIGNAL PROCESSING due to good match with the AUDITORY FREQUENCY RESOLUTION . KAUTZ FILTERS are an extension where the FREQUENCY WARPING and RELATED RESOLUTION can have more freedom . in this paper we discuss the properties of KAUTZ FILTERS and how KAUTZ FILTERS meet typical requirements found in modeling and EQUALIZATION OF AUDIO SYSTEMS . case studies include TRANSFER FUNCTION MODELING of the GUITAR BODY and LOUDSPEAKER RESPONSE EQUALIZATION . \n",
            "this paper addresses the problem of AUDITORY FREQUENCY RESOLUTION in AUDIO SIGNAL PROCESSING . we propose a method for TRANSFER FUNCTION MODELING based on FREQUENCY WARPING and KAUTZ FILTERS . the proposed method is based on the use of LAGUERRE FILTERS and FREQUENCY WARPING . the proposed method is based on the use of LAGUERRE FILTERS and FREQUENCY WARPING . the proposed method is evaluated on the GUITAR BODY and the EQUALIZATION OF AUDIO SYSTEMS .\n",
            "\n",
            "514 1000\n",
            "in this paper we present a new approach to VARIANCE MODELLING in AUTOMATIC SPEECH RECOGNITION that is based on TANGENT DISTANCE . using TANGENT DISTANCE , CLASSIFIERS can be made invariant w.r.t. small CLASSIFIERS of the data . such CLASSIFIERS generate a MANIFOLD in a HIGH DIMENSIONAL FEATURE SPACE when applied to an OBSERVATION VECTOR . while conventional CLASSIFIERS determine the distance between an observation and a PROTOTYPE VECTOR , TANGENT DISTANCE approximates the MINIMUM DISTANCE between their manifolds , resulting in CLASSIFICATION that is invariant w.r.t. the underlying transformation . recently , this approach was successfully applied in IMAGE OBJECT RECOGNITION . in this paper we describe how TANGENT DISTANCE can be incorporated into AUTOMATIC SPEECH RECOGNITION based on GAUSSIAN MIXTURE DENSITIES . the proposed method is embedded into a PROBABILISTIC FRAMEWORK . experiments performed on the SIETILL CORPUS for TELEPHONE LINE RECORDED GERMAN DIGIT STRINGS show a significant improvement in comparison with a conventional GMD APPROACH using a comparable amount of MODEL PARAMETERS . \n",
            "this paper proposes a new PROBABILISTIC FRAMEWORK for IMAGE OBJECT RECOGNITION . the proposed PROBABILISTIC FRAMEWORK is based on the MINIMUM DISTANCE of the MANIFOLD and the MODEL PARAMETERS . the proposed GMD APPROACH is based on the MINIMUM DISTANCE of the MANIFOLD . the proposed GMD APPROACH is based on the TANGENT DISTANCE . the proposed GMD APPROACH is evaluated on the SIETILL CORPUS and on the SIETILL CORPUS . the proposed GMD APPROACH is shown to outperform the conventional GMD APPROACH in terms of CLASSIFICATION and CLASSIFICATION .\n",
            "\n",
            "515 1000\n",
            "this paper describes the performance of the I4U SPEAKER RECOGNITION SYSTEM in the nist 2008 speaker recognition evaluation . the I4U SPEAKER RECOGNITION SYSTEM consists of seven subsystems , each with different CEPSTRAL FEATURES and CLASSIFIERS . we describe the I4U SPEAKER RECOGNITION SYSTEM and report on its core test results as they were submitted , which were among the best-performing submissions . the <unk> effort was led by the \n",
            "this paper presents a new I4U SPEAKER RECOGNITION SYSTEM for CLASSIFIERS . the I4U SPEAKER RECOGNITION SYSTEM consists of a I4U SPEAKER RECOGNITION SYSTEM and a I4U SPEAKER RECOGNITION SYSTEM . the I4U SPEAKER RECOGNITION SYSTEM is evaluated on the I4U SPEAKER RECOGNITION SYSTEM and the I4U SPEAKER RECOGNITION SYSTEM .\n",
            "\n",
            "516 1000\n",
            "we describe our approach for generating EXPRESSIVE MUSIC PERFORMANCES OF MONOPHONIC JAZZ MELODIES . it consists of three components : -lrb- a -rrb- a MELODIC TRANSCRIPTION COMPONENT which extracts a set of ACOUSTIC FEATURES from MONOPHONIC RECORDINGS , -lrb- b -rrb- a MACHINE LEARNING COMPONENT which induces an EXPRESSIVE TRANSFORMATION MODEL from the set of EXTRACTED ACOUSTIC FEATURES , and -lrb- c -rrb- a MELODY SYNTHESIS COMPONENT which generates expressive <unk> output -lrb- <unk> or audio -rrb- from INEXPRESSIVE MELODY DESCRIPTIONS using the INDUCED EXPRESSIVE TRANSFORMATION MODEL . in this paper we concentrate on the MACHINE LEARNING COMPONENT , in particular , on the LEARNING SCHEME we use for generating EXPRESSIVE AUDIO from a score . \n",
            "this paper presents a LEARNING SCHEME for EXPRESSIVE MUSIC PERFORMANCES OF MONOPHONIC JAZZ MELODIES . the INDUCED EXPRESSIVE TRANSFORMATION MODEL is based on the INDUCED EXPRESSIVE TRANSFORMATION MODEL and the INDUCED EXPRESSIVE TRANSFORMATION MODEL . the INDUCED EXPRESSIVE TRANSFORMATION MODEL is based on the INDUCED EXPRESSIVE TRANSFORMATION MODEL and the INDUCED EXPRESSIVE TRANSFORMATION MODEL . the INDUCED EXPRESSIVE TRANSFORMATION MODEL is based on the INDUCED EXPRESSIVE TRANSFORMATION MODEL and the INDUCED EXPRESSIVE TRANSFORMATION MODEL . the INDUCED EXPRESSIVE TRANSFORMATION MODEL is based on the INDUCED EXPRESSIVE TRANSFORMATION MODEL and the INDUCED EXPRESSIVE TRANSFORMATION MODEL . the INDUCED EXPRESSIVE TRANSFORMATION MODEL is based on the INDUCED EXPRESSIVE TRANSFORMATION MODEL and the INDUCED EXPRESSIVE TRANSFORMATION MODEL . the INDUCED EXPRESSIVE TRANSFORMATION MODEL is based on the INDUCED EXPRESSIVE TRANSFORMATION MODEL and the INDUCED EXPRESSIVE TRANSFORMATION MODEL .\n",
            "\n",
            "517 1000\n",
            "opponent modeling is necessary in MULTI-AGENT SETTINGS where SECONDARY AGENTS with competing goals also adapt their strategies , yet it remains challenging because strategies interact with each other and change . most previous work focuses on developing PROBABILISTIC MODELS or PARAMETERIZED STRATEGIES for specific applications . inspired by the recent success of DEEP REINFORCEMENT LEARNING , we present NEURAL-BASED MODELS that jointly learn a POLICY and the behavior of opponents . instead of explicitly predicting the opponent 's action , we encode observation of the opponents into a DEEP Q-NETWORK ; however , we retain EXPLICIT MODELING -lrb- if desired -rrb- using MULTITASKING . by using a MIXTURE-OF-EXPERTS ARCHITECTURE , our model automatically discovers different strategy patterns of opponents without extra SUPERVISION . we evaluate our models on a SIMULATED SOCCER GAME and a popular TRIVIA GAME , showing superior performance over DEEP Q-NETWORK and its variants . \n",
            "this paper addresses the problem of DEEP REINFORCEMENT LEARNING for MULTI-AGENT SETTINGS in MULTI-AGENT SETTINGS . we propose a method for DEEP REINFORCEMENT LEARNING based on the MIXTURE-OF-EXPERTS ARCHITECTURE . the proposed method is based on the MIXTURE-OF-EXPERTS ARCHITECTURE and the MIXTURE-OF-EXPERTS ARCHITECTURE . the proposed method is based on the MIXTURE-OF-EXPERTS ARCHITECTURE and the MIXTURE-OF-EXPERTS ARCHITECTURE . the proposed method is based on a SIMULATED SOCCER GAME and a SIMULATED SOCCER GAME .\n",
            "\n",
            "518 1000\n",
            "this paper provides an <unk> tutorial for the <unk> special session on '' <unk> and <unk> automatic speech recognition '' . the purpose of the special session is to bring together researchers who have special interest in novel techniques that are aimed at overcoming weaknesses of HMMS for ACOUSTIC MODELING in SPEECH RECOGNITION . numerous such approaches have been taken over the past dozen years , which can be broadly classified into <unk> -lrb- parametric -rrb- and <unk> -lrb- non-parametric -rrb- ones . in this paper , we will provide an overview of both approaches , focusing on the incorporation of long-range temporal dependencies of the SPEECH FEATURES and PHONETIC DETAIL in SPEECH RECOGNITION ALGORITHMS . we will provide a high-level survey on major existing work and systems using these two types of '' BEYOND-HMM '' FRAMEWORKS . the contributed papers in this special session will elaborate further on the related topics . \n",
            "this paper addresses the problem of SPEECH RECOGNITION in SPEECH RECOGNITION . in this paper , we present a method for SPEECH RECOGNITION based on HMMS . the proposed method is based on the use of SPEECH FEATURES in the ACOUSTIC MODELING . the proposed method is based on the use of SPEECH FEATURES in the ACOUSTIC MODELING . experimental results show that the proposed method is effective for SPEECH RECOGNITION .\n",
            "\n",
            "519 1000\n",
            "in this paper we first propose a new STATISTICAL PARSING MODEL , which is a GENERA-TIVE MODEL of LEXICALISED CONTEXT-FREE GRAMMAR . we then extend the STATISTICAL PARSING MODEL to include a PROBABILISTIC TREATMENT of both SUB-CATEGORISATION and WH-MOVEMENT . results on WALL STREET JOURNAL TEXT show that the STATISTICAL PARSING MODEL performs at <unk> / <unk> % CONSTITUENT PRECISION/RECALL , an average improvement of 2.3 % over -lrb- collins 96 -rrb- . \n",
            "this paper presents a novel STATISTICAL PARSING MODEL for WALL STREET JOURNAL TEXT . the proposed STATISTICAL PARSING MODEL is based on the GENERA-TIVE MODEL . the proposed STATISTICAL PARSING MODEL is based on the GENERA-TIVE MODEL . the proposed STATISTICAL PARSING MODEL is evaluated on the WALL STREET JOURNAL TEXT and on the WALL STREET JOURNAL TEXT . the proposed STATISTICAL PARSING MODEL achieves a better performance than the conventional STATISTICAL PARSING MODEL .\n",
            "\n",
            "520 1000\n",
            "curvature has received increasing attention as an important alternative to LENGTH BASED REGULARIZATION in COMPUTER VISION . in contrast to LENGTH , it preserves ELONGATED STRUCTURES and fine details . existing approaches are either inefficient , or have LOW ANGULAR RESOLUTION and yield results with strong block artifacts . we derive a new model for COMPUTING SQUARED CURVATURE based on INTEGRAL GEOMETRY . the model counts responses of straight line triple cliques . the corresponding energy decomposes into SUBMODULAR AND SUPER-MODULAR PAIRWISE POTENTIALS . we show that this energy can be efficiently minimized even for HIGH ANGULAR RESOLUTIONS using the TRUST REGION FRAMEWORK . our results confirm that we obtain accurate and visually pleasing solutions without strong artifacts at reasonable runtimes . \n",
            "this paper presents a TRUST REGION FRAMEWORK for COMPUTING SQUARED CURVATURE . the TRUST REGION FRAMEWORK is based on a TRUST REGION FRAMEWORK of the INTEGRAL GEOMETRY . the proposed TRUST REGION FRAMEWORK is based on a TRUST REGION FRAMEWORK . the proposed TRUST REGION FRAMEWORK is based on a TRUST REGION FRAMEWORK of the INTEGRAL GEOMETRY . the proposed TRUST REGION FRAMEWORK is based on a TRUST REGION FRAMEWORK and is shown to be more robust to HIGH ANGULAR RESOLUTIONS than the conventional CURVATURE .\n",
            "\n",
            "521 1000\n",
            "sponsored search is an important MONETIZATION CHANNEL for SEARCH ENGINES , in which an AUCTION MECHANISM is used to select the ads shown to users and determine the prices charged from advertisers . there have been several pieces of work in the literature that investigate how to design an AUCTION MECHANISM in order to optimize the revenue of the SEARCH ENGINES . however , due to some unrealistic assumptions used , the practical values of these studies are not very clear . in this paper , we propose a novel GAME-THEORETIC MACHINE LEARNING APPROACH , which naturally combines MACHINE LEARNING and GAME THEORY , and learns the AUCTION MECHANISM using a BILEVEL OPTIMIZATION FRAMEWORK . in particular , we first learn a MARKOV MODEL from HISTORICAL DATA to describe how advertisers change their bids in response to an AUCTION MECHANISM , and then for any given AUCTION MECHANISM , we use the learnt MARKOV MODEL to predict its corresponding future bid sequences . next we learn the AUCTION MECHANISM through EMPIRICAL REVENUE MAXIMIZATION on the PREDICTED BID SEQUENCES . we show that the empirical revenue will converge when the PREDICTION PERIOD approaches infinity , and a GENETIC PROGRAMMING ALGORITHM can effectively optimize this empirical revenue . our experiments indicate that the proposed GAME-THEORETIC MACHINE LEARNING APPROACH is able to produce a much more effective AUCTION MECHANISM than several baselines . \n",
            "this paper presents a GAME-THEORETIC MACHINE LEARNING APPROACH for MACHINE LEARNING . the GAME-THEORETIC MACHINE LEARNING APPROACH is based on a AUCTION MECHANISM and a AUCTION MECHANISM . the AUCTION MECHANISM is based on the AUCTION MECHANISM and the AUCTION MECHANISM . the AUCTION MECHANISM is based on the AUCTION MECHANISM and the AUCTION MECHANISM . the AUCTION MECHANISM is based on a AUCTION MECHANISM and a AUCTION MECHANISM . the proposed GAME-THEORETIC MACHINE LEARNING APPROACH is based on a MARKOV MODEL and a AUCTION MECHANISM .\n",
            "\n",
            "522 1000\n",
            "state-of-the-art FACTOR ANALYSIS BASED CHANNEL COMPENSATION METHODS for SPEAKER RECOGNITION are based on the assumption that SPEAKER/UTTERANCE DEPENDENT GAUSSIAN MIXTURE MODEL mean SUPER-VECTORS can be constrained to lie in a LOWER DIMENSIONAL SUBSPACE , which does not consider the fact that conventional ACOUSTIC FEATURES may also be constrained in a similar way in the FEATURE SPACE . in this study , motivated by the LOW-RANK COVARIANCE STRUCTURE OF CEPSTRAL FEATURES , we propose a FACTOR ANALYSIS MODEL in the ACOUSTIC FEATURE SPACE instead of the SUPER-VECTOR DOMAIN and derive a MIXTURE DEPENDENT FEATURE TRANSFORMATION . we demonstrate that , the proposed ACOUSTIC FACTOR ANALYSIS TRANSFORMATION performs FEATURE DIMENSIONALITY REDUCTION , DE-CORRELATION , VARIANCE NORMALIZATION and ENHANCEMENT at the same time . the transform applies a <unk> wiener gain on the ACOUSTIC FEATURE EIGENVECTOR DIRECTIONS , and is similar to the SIGNAL SUB-SPACE BASED SPEECH ENHANCEMENT SCHEMES . we also propose several methods of adaptively selecting the AFA PARAMETER for each mixture . the proposed feature transform is applied using a PROBABILISTIC MIXTURE ALIGNMENT , and is integrated with a conventional I-VECTOR SYSTEM . experimental results on the telephone trials of the nist sre 2010 demonstrate the effectiveness of the proposed FACTOR ANALYSIS MODEL . \n",
            "this paper addresses the problem of SPEAKER RECOGNITION in SPEAKER RECOGNITION . we propose a FACTOR ANALYSIS MODEL for SPEAKER RECOGNITION , which is based on the LOW-RANK COVARIANCE STRUCTURE OF CEPSTRAL FEATURES and the LOW-RANK COVARIANCE STRUCTURE OF CEPSTRAL FEATURES . the proposed method is based on a FACTOR ANALYSIS MODEL and a FACTOR ANALYSIS MODEL . the proposed method is based on the LOW-RANK COVARIANCE STRUCTURE OF CEPSTRAL FEATURES and the LOW-RANK COVARIANCE STRUCTURE OF CEPSTRAL FEATURES . the proposed method is based on the LOW-RANK COVARIANCE STRUCTURE OF CEPSTRAL FEATURES and the LOW-RANK COVARIANCE STRUCTURE OF CEPSTRAL FEATURES . the proposed method is based on the LOW-RANK COVARIANCE STRUCTURE OF CEPSTRAL FEATURES and the LOW-RANK COVARIANCE STRUCTURE OF CEPSTRAL FEATURES . the proposed method is based on a FACTOR ANALYSIS MODEL and a FACTOR ANALYSIS MODEL . the proposed method is compared with the conventional FACTOR ANALYSIS BASED CHANNEL COMPENSATION METHODS and the FACTOR ANALYSIS BASED CHANNEL COMPENSATION METHODS .\n",
            "\n",
            "523 1000\n",
            "a KNOWLEDGE-BASED PROGRAM defines the behavior of an agent by combining PRIMITIVE ACTIONS , PROGRAMMING CONSTRUCTS and TEST CONDITIONS that make explicit reference to the AGENT 'S KNOWLEDGE . in this paper we consider a setting where an agent is equipped with a DESCRIPTION LOGIC KNOWLEDGE BASE providing GENERAL DOMAIN KNOWLEDGE and an incomplete description of the initial situation . we introduce a corresponding new DL-BASED ACTION LANGUAGE that allows for representing both PHYSICAL AND SENSING ACTIONS , and that we then use to build KNOWLEDGE-BASED PROGRAMS with TEST CONDITIONS expressed in the EPISTEMIC DL . after proving undecidability for the general case , we then discuss a RESTRICTED FRAGMENT where VERIFICATION becomes decidable . the provided proof is constructive and comes with an upper bound on the PROCE-DURE 'S COMPLEXITY . \n",
            "this paper addresses the problem of VERIFICATION in DL-BASED ACTION LANGUAGE . we propose a method for VERIFICATION based on AGENT 'S KNOWLEDGE and AGENT 'S KNOWLEDGE . the proposed method is based on a DESCRIPTION LOGIC KNOWLEDGE BASE and a DESCRIPTION LOGIC KNOWLEDGE BASE . the proposed method is based on a DESCRIPTION LOGIC KNOWLEDGE BASE and a KNOWLEDGE-BASED PROGRAM . the proposed method is based on a DESCRIPTION LOGIC KNOWLEDGE BASE and a KNOWLEDGE-BASED PROGRAM . the proposed method is based on a DESCRIPTION LOGIC KNOWLEDGE BASE and a DESCRIPTION LOGIC KNOWLEDGE BASE .\n",
            "\n",
            "524 1000\n",
            "we propose a method that uses a FILLER PREDICTION MODEL for building a LANGUAGE MODEL that includes FILLERS from a corpus without FILLERS . in our method , a FILLER PREDICTION MODEL is trained from a corpus that does not cover DOMAIN-RELEVANT TOPICS . it recovers FILLERS in inexact TRANSCRIBED CORPORA in the target domain , and then a LANGUAGE MODEL that includes FILLERS is built from the corpora . the results of an evaluation of the JAPANESE NATIONAL DIET RECORD showed that a model using our method achieves higher RECOGNITION performance than conventional ones . \n",
            "this paper presents a FILLER PREDICTION MODEL for RECOGNITION . the FILLER PREDICTION MODEL is based on the FILLER PREDICTION MODEL . the FILLER PREDICTION MODEL is based on the FILLER PREDICTION MODEL . the FILLER PREDICTION MODEL is based on the FILLER PREDICTION MODEL . the FILLER PREDICTION MODEL is based on the FILLER PREDICTION MODEL and the FILLER PREDICTION MODEL . the experimental results show that the proposed FILLER PREDICTION MODEL is robust and robust to RECOGNITION .\n",
            "\n",
            "525 1000\n",
            "most INFORMATION EXTRACTION SYSTEMS treat separate potential <unk> as independent . however , in many cases , considering influences between different potential <unk> could improve OVERALL ACCURACY . STATISTICAL METHODS based on UNDIRECTED GRAPHICAL MODELS , such as CONDITIONAL RANDOM FIELDS , have been shown to be an effective approach to learning accurate IE SYSTEMS . we present a new ie method that employs RELATIONAL MARKOV NETWORKS -lrb- a GENERALIZATION OF CRFS -RRB- , which can represent ARBITRARY DEPENDENCIES between <unk> . this allows for '' collective information extraction '' that exploits the mutual influence between possible <unk> . experiments on learning to extract PROTEIN NAMES from BIOMEDICAL TEXT demonstrate the advantages of this approach . \n",
            "this paper addresses the problem of INFORMATION EXTRACTION SYSTEMS for BIOMEDICAL TEXT . we propose a GENERALIZATION OF CRFS -RRB- based on CONDITIONAL RANDOM FIELDS , which is based on the GENERALIZATION OF CRFS -RRB- . the proposed method is based on the use of CONDITIONAL RANDOM FIELDS , which is based on the GENERALIZATION OF CRFS -RRB- . the proposed method is based on the GENERALIZATION OF CRFS -RRB- . the proposed method is based on the use of CONDITIONAL RANDOM FIELDS , which are used for INFORMATION EXTRACTION SYSTEMS . experimental results show that the proposed method is robust and robust to ARBITRARY DEPENDENCIES .\n",
            "\n",
            "526 1000\n",
            "in this paper , we present a HYBRID SPEECH RECOGNIZER combining HIDDEN MARKOV MODELS and a POLYNOMIAL CLASSIFIER . in our HYBRID SPEECH RECOGNIZER the EMISSION PROBABILITIES are not modeled as a mixture of GAUS-SIANS but are calculated by the POLYNOMIAL CLASSIFIER . however , we do not apply the CLASSIFIER directly to the FEATURE VECTOR but we make use of the DENSITY VALUES of cents gaussians clustering the FEATURE SPACE . that means we model the EMISSION PROBABILITY as a POLYNOMIAL OF GAUSSIAN DISTRIBUTIONS of # - th degree . as most of these DENSITY VALUES are approximately zero for a single FEATURE VECTOR the calculation of a POLYNOMIAL can be done very efficiently . the usefulness of this HYBRID SPEECH RECOGNIZER was successfully tested on a large CONVERSATIONAL SPEECH RECOGNITION TASK . \n",
            "this paper presents a new CLASSIFIER for CONVERSATIONAL SPEECH RECOGNITION TASK . the CLASSIFIER is based on a POLYNOMIAL CLASSIFIER and a POLYNOMIAL CLASSIFIER . the CLASSIFIER is based on a POLYNOMIAL CLASSIFIER and a POLYNOMIAL CLASSIFIER . the CLASSIFIER is based on a POLYNOMIAL CLASSIFIER and a POLYNOMIAL CLASSIFIER . the CLASSIFIER is based on a POLYNOMIAL CLASSIFIER and a POLYNOMIAL CLASSIFIER . experimental results show the effectiveness of the proposed CLASSIFIER .\n",
            "\n",
            "527 1000\n",
            "many PITCH TRACKERS based on DYNAMIC PROGRAMMING require <unk> design of local cost and TRANSITION COST FUNCTIONS . the forms of these functions are often empirically determined and their parameters are tuned accordingly . PARAMETER TUNING usually requires great effort without a guarantee of optimal performance . this work presents a GRAPHICAL MODEL FRAMEWORK to automatically optimize PITCH TRACKING PARAMETERS in the MAXIMUM LIKELIHOOD SENSE . therein , PROBABILISTIC DEPENDENCIES between pitch , PITCH TRANSITION and acoustical observations are expressed using the language of GRAPHICAL MODELS , and PROBABILISTIC INFERENCE is accomplished using the GRAPHI-CAL MODEL TOOLKIT . experiments show that this GRAPHICAL MODEL FRAMEWORK not only <unk> the design of a PITCH TRACKERS , but also yields remarkably good performance for both PITCH ESTIMATION and VOICING DECISION . \n",
            "this paper presents a GRAPHICAL MODEL FRAMEWORK for PITCH ESTIMATION . the GRAPHI-CAL MODEL TOOLKIT is based on the GRAPHI-CAL MODEL TOOLKIT and the GRAPHI-CAL MODEL TOOLKIT . the GRAPHI-CAL MODEL TOOLKIT is based on the GRAPHI-CAL MODEL TOOLKIT and the GRAPHI-CAL MODEL TOOLKIT . the GRAPHI-CAL MODEL TOOLKIT is based on the GRAPHI-CAL MODEL TOOLKIT and the GRAPHI-CAL MODEL TOOLKIT . the GRAPHI-CAL MODEL TOOLKIT is based on the GRAPHI-CAL MODEL TOOLKIT . the GRAPHI-CAL MODEL TOOLKIT is based on the GRAPHI-CAL MODEL TOOLKIT and the GRAPHI-CAL MODEL TOOLKIT . the proposed GRAPHI-CAL MODEL TOOLKIT is based on a GRAPHICAL MODEL FRAMEWORK and is shown to outperform the conventional GRAPHI-CAL MODEL TOOLKIT .\n",
            "\n",
            "528 1000\n",
            "this paper proposes a novel approach for effectively utilizing UNSUPERVISED DATA in addition to SUPERVISED DATA for SUPERVISED LEARNING . we use UNSUPERVISED DATA to generate INFORMATIVE ` CONDENSED FEATURE REPRESEN-TATIONS ' from the original feature set used in SUPERVISED NLP SYSTEMS . the main contribution of our method is that it can offer DENSE AND LOW-DIMENSIONAL FEATURE SPACES for NLP TASKS while maintaining the state-of-the-art performance provided by the recently developed high-performance SEMI-SUPERVISED LEARNING TECHNIQUE . our method matches the results of current state-of-the-art systems with very few FEATURES , i.e. , f-score <unk> with <unk> FEATURES for CONLL-2003 NER DATA , and <unk> <unk> with 12.5 k FEATURES for DEPENDENCY PARSING DATA derived from PTB-III . \n",
            "this paper addresses the problem of SUPERVISED LEARNING in SUPERVISED NLP SYSTEMS . we propose a method for SUPERVISED LEARNING based on CONLL-2003 NER DATA . the proposed method is based on the use of FEATURES and FEATURES . the proposed method is based on the use of FEATURES and FEATURES . the proposed method is based on the use of FEATURES and FEATURES . experimental results demonstrate the effectiveness of the proposed method .\n",
            "\n",
            "529 1000\n",
            "ordinary least squares -lrb- ORDINARY LEAST SQUARES -rrb- is the DEFAULT METHOD for fitting LINEAR MODELS , but is not applicable for problems with DIMENSIONALITY larger than the SAMPLE SIZE . for these problems , we advocate the use of a GENERALIZED VERSION of ORDINARY LEAST SQUARES motivated by RIDGE REGRESSION , and propose two novel THREE-STEP ALGORITHMS involving LEAST SQUARES FITTING and HARD THRESHOLDING . the THREE-STEP ALGORITHMS are <unk> simple to understand intuitively , computationally easy to implement efficiently , and theoretically appealing for choosing models consistently . NUMERICAL EXERCISES comparing our methods with PENALIZATION-BASED APPROACHES in simulations and data analyses illustrate the great potential of the proposed THREE-STEP ALGORITHMS . \n",
            "this paper addresses the problem of LEAST SQUARES FITTING for LEAST SQUARES FITTING . we propose a new DEFAULT METHOD based on the GENERALIZED VERSION . the proposed DEFAULT METHOD is based on a GENERALIZED VERSION and a GENERALIZED VERSION . the proposed DEFAULT METHOD is based on a GENERALIZED VERSION , which is based on the GENERALIZED VERSION . the proposed DEFAULT METHOD is based on a GENERALIZED VERSION and is shown to be robust to SAMPLE SIZE and SAMPLE SIZE .\n",
            "\n",
            "530 1000\n",
            "we investigate the effect of CORPUS SIZE in combining SUPERVISED AND UNSUPER-VISED LEARNING for two types of ATTACHMENT DECISIONS : RELATIVE CLAUSE ATTACHMENT and PREPOSITIONAL PHRASE ATTACHMENT . the SUPERVISED COMPONENT is COLLINS ' PARSER , trained on the WALL STREET JOURNAL . the UNSUPERVISED COMPONENT gathers LEXICAL STATISTICS from an UNANNOTATED CORPUS OF NEWSWIRE TEXT . we find that the combined system only improves the performance of the COLLINS ' PARSER for small training sets . surprisingly , the size of the UNANNOTATED CORPUS has little effect due to the <unk> of the LEXICAL STATISTICS acquired by UNSUPERVISED LEARNING . \n",
            "this paper presents a COLLINS ' PARSER based on LEXICAL STATISTICS . the COLLINS ' PARSER is based on a UNANNOTATED CORPUS OF NEWSWIRE TEXT and a SUPERVISED COMPONENT . the UNSUPERVISED COMPONENT is based on a UNANNOTATED CORPUS OF NEWSWIRE TEXT and a SUPERVISED COMPONENT . the UNSUPERVISED COMPONENT is based on a UNANNOTATED CORPUS OF NEWSWIRE TEXT and a CORPUS SIZE . the UNSUPERVISED COMPONENT is applied to the UNANNOTATED CORPUS OF NEWSWIRE TEXT , and the results show that the proposed COLLINS ' PARSER is effective for PREPOSITIONAL PHRASE ATTACHMENT such as PREPOSITIONAL PHRASE ATTACHMENT , RELATIVE CLAUSE ATTACHMENT , and RELATIVE CLAUSE ATTACHMENT .\n",
            "\n",
            "531 1000\n",
            "automatic topic segmentation is an important t e c <unk> for MULTIMEDIA ARCHIVAL AND RETRIEVAL SYSTEMS . in this paper we present an algorithm for TOPIC SEGMENTATION which uses a combination of MACHINE LEARNING , STATISTICAL NATURAL LANGUAGE PROCESSING , and INFORMATION RETRIEVAL TECHNIQUES . the performance of this algorithm is measured by considering the misses and false alarms on a MANUALLY SEGMENTED CORPUS . we present our results on the widely used <unk> and <unk> corpora provided by NIST . most of the techniques described are independent of the source language . we demonstrate this by applying the algorithm on both the ENGLISH AND MANDARIN TDT3 CORPORA with only minor changes . \n",
            "this paper addresses the problem of AUTOMATIC TOPIC SEGMENTATION in MULTIMEDIA ARCHIVAL AND RETRIEVAL SYSTEMS . in this paper , we propose a method for AUTOMATIC TOPIC SEGMENTATION based on MACHINE LEARNING and INFORMATION RETRIEVAL TECHNIQUES . the proposed method is based on a MANUALLY SEGMENTED CORPUS and a MANUALLY SEGMENTED CORPUS . the proposed method is based on the use of INFORMATION RETRIEVAL TECHNIQUES and INFORMATION RETRIEVAL TECHNIQUES . experimental results show that the proposed method can improve the performance of MULTIMEDIA ARCHIVAL AND RETRIEVAL SYSTEMS and TOPIC SEGMENTATION .\n",
            "\n",
            "532 1000\n",
            "recognizing materials in REAL-WORLD IMAGES is a challenging task . REAL-WORLD MATERIALS have RICH SURFACE TEXTURE , GEOMETRY , LIGHTING CONDITIONS , and CLUTTER , which combine to make the problem particularly difficult . in this paper , we introduce a new , LARGE-SCALE , OPEN DATASET OF MATERIALS in the wild , the MATERIALS IN CONTEXT DATABASE , and combine this dataset with DEEP LEARNING to achieve MATERIAL RECOGNITION and segmentation of images in the wild . MATERIALS IN CONTEXT DATABASE is an order of magnitude larger than previous MATERIAL DATABASES , while being more diverse and <unk> across its 23 categories . using MATERIALS IN CONTEXT DATABASE , we train CONVOLU-TIONAL NEURAL NETWORKS -lrb- cnns -rrb- for two tasks : CLASSIFYING MATERIALS from PATCHES , and SIMULTANEOUS MATERIAL RECOGNITION and segmentation in FULL IMAGES . for PATCH-BASED CLASSIFICATION on MATERIALS IN CONTEXT DATABASE we found that the best performing MATERIALS IN CONTEXT DATABASE can achieve <unk> % MEAN CLASS ACCURACY . we convert these trained CNN CLASSIFIERS into an efficient fully convolutional framework combined with a fully connected conditional random field -lrb- crf -rrb- to predict the material at every PIXEL in an image , achieving <unk> % MEAN CLASS ACCURACY . our experiments demonstrate that having a large , WELL-SAMPLED DATASET such as MATERIALS IN CONTEXT DATABASE is crucial for REAL-WORLD MATERIAL RECOGNITION and segmentation . \n",
            "this paper addresses the problem of RECOGNIZING MATERIALS in REAL-WORLD IMAGES . we propose a method for CLASSIFYING MATERIALS based on CONVOLU-TIONAL NEURAL NETWORKS . the proposed method is based on the use of CONVOLU-TIONAL NEURAL NETWORKS and DEEP LEARNING . the proposed method is based on the use of CONVOLU-TIONAL NEURAL NETWORKS and DEEP LEARNING . the proposed method is based on the MATERIALS IN CONTEXT DATABASE and the CNN CLASSIFIERS . the proposed method is based on the MATERIALS IN CONTEXT DATABASE and the CNN CLASSIFIERS . the proposed method is evaluated on the WELL-SAMPLED DATASET and the WELL-SAMPLED DATASET . the results show that the proposed method is robust and robust to LIGHTING CONDITIONS , CLUTTER , and CLUTTER .\n",
            "\n",
            "533 1000\n",
            "the ideal BINARY MASK , often used in robust speech recognition applications , requires an estimate of the LOCAL SNR in each TIME-FREQUENCY UNIT . a DATA-DRIVEN APPROACH is proposed for estimating the INSTANTANEOUS SNR of each t-f unit . by assuming that the a PRIORI SNR and a POSTERIORI SNR are uniformly distributed within a small region , the INSTANTANEOUS SNR is estimated by minimizing the LOCALIZED BAYES RISK . the BINARY MASK ESTIMATOR derived by the proposed DATA-DRIVEN APPROACH is evaluated in terms of hit and FALSE ALARM RATES . compared to the BINARY MASK ESTIMATOR that uses the DECISION-DIRECTED APPROACH to compute the SNR , the proposed DATA-DRIVEN APPROACH yielded substantial improvements -lrb- up to 40 % -rrb- in CLASSIFICATION performance , when assessed in terms of a SENSITIVITY METRIC which is based on the difference between the hit and FALSE ALARM RATES . \n",
            "this paper presents a DATA-DRIVEN APPROACH for CLASSIFICATION . the proposed DATA-DRIVEN APPROACH is based on the SENSITIVITY METRIC of the TIME-FREQUENCY UNIT and the SENSITIVITY METRIC of the TIME-FREQUENCY UNIT . the proposed DATA-DRIVEN APPROACH is based on the SENSITIVITY METRIC of the TIME-FREQUENCY UNIT and the SENSITIVITY METRIC of the BINARY MASK ESTIMATOR . the SENSITIVITY METRIC of the proposed DATA-DRIVEN APPROACH is compared with the conventional DATA-DRIVEN APPROACH and the DATA-DRIVEN APPROACH .\n",
            "\n",
            "534 1000\n",
            "in the JAPANESE LANGUAGE , as a predicate is placed at the end of a sentence , the content of a sentence can not be inferred until reaching the end . however , when the content is complicated and the sentence is long , people want to know at an earlier stage in the sentence whether the content is negative , affirmative , or <unk> . in JAPANESE , the GRAMMATICAL FORM called the KO-OU RELATION exists . the KO-OU RELATION is a kind of CONCORD . if a KO ELEMENT appears , then an OU ELEMENT appears in the latter part of a sentence . it is being pointed out that the KO-OU RELATION gives advance notice to the element that appears in the latter part of a sentence . in this paper , we present the method of extracting automatically the KO-OU EXPRESSION DATA from LARGE-SCALE ELECTRONIC CORPUS and verify the usefulness of the KO-OU EXPRESSION DATA . \n",
            "this paper presents a new method for JAPANESE , which is based on the KO-OU RELATION . the method is based on a KO-OU RELATION , which is a KO-OU RELATION , which is a KO-OU RELATION . the KO ELEMENT is a KO-OU RELATION , which is a KO-OU RELATION . the method is based on the KO-OU RELATION and the KO-OU RELATION . the proposed method is tested on a LARGE-SCALE ELECTRONIC CORPUS and on a LARGE-SCALE ELECTRONIC CORPUS .\n",
            "\n",
            "535 1000\n",
            "we approach the ZERO-ANAPHORA RESOLUTION PROBLEM by decomposing ZERO-ANAPHORA RESOLUTION PROBLEM into INTRA-SENTENTIAL AND INTER-SENTENTIAL ZERO-ANAPHORA RESOLUTION . for the former problem , SYNTACTIC PATTERNS of the appearance of ZERO-PRONOUNS and their antecedents are useful clues . taking JAPANESE as a target language , we empirically demonstrate that incorporating RICH SYNTACTIC PATTERN FEATURES in a state-of-the-art LEARNING-BASED ANAPHORA RESOLUTION MODEL dramatically improves the ACCURACY of INTRA-SENTENTIAL ZERO-ANAPHORA , which consequently improves the overall performance of ZERO-ANAPHORA RESOLUTION . \n",
            "this paper presents a novel LEARNING-BASED ANAPHORA RESOLUTION MODEL for ZERO-ANAPHORA RESOLUTION . the LEARNING-BASED ANAPHORA RESOLUTION MODEL is based on a LEARNING-BASED ANAPHORA RESOLUTION MODEL . the proposed LEARNING-BASED ANAPHORA RESOLUTION MODEL is based on a LEARNING-BASED ANAPHORA RESOLUTION MODEL . the proposed LEARNING-BASED ANAPHORA RESOLUTION MODEL is based on a LEARNING-BASED ANAPHORA RESOLUTION MODEL . the proposed LEARNING-BASED ANAPHORA RESOLUTION MODEL is based on a LEARNING-BASED ANAPHORA RESOLUTION MODEL . the proposed LEARNING-BASED ANAPHORA RESOLUTION MODEL is based on a LEARNING-BASED ANAPHORA RESOLUTION MODEL . the proposed LEARNING-BASED ANAPHORA RESOLUTION MODEL is based on a LEARNING-BASED ANAPHORA RESOLUTION MODEL and is shown to be robust to ZERO-ANAPHORA RESOLUTION .\n",
            "\n",
            "536 1000\n",
            "we propose a GESTURE RECOGNITION SYSTEM based primarily on a single 3-AXIS ACCELEROMETER . the GESTURE RECOGNITION SYSTEM employs DYNAMIC TIME WARPING and AFFINITY PROPAGATION ALGORITHMS for training and utilizes the sparse nature of the GESTURE SEQUENCE by implementing COMPRESSIVE SENSING for GESTURE RECOGNITION . a dictionary of 18 gestures or classes is defined and a database of over <unk> repetitions is created from 7 users . our DICTIONARY OF GESTURES is the largest in PUBLISHED STUDIES related to ACCELERATION-BASED GESTURE RECOGNITION , to the best of our knowledge . the proposed GESTURE RECOGNITION SYSTEM achieves almost perfect USER-DEPENDENT RECOGNITION and a USER-INDEPENDENT RECOGNITION ACCURACY that is competitive with the STATISTICAL METHODS that require significantly a large number of training samples and with the other ACCELEROMETER-BASED GESTURE RECOGNITION SYSTEMS available in literature . \n",
            "this paper presents a novel GESTURE RECOGNITION SYSTEM for ACCELERATION-BASED GESTURE RECOGNITION . the proposed GESTURE RECOGNITION SYSTEM is based on the use of COMPRESSIVE SENSING and AFFINITY PROPAGATION ALGORITHMS . the proposed GESTURE RECOGNITION SYSTEM is based on the DICTIONARY OF GESTURES , which is based on the DICTIONARY OF GESTURES . the proposed GESTURE RECOGNITION SYSTEM is compared with conventional STATISTICAL METHODS and STATISTICAL METHODS . the proposed GESTURE RECOGNITION SYSTEM is compared with other STATISTICAL METHODS and STATISTICAL METHODS . the proposed GESTURE RECOGNITION SYSTEM is compared with conventional STATISTICAL METHODS and AFFINITY PROPAGATION ALGORITHMS .\n",
            "\n",
            "537 1000\n",
            "in beijing , most TAXI DRIVERS intentionally avoid working during peak hours despite of the huge customer demand within these peak periods . this dilemma is mainly due to the fact that TAXI DRIVERS ' CONGESTION COSTS are not reflected in the current TAXI FARE STRUCTURE . to resolve this problem , we propose a new ATOM SCHEDULE METHOD to provide TAXI DRIVERS with extra incentives to work during peak hours . this differs from previous studies of <unk> market by considering MARKET VARIANCE over multiple periods , TAXI DRIVERS ' PROFIT-DRIVEN DECISIONS , and their SCHEDULING CONSTRAINTS regarding the interdependence among different periods . the major challenge of this research is the COMPUTATIONAL INTEN-SIVENESS to identify optimal strategy due to the exponentially large size of a TAXI DRIVER 'S STRATEGY SPACE and the SCHEDULING CONSTRAINTS . we develop an ATOM SCHEDULE METHOD to overcome these issues . it reduces the magnitude of the problem while satisfying the constraints to filter out INFEASIBLE PURE STRATEGIES . simulation results based on real data show the effectiveness of the proposed ATOM SCHEDULE METHOD , which opens up a new door to improving the efficiency of <unk> market in <unk> -lrb- e.g. , beijing -rrb- . \n",
            "this paper presents a ATOM SCHEDULE METHOD for SCHEDULING CONSTRAINTS . the ATOM SCHEDULE METHOD is based on the ATOM SCHEDULE METHOD . the ATOM SCHEDULE METHOD is based on the ATOM SCHEDULE METHOD . the ATOM SCHEDULE METHOD is based on the ATOM SCHEDULE METHOD . the proposed ATOM SCHEDULE METHOD is based on the ATOM SCHEDULE METHOD . the proposed ATOM SCHEDULE METHOD is based on the ATOM SCHEDULE METHOD and the ATOM SCHEDULE METHOD .\n",
            "\n",
            "538 1000\n",
            "we describe a SPOKEN DIALOGUE SYSTEM which can engage in CALL FOR FIRE RADIO DIALOGUES to help train <unk> in proper procedures for requesting ARTILLERY FIRE MISSIONS . we describe the domain , an INFORMATION-STATE DIALOGUE MANAGER with a novel system of INTERACTIVE INFORMATION COMPONENTS , and provide evaluation results . \n",
            "this paper presents a new method for CALL FOR FIRE RADIO DIALOGUES . the proposed method is based on a INFORMATION-STATE DIALOGUE MANAGER with INTERACTIVE INFORMATION COMPONENTS . the proposed method is based on the INFORMATION-STATE DIALOGUE MANAGER . the proposed method is based on the INFORMATION-STATE DIALOGUE MANAGER . the experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "539 1000\n",
            "<unk> patients -lrb- <unk> <unk> <unk> with PRIMARY CLOSURE , the red lines indicate the <unk> of the <unk> -rrb- abstract <unk> changes properties of the tongue and negatively affects patients ' speech production . among the most difficult consonants to produce in the POST-GLOSSECTOMY SPEAKERS , the SIBILANT FRICATIVES / s / and / <unk> / are often problematic . to better understand these problems in production , this study analyzed ACOUSTIC AND ARTICULATORY DATA of / s / and / <unk> / from three subjects : one normal speaker and two POST-GLOSSECTOMY SPEAKERS with abnormal / s / or / <unk> . based on CINE MAGNETIC RESONANCE IMAGES , three DIMENSIONAL VOCAL TRACT RECONSTRUCTIONS , TONGUE SURFACE SHAPES behind CONSTRICTIONS , and AREA FUNCTIONS were analyzed . our results show that in each patient , contrary to normal , / s / and / <unk> / were quite similar in ACOUSTIC SPECTRA , TONGUE SURFACE SHAPES , and CONSTRICTION LOCATIONS . in the abnormal / s / , the MISSING UNILATERAL TONGUE TISSUE created an AIR FLOW BYPASS which made the CONSTRICTION further backward . the abnormal / <unk> / may be explained by the lack of PRECISE TONGUE CONTROL after surgery . in addition , the TONGUE SURFACES in the patients were more asymmetric in the back and were not <unk> for / s / <unk> to the CONSTRICTION . \n",
            "this paper presents a new method for DIMENSIONAL VOCAL TRACT RECONSTRUCTIONS in CINE MAGNETIC RESONANCE IMAGES . the proposed method is based on the use of AREA FUNCTIONS and PRECISE TONGUE CONTROL . the proposed method is based on the use of AREA FUNCTIONS and PRECISE TONGUE CONTROL . the proposed method is based on the use of AREA FUNCTIONS and PRECISE TONGUE CONTROL . the proposed method is based on the use of CINE MAGNETIC RESONANCE IMAGES and PRECISE TONGUE CONTROL .\n",
            "\n",
            "540 1000\n",
            "in this paper , we describe a new JAVA FRAMEWORK for an easy and efficient way of developing new GUI BASED SPEECH PROCESSING APPLICATIONS . STANDARD COMPONENTS are provided to display the SPEECH SIGNAL , the POWER PLOT , and the SPECTROGRAM . furthermore , a component to create a new TRANSCRIPTION and to display and manipulate an existing TRANSCRIPTION is provided , as well as a component to display and manually correct EXTERNAL PITCH VALUES . these STANDARD COMPONENTS can be easily embedded into own JAVA PROGRAMS . STANDARD COMPONENTS can be synchronized to display the same region of the SPEECH FILE . the OBJECT-ORIENTED DESIGN provides base classes for rapid development of own components . \n",
            "this paper presents a JAVA FRAMEWORK for GUI BASED SPEECH PROCESSING APPLICATIONS . the JAVA FRAMEWORK is based on a JAVA FRAMEWORK and a JAVA FRAMEWORK . the JAVA FRAMEWORK is based on a JAVA FRAMEWORK and a JAVA FRAMEWORK . the OBJECT-ORIENTED DESIGN is based on a JAVA FRAMEWORK and a JAVA FRAMEWORK . the OBJECT-ORIENTED DESIGN is based on a JAVA FRAMEWORK and a JAVA FRAMEWORK . the OBJECT-ORIENTED DESIGN is applied to the JAVA FRAMEWORK and the OBJECT-ORIENTED DESIGN . the experimental results show the effectiveness of the proposed JAVA FRAMEWORK .\n",
            "\n",
            "541 1000\n",
            "in this article we propose a NETWORK TOPOLOGY ESTIMATION STRATEGY using UNICAST END-TO-END PACKET PAIR DELAY MEASUREMENTS that is based on MIXTURE MODELS for the DELAY CO-VARIANCES . an UNSUPERVISED LEARNING ALGORITHMS is applied to estimate the number of MIXTURE COMPONENTS and delay covariances . the LEAF PAIRS are clustered by a MAP CRITERION and passed to a HIERARCHICAL TOPOLOGY CONSTRUCTION ALGORITHM to rebuild the TREE . results from an ns simulation show that our NETWORK TOPOLOGY ESTIMATION STRATEGY can identify a NETWORK TREE with 8 LEAF NODES . \n",
            "this paper presents a NETWORK TOPOLOGY ESTIMATION STRATEGY for UNICAST END-TO-END PACKET PAIR DELAY MEASUREMENTS based on MIXTURE MODELS . the HIERARCHICAL TOPOLOGY CONSTRUCTION ALGORITHM is based on the HIERARCHICAL TOPOLOGY CONSTRUCTION ALGORITHM . the HIERARCHICAL TOPOLOGY CONSTRUCTION ALGORITHM is based on the HIERARCHICAL TOPOLOGY CONSTRUCTION ALGORITHM . the HIERARCHICAL TOPOLOGY CONSTRUCTION ALGORITHM is based on the HIERARCHICAL TOPOLOGY CONSTRUCTION ALGORITHM . the HIERARCHICAL TOPOLOGY CONSTRUCTION ALGORITHM is based on the HIERARCHICAL TOPOLOGY CONSTRUCTION ALGORITHM . the proposed NETWORK TOPOLOGY ESTIMATION STRATEGY is based on the HIERARCHICAL TOPOLOGY CONSTRUCTION ALGORITHM . the proposed NETWORK TOPOLOGY ESTIMATION STRATEGY is based on the HIERARCHICAL TOPOLOGY CONSTRUCTION ALGORITHM . the proposed NETWORK TOPOLOGY ESTIMATION STRATEGY is based on the HIERARCHICAL TOPOLOGY CONSTRUCTION ALGORITHM . the proposed NETWORK TOPOLOGY ESTIMATION STRATEGY is based on the HIERARCHICAL TOPOLOGY CONSTRUCTION ALGORITHM .\n",
            "\n",
            "542 1000\n",
            "speech SPEECH RECOGNITION continues to be a challenging problem particularly for SPEECH RECOGNITION in NOISY ENVIRONMENTS . in this paper , we address this problem from the point of view of <unk> and chaos . by studying RECURRENCE TIME STATISTICS for CHAOTIC SYSTEMS , we find the nonstationarity and <unk> in a TIME SERIES are due to <unk> and lack of FRACTAL STRUCTURE in the signal . a POINCARÉ RECURRENCE METRIC is designed to determine the STATIONARITY CHANGE for SPEECH RECOGNITION . we consider the small area of beginning and ending of an utterance as transient . for NONSTATIONARY AND TRANSIENT TIME SERIES , we expect the average number of POINCARÉ RECURRENCE POINTS for each given small block will be different for different blocks of data subsets . however , the average number of RECURRENCE POINTS will stay nearly constant . the resulting RECURRENCE POINT VARIABILITY ALGORITHM is shown to be well suited for the DETECTION OF STATE TRANSITIONS in a TIME SERIES and is very robust for different types of NOISE , especially for LOW SNR . \n",
            "this paper presents a novel method for SPEECH ENDPOINT DETECTION in NOISY ENVIRONMENTS . the proposed method is based on a POINCARÉ RECURRENCE METRIC that is based on the POINCARÉ RECURRENCE METRIC of the TIME SERIES . the proposed method is based on a POINCARÉ RECURRENCE METRIC that is based on the DETECTION OF STATE TRANSITIONS . the proposed method is based on the POINCARÉ RECURRENCE METRIC and the DETECTION OF STATE TRANSITIONS . the proposed method is based on the POINCARÉ RECURRENCE METRIC and the DETECTION OF STATE TRANSITIONS . the proposed method is based on the POINCARÉ RECURRENCE METRIC and the DETECTION OF STATE TRANSITIONS .\n",
            "\n",
            "543 1000\n",
            "this paper explores the use of ANSWER SET PROGRAMMING in solving DISTRIBUTED CONSTRAINT OPTIMIZATION PROBLEMS . it makes the following contributions : -lrb- i -rrb- it shows how one can formulate ANSWER SET PROGRAMMING as LOGIC PROGRAMS ; -lrb- ii -rrb- it introduces ASP-DPOP , the first DCOP ALGORITHM that is based on LOGIC PROGRAMMING ; -lrb- iii -rrb- it experimentally shows that ASP-DPOP can be up to two orders of magnitude faster than ANSWER SET PROGRAMMING -lrb- its <unk> counterpart -rrb- as well as solve some problems that ANSWER SET PROGRAMMING fails to solve due to MEMORY LIMITATIONS ; and -lrb- iv -rrb- it demonstrates the applicability of ANSWER SET PROGRAMMING in the wide array of MULTI-AGENT PROBLEMS currently modeled as ANSWER SET PROGRAMMING . \n",
            "this paper addresses the problem of DISTRIBUTED CONSTRAINT OPTIMIZATION PROBLEMS in MULTI-AGENT PROBLEMS . in this paper , we propose a new DCOP ALGORITHM based on the DCOP ALGORITHM . the proposed DCOP ALGORITHM is based on the use of LOGIC PROGRAMMING to estimate the MEMORY LIMITATIONS and the MEMORY LIMITATIONS . experimental results show that the proposed method outperforms the conventional DCOP ALGORITHM in terms of MEMORY LIMITATIONS and MEMORY LIMITATIONS .\n",
            "\n",
            "544 1000\n",
            "a new BOOSTING ALGORITHM of <unk> and <unk> is used to improve the performance of DECISION TREES which are constructed <unk> : the INFORMATION RATIO CRITERION of QUINLAN 'S C4 .5 ALGORITHM . this BOOSTING ALGORITHM iteratively constructs a series of DECISION TREES , each DECISION TREE being trained and pruned on examples that have been filtered by previously trained TREES . examples that have been incorrectly classified by the previous TREES in the ensemble are resampled with higher probability to give a new PROBABILITY DISTRIBUTION for the next ace in the ensemble to <unk> on . results from optical <unk> : er <unk> ~ tion -lrb- ocr -rrb- , and KNOWLEDGE DISCOVERY and DATA MINING PROBLEMS show that in comparison to single TREES , or to TREES trained <unk> <unk> or to TREES trained on subsets of the feature space , the BOOSRING ENSEMBLE is much better . \n",
            "this paper proposes a new BOOSTING ALGORITHM for DATA MINING PROBLEMS . the proposed BOOSTING ALGORITHM is based on the INFORMATION RATIO CRITERION and the INFORMATION RATIO CRITERION of the BOOSRING ENSEMBLE . the proposed BOOSTING ALGORITHM is based on the INFORMATION RATIO CRITERION of the DECISION TREE and the PROBABILITY DISTRIBUTION . the proposed BOOSTING ALGORITHM is based on the INFORMATION RATIO CRITERION of the QUINLAN 'S C4 .5 ALGORITHM and the BOOSTING ALGORITHM .\n",
            "\n",
            "545 1000\n",
            "in many MULTILINGUAL TEXT CLASSIFICATION PROBLEMS , the documents in different languages often share the same set of categories . to reduce the LABELING COST of training a CLASSIFICATION MODEL for each individual language , it is important to transfer the LABEL KNOWLEDGE gained from one language to another language by conducting CROSS LANGUAGE CLASSIFICATION . in this paper we develop a novel SUBSPACE CO-REGULARIZED MULTI-VIEW LEARNING METHOD for CROSS LANGUAGE TEXT CLASSIFICATION . this SUBSPACE CO-REGULARIZED MULTI-VIEW LEARNING METHOD is built on PARALLEL CORPORA produced by MACHINE TRANSLATION . SUBSPACE CO-REGULARIZED MULTI-VIEW LEARNING METHOD jointly minimizes the training error of each CLASSIFIER in each language while penalizing the distance between the subspace representations of PARALLEL DOCUMENTS . our empirical study on a large set of CROSS LANGUAGE TEXT CLASSIFICATION TASKS shows the proposed SUBSPACE CO-REGULARIZED MULTI-VIEW LEARNING METHOD consistently outperforms a number of INDUCTIVE METHODS , DOMAIN ADAPTATION METHODS , and MULTI-VIEW LEARNING METHODS . \n",
            "this paper presents a SUBSPACE CO-REGULARIZED MULTI-VIEW LEARNING METHOD for MULTILINGUAL TEXT CLASSIFICATION PROBLEMS . the proposed SUBSPACE CO-REGULARIZED MULTI-VIEW LEARNING METHOD is based on the SUBSPACE CO-REGULARIZED MULTI-VIEW LEARNING METHOD and the SUBSPACE CO-REGULARIZED MULTI-VIEW LEARNING METHOD . the proposed SUBSPACE CO-REGULARIZED MULTI-VIEW LEARNING METHOD is based on the use of LABEL KNOWLEDGE and LABEL KNOWLEDGE . the proposed SUBSPACE CO-REGULARIZED MULTI-VIEW LEARNING METHOD is based on the use of LABEL KNOWLEDGE and LABEL KNOWLEDGE . the proposed SUBSPACE CO-REGULARIZED MULTI-VIEW LEARNING METHOD is compared with conventional MULTI-VIEW LEARNING METHODS and MULTI-VIEW LEARNING METHODS . the proposed SUBSPACE CO-REGULARIZED MULTI-VIEW LEARNING METHOD is compared with conventional INDUCTIVE METHODS and MULTI-VIEW LEARNING METHODS .\n",
            "\n",
            "546 1000\n",
            "in this paper a novel method is introduced for SEMI-SUPERVISED DIMENSIONALITY REDUCTION on FACIAL IMAGES extracted from STEREO VIDEOS . it operates on IMAGE DATA with multiple representations and calculates a PROJECTION MATRIX that preserves LOCALITY INFORMATION and a PRIORI PAIRWISE INFORMATION , in the form of MUST-LINK AND CANNOT-LINK CONSTRAINTS between the various DATA REPRESENTATIONS , as well as LABEL INFORMATION for a percentage of the data . the final DATA REPRESENTATIONS is a LINEAR COMBINATION of the projections of all DATA REPRESENTATIONS . the performance of the proposed semi-supervised multiple locality preserving projections method was evaluated in PERSON IDENTITY LABEL PROPAGATION on FACIAL IMAGES extracted from STEREO MOVIES . experimental results showed that the proposed method outperforms STATE OF THE ART METHODS . \n",
            "this paper addresses the problem of SEMI-SUPERVISED DIMENSIONALITY REDUCTION in STEREO VIDEOS . we propose a method for SEMI-SUPERVISED DIMENSIONALITY REDUCTION based on PERSON IDENTITY LABEL PROPAGATION . the proposed method is based on a PRIORI PAIRWISE INFORMATION and a PRIORI PAIRWISE INFORMATION . the proposed method is based on PERSON IDENTITY LABEL PROPAGATION and PERSON IDENTITY LABEL PROPAGATION . the proposed method is based on a PRIORI PAIRWISE INFORMATION and a PRIORI PAIRWISE INFORMATION . the proposed method is based on a PRIORI PAIRWISE INFORMATION and a PRIORI PAIRWISE INFORMATION . experimental results show that the proposed method outperforms the conventional STATE OF THE ART METHODS in terms of SEMI-SUPERVISED DIMENSIONALITY REDUCTION and SEMI-SUPERVISED DIMENSIONALITY REDUCTION .\n",
            "\n",
            "547 1000\n",
            "we show how DEEP LEARNING METHODS can be applied in the context of CROWDSOURCING and UNSUPERVISED ENSEMBLE LEARNING . first , we prove that the popular model of <unk> and <unk> , which assumes that all CLASSIFIERS are conditionally independent , is equivalent to a RESTRICTED BOLTZMANN MACHINE with a single HIDDEN NODE . hence , under this model , the POSTERIOR PROBABILITIES of the true labels can be instead estimated via a trained RESTRICTED BOLTZMANN MACHINE . next , to address the more general case , where CLASSIFIERS may strongly violate the CONDITIONAL INDEPENDENCE ASSUMPTION , we propose to apply RBM-BASED DEEP NEURAL NET . experimental results on various SIMULATED AND REAL-WORLD DATASETS demonstrate that our proposed dnn approach outperforms other state-of-the-art methods , in particular when the data violates the CONDITIONAL INDEPENDENCE ASSUMPTION . \n",
            "this paper presents a new method for UNSUPERVISED ENSEMBLE LEARNING based on a RESTRICTED BOLTZMANN MACHINE . the proposed method is based on a RESTRICTED BOLTZMANN MACHINE , which is based on the CONDITIONAL INDEPENDENCE ASSUMPTION . the proposed method is based on the CONDITIONAL INDEPENDENCE ASSUMPTION . the proposed method is based on the RBM-BASED DEEP NEURAL NET . the proposed method is based on the RBM-BASED DEEP NEURAL NET . the proposed method is based on the RBM-BASED DEEP NEURAL NET . the proposed method is based on the RBM-BASED DEEP NEURAL NET .\n",
            "\n",
            "548 1000\n",
            "we consider an important class of SIGNAL PROCESSING PROBLEMS where the SIGNAL OF INTEREST is known to be sparse , and can be recovered from data given AUXILIARY INFORMATION about how this data was generated . for example , a SPARSE GREEN 'S FUNCTION may be recovered from SEISMIC EXPERIMENTAL DATA using SPARSITY OPTIMIZATION when the SOURCE SIGNATURE is known . unfortunately , in practice this information is often missing , and must be recovered from data along with the signal using DECONVOLUTION TECHNIQUES . in this paper , we present a novel methodology to simultaneously solve for the SPARSE SIGNAL AND AUXILIARY PARAMETERS using a recently proposed VARIABLE PROJECTION TECHNIQUE . our main contribution is to combine VARIABLE PROJECTION with SPAR-SITY PROMOTING OPTIMIZATION , obtaining an efficient algorithm for LARGE-SCALE SPARSE DECONVOLUTION PROBLEMS . we demonstrate the algorithm on a SEISMIC IMAGING EXAMPLE . \n",
            "this paper proposes a VARIABLE PROJECTION TECHNIQUE for LARGE-SCALE SPARSE DECONVOLUTION PROBLEMS . the proposed VARIABLE PROJECTION TECHNIQUE is based on a VARIABLE PROJECTION TECHNIQUE and a VARIABLE PROJECTION TECHNIQUE . the proposed VARIABLE PROJECTION TECHNIQUE is based on a VARIABLE PROJECTION TECHNIQUE and a VARIABLE PROJECTION TECHNIQUE . the proposed VARIABLE PROJECTION TECHNIQUE is based on a VARIABLE PROJECTION TECHNIQUE and a VARIABLE PROJECTION TECHNIQUE . the proposed VARIABLE PROJECTION TECHNIQUE is based on a VARIABLE PROJECTION TECHNIQUE and a VARIABLE PROJECTION TECHNIQUE . the proposed VARIABLE PROJECTION TECHNIQUE is based on a VARIABLE PROJECTION TECHNIQUE . the proposed VARIABLE PROJECTION TECHNIQUE is based on a VARIABLE PROJECTION TECHNIQUE and is shown to be robust to SPARSITY OPTIMIZATION and DECONVOLUTION TECHNIQUES .\n",
            "\n",
            "549 1000\n",
            "this paper studies conditions under which a signal can be reconstructed from PARTIAL FREQUENCY CONTENT . we focus on signals in SHIFT-INVARIANT SPACES generated by multiple generators . for these signals , we derive a lower bound on the necessary SIGNAL BANDWIDTH as well as sufficient conditions on the generators such that SIGNAL RECOVERY is possible . when the available FREQUENCY CONTENT is not sufficient to recover the signal , we propose appropriate PRE-PROCESSING that can improve the RECONSTRUCTION ABILITY . \n",
            "this paper presents a method for SIGNAL RECOVERY in SHIFT-INVARIANT SPACES . the proposed method is based on the PRE-PROCESSING of the FREQUENCY CONTENT . the proposed method is based on the PRE-PROCESSING . the proposed method is based on a PRE-PROCESSING of the FREQUENCY CONTENT .\n",
            "\n",
            "550 1000\n",
            "two approaches for parallelization of h. 264 decoder , DATA PARTITION and FUNCTION PARTITION , are realized on a PAC DUO PLATFORM , which contains two parallel architecture core digital signal processors -lrb- <unk> 's -rrb- . eight baseline <unk> sequences are decoded and their execution cycles and waiting cycles are examined . there are three roots hindering the performance of DUAL-CORE DECODERS : INTER-CORE SYNCHRONIZATION , RESOURCE CONTENTION , and CACHE MISS . through the WAITING CYCLE ANALYSIS , the major reasons causing the degradation of DUAL CORE h. <unk> decoders are found . the INTER CORE SYNCHRONIZATION and RESOURCE CONTENTION principally slow down the EXECUTION SPEED of the DUAL CORE with FUNCTION PARTITION and DUAL CORE DATA PARTITION , respectively . the precious experience and analysis will help the software and hardware designers explore the mechanisms to improve performance of the MULTI CORE SCENARIOS . \n",
            "this paper addresses the problem of INTER CORE SYNCHRONIZATION in MULTI CORE SCENARIOS , such as INTER CORE SYNCHRONIZATION , INTER-CORE SYNCHRONIZATION , and INTER-CORE SYNCHRONIZATION . we propose a method for estimating the FUNCTION PARTITION from a PAC DUO PLATFORM . the proposed method is based on the DUAL CORE DATA PARTITION and the DUAL CORE DATA PARTITION . the proposed method is based on the DUAL CORE DATA PARTITION and the DUAL CORE DATA PARTITION . the proposed method is evaluated on the PAC DUO PLATFORM and the results show that the proposed method is robust to EXECUTION SPEED and EXECUTION SPEED .\n",
            "\n",
            "551 1000\n",
            "pathological speech usually refers to the VOICE DISORDERS resulting from <unk> in VOICE AND/OR in the ARTICULATORY MECHANISMS due to DISEASE , ILLNESS or other PHYSICAL PROBLEM in the SPEECH PRODUCTION SYSTEM . it may increase UNHEALTHY SOCIAL BEHAVIOR and VOICE ABUSE , and dramatically affect the patients ' quality of life . therefore , AUTOMATIC INTEL-LIGIBILITY DETECTION of PATHOLOGICAL SPEECH has an important role in the <unk> treatment of PATHOLOGICAL VOICES . this paper proposes to use asymmetric sparse kernel partial least squares classifier -lrb- <unk> -rrb- for INTELLIGIBILITY DETECTION of PATHOLOGICAL SPEECH . the proposed approach achieves an UN-WEIGHTED ACCURACY of <unk> % , which is <unk> % relative improvement of baseline system of an UN-WEIGHTED ACCURACY of <unk> % for the <unk> sub-challenge of interspeech 2012 speaker trait challenge . \n",
            "this paper presents a method for AUTOMATIC INTEL-LIGIBILITY DETECTION in PATHOLOGICAL SPEECH . the proposed method is based on the ARTICULATORY MECHANISMS and the ARTICULATORY MECHANISMS . the proposed method is based on the ARTICULATORY MECHANISMS and the ARTICULATORY MECHANISMS . the proposed method is based on the ARTICULATORY MECHANISMS and the ARTICULATORY MECHANISMS . the proposed method is based on the ARTICULATORY MECHANISMS and the ARTICULATORY MECHANISMS . experimental results show the effectiveness of the proposed method in terms of INTELLIGIBILITY DETECTION and INTELLIGIBILITY DETECTION .\n",
            "\n",
            "552 1000\n",
            "we use a combination of LINEAR SUPPORT VECTOR MACHINES and HIDDEN MARKOV MODELS for DIALOG ACT TAGGING in the HCRC MAPTASK CORPUS , and obtain better results than those previously reported . SUPPORT VECTOR MACHINES allow easy integration of SPARSE HIGH-DIMENSIONAL TEXT FEATURES and DENSE LOW-DIMENSIONAL ACOUSTIC FEATURES , and produce POSTERIOR PROBABILITIES usable by SEQUENCE LABELLING ALGORITHMS . the relative contribution of TEXT AND ACOUSTIC FEATURES for each class of dialog act is analyzed . \n",
            "this paper addresses the problem of DIALOG ACT TAGGING in HIDDEN MARKOV MODELS . we propose a new method for DIALOG ACT TAGGING based on LINEAR SUPPORT VECTOR MACHINES and LINEAR SUPPORT VECTOR MACHINES . the proposed method is based on the use of HIDDEN MARKOV MODELS and LINEAR SUPPORT VECTOR MACHINES . the proposed method is based on the HCRC MAPTASK CORPUS . the proposed method is based on the use of LINEAR SUPPORT VECTOR MACHINES and LINEAR SUPPORT VECTOR MACHINES . the experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "553 1000\n",
            "efficient LEARNING with NON-LINEAR KERNELS is often based on extracting FEATURES from the data that '' <unk> '' the kernel . while most constructions aim at obtaining LOW-DIMENSIONAL AND DENSE FEATURES , in this work we explore HIGH-DIMENSIONAL AND SPARSE ONES . we give a method to compute SPARSE FEATURES for ARBITRARY KERNELS , <unk> as a special case a popular map for the INTERSECTION KERNEL and extending it to ARBITRARY ADDITIVE KERNELS . we show that BUNDLE OPTIMISATION METHODS can handle efficiently these SPARSE FEATURES in LEARNING . as an application , we show that PRODUCT QUANTISATION can be interpreted as a SPARSE FEATURE ENCODING , and use this to significantly accelerate LEARNING with this technique . we demonstrate these ideas on IMAGE CLASSIFICATION with FISHER KERNELS and OBJECT DETECTION with DEFORMABLE PART MODELS on the challenging PASCAL VOC DATA , obtaining five to tenfold speed-ups as well as reducing MEMORY USE by an order of magnitude . \n",
            "this paper addresses the problem of OBJECT DETECTION in PASCAL VOC DATA . we propose a method for OBJECT DETECTION based on FISHER KERNELS . the proposed method is based on the use of SPARSE FEATURES and FISHER KERNELS . the proposed method is based on the use of SPARSE FEATURES and FISHER KERNELS . the proposed method is based on the use of SPARSE FEATURES and FISHER KERNELS . the proposed method is based on the use of SPARSE FEATURES and FISHER KERNELS . the proposed method is evaluated on PASCAL VOC DATA and PASCAL VOC DATA . the results show that the proposed method is robust and robust to OBJECT DETECTION and LEARNING .\n",
            "\n",
            "554 1000\n",
            "we consider the WEIGHTED SUM RATE OPTIMIZATION of WEIGHTED SUM RATE OPTIMIZATION in a MIMO INTERFERING MULTIPLE ACCESS CHANNEL . we propose to jointly optimize the users ' LINEAR PROCODERS as well as their base station -lrb- bs -rrb- associations . this approach enables the users to avoid CONGESTED BSS and can improve system performance as well as USER FAIRNESS . we formulate the WEIGHTED SUM RATE OPTIMIZATION into a NONCOOPERATIVE GAME , and develop an algorithm that allows the players to <unk> reach the NASH EQUILIBRIUM of the game . we show that every NE of the game is a STATIONARY SOLUTION of the WEIGHTED SUM RATE OPTIMIZATION PROBLEM , and propose an algorithm to compute the NE of the game . simulation results show that the proposed algorithm performs well in the presence of BS CONGESTION . \n",
            "this paper presents a new method for WEIGHTED SUM RATE OPTIMIZATION based on a WEIGHTED SUM RATE OPTIMIZATION PROBLEM . the proposed method is based on a WEIGHTED SUM RATE OPTIMIZATION PROBLEM . the proposed method is based on a WEIGHTED SUM RATE OPTIMIZATION PROBLEM . the proposed method is based on the WEIGHTED SUM RATE OPTIMIZATION PROBLEM . the proposed method is based on a WEIGHTED SUM RATE OPTIMIZATION PROBLEM . the proposed method is based on the WEIGHTED SUM RATE OPTIMIZATION PROBLEM . the proposed method is based on the WEIGHTED SUM RATE OPTIMIZATION PROBLEM . the proposed method is based on the WEIGHTED SUM RATE OPTIMIZATION PROBLEM . the proposed method is based on a WEIGHTED SUM RATE OPTIMIZATION PROBLEM . the proposed method is based on a WEIGHTED SUM RATE OPTIMIZATION PROBLEM .\n",
            "\n",
            "555 1000\n",
            "examples are often used along with TEXTUAL DESCRIPTIONS to help convey particular <unk> in INSTRUCTIONAL OR EXPLANATORY CONTEXTS . these accompanying examples reflect information in the surrounding text , and in turn , also influence the text . sometimes , examples replace possible -lrb- textual -rrb- <unk> in the description . it is thus clear that if OBJECT DESCRIPTIONS are to be generated , the system must incorporate strategies to handle examples . in this work , we shall investigate some of these issues in the generation of OBJECT DESCRIPTIONS . \n",
            "this paper presents a method for OBJECT DESCRIPTIONS in INSTRUCTIONAL OR EXPLANATORY CONTEXTS . the proposed method is based on the use of TEXTUAL DESCRIPTIONS , which can be used to improve the performance of TEXTUAL DESCRIPTIONS .\n",
            "\n",
            "556 1000\n",
            "in applications of GAUSSIAN PROCESSES where QUANTIFICATION OF UNCERTAINTY is of primary interest , it is necessary to accurately characterize the POSTERIOR DISTRIBUTION over COVARIANCE PARAMETERS . this paper proposes an adaptation of the STOCHASTIC GRADIENT LANGEVIN DYNAMICS ALGORITHM to draw samples from the POSTERIOR DISTRIBUTION over COVARIANCE PARAMETERS with NEGLIGIBLE BIAS and without the need to compute the MARGINAL LIKELIHOOD . in GAUSSIAN PROCESS REGRESSION , this has the enormous advantage that STOCHASTIC GRADIENTS can be computed by solving LINEAR SYSTEMS only . a novel UNBIASED LINEAR SYSTEMS SOLVER based on PARALLELIZABLE COVARIANCE MATRIX-VECTOR PRODUCTS is developed to accelerate the UNBIASED ESTIMATION OF GRADIENTS . the results demonstrate the possibility to enable scal-able and exact -lrb- in a monte carlo sense -rrb- <unk> of uncertainty in GAUSSIAN PROCESSES without imposing any special structure on the COVARI-ANCE or reducing the number of input vectors . \n",
            "this paper proposes a new method for UNBIASED ESTIMATION OF GRADIENTS based on GAUSSIAN PROCESS REGRESSION . the proposed STOCHASTIC GRADIENT LANGEVIN DYNAMICS ALGORITHM is based on a STOCHASTIC GRADIENT LANGEVIN DYNAMICS ALGORITHM of the POSTERIOR DISTRIBUTION . the proposed STOCHASTIC GRADIENT LANGEVIN DYNAMICS ALGORITHM is based on the STOCHASTIC GRADIENT LANGEVIN DYNAMICS ALGORITHM . the proposed STOCHASTIC GRADIENT LANGEVIN DYNAMICS ALGORITHM is based on the STOCHASTIC GRADIENT LANGEVIN DYNAMICS ALGORITHM . the proposed method is based on the STOCHASTIC GRADIENT LANGEVIN DYNAMICS ALGORITHM . the proposed STOCHASTIC GRADIENT LANGEVIN DYNAMICS ALGORITHM is based on the STOCHASTIC GRADIENT LANGEVIN DYNAMICS ALGORITHM . the proposed STOCHASTIC GRADIENT LANGEVIN DYNAMICS ALGORITHM is based on the STOCHASTIC GRADIENT LANGEVIN DYNAMICS ALGORITHM . the proposed STOCHASTIC GRADIENT LANGEVIN DYNAMICS ALGORITHM is based on the STOCHASTIC GRADIENT LANGEVIN DYNAMICS ALGORITHM . the proposed STOCHASTIC GRADIENT LANGEVIN DYNAMICS ALGORITHM is based on the STOCHASTIC GRADIENT LANGEVIN DYNAMICS ALGORITHM .\n",
            "\n",
            "557 1000\n",
            "<unk> rules , also known as DATA DEPENDENCIES in DATABASES , have been recently rediscovered as a promising family of languages for ONTOLOGY-BASED QUERY ANSWERING . in this paper , we prove that DISJUNCTIVE EMBEDDED DEPENDENCIES exactly capture the class of recursively <unk> ontologies in ONTOLOGY-BASED CONJUNCTIVE QUERY ANSWERING . our EXPRESSIVE COMPLETENESS result does not rely on any BUILT-IN LINEAR ORDER on the database . to establish the EXPRESSIVE COMPLETENESS , we introduce a novel SEMANTIC DEFINITION for ONTOLOGY-BASED CONJUNCTIVE QUERY ANSWERING . we also show that neither the CLASS OF DISJUNC-TIVE TUPLE-GENERATING DEPENDENCIES nor the CLASS OF EMBEDDED DEPENDENCIES is <unk> complete for recursively <unk> OCQA ONTOLOGIES . \n",
            "this paper addresses the problem of ONTOLOGY-BASED CONJUNCTIVE QUERY ANSWERING in DATABASES . we propose a method for ONTOLOGY-BASED CONJUNCTIVE QUERY ANSWERING based on the CLASS OF DISJUNC-TIVE TUPLE-GENERATING DEPENDENCIES . the proposed method is based on a SEMANTIC DEFINITION , which is based on the CLASS OF EMBEDDED DEPENDENCIES . the proposed method is based on a SEMANTIC DEFINITION , which is based on the CLASS OF DISJUNC-TIVE TUPLE-GENERATING DEPENDENCIES . the proposed method is based on a SEMANTIC DEFINITION , which is based on the CLASS OF DISJUNC-TIVE TUPLE-GENERATING DEPENDENCIES . the proposed method is evaluated on the DATABASES .\n",
            "\n",
            "558 1000\n",
            "we consider the problem of the SHAPE RECONSTRUCTION of a COMPACT OBJECT in X RAY TOMOGRAPHY when the CONTOUR of the object is modeled by a POLYGON . the problem is then to estimate the vertices of that POLYGON from a limited number of projections . the main objectives of this paper are : -lcb- to show how this SHAPE RECONSTRUCTION becomes equivalent to a generic MATHEMATICAL INVERSION PROBLEM which arises also in LINEAR ANTENNA ARRAY PROCESSING ; -lcb- to evaluate the performances of the CLASSICAL AP TECHNIQUES to handle with this SHAPE RECONSTRUCTION , and , -lcb- to propose a new method based on BAYESIAN ESTIMATION APPROACH for the resolution of this INVERSE PROBLEM . \n",
            "this paper presents a BAYESIAN ESTIMATION APPROACH for X RAY TOMOGRAPHY . the BAYESIAN ESTIMATION APPROACH is based on a BAYESIAN ESTIMATION APPROACH . the BAYESIAN ESTIMATION APPROACH is based on a BAYESIAN ESTIMATION APPROACH . the BAYESIAN ESTIMATION APPROACH is based on a BAYESIAN ESTIMATION APPROACH . the proposed BAYESIAN ESTIMATION APPROACH is based on a BAYESIAN ESTIMATION APPROACH . the proposed BAYESIAN ESTIMATION APPROACH is based on a BAYESIAN ESTIMATION APPROACH . the proposed BAYESIAN ESTIMATION APPROACH is based on a BAYESIAN ESTIMATION APPROACH . the proposed BAYESIAN ESTIMATION APPROACH is based on a BAYESIAN ESTIMATION APPROACH and is shown to be robust to SHAPE RECONSTRUCTION .\n",
            "\n",
            "559 1000\n",
            "the ability to MAP DESCRIPTIONS OF SCENES to 3D GEOMETRIC REPRESENTATIONS has many applications in areas such as ART , EDUCATION , and ROBOTICS . however , prior work on the text to 3D SCENE GENERATION TASK has used MANUALLY SPECIFIED OBJECT CATEGORIES and language that identifies them . we introduce a dataset of 3d scenes annotated with NATURAL LANGUAGE DESCRIPTIONS and learn from this data how to ground TEX-TUAL DESCRIPTIONS to PHYSICAL OBJECTS . our method successfully grounds a variety of LEXICAL TERMS to CONCRETE REFERENTS , and we show quantitatively that our method improves 3D SCENE GENERATION over previous work using purely RULE-BASED METHODS . we evaluate the fidelity and <unk> of 3d scenes generated with our GROUNDING APPROACH through HUMAN JUDGMENTS . to ease evaluation on this task , we also introduce an GROUNDING APPROACH that strongly correlates with HUMAN JUDGMENTS . \n",
            "this paper presents a new GROUNDING APPROACH for 3D SCENE GENERATION . the proposed GROUNDING APPROACH is based on the use of 3D GEOMETRIC REPRESENTATIONS and EDUCATION . the proposed GROUNDING APPROACH is based on the GROUNDING APPROACH and the GROUNDING APPROACH . the proposed GROUNDING APPROACH is based on the GROUNDING APPROACH and the GROUNDING APPROACH . the proposed GROUNDING APPROACH is compared with other RULE-BASED METHODS and RULE-BASED METHODS . the proposed GROUNDING APPROACH is compared with conventional RULE-BASED METHODS and RULE-BASED METHODS .\n",
            "\n",
            "560 1000\n",
            "layered LAYERED MODELS are a powerful way of describing NATURAL SCENES containing SMOOTH SURFACES that may overlap and occlude each other . for IMAGE MOTION ESTIMATION , such LAYERED MODELS have a long history but have not achieved the wide use or ACCURACY of NON-LAYERED METHODS . we present a new PROBABILISTIC MODEL of OPTICAL FLOW IN LAYERS that addresses many of the shortcomings of previous approaches . in particular , we define a PROBABILISTIC GRAPHICAL MODEL that explicitly captures : 1 -rrb- OCCLUSIONS and DISOCCLUSIONS ; 2 -rrb- depth ordering of the layers ; 3 -rrb- TEMPORAL CONSISTENCY of the LAYER SEGMENTATION . additionally the OPTICAL FLOW in each layer is modeled by a combination of a PARAMETRIC MODEL and a SMOOTH DEVIATION based on an MRF with a ROBUST SPATIAL PRIOR ; the resulting PROBABILISTIC GRAPHICAL MODEL allows <unk> in layers . finally , a key contribution is the formulation of the layers using an IMAGE-DEPENDENT HIDDEN FIELD PRIOR based on recent LAYERED MODELS for STATIC SCENE SEGMENTATION . the PROBABILISTIC GRAPHICAL MODEL achieves state-of-the-art results on the MIDDLEBURY BENCHMARK and produces MEANINGFUL SCENE SEGMENTATIONS as well as DETECTED OCCLUSION REGIONS . \n",
            "this paper presents a novel PROBABILISTIC MODEL for IMAGE MOTION ESTIMATION . the proposed PROBABILISTIC GRAPHICAL MODEL is based on a PROBABILISTIC GRAPHICAL MODEL and a PROBABILISTIC GRAPHICAL MODEL . the proposed PROBABILISTIC MODEL is based on the ROBUST SPATIAL PRIOR and the TEMPORAL CONSISTENCY of the PARAMETRIC MODEL . the proposed PROBABILISTIC MODEL is based on the IMAGE-DEPENDENT HIDDEN FIELD PRIOR and the TEMPORAL CONSISTENCY of the MRF . the proposed PROBABILISTIC MODEL is based on the IMAGE-DEPENDENT HIDDEN FIELD PRIOR and the TEMPORAL CONSISTENCY of the PARAMETRIC MODEL . the ACCURACY of the proposed PROBABILISTIC MODEL is demonstrated by simulation results .\n",
            "\n",
            "561 1000\n",
            "speech separation is a challenging problem at LOW SIGNAL-TO-NOISE RATIOS . SEPARATION can be formulated as a CLASSIFICATION PROBLEM . in this study , we focus on the SNR LEVEL OF-5 DB in which SPEECH is generally dominated by BACKGROUND NOISE . in such a LOW SNR CONDITION , extracting ROBUST FEATURES from a NOISY MIXTURE is crucial for successful CLASSIFICATION . using a common NEURAL NETWORK CLASSIFIER , we systematically compare separation performance of many MONAURAL FEATURES . in addition , we propose a new MULTI-RESOLUTION COCHLEAGRAM called MULTI-RESOLUTION COCHLEAGRAM , which is extracted from four <unk> of different resolutions to capture both LOCAL INFORMATION and <unk> context . comparisons using two NON-STATIONARY NOISES show a range of FEATURE ROBUSTNESS for SPEECH SEPARATION with the proposed MULTI-RESOLUTION COCHLEAGRAM performing the best . we also find that MULTI-RESOLUTION COCHLEAGRAM , a POST-PROCESSING TECHNIQUE previously used for ROBUST SPEECH RECOGNITION , improves SPEECH SEPARATION performance by smoothing the TEMPORAL TRAJECTORIES OF FEATURE DIMENSIONS . \n",
            "this paper presents a POST-PROCESSING TECHNIQUE for ROBUST SPEECH RECOGNITION . the NEURAL NETWORK CLASSIFIER is based on a NEURAL NETWORK CLASSIFIER of the NOISY MIXTURE and the LOCAL INFORMATION . the CLASSIFICATION PROBLEM is performed by a NEURAL NETWORK CLASSIFIER . the proposed POST-PROCESSING TECHNIQUE is based on a POST-PROCESSING TECHNIQUE of the NOISY MIXTURE . the proposed POST-PROCESSING TECHNIQUE is based on a POST-PROCESSING TECHNIQUE and is applied to the CLASSIFICATION PROBLEM . experimental results show the effectiveness of the proposed POST-PROCESSING TECHNIQUE for ROBUST SPEECH RECOGNITION .\n",
            "\n",
            "562 1000\n",
            "we construct a mixture of LOCALLY LINEAR GENERATIVE MODELS of a collection of PIXEL-BASED IMAGES OF DIGITS , and use LOCALLY LINEAR GENERATIVE MODELS for RECOGNITION . different models of a given digit are used to capture different styles of writing , and new images are classified by evaluating their <unk> under each LOCALLY LINEAR GENERATIVE MODELS . we use an EM-BASED ALGORITHM in which the M-STEP is computationally straightforward principal components analysis -lrb- M-STEP -rrb- . incorporating <unk> information -lsb- 12 -rsb- about expected LOCAL DEFORMATIONS only requires adding TANGENT VECTORS into the SAMPLE COVARIANCE MATRICES for the M-STEP , and it demonstrably improves performance . \n",
            "this paper addresses the problem of RECOGNITION in PIXEL-BASED IMAGES OF DIGITS . we propose a method for RECOGNITION based on LOCALLY LINEAR GENERATIVE MODELS . the proposed method is based on the EM-BASED ALGORITHM . the proposed method is based on the EM-BASED ALGORITHM . the proposed method is based on the use of LOCALLY LINEAR GENERATIVE MODELS . the proposed method is based on the EM-BASED ALGORITHM . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "563 1000\n",
            "an empirical comparison of CFG FILTERING TECHNIQUES for LTAG and HPSG is presented . we demonstrate that an approximation of HPSG produces a more effective CFG FILTER than that of LTAG . we also investigate the reason for that difference . \n",
            "this paper addresses the problem of HPSG for HPSG . in this paper , we show that the CFG FILTER can be used to derive CFG FILTERING TECHNIQUES for HPSG .\n",
            "\n",
            "564 1000\n",
            "parallel named entity PARALLEL NAMED ENTITY PAIRS are important resources in several NLP TASKS , such as , <unk> and mt systems . further , such PARALLEL NAMED ENTITY PAIRS may also be used for TRAINING TRANSLITERATION SYSTEMS , if PARALLEL NAMED ENTITY PAIRS are <unk> of each other . in this paper , we profile the performance of a MINING METHODOLOGY in mining PARALLEL NAMED ENTITY TRANSLITERATION PAIRS in ENGLISH and an INDIAN LANGUAGE , TAMIL , leveraging LINGUISTIC TOOLS in ENGLISH , and ARTICLE-ALIGNED COMPARABLE CORPORA in the two languages . we adopt a MINING METHODOLOGY parallel to that of -lsb- <unk> and <unk> , 2006 -rsb- , but we focus instead on mining PARALLEL NAMED ENTITY TRANSLITERATION PAIRS , using a WELL-TRAINED LINEAR CLASSIFIER to identify TRANSLITERATION PAIRS . we profile the performance at several operating parameters of our MINING METHODOLOGY and present the results that show the potential of the MINING METHODOLOGY in MINING TRANSLITERATIONS PAIRS ; in addition , we uncover a host of issues that need to be resolved , for effective mining of PARALLEL NAMED ENTITY TRANSLITERATION PAIRS . \n",
            "this paper presents a new method for TRAINING TRANSLITERATION SYSTEMS from ARTICLE-ALIGNED COMPARABLE CORPORA . the MINING METHODOLOGY is based on a WELL-TRAINED LINEAR CLASSIFIER , which is based on a WELL-TRAINED LINEAR CLASSIFIER . the proposed method is based on the use of PARALLEL NAMED ENTITY PAIRS , which is based on the WELL-TRAINED LINEAR CLASSIFIER . the proposed method is based on a WELL-TRAINED LINEAR CLASSIFIER . the proposed method is based on a WELL-TRAINED LINEAR CLASSIFIER . the proposed method is based on a WELL-TRAINED LINEAR CLASSIFIER . the proposed method is based on a WELL-TRAINED LINEAR CLASSIFIER , which is based on the WELL-TRAINED LINEAR CLASSIFIER . the proposed method is evaluated on the NLP TASKS .\n",
            "\n",
            "565 1000\n",
            "in many ESTIMATION PROBLEMS , the set of UNKNOWN PARAMETERS can be divided into a subset of desired parameters and a subset of NUISANCE PARAMETERS . using a maximum a POS-TERIORI APPROACH to PARAMETER ESTIMATION , these NUISANCE PARAMETERS are integrated out in the ESTIMATION PROCESS . this can result in an extremely COMPUTATIONALLY-INTENSIVE ESTI-MATOR . this paper proposes a method by which COMPUTATIONALLY-INTENSIVE I N TEGRATIONS over the NUISANCE PARAMETERS required in BAYESIAN ESTIMATION may be avoided under certain conditions . the <unk> method is an APPROXIMATE MAP ESTIMATOR which is much more compu-tationally ecient than direct , or even MONTE CARLO , integration of the joint posteriori distribution of the desired and NUISANCE PARAMETERS . as an example of its eciency , we apply the fast algorithm to MATCHED-ELD SOURCE LOCALIZA-TION in an uncertain environment . \n",
            "this paper addresses the problem of MATCHED-ELD SOURCE LOCALIZA-TION in MATCHED-ELD SOURCE LOCALIZA-TION . we propose a method for BAYESIAN ESTIMATION , which is based on a COMPUTATIONALLY-INTENSIVE ESTI-MATOR . the proposed method is based on the COMPUTATIONALLY-INTENSIVE ESTI-MATOR . the proposed method is based on the APPROXIMATE MAP ESTIMATOR . the proposed method is based on the COMPUTATIONALLY-INTENSIVE ESTI-MATOR . the proposed method is based on the POS-TERIORI APPROACH . the proposed method is based on a COMPUTATIONALLY-INTENSIVE ESTI-MATOR . the proposed method is based on the COMPUTATIONALLY-INTENSIVE ESTI-MATOR . the proposed method is based on the POS-TERIORI APPROACH . the proposed method is compared with other state-of-the-art methods .\n",
            "\n",
            "566 1000\n",
            "in this paper we are studying the use of two microphones for ACOUSTIC FEEDBACK CANCELLATION in HEARING AIDS . with the two MICROPHONES APPROACH , an additional microphone is employed to provide added information about the signals which is then utilized to obtain an INCOMING SIGNAL ESTIMATE . this INCOMING SIGNAL ESTIMATE is removed from the ERROR SIGNAL prior to adapting the CANCELER , thus removing the UNDESIRED SIGNAL CORRELATION . in this paper , we propose to use ORTHOGONAL TRANSFORMS with the two MICROPHONES APPROACH . the discrete fourier transform and the DISCRETE COSINE TRANSFORM are implemented to transform the ADAPTIVE FILTER SIGNALS . also , a bank of ADAPTIVE FILTERS is employed , each adapting to different portions of the spectrum for a finer control of the ADAPTATION PROCESS . simulation results based on REAL MEASURED FEEDBACK PATHS and SPEECH SIGNALS show improved CONVERGENCE RATES and stable solutions . \n",
            "this paper addresses the problem of ACOUSTIC FEEDBACK CANCELLATION in HEARING AIDS . we propose a method for ACOUSTIC FEEDBACK CANCELLATION based on the DISCRETE COSINE TRANSFORM . the proposed method is based on the DISCRETE COSINE TRANSFORM of the DISCRETE COSINE TRANSFORM and the ADAPTATION PROCESS . the proposed method is based on the DISCRETE COSINE TRANSFORM and the DISCRETE COSINE TRANSFORM . the proposed method is based on the DISCRETE COSINE TRANSFORM and the ADAPTATION PROCESS . the proposed method is based on the DISCRETE COSINE TRANSFORM and the ADAPTATION PROCESS .\n",
            "\n",
            "567 1000\n",
            "goal recognition design -lrb- <unk> -rrb- problems involve identifying the best ways to modify the underlying environment that the agents operate in , typically by making a subset of feasible actions infeasible , in such a way that agents are forced to reveal their goals as early as possible . thus far , existing work assumes that the outcomes of the actions of the agents are deterministic , which might be unrealistic in REAL-WORLD PROBLEMS . for example , WHEEL SLIPPAGE in robots cause the outcomes of their movements to be stochastic . in this paper , we generalize the GOAL RECOGNITION DESIGN PROBLEMS to STOCHASTIC GRD PROBLEMS , which handle STOCHASTIC ACTION OUTCOMES . we also generalize the WORST-CASE DISTINCTIVENESS MEASURE , which measures the goodness of a solution , to take STOCHASTIC-ITY into account . finally , we introduce MARKOV DECISION PROCESS BASED ALGORITHMS to compute the WCD and minimize MARKOV DECISION PROCESS BASED ALGORITHMS by making up to k actions infeasible . \n",
            "this paper presents a new method for GOAL RECOGNITION DESIGN PROBLEMS in REAL-WORLD PROBLEMS . the method is based on the WORST-CASE DISTINCTIVENESS MEASURE of the WORST-CASE DISTINCTIVENESS MEASURE of the WORST-CASE DISTINCTIVENESS MEASURE . the proposed method is based on the use of STOCHASTIC GRD PROBLEMS in the presence of STOCHASTIC ACTION OUTCOMES . the proposed method is based on the WORST-CASE DISTINCTIVENESS MEASURE of the MARKOV DECISION PROCESS BASED ALGORITHMS .\n",
            "\n",
            "568 1000\n",
            "we show that the approaches to 3D RECONSTRUCTION that use VOLUMETRIC GRAPH CUTS to minimize a COST FUNCTION over the OBJECT SURFACE have two types of biases , the MINIMAL SURFACE BIAS and the DISCRETIZATION BIAS . these biases make it difficult to recover SURFACE EXTRUSIONS and other details , especially when a NON-LAMBERTIAN PHOTO-CONSISTENCY MEASURE is used . to reduce these biases , we propose a new ITERATIVE GRAPH CUTS BASED ALGORITHM that operates on the SURFACE DISTANCE GRID , which is a special discretization of the 3D SPACE , constructed using a SIGNED DISTANCE TRANSFORM of the current surface estimate . it can be shown that SURFACE DISTANCE GRID significantly reduces the MINIMAL SURFACE BIAS , and transforms the DISCRETIZATION BIAS into a controllable degree of SURFACE SMOOTHNESS . experiments on 3D RECONSTRUCTION OF NON-LAMBERTIAN OBJECTS confirm the effectiveness of our ITERATIVE GRAPH CUTS BASED ALGORITHM over previous methods . \n",
            "this paper proposes a new ITERATIVE GRAPH CUTS BASED ALGORITHM for 3D RECONSTRUCTION OF NON-LAMBERTIAN OBJECTS . the proposed ITERATIVE GRAPH CUTS BASED ALGORITHM is based on the SIGNED DISTANCE TRANSFORM and the SIGNED DISTANCE TRANSFORM . the proposed ITERATIVE GRAPH CUTS BASED ALGORITHM is based on the SIGNED DISTANCE TRANSFORM and the SIGNED DISTANCE TRANSFORM . the proposed ITERATIVE GRAPH CUTS BASED ALGORITHM is based on the SIGNED DISTANCE TRANSFORM and the SIGNED DISTANCE TRANSFORM . the proposed ITERATIVE GRAPH CUTS BASED ALGORITHM is compared with the conventional ITERATIVE GRAPH CUTS BASED ALGORITHM and the ITERATIVE GRAPH CUTS BASED ALGORITHM . the proposed ITERATIVE GRAPH CUTS BASED ALGORITHM is compared with the conventional ITERATIVE GRAPH CUTS BASED ALGORITHM and the ITERATIVE GRAPH CUTS BASED ALGORITHM .\n",
            "\n",
            "569 1000\n",
            "<unk> is a useful SIMRANK-LIKE MEASURE OF SIMILARITY based on GRAPH STRUCTURE . the existing method iteratively computes each pair of CO-SIMRANK SCORE from a dot product of two PAGERANK VECTORS , <unk> o -lrb- log -lrb- 1 / ǫ -rrb- n 3 -rrb- time to compute all pairs of CO-SIMRANK SCORE in a GRAPH with n nodes , to attain a desired accuracy ǫ . in this study , we devise a model , CO-SIMRANK , to speed up the retrieval of all pairs of CO-SIMRANK SCORE to o -lrb- log 2 -lrb- log -lrb- 1 / ǫ -rrb- -rrb- n 3 -rrb- time . moreover , we show the optimality of CO-SIMRANK among other <unk> - -lrb- u k -rrb- variations , and integrate it with a MATRIX DECOMPOSITION BASED METHOD on SINGULAR GRAPHS to attain higher efficiency . the viable experiments verify the superiority of CO-SIMRANK to others . \n",
            "this paper presents a new method for SIMRANK-LIKE MEASURE OF SIMILARITY , which is based on the SIMRANK-LIKE MEASURE OF SIMILARITY of a GRAPH . the CO-SIMRANK is based on the SIMRANK-LIKE MEASURE OF SIMILARITY of the GRAPH . the proposed MATRIX DECOMPOSITION BASED METHOD is based on a MATRIX DECOMPOSITION BASED METHOD , which is based on a MATRIX DECOMPOSITION BASED METHOD . the CO-SIMRANK SCORE of the proposed method is compared with the conventional MATRIX DECOMPOSITION BASED METHOD .\n",
            "\n",
            "570 1000\n",
            "automated musical accompaniment of human performers often requires an agent be able to follow a musical score with similar facility to that of a HUMAN PERFORMER . systems described in the literature represent musical scores in a way that assumes no LARGE-SCALE STRUCTURAL VARIATION of the piece during performance . if the performer deviates from the expected path by <unk> or repeating a section , the system may become lost . we describe a way to automatically generate a MARKOV MODEL from a WRITTEN SCORE that models the SCORE FORM , and an ON-LINE ALGORITHM to align a performance to a score . the resulting system can follow performances that take alternate paths through the score without losing its place . we compare the performance of our system to that of SEQUENCE-BASED SCORE FOLLOWERS on a MELODIC CORPUS OF 98 JAZZ MELODIES . results show that explicitly representing the BRANCHING STRUCTURE of a score significantly improves score following when the branch a performer may take is unknown beforehand . \n",
            "this paper addresses the problem of AUTOMATED MUSICAL ACCOMPANIMENT OF HUMAN PERFORMERS in HUMAN PERFORMER . we propose a MARKOV MODEL based on the MELODIC CORPUS OF 98 JAZZ MELODIES . the proposed ON-LINE ALGORITHM is based on the MELODIC CORPUS OF 98 JAZZ MELODIES . the proposed ON-LINE ALGORITHM is based on the MELODIC CORPUS OF 98 JAZZ MELODIES . the proposed ON-LINE ALGORITHM is evaluated on the MELODIC CORPUS OF 98 JAZZ MELODIES . the experimental results show that the proposed method outperforms the conventional ON-LINE ALGORITHM in terms of WRITTEN SCORE .\n",
            "\n",
            "571 1000\n",
            "this paper describes the application of RAO-BLACKWELLISED GIBBS SAMPLING to SPEECH RECOGNITION using SWITCHING LINEAR DY-NAMICAL SYSTEMS -lrb- RAO-BLACKWELLISED GIBBS SAMPLING -rrb- . the RAO-BLACKWELLISED GIBBS SAMPLING is a hybrid of standard HIDDEN MARKOV MODELS and LINEAR DYNAMICAL SYSTEMS . RAO-BLACKWELLISED GIBBS SAMPLING is an extension of the STOCHASTIC SEGMENT MODEL as RAO-BLACKWELLISED GIBBS SAMPLING relaxes the ASSUMPTION OF INDEPENDENT SEGMENTS . RAO-BLACKWELLISED GIBBS SAMPLING explicitly take into account the strong co-articulation present in SPEECH . unfortunately , INFERENCE in RAO-BLACKWELLISED GIBBS SAMPLING is intractable unless the DISCRETE STATE SEQUENCE is known . RAO-BLACKWELLISED GIBBS SAMPLING is one approach that may be applied for both improved training and decoding for this form of INTRACTABLE MODEL . the theory of RAO-BLACKWELLISED GIBBS SAMPLING and RAO-BLACKWELLISED GIBBS SAMPLING is described , along with an efficient PROPOSAL MECHANISM . the performance of the RAO-BLACKWELLISED GIBBS SAMPLING using RAO-BLACKWELLISED GIBBS SAMPLING for training and INFERENCE is evaluated on the ARPA RESOURCE MANAGEMENT TASK . \n",
            "this paper proposes a new PROPOSAL MECHANISM for SPEECH RECOGNITION . the PROPOSAL MECHANISM is based on the ASSUMPTION OF INDEPENDENT SEGMENTS and the ASSUMPTION OF INDEPENDENT SEGMENTS . the proposed PROPOSAL MECHANISM is based on RAO-BLACKWELLISED GIBBS SAMPLING and RAO-BLACKWELLISED GIBBS SAMPLING . the proposed PROPOSAL MECHANISM is based on RAO-BLACKWELLISED GIBBS SAMPLING and RAO-BLACKWELLISED GIBBS SAMPLING . the proposed PROPOSAL MECHANISM is evaluated on the ARPA RESOURCE MANAGEMENT TASK and on the ARPA RESOURCE MANAGEMENT TASK . experimental results show that the proposed PROPOSAL MECHANISM outperforms the conventional INTRACTABLE MODEL in terms of SPEECH RECOGNITION and INFERENCE .\n",
            "\n",
            "572 1000\n",
            "energy detection is widely used by COGNITIVE RADIOS for SPECTRUM SENSING . during a silent period , SECONDARY USERS are kept silent so that the ENERGY DETECTOR does not confuse SU SIGNALS for primary user -lrb- pu -rrb- signals . due to imperfect coordination , an SECONDARY USERS may transmit during a silent period and cause possible false alarms . we propose to leverage matched filters that already exist in many SUS to alleviate the impact of such SU INTERFERENCE by combining the matched filtering result and the ENERGY DETECTION result . the analysis shows that for practical purposes , our algorithm virtually eliminates all of the negative impact of SU INTERFERENCE with only negligible penalty in DELAY and energy consumption . \n",
            "this paper presents a method for ENERGY DETECTION from COGNITIVE RADIOS . the ENERGY DETECTOR is based on the ENERGY DETECTOR and the ENERGY DETECTOR . the proposed ENERGY DETECTOR is based on the ENERGY DETECTOR , which is based on the ENERGY DETECTOR . the proposed method is based on the ENERGY DETECTOR . the proposed method is based on the use of SU SIGNALS , which are used for ENERGY DETECTION . the experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "573 1000\n",
            "this paper proposes a DICTIONARY LEARNING FRAMEWORK that combines the proposed <unk> -lrb- <unk> -rrb- or RECONSTRUCTED BLOCK/GROUP SPARSE CODING SCHEMES with the novel <unk> coherence suppression dictionary learning -lrb- <unk> -rrb- algorithm . an important and distinguishing DICTIONARY BLOCKS of the proposed DICTIONARY LEARNING FRAMEWORK is that all DICTIONARY BLOCKS are trained simultaneously with respect to each data group while the INTRA-BLOCK COHERENCE being explicitly minimized as an important objective . we provide both empirical evidence and heuristic support for this DICTIONARY BLOCKS that can be considered as a direct consequence of incorporating both the GROUP STRUCTURE for the INPUT DATA and the BLOCK STRUCTURE for the dictionary in the LEARNING PROCESS . the OPTIMIZATION PROBLEMS for both the DICTIONARY LEARNING and SPARSE CODING can be solved efficiently using BLOCK-GRADIENT DESCENT , and the details of the OPTIMIZATION ALGORITHMS are presented . we evaluate the proposed methods using WELL-KNOWN DATASETS , and favorable comparisons with state-of-the-art DICTIONARY LEARNING METHODS demonstrate the viability and validity of the proposed DICTIONARY LEARNING FRAMEWORK . \n",
            "this paper addresses the problem of SPARSE CODING from INPUT DATA . we propose a DICTIONARY LEARNING FRAMEWORK based on BLOCK-GRADIENT DESCENT and BLOCK-GRADIENT DESCENT . the LEARNING PROCESS is formulated as a LEARNING PROCESS . the LEARNING PROCESS is formulated as a DICTIONARY LEARNING FRAMEWORK . the LEARNING PROCESS is formulated as a LEARNING PROCESS . the proposed method is based on the use of DICTIONARY BLOCKS and BLOCK-GRADIENT DESCENT . experimental results on WELL-KNOWN DATASETS demonstrate the effectiveness of the proposed method .\n",
            "\n",
            "574 1000\n",
            "we propose an ADAPTIVE DIFFUSION STRATEGY with LIMITED COMMUNICATION OVERHEAD by cutting off all links but one for each NODE in the network . we keep the '' best '' neighbor that has the smallest estimated <unk> measure and ignore the other neighbors . the COMBINATION COEFFICIENTS for the INTERACTING NODES are calculated via a MAXIMAL-RATIO-COMBINING RULE to minimize the STEADY-STATE MEAN-SQUARE-DEVIATION . simulation results illustrate that , with less COMMUNICATION OVERHEAD and less computations , the proposed ADAPTIVE DIFFUSION STRATEGY performs well and outperforms other related methods with similar overheads . \n",
            "this paper presents a new method for STEADY-STATE MEAN-SQUARE-DEVIATION in INTERACTING NODES . the proposed method is based on the ADAPTIVE DIFFUSION STRATEGY with a MAXIMAL-RATIO-COMBINING RULE . the proposed method is based on the ADAPTIVE DIFFUSION STRATEGY . the proposed method is based on the ADAPTIVE DIFFUSION STRATEGY . the proposed method is compared with the conventional ADAPTIVE DIFFUSION STRATEGY .\n",
            "\n",
            "575 1000\n",
            "conventional VIDEO SUMMARIZATION METHODS <unk> -rrb- cus predominantly on summarizing videos along the TIME AXIS , such as building a MOVIE TRAILER : the resulting VIDEO TRAILER tends to retain much EMPTY SPUCE in the background of the video <unk> while discarding much <unk> video content due lo size limit . in this <unk> we propose a novel SPACE-TIME VIDEO SUMMARIZATION METHOD which we call SPACE-TIME VIDEO MONTAGE . the SPACE-TIME VIDEO SUMMARIZATION METHOD simultaneously analyzes both the SPATIAL AND TEMPORAL INJBRMATION DISTRIBUTION in a VIDEO SEQUENCE , and <unk> the visually informative space-time portions of the input videos . the informative video <unk> are represented in VOLUMETRIC LA.YERS . the LAYERS are then <unk> together in a SMULL OUZPUT VIDEO VOLUME such <unk> the total amount of VISUAL INFORMATION in the VIDEO VOLUME is maximized . to achieve the PACKING PROCESS , we develop a new SPACE-TIME VIDEO SUMMARIZATION METHOD based upon THE3RST-JT UND GRAPH CUT OPTIMIZATION TECHNIQUES . since our SPACE-TIME VIDEO SUMMARIZATION METHOD is <unk> to <unk> <unk> spatially <unk> temporally less informative portions , it is <unk> to <unk> much more <unk> yet highly informative output videos . the <unk> -lrb- $ our SPACE-TIME VIDEO SUMMARIZATION METHOD is validated by extensive experiments over a wide variety c ~ videos . \n",
            "this paper presents a novel SPACE-TIME VIDEO SUMMARIZATION METHOD for SPACE-TIME VIDEO MONTAGE . the proposed method is based on a SPACE-TIME VIDEO SUMMARIZATION METHOD , which is based on a SPACE-TIME VIDEO SUMMARIZATION METHOD . the proposed method is based on a SPACE-TIME VIDEO SUMMARIZATION METHOD . the proposed method is based on the THE3RST-JT UND GRAPH CUT OPTIMIZATION TECHNIQUES . the proposed method is based on the THE3RST-JT UND GRAPH CUT OPTIMIZATION TECHNIQUES . the proposed method is based on a SPACE-TIME VIDEO SUMMARIZATION METHOD . the proposed method is based on a SPACE-TIME VIDEO SUMMARIZATION METHOD . the proposed method is based on the THE3RST-JT UND GRAPH CUT OPTIMIZATION TECHNIQUES . the proposed method is based on the THE3RST-JT UND GRAPH CUT OPTIMIZATION TECHNIQUES . the proposed method is based on the THE3RST-JT UND GRAPH CUT OPTIMIZATION TECHNIQUES . the proposed method is based on the THE3RST-JT UND GRAPH CUT OPTIMIZATION TECHNIQUES . the proposed method is based on a SPACE-TIME VIDEO SUMMARIZATION METHOD . the proposed method is based on the THE3RST-JT UND GRAPH CUT OPTIMIZATION TECHNIQUES . the proposed method is based on a SPACE-TIME VIDEO SUMMARIZATION METHOD . the proposed method is based on the THE3RST-JT UND GRAPH CUT OPTIMIZATION TECHNIQUES .\n",
            "\n",
            "576 1000\n",
            "this paper addresses the SEARCH PROBLEM in TEXTUAL INFERENCE , where systems need to infer one piece of text from another . a prominent approach to this SEARCH PROBLEM is attempts to transform one text into the other through a sequence of INFERENCE-PRESERVING TRANSFORMATIONS , a.k.a. a proof , while estimating the proof 's validity . this raises a search challenge of finding the best possible proof . we explore this challenge through a comprehensive investigation of prominent SEARCH ALGORITHMS and propose two novel ALGORITHMIC COMPONENTS specifically designed for TEXTUAL INFERENCE : a GRADIENT-STYLE EVALUATION FUNCTION , and a LOCAL-LOOKAHEAD NODE EXPANSION METHOD . evaluations , using the OPEN-SOURCE SYSTEM , BIUTEE , show the contribution of these ideas to search efficiency and PROOF QUALITY . \n",
            "this paper addresses the problem of TEXTUAL INFERENCE in TEXTUAL INFERENCE . in this paper , we propose a new method for TEXTUAL INFERENCE based on INFERENCE-PRESERVING TRANSFORMATIONS . the proposed method is based on the use of INFERENCE-PRESERVING TRANSFORMATIONS such as BIUTEE , and a LOCAL-LOOKAHEAD NODE EXPANSION METHOD . the proposed method is compared with state-of-the-art SEARCH ALGORITHMS such as BIUTEE , and the LOCAL-LOOKAHEAD NODE EXPANSION METHOD .\n",
            "\n",
            "577 1000\n",
            "this paper presents a new ANALYTICAL MODEL for the normalized least mean square -lrb- nlms -rrb- adaptive algorithm . the new ANALYTICAL MODEL is derived using a STOCHASTIC DIFFERENTIAL EQUATION APPROACH . an accurate estimate of the STEADY-STATE WEIGHT-ERROR CORRELATIONS is also derived , which leads to an improved ANALYTICAL MODEL performance for medium and large step sizes . NUMERICAL SIMULATIONS compare the new ANALYTICAL MODEL with existing ANALYTICAL MODEL and show better agreement with MONTE CARLO SIMULATIONS . \n",
            "this paper presents a new ANALYTICAL MODEL for STEADY-STATE WEIGHT-ERROR CORRELATIONS . the proposed STOCHASTIC DIFFERENTIAL EQUATION APPROACH is based on a STOCHASTIC DIFFERENTIAL EQUATION APPROACH . the proposed STOCHASTIC DIFFERENTIAL EQUATION APPROACH is compared with the conventional ANALYTICAL MODEL and the ANALYTICAL MODEL .\n",
            "\n",
            "578 1000\n",
            "machine learning contains many COMPUTATIONAL BOTTLENECKS in the form of NESTED SUMMATIONS OVER DATASETS . computation of these MACHINE LEARNING is typically o -lrb- n 2 -rrb- or higher , which severely limits application to large datasets . we present a MULTI-STAGE STRATIFIED MONTE CARLO METHOD for approximating such MACHINE LEARNING with PROBABILISTIC RELATIVE ERROR CONTROL . the essential idea is fast approximation by sampling in trees . this MULTI-STAGE STRATIFIED MONTE CARLO METHOD differs from many previous SCALABILITY TECHNIQUES -lrb- such as MULTI-TREE METHODS -rrb- in that its error is stochastic , but we derive conditions for ERROR CONTROL and demonstrate that they work . further , we give a THEORETICAL SAMPLE COMPLEXITY for the MULTI-STAGE STRATIFIED MONTE CARLO METHOD that is independent of DATASET SIZE , and show that this appears to hold in experiments , where SPEEDUPS reach as high as 10 14 , many orders of magnitude beyond the previous state of the art . \n",
            "this paper proposes a new MULTI-STAGE STRATIFIED MONTE CARLO METHOD for MACHINE LEARNING . the proposed MULTI-STAGE STRATIFIED MONTE CARLO METHOD is based on the MULTI-STAGE STRATIFIED MONTE CARLO METHOD . the proposed MULTI-STAGE STRATIFIED MONTE CARLO METHOD is based on the MULTI-STAGE STRATIFIED MONTE CARLO METHOD . the proposed MULTI-STAGE STRATIFIED MONTE CARLO METHOD is based on the MULTI-STAGE STRATIFIED MONTE CARLO METHOD . the proposed MULTI-STAGE STRATIFIED MONTE CARLO METHOD is based on the THEORETICAL SAMPLE COMPLEXITY of the MULTI-STAGE STRATIFIED MONTE CARLO METHOD . the proposed MULTI-STAGE STRATIFIED MONTE CARLO METHOD is compared with the conventional MULTI-STAGE STRATIFIED MONTE CARLO METHOD .\n",
            "\n",
            "579 1000\n",
            "we present in this paper a novel approach for training DETERMINISTIC AUTO-ENCODERS . we show that by adding a well chosen penalty term to the CLASSICAL RECONSTRUCTION COST FUNCTION , we can achieve results that equal or surpass those attained by other REGULARIZED AUTO-ENCODERS as well as DENOISING AUTO-ENCODERS on a range of datasets . this penalty term corresponds to the FROBENIUS NORM of the JACOBIAN MATRIX of the ENCODER ACTIVATIONS with respect to the input . we show that this penalty term results in a LOCALIZED SPACE CONTRACTION which in turn yields robust FEATURES on the ACTIVATION LAYER . furthermore , we show how this penalty term is related to both REGULARIZED AUTO-ENCODERS and DENOISING AUTO-ENCODERS and how it can be seen as a link between DETERMINISTIC AND NON-DETERMINISTIC AUTO-ENCODERS . we find empirically that this penalty helps to <unk> a representation that better captures the LOCAL DIRECTIONS OF VARIATION dictated by the data , corresponding to a LOWER-DIMENSIONAL NON-LINEAR MANIFOLD , while being more invariant to the vast majority of directions orthogonal to the MANIFOLD . finally , we show that by using the learned FEATURES to initialize a MLP , we achieve state of the art classification error on a range of datasets , surpassing other methods of PRE-TRAINING . \n",
            "this paper presents a new method for DENOISING AUTO-ENCODERS based on REGULARIZED AUTO-ENCODERS . the proposed method is based on the use of REGULARIZED AUTO-ENCODERS and REGULARIZED AUTO-ENCODERS . the proposed method is based on the use of REGULARIZED AUTO-ENCODERS and REGULARIZED AUTO-ENCODERS . the proposed method is based on the use of REGULARIZED AUTO-ENCODERS and REGULARIZED AUTO-ENCODERS . the proposed method is based on the JACOBIAN MATRIX and the MLP . the proposed method is based on the use of DETERMINISTIC AUTO-ENCODERS and REGULARIZED AUTO-ENCODERS . the proposed method is based on the MLP and the MLP . the proposed method is based on the DETERMINISTIC AND NON-DETERMINISTIC AUTO-ENCODERS and the MLP . the proposed method is based on the DETERMINISTIC AND NON-DETERMINISTIC AUTO-ENCODERS and the MLP .\n",
            "\n",
            "580 1000\n",
            "we present an overview of recent work in which EYE MOVEMENTS are monitored as people follow SPOKEN INSTRUCTIONS to move objects or pictures in a VISUAL WORKSPACE . subjects naturally make SACCADIC EYE-MOVEMENTS to objects that are closely <unk> to relevant information in the instruction . thus the <unk> provide a WINDOW into the RAPID MENTAL PROCESSES that underlie SPOKEN LANGUAGE COMPREHENSION . we review studies of REFERENCE RESOLUTION , WORD RECOGNITION , and pragmatic effects on SYNTACTIC AMBIGUITY RESOLUTION . our studies show that people seek to establish reference with respect to their behavioral goals during the earliest moments of LINGUISTIC PROCESSING . moreover , <unk> relevant NON-LINGUISTIC INFORMATION immediately affects how the LINGUISTIC INPUT is initially structured . \n",
            "this paper presents a method for SPOKEN LANGUAGE COMPREHENSION in SPOKEN LANGUAGE COMPREHENSION . the method is based on the use of NON-LINGUISTIC INFORMATION and NON-LINGUISTIC INFORMATION . the proposed method is based on the use of NON-LINGUISTIC INFORMATION and NON-LINGUISTIC INFORMATION . the proposed method is based on the NON-LINGUISTIC INFORMATION and the NON-LINGUISTIC INFORMATION . the proposed method is based on the NON-LINGUISTIC INFORMATION and the NON-LINGUISTIC INFORMATION . the proposed method is compared with the conventional method for WORD RECOGNITION and WORD RECOGNITION .\n",
            "\n",
            "581 1000\n",
            "matrix factorization techniques have been frequently applied in INFORMATION PROCESSING TASKS . among them , NON-NEGATIVE MATRIX FACTORIZATION have received considerable attentions due to its psychological and physiological interpretation of naturally occurring data whose representation may be <unk> in HUMAN BRAIN . on the other hand , from GEOMETRIC PERSPECTIVE the data is usually sampled from a LOW DIMENSIONAL MANIFOLD embedded in HIGH DIMENSIONAL AMBIENT SPACE . one hopes then to find a COMPACT REPRESENTATION which uncovers the HIDDEN TOPICS and simultaneously respects the INTRINSIC GEOMETRIC STRUCTURE . in this paper , we propose a novel algorithm , called locality preserving non-negative matrix factorization -lrb- <unk> -rrb- , for this purpose . for two data points , we use KL-DIVERGENCE to evaluate their similarity on the HIDDEN TOPICS . the optimal maps are obtained such that the FEATURE VALUES on HIDDEN TOPICS are restricted to be non-negative and vary smoothly along the GEODESICS OF THE DATA MANIFOLD . our empirical study shows the encouraging results of the proposed algorithm in comparisons to the state-of-the-art algorithms on two large HIGH-DIMENSIONAL DATABASES . \n",
            "this paper presents a method for NON-NEGATIVE MATRIX FACTORIZATION based on NON-NEGATIVE MATRIX FACTORIZATION . the proposed method is based on the GEODESICS OF THE DATA MANIFOLD of the HUMAN BRAIN . the KL-DIVERGENCE is based on the GEODESICS OF THE DATA MANIFOLD of the HUMAN BRAIN . the proposed method is based on the GEODESICS OF THE DATA MANIFOLD of the HUMAN BRAIN . the proposed method is based on the GEODESICS OF THE DATA MANIFOLD . the proposed method is based on the GEODESICS OF THE DATA MANIFOLD . the proposed method is based on the GEODESICS OF THE DATA MANIFOLD . the proposed method is based on a COMPACT REPRESENTATION .\n",
            "\n",
            "582 1000\n",
            "kernel functions have become an extremely popular tool in MACHINE LEARNING , with an attractive theory as well . this theory views a kernel as implicitly mapping data points into a possibly very high dimensional space , and describes a KERNEL FUNCTION as being good for a given LEARNING PROBLEM if data is separable by a large margin in that IMPLICIT SPACE . however , while quite elegant , this theory does not directly correspond to one 's intuition of a good kernel as a good SIMILARITY FUNCTION . furthermore , it may be difficult for a domain expert to use the theory to help design an appropriate kernel for the LEARNING PROBLEM at hand since the IMPLICIT MAPPING may not be easy to calculate . finally , the requirement of POSITIVE SEMI-DEFINITENESS may rule out the most NATURAL PAIRWISE SIMILARITY FUNCTIONS for the given problem <unk> this work we develop an alternative , more general theory of learning with SIMILARITY FUNCTIONS -lrb- i.e. , sufficient conditions for a SIMILARITY FUNCTION to allow one to learn well -rrb- that does not require reference to IMPLICIT SPACES , and does not require the function to be positive semi-definite -lrb- or even symmetric -rrb- . our results also generalize the standard theory in the sense that any good KERNEL FUNCTION under the usual definition can be shown to also be a good SIMILARITY FUNCTION under our definition -lrb- though with some loss in the parameters -rrb- . in this way , we provide the first steps towards a THEORY OF KERNELS that describes the effectiveness of a given KERNEL FUNCTION in terms of NATURAL SIMILARITY-BASED PROPERTIES . \n",
            "this paper presents a method for MACHINE LEARNING in IMPLICIT SPACES . the proposed method is based on a THEORY OF KERNELS , which is a SIMILARITY FUNCTION with a SIMILARITY FUNCTION . the proposed method is based on the THEORY OF KERNELS of the KERNEL FUNCTION . the proposed method is based on the THEORY OF KERNELS of the KERNEL FUNCTION . the proposed method is based on the THEORY OF KERNELS . the proposed method is applied to the LEARNING PROBLEM in IMPLICIT SPACES .\n",
            "\n",
            "583 1000\n",
            "in this paper , we propose a method -- based on the DISCRETE EVOLUTIONARY TRANSFORM -- to estimate the INSTANTANEOUS FREQUENCY of a signal embedded in noise or <unk> signals . the DISCRETE EVOLUTIONARY TRANSFORM provides a REPRESENTATION for NON-STATIONARY SIGNALS and a TIME-FREQUENCY KERNEL that permit us to obtain the TIME-DEPENDENT SPECTRUM of the signal . we will show the INSTANTANEOUS PHASE and the corresponding INSTANTANEOUS FREQUENCY can also be computed from the EVOLUTIONARY KERNEL . estimation of INSTANTANEOUS FREQUENCY is of general interest in TIME-FREQUENCY ANALYSIS , and of special interest in the EXCISION OF JAMMERS in DIRECT SEQUENCE SPREAD SPECTRUM . implementation of the INSTANTANEOUS FREQUENCY is done by MASKING and a RECURSIVE NON-LINEAR CORRECTION PROCEDURE . the proposed INSTANTANEOUS FREQUENCY is valid for <unk> as well as <unk> signals in the NOISELESS AND NOISY SITUATIONS . its application to JAMMER EXCISION in DIRECT SEQUENCE SPREAD SPECTRUM COMMUNICATION is considered as an important application . the INSTANTANEOUS FREQUENCY procedure is illustrated with several examples . \n",
            "this paper proposes a RECURSIVE NON-LINEAR CORRECTION PROCEDURE for DIRECT SEQUENCE SPREAD SPECTRUM COMMUNICATION . the DISCRETE EVOLUTIONARY TRANSFORM is based on a RECURSIVE NON-LINEAR CORRECTION PROCEDURE and a RECURSIVE NON-LINEAR CORRECTION PROCEDURE . the proposed RECURSIVE NON-LINEAR CORRECTION PROCEDURE is based on a RECURSIVE NON-LINEAR CORRECTION PROCEDURE and a RECURSIVE NON-LINEAR CORRECTION PROCEDURE . the proposed RECURSIVE NON-LINEAR CORRECTION PROCEDURE is based on a RECURSIVE NON-LINEAR CORRECTION PROCEDURE and a RECURSIVE NON-LINEAR CORRECTION PROCEDURE . the proposed RECURSIVE NON-LINEAR CORRECTION PROCEDURE is based on a RECURSIVE NON-LINEAR CORRECTION PROCEDURE and a RECURSIVE NON-LINEAR CORRECTION PROCEDURE . the proposed RECURSIVE NON-LINEAR CORRECTION PROCEDURE is based on a RECURSIVE NON-LINEAR CORRECTION PROCEDURE and a RECURSIVE NON-LINEAR CORRECTION PROCEDURE . the proposed RECURSIVE NON-LINEAR CORRECTION PROCEDURE is based on a RECURSIVE NON-LINEAR CORRECTION PROCEDURE and a RECURSIVE NON-LINEAR CORRECTION PROCEDURE . the proposed RECURSIVE NON-LINEAR CORRECTION PROCEDURE is based on a RECURSIVE NON-LINEAR CORRECTION PROCEDURE and a RECURSIVE NON-LINEAR CORRECTION PROCEDURE .\n",
            "\n",
            "584 1000\n",
            "we describe an approach to SPEED-UP INFERENCE with LATENT-VARIABLE PCFGS , which have been shown to be highly effective for NATURAL LANGUAGE PARSING . our approach is based on a TENSOR FORMULATION recently introduced for spectral estimation of LATENT-VARIABLE PCFGS coupled with a TENSOR DECOMPOSITION ALGORITHM well-known in the MULTILINEAR ALGEBRA LITERATURE . we also describe an error bound for this approximation , which gives guarantees showing that if the underlying tensors are well approximated , then the PROBABILITY DISTRIBUTION OVER TREES will also be well approximated . empirical evaluation on REAL-WORLD NATURAL LANGUAGE PARSING DATA demonstrates a significant speed-up at minimal cost for PARSING performance . \n",
            "this paper addresses the problem of NATURAL LANGUAGE PARSING in NATURAL LANGUAGE PARSING . in this paper , we propose a new method for SPEED-UP INFERENCE based on the TENSOR DECOMPOSITION ALGORITHM . the proposed method is based on the TENSOR DECOMPOSITION ALGORITHM . the proposed method is based on the TENSOR DECOMPOSITION ALGORITHM . the proposed method is based on the TENSOR DECOMPOSITION ALGORITHM . the proposed method is based on the TENSOR DECOMPOSITION ALGORITHM . the experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "585 1000\n",
            "discriminative approaches to HUMAN POSE INFERENCE involve MAPPING VISUAL OBSERVATIONS to ARTICULATED BODY CONFIGURATIONS . current PROBABILISTIC APPROACHES to learn this MAPPING VISUAL OBSERVATIONS have been limited in their ability to handle domains with a large number of activities that require very large training sets . we propose an ONLINE PROBABILISTIC REGRESSION SCHEME for efficient INFERENCE of complex , high-dimensional , and multimodal mappings . our ONLINE PROBABILISTIC REGRESSION SCHEME is based on a local mixture of GAUSSIAN PROCESSES , where LOCALITY is defined based on both APPEARANCE AND POSE , and where the MAPPING HYPERPARAMETERS can vary across LOCAL NEIGHBORHOODS to better adapt to specific regions in the POSE SPACE . the MAPPING HYPERPARAMETERS are defined online in very small neighborhoods , so learning and INFERENCE is extremely efficient . when the MAPPING VISUAL OBSERVATIONS is <unk> , we derive a bound on the approximation error of LOCAL REGRESSION -lrb- vs. GLOBAL REGRESSION -rrb- for MONOTONICALLY DECREASING CO-VARIANCE FUNCTIONS . our ONLINE PROBABILISTIC REGRESSION SCHEME can determine when training examples are redundant given the rest of the database , and use this criteria for PRUNING . we report results on synthetic -lrb- <unk> -rrb- and REAL POSE DATABASES , obtaining fast and accurate pose estimates using training set sizes up to 10 5 . \n",
            "this paper addresses the problem of HUMAN POSE INFERENCE in REAL POSE DATABASES . we propose a method for HUMAN POSE INFERENCE based on GAUSSIAN PROCESSES . the proposed method is based on the use of MONOTONICALLY DECREASING CO-VARIANCE FUNCTIONS in the POSE SPACE . the proposed method is based on the use of MONOTONICALLY DECREASING CO-VARIANCE FUNCTIONS in the POSE SPACE . the proposed method is based on the use of MONOTONICALLY DECREASING CO-VARIANCE FUNCTIONS in the POSE SPACE . the proposed method is based on the use of MONOTONICALLY DECREASING CO-VARIANCE FUNCTIONS . the proposed method is based on the use of MONOTONICALLY DECREASING CO-VARIANCE FUNCTIONS in the POSE SPACE . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "586 1000\n",
            "we propose a novel strategy for training NEURAL NETWORKS using SEQUENTIAL SAMPLING-IMPORTANCE RESAMPLING ALGORITHMS . this global optimisation strategy allows us to learn the PROBABILITY DISTRIBUTION of the NETWORK WEIGHTS in a SEQUENTIAL FRAMEWORK . it is well suited to applications involving on-line , nonlinear , non-gaussian or non-stationary signal processing . \n",
            "this paper addresses the problem of NEURAL NETWORKS for NEURAL NETWORKS . we propose a new SEQUENTIAL FRAMEWORK based on the PROBABILITY DISTRIBUTION . the proposed SEQUENTIAL SAMPLING-IMPORTANCE RESAMPLING ALGORITHMS is based on the use of a PROBABILITY DISTRIBUTION and a PROBABILITY DISTRIBUTION .\n",
            "\n",
            "587 1000\n",
            "we present an INCREMENTAL ADAPTATION APPROACH for STATISTICAL MACHINE TRANSLATION that maintains a FLEXIBLE HIERARCHICAL DOMAIN STRUCTURE within a single CONSISTENT MODEL . both weights and RULES are updated incrementally on a STREAM OF POST-EDITS . our MULTI-LEVEL DOMAIN HIERARCHY allows the INCREMENTAL ADAPTATION APPROACH to adapt simultaneously towards LOCAL CONTEXT at different levels of GRANULARITY , including genres and individual documents . our experiments show consistent improvements in TRANSLATION QUALITY from all components of our INCREMENTAL ADAPTATION APPROACH . \n",
            "this paper presents a INCREMENTAL ADAPTATION APPROACH for STATISTICAL MACHINE TRANSLATION . the INCREMENTAL ADAPTATION APPROACH is based on the STREAM OF POST-EDITS . the proposed INCREMENTAL ADAPTATION APPROACH is based on the STREAM OF POST-EDITS . the proposed INCREMENTAL ADAPTATION APPROACH is based on the STREAM OF POST-EDITS . the proposed INCREMENTAL ADAPTATION APPROACH is based on the INCREMENTAL ADAPTATION APPROACH . the proposed INCREMENTAL ADAPTATION APPROACH is evaluated on the MULTI-LEVEL DOMAIN HIERARCHY and the TRANSLATION QUALITY of the proposed INCREMENTAL ADAPTATION APPROACH .\n",
            "\n",
            "588 1000\n",
            "we discuss a model for IMAGE SEGMENTATION that is able to overcome the SHORT-BOUNDARY BIAS observed in standard PAIRWISE RANDOM FIELD BASED APPROACHES . to <unk> , we show that a RANDOM FIELD with MULTI-LAYERED HIDDEN UNITS can encode boundary preserving higher order potentials such as the ones used in the COOPERATIVE CUTS MODEL of -lsb- 12 -rsb- while still allowing for fast and exact MAP INFERENCE . EXACT INFERENCE allows our model to outperform previous IMAGE SEG-MENTATION METHODS , and to see the true effect of COUPLING GRAPH EDGES . finally , our model can be easily extended to handle SEGMENTATION INSTANCES with multiple labels , for which it yields promising results . \n",
            "this paper addresses the problem of IMAGE SEGMENTATION in IMAGE SEGMENTATION . we propose a COOPERATIVE CUTS MODEL based on the COOPERATIVE CUTS MODEL . the proposed method is based on the use of MULTI-LAYERED HIDDEN UNITS in the RANDOM FIELD . the proposed method is based on the use of MULTI-LAYERED HIDDEN UNITS in the RANDOM FIELD . the proposed method is based on the COOPERATIVE CUTS MODEL . the experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "589 1000\n",
            "we study PARTICLE FILTERING ALGORITHMS for TRACKING on infinite -lrb- in practice , large -rrb- dimensional state spaces . PARTICLE FILTERING -LRB- MONTE CARLO SAMPLING -RRB- from a LARGE DIMENSIONAL SYSTEM NOISE DISTRIBUTION is computationally expensive . but , in most large dim TRACKING applications , it is fair to assume that '' most of the state change '' occurs in a small dimensional basis and the basis itself may be slowly time varying -lrb- approximated as piecewise constant -rrb- . we have proposed a PF ALGORITHM with BASIS CHANGE DETECTION and RE-ESTIMATION STEPS that uses this idea . the implicit assumptions in defining this PF ALGORITHM are very strong . we study here the implications of WEAKER ASSUMPTIONS and how to handle them . we propose to use a simple modification of the ASYMPTOTICALLY STABLE ADAPTIVE PARTICLE FILTER to handle errors in estimating the basis dimension . \n",
            "this paper addresses the problem of TRACKING in a LARGE DIMENSIONAL SYSTEM NOISE DISTRIBUTION . in this paper , we propose a new PF ALGORITHM based on PARTICLE FILTERING -LRB- MONTE CARLO SAMPLING -RRB- and PARTICLE FILTERING -LRB- MONTE CARLO SAMPLING -RRB- . the proposed method is based on the use of RE-ESTIMATION STEPS and PARTICLE FILTERING -LRB- MONTE CARLO SAMPLING -RRB- . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "590 1000\n",
            "recently , the MODULATION SPECTRUM has been proposed and found to be a useful source of SPEECH INFORMATION . the MODULATION SPECTRUM represents longer term variations in the spectrum and thus implicitly requires FEATURES extracted from much longer speech segments compared to MFCCS and their delta terms . in this paper , a DISCRETE COSINE TRANSFORM ANALYSIS of the LOG MAGNITUDE SPECTRUM combined with a discrete cosine series -lrb- <unk> -rrb- expansion of dct coefficients over time is proposed as a method for capturing both the SPECTRAL AND MODULATION INFORMATION . these DCT/DCS FEATURES can be computed so as to emphasize FREQUENCY RESOLUTION or TIME RESOLUTION or a combination of the two factors . several variations of the DCT/DCS FEATURES were evaluated with PHONETIC RECOGNITION experiments using TIMIT and its telephone version -lrb- TIMIT -rrb- . best results obtained with a combined feature set are <unk> % for TIMIT and <unk> % for TIMIT . the MODULATION FEATURES are shown to be far more important than the SPECTRAL FEATURES for AUTOMATIC SPEECH RECOGNITION and far more noise robust . \n",
            "this paper presents a new method for AUTOMATIC SPEECH RECOGNITION based on DISCRETE COSINE TRANSFORM ANALYSIS . the proposed method is based on the use of MODULATION FEATURES and SPECTRAL FEATURES . the proposed method is based on the use of MODULATION FEATURES and MODULATION FEATURES . the proposed method is based on the use of MODULATION FEATURES and MODULATION FEATURES . the proposed method is compared with conventional FEATURES and MFCCS . the experimental results show that the proposed method outperforms the conventional MFCCS and the MFCCS .\n",
            "\n",
            "591 1000\n",
            "in SUPERVISED LEARNING , many techniques focus on OPTIMIZING TRAINING PHASE to increase PREDICTION performance . ACTIVE INFERENCE , a relatively novel paradigm , aims to decrease overall PREDICTION error via SELECTIVE COLLECTION of some labels based on relations among instances . in this research , we use DYNAMIC BAYESIAN NETWORKS to model TEMPORAL SYSTEMS and we apply ACTIVE INFERENCE to dynamically choose variables for observation so as to improve PREDICTION on UNOBSERVED VARIABLES . \n",
            "this paper addresses the problem of ACTIVE INFERENCE in TEMPORAL SYSTEMS . we propose a method for ACTIVE INFERENCE based on DYNAMIC BAYESIAN NETWORKS . the proposed method is based on the DYNAMIC BAYESIAN NETWORKS . the proposed method is based on the use of DYNAMIC BAYESIAN NETWORKS . the proposed method is based on the use of DYNAMIC BAYESIAN NETWORKS . the proposed method is based on the use of DYNAMIC BAYESIAN NETWORKS .\n",
            "\n",
            "592 1000\n",
            "as SPOKEN DIALOGUE SYSTEMS become deployed in increasingly complex domains , SPOKEN DIALOGUE SYSTEMS face rising demands on the NATURALNESS OF INTERACTION . we focus on SYSTEM RESPONSIVENESS , aiming to mimic HUMAN-LIKE DIALOGUE FLOW CONTROL by PREDICTING SPEAKER CHANGES as observed in REAL HUMAN-HUMAN CONVERSATIONS . we derive an INSTANTANEOUS VECTOR REPRESENTATION OF PITCH VARIATION and show that SPOKEN DIALOGUE SYSTEMS is amenable to standard ACOUSTIC MODELING TECHNIQUES . using a small amount of AUTOMATICALLY LABELED DATA , we train models which significantly outperform current state-of-the-art PAUSE-ONLY SYSTEMS , and replicate to within 1 % absolute the performance of our previously published HAND-CRAFTED BASELINE . the new system additionally offers scope for run-time control over the PRECISION or RECALL OF LOCATIONS at which to speak . \n",
            "this paper presents a novel approach for PREDICTING SPEAKER CHANGES based on INSTANTANEOUS VECTOR REPRESENTATION OF PITCH VARIATION . the proposed method is based on the INSTANTANEOUS VECTOR REPRESENTATION OF PITCH VARIATION , which is based on the INSTANTANEOUS VECTOR REPRESENTATION OF PITCH VARIATION . the proposed ACOUSTIC MODELING TECHNIQUES is based on the NATURALNESS OF INTERACTION of the PAUSE-ONLY SYSTEMS and the NATURALNESS OF INTERACTION of the SPOKEN DIALOGUE SYSTEMS . the proposed ACOUSTIC MODELING TECHNIQUES is evaluated on REAL HUMAN-HUMAN CONVERSATIONS and compared to the conventional HAND-CRAFTED BASELINE .\n",
            "\n",
            "593 1000\n",
            "model learning and TRACKING are two important topics in COMPUTER VISION . while there are many applications where one of them is used to support the other , there are currently only few where both aid each other simultaneously . in this work , we seek to incrementally learn a GRAPHICAL MODEL from TRACKING and to simultaneously use whatever has been learned to improve the TRACKING in the next frames . the main problem encountered in this situation is that the current GRAPHICAL MODEL may be inconsistent with future observations , creating a bias in the TRACKING results . we propose an uncertain model that explicitly accounts for such uncertainties by representing relations by an appropriately weighted sum of informative -lrb- parametric -rrb- and UNINFORMATIVE COMPONENTS . the method is completely unsupervised and operates in real time . \n",
            "this paper presents a new method for TRACKING in COMPUTER VISION . the GRAPHICAL MODEL is based on the use of a GRAPHICAL MODEL and a GRAPHICAL MODEL for TRACKING . experimental results show the effectiveness of the proposed GRAPHICAL MODEL .\n",
            "\n",
            "594 1000\n",
            "widespread use of efficient and successful solutions of COMPUTER VISION PROBLEMS based on PAIRWISE MARKOV RANDOM FIELD MODELS raises a question : does any link exist between the pairwise and higher order mrfs such that the like solutions can be applied to the latter models ? this work explores such a link for BINARY MRFS that allow us to represent GIBBS ENERGY OF SIGNAL INTERACTION with a POLYNOMIAL FUNCTION . we show how a higher ORDER POLYNOMIAL can be efficiently transformed into a QUADRATIC FUNCTION . then ENERGY MINIMIZATION TOOLS for the PAIRWISE MRF MODELS can be easily applied to the HIGHER ORDER COUNTERPARTS . also , we propose a method to analytically estimate the potential parameter of the ASYMMETRIC POTTS PRIOR . the proposed framework demonstrates very promising experimental results of IMAGE SEGMENTATION and can be used to solve other COMPUTER VISION PROBLEMS . \n",
            "this paper addresses the problem of IMAGE SEGMENTATION in COMPUTER VISION PROBLEMS . we propose a method for IMAGE SEGMENTATION based on PAIRWISE MARKOV RANDOM FIELD MODELS . the proposed method is based on the use of PAIRWISE MRF MODELS to estimate the HIGHER ORDER COUNTERPARTS from the QUADRATIC FUNCTION . the proposed method is based on the use of PAIRWISE MRF MODELS for IMAGE SEGMENTATION . the proposed method is based on the use of ENERGY MINIMIZATION TOOLS for IMAGE SEGMENTATION . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "595 1000\n",
            "this paper proposes a novel approach that utilizes a MACHINE LEARNING METHOD to improve PIVOT-BASED STATISTICAL MACHINE TRANSLATION . for language pairs with few BILINGUAL DATA , a possible solution in PIVOT-BASED SMT using another language as a `` bridge '' to generate SOURCE-TARGET TRANSLATION . however , one of the weaknesses is that some useful SOURCE-TARGET TRANSLATIONS can not be generated if the corresponding source phrase and target phrase connect to different PIVOT PHRASES . to alleviate the problem , we utilize MARKOV RANDOM WALKS to connect possible translation phrases between source and target language . experimental results on EUROPEAN PARLIAMENT DATA , SPOKEN LANGUAGE DATA and WEB DATA show that our method leads to significant improvements on all the tasks over the baseline system . \n",
            "this paper presents a new MACHINE LEARNING METHOD for SPOKEN LANGUAGE DATA . the MACHINE LEARNING METHOD is based on MARKOV RANDOM WALKS and MARKOV RANDOM WALKS . the proposed MACHINE LEARNING METHOD is based on MARKOV RANDOM WALKS and MARKOV RANDOM WALKS . the proposed MACHINE LEARNING METHOD is based on MARKOV RANDOM WALKS and MARKOV RANDOM WALKS . the proposed MACHINE LEARNING METHOD is based on MARKOV RANDOM WALKS and MARKOV RANDOM WALKS .\n",
            "\n",
            "596 1000\n",
            "plant traits are a key to understanding and predicting the ADAPTATION OF ECOSYSTEMS to environmental changes , which motivates the try project aiming at constructing a GLOBAL DATABASE for PLANT TRAITS and becoming a standard resource for the ECOLOGICAL COMMUNITY . despite its unprecedented coverage , a large percentage of MISSING DATA substantially constrains JOINT TRAIT ANALYSIS . meanwhile , the TRAIT DATA is characterized by the HIERARCHICAL PHYLOGENETIC STRUCTURE of the plant <unk> . while FACTORIZATION BASED MATRIX COMPLETION TECHNIQUES have been widely used to address the MISSING DATA PROBLEM , traditional MATRIX FACTORIZATION METHODS are unable to leverage the PHYLOGENETIC STRUCTURE . we propose HIERARCHICAL PROBABILISTIC MATRIX FACTORIZATION , which effectively uses HIERARCHICAL PHYLOGENETIC INFORMATION for TRAIT PREDICTION . we demonstrate HIERARCHICAL PROBABILISTIC MATRIX FACTORIZATION 's high accuracy , effectiveness of incorporating HIERARCHICAL STRUCTURE and ability to capture TRAIT CORRELATION through experiments . \n",
            "this paper addresses the problem of TRAIT PREDICTION in ECOLOGICAL COMMUNITY . we propose a HIERARCHICAL PROBABILISTIC MATRIX FACTORIZATION based on HIERARCHICAL PROBABILISTIC MATRIX FACTORIZATION for TRAIT PREDICTION . the proposed FACTORIZATION BASED MATRIX COMPLETION TECHNIQUES is based on a HIERARCHICAL PROBABILISTIC MATRIX FACTORIZATION , which is based on the HIERARCHICAL PROBABILISTIC MATRIX FACTORIZATION . the proposed FACTORIZATION BASED MATRIX COMPLETION TECHNIQUES is based on a HIERARCHICAL PROBABILISTIC MATRIX FACTORIZATION , which is based on the HIERARCHICAL PHYLOGENETIC INFORMATION . the proposed FACTORIZATION BASED MATRIX COMPLETION TECHNIQUES is based on the HIERARCHICAL PROBABILISTIC MATRIX FACTORIZATION . the proposed FACTORIZATION BASED MATRIX COMPLETION TECHNIQUES is based on a HIERARCHICAL PROBABILISTIC MATRIX FACTORIZATION . the proposed FACTORIZATION BASED MATRIX COMPLETION TECHNIQUES is applied to the MISSING DATA PROBLEM . experimental results demonstrate the effectiveness of the proposed JOINT TRAIT ANALYSIS .\n",
            "\n",
            "597 1000\n",
            "in this paper , we present a VISION SYSTEM for OBJECT RECOGNITION in AERIAL IMAGES , which enables broader mission profiles for MICRO AIR VEHICLES . the most important factors that inform our DESIGN CHOICES are : REAL-TIME CONSTRAINTS , ROBUSTNESS to VIDEO NOISE , and COMPLEXITY OF OBJECT APPEARANCES . as such , we first propose the HSI COLOR SPACE and the COMPLEX WAVELET TRANSFORM as a set of sufficiently DISCRIMINATING FEATURES . for each FEATURE , we then build TREE-STRUCTURED BELIEF NETWORKS as our underlying STATISTICAL MODELS OF OBJECT APPEARANCES . to perform OBJECT RECOGNITION , we develop the novel MULTISCALE VITERBI CLASSIFICATION ALGORITHM , as an improvement to MULTI-SCALE BAYESIAN CLASSIFICATION . next , we show how to globally optimize TREE-STRUCTURED BELIEF NETWORKS with respect to the FEATURE SET , using an ADAPTIVE FEATURE SELECTION ALGORITHM . finally , we discuss CONTEXT-BASED OBJECT RECOGNITION , where VISUAL CONTEXTS help to disambiguate the identity of an object despite the RELATIVE POVERTY OF SCENE DETAIL in FLIGHT IMAGES , and <unk> the need for an exhaustive search of objects over various scales and locations in the image . experimental results show that the proposed VISION SYSTEM achieves smaller classification error and fewer false positives than systems using the MSBC PARADIGM on challenging REAL-WORLD TEST IMAGES . \n",
            "this paper presents a novel ADAPTIVE FEATURE SELECTION ALGORITHM for CONTEXT-BASED OBJECT RECOGNITION . the proposed ADAPTIVE FEATURE SELECTION ALGORITHM is based on a MULTISCALE VITERBI CLASSIFICATION ALGORITHM and a MULTISCALE VITERBI CLASSIFICATION ALGORITHM . the proposed ADAPTIVE FEATURE SELECTION ALGORITHM is based on the MULTISCALE VITERBI CLASSIFICATION ALGORITHM and the MULTISCALE VITERBI CLASSIFICATION ALGORITHM . the proposed ADAPTIVE FEATURE SELECTION ALGORITHM is based on the MULTISCALE VITERBI CLASSIFICATION ALGORITHM and the MULTISCALE VITERBI CLASSIFICATION ALGORITHM . the proposed ADAPTIVE FEATURE SELECTION ALGORITHM is based on a MULTISCALE VITERBI CLASSIFICATION ALGORITHM and a MULTISCALE VITERBI CLASSIFICATION ALGORITHM . the proposed ADAPTIVE FEATURE SELECTION ALGORITHM is evaluated on REAL-WORLD TEST IMAGES and REAL-WORLD TEST IMAGES . it is shown that the proposed ADAPTIVE FEATURE SELECTION ALGORITHM is robust and robust to VIDEO NOISE and VIDEO NOISE .\n",
            "\n",
            "598 1000\n",
            "this paper presents a new method for MUSICAL CHORD RECOGNITION based on a MODEL OF HUMAN PERCEPTION . we classify the chords directly from the sound without the INFORMATION OF TIMBRES and notes . a WAVELET-BASED TRANSFORM as well as a SELF-ORGANIZED MAP NEURAL NETWORK is adopted to imitate HUMAN EARS and <unk> , respectively . the resultant system can classify chords very well even in a noisy environment . \n",
            "this paper presents a method for MUSICAL CHORD RECOGNITION from HUMAN EARS . the SELF-ORGANIZED MAP NEURAL NETWORK is based on a SELF-ORGANIZED MAP NEURAL NETWORK of the SELF-ORGANIZED MAP NEURAL NETWORK . the proposed method is based on the SELF-ORGANIZED MAP NEURAL NETWORK . the proposed method is based on the SELF-ORGANIZED MAP NEURAL NETWORK .\n",
            "\n",
            "599 1000\n",
            "human speakers plan and deliver their utterances incremen-tally , <unk> , and it is obvious that their choice regarding PHONETIC DETAILS -lrb- and the details ' <unk> -rrb- is rarely determined by globally optimal solutions . in contrast , PARA-METRIC SPEECH SYNTHESIZERS use a FULL-UTTERANCE CONTEXT when optimizing VOCODING PARAMETERS and when DETERMING HMM STATES . apart from being cognitively implausible , this impedes INCREMENTAL USE-CASES , where the future context is often at least partially unavailable . this paper investigates the ` locality ' of FEATURES in PARAMETRIC SPEECH SYNTHESIS VOICES and takes some missing steps towards better HMM STATE SELECTION and PROSODY MODELLING for INCREMENTAL SPEECH SYNTHESIS . \n",
            "this paper addresses the problem of PROSODY MODELLING in PARA-METRIC SPEECH SYNTHESIZERS . we propose a method for PROSODY MODELLING based on HMM STATE SELECTION . the proposed method is based on the use of FEATURES and FEATURES . the proposed method is based on the use of DETERMING HMM STATES and PROSODY MODELLING . the proposed method is based on the use of DETERMING HMM STATES and PROSODY MODELLING . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "600 1000\n",
            "burst detection is an important topic in TEMPORAL STREAM ANALYSIS . usually , only the TEXTUAL FEATURES are used in BURST DETECTION . in the theme extraction from current prevailing SOCIAL MEDIA CONTENT , it is necessary to consider not only TEXTUAL FEATURES but also the PERVASIVE COLLABORATIVE CONTEXT , e.g. , RESOURCE LIFETIME and user activity . this paper explores novel approaches to combine multiple sources of such indication for better BURST EXTRACTION . we systematically investigate the characters of COLLABORATIVE CONTEXT , i.e. , METADATA FREQUENCY , TOPIC COVERAGE and USER ATTRACTIVENESS . first , a ROBUST STATE BASED MODEL is utilized to detect bursts from individual streams . we then propose a LEARNING METHOD to combine these BURST PULSES . experiments on a large real dataset demonstrate the remarkable improvements over the traditional methods . \n",
            "this paper addresses the problem of BURST EXTRACTION in SOCIAL MEDIA CONTENT . we propose a ROBUST STATE BASED MODEL , called ROBUST STATE BASED MODEL , which is based on a ROBUST STATE BASED MODEL and a ROBUST STATE BASED MODEL . the ROBUST STATE BASED MODEL is based on a ROBUST STATE BASED MODEL and a ROBUST STATE BASED MODEL . the LEARNING METHOD is a ROBUST STATE BASED MODEL and a ROBUST STATE BASED MODEL . the LEARNING METHOD is a ROBUST STATE BASED MODEL , which is a ROBUST STATE BASED MODEL for BURST EXTRACTION . the LEARNING METHOD is a ROBUST STATE BASED MODEL and a ROBUST STATE BASED MODEL .\n",
            "\n",
            "601 1000\n",
            "we have trained networks of E-II UNITS with SHORT-RANGE CONNECTIONS to simulate simple CELLULAR AUTOMATA that exhibit COMPLEX OR CHAOTIC BEHAVIOUR . three levels of LEARNING are possible -lrb- in decreasing order of difficulty -rrb- : LEARNING the underlying AUTOMATON RULE , learning ASYMPTOTIC DYNAMICAL BEHAVIOUR , and LEARNING to extrapolate the training history . the levels of LEARNING achieved with and without WEIGHT SHARING for different automata provide new insight into their dynamics . \n",
            "this paper addresses the problem of ASYMPTOTIC DYNAMICAL BEHAVIOUR in COMPLEX OR CHAOTIC BEHAVIOUR . we propose a method for estimating the ASYMPTOTIC DYNAMICAL BEHAVIOUR from E-II UNITS . the proposed method is based on the AUTOMATON RULE of the E-II UNITS . the proposed method is based on a AUTOMATON RULE and is shown to be more robust to COMPLEX OR CHAOTIC BEHAVIOUR than the conventional AUTOMATON RULE .\n",
            "\n",
            "602 1000\n",
            "nearest neighbor -lrb- nn -rrb- classification relies on the assumption that CLASS CONDITIONAL PROBABILITIES are locally constant . this assumption becomes false in high dimensions with FINITE SAMPLES due to the CURSE OF DIMENSIONALITY . the NEAREST NEIGHBOR CLASSIFICATION introduces SEVERE BIAS under these conditions . we propose a LOCALLY ADAPTIVE NEIGHBORHOOD MORPHING CLASSIFICATION METHOD to try to minimize BIAS . we use LOCAL SUPPORT VECTOR MACHINE LEARNING to estimate an effective metric for producing neighborhoods that are <unk> along less DISCRIMINANT FEATURE DIMENSIONS and <unk> along most discriminant ones . as a result , the CLASS CONDITIONAL PROBABILITIES can be expected to be approximately constant in the modified neighborhoods , whereby better classification performance can be achieved . the efficacy of our LOCALLY ADAPTIVE NEIGHBORHOOD MORPHING CLASSIFICATION METHOD is validated and compared against other competing techniques using a number of datasets . \n",
            "this paper presents a LOCALLY ADAPTIVE NEIGHBORHOOD MORPHING CLASSIFICATION METHOD for NEAREST NEIGHBOR CLASSIFICATION . the LOCALLY ADAPTIVE NEIGHBORHOOD MORPHING CLASSIFICATION METHOD is based on the LOCALLY ADAPTIVE NEIGHBORHOOD MORPHING CLASSIFICATION METHOD . the LOCALLY ADAPTIVE NEIGHBORHOOD MORPHING CLASSIFICATION METHOD is based on the LOCALLY ADAPTIVE NEIGHBORHOOD MORPHING CLASSIFICATION METHOD . the proposed LOCALLY ADAPTIVE NEIGHBORHOOD MORPHING CLASSIFICATION METHOD is based on a LOCALLY ADAPTIVE NEIGHBORHOOD MORPHING CLASSIFICATION METHOD . the LOCALLY ADAPTIVE NEIGHBORHOOD MORPHING CLASSIFICATION METHOD is based on the LOCALLY ADAPTIVE NEIGHBORHOOD MORPHING CLASSIFICATION METHOD . the proposed LOCALLY ADAPTIVE NEIGHBORHOOD MORPHING CLASSIFICATION METHOD is based on the LOCALLY ADAPTIVE NEIGHBORHOOD MORPHING CLASSIFICATION METHOD . the proposed LOCALLY ADAPTIVE NEIGHBORHOOD MORPHING CLASSIFICATION METHOD is based on a LOCALLY ADAPTIVE NEIGHBORHOOD MORPHING CLASSIFICATION METHOD . the proposed LOCALLY ADAPTIVE NEIGHBORHOOD MORPHING CLASSIFICATION METHOD is based on the LOCALLY ADAPTIVE NEIGHBORHOOD MORPHING CLASSIFICATION METHOD .\n",
            "\n",
            "603 1000\n",
            "deconvolution problems are encountered in SIGNAL PROCESSING APPLICATIONS where an UNKNOWN INPUT SIGNAL can only be observed after propagation through one or more NOISE CORRUPTED FIR CHANNELS . the first step in recovering the input usually entails an estimation of the FIR CHANNELS through training based or BLIND ALGORITHMS . the ` standard ' procedure then uses LEAST SQUARES ESTIMATION to recover the input . a RECURSIVE IMPLEMENTATION with CONSTANT COMPUTATIONAL COST is based on the KALMAN FILTER . in this paper we focus on a TOTAL LEAST SQUARES BASED APPROACH , which is more appropriate if errors are expected both on the output samples and the estimates of the FIR CHANNELS . we will develop a RE-CURSIVE TOTAL LEAST SQUARES ALGORITHM which closely approximates the performance of the NON-RECURSIVE TLS ALGORITHM and this at a much lower COMPUTATIONAL COST . \n",
            "this paper addresses the problem of DECONVOLUTION PROBLEMS in NOISE CORRUPTED FIR CHANNELS . we propose a TOTAL LEAST SQUARES BASED APPROACH for LEAST SQUARES ESTIMATION with LEAST SQUARES ESTIMATION . the proposed RE-CURSIVE TOTAL LEAST SQUARES ALGORITHM is based on the RE-CURSIVE TOTAL LEAST SQUARES ALGORITHM . the proposed TOTAL LEAST SQUARES BASED APPROACH is based on the RE-CURSIVE TOTAL LEAST SQUARES ALGORITHM and the RE-CURSIVE TOTAL LEAST SQUARES ALGORITHM . the proposed NON-RECURSIVE TLS ALGORITHM is based on the RE-CURSIVE TOTAL LEAST SQUARES ALGORITHM with LEAST SQUARES ESTIMATION . the proposed algorithm is shown to be robust to CONSTANT COMPUTATIONAL COST and is robust to FIR CHANNELS .\n",
            "\n",
            "604 1000\n",
            "an efficient realization of a LOW DELAY FILTER-BANK , termed as GENERALIZED FILTER-BANK EQUALIZER , will be proposed for NOISE REDUCTION with LOW SIGNAL DELAY . the GENERALIZED FILTER-BANK EQUALIZER is equivalent to a TIME-DOMAIN FILTER with COEFFICIENTS adapted in the FREQUENCY-DOMAIN . this FILTER-BANK STRUCTURE ensures perfect SIGNAL RECONSTRUCTION for a variety of spectral transforms with less restrictions than for an ANALYSIS-SYNTHESIS FILTER-BANK -LRB- AS FB -RRB- . a NON-UNIFORM FREQUENCY RESOLUTION can be achieved by an ALLPASS TRANSFORMATION . in this case , the GENERALIZED FILTER-BANK EQUALIZER has not only a lower signal delay than the GENERALIZED FILTER-BANK EQUALIZER , but also a lower ALGORITHMIC COMPLEXITY for most PARAMETER CONFIGURATIONS . another advantage of the GENERALIZED FILTER-BANK EQUALIZER is the lower number of required delay elements -lrb- memory -rrb- compared to the GENERALIZED FILTER-BANK EQUALIZER . the NOISE REDUCTION achieved by means of the GENERALIZED FILTER-BANK EQUALIZER and the GENERALIZED FILTER-BANK EQUALIZER is approximately equal . \n",
            "this paper proposes a new GENERALIZED FILTER-BANK EQUALIZER for SIGNAL RECONSTRUCTION . the GENERALIZED FILTER-BANK EQUALIZER is based on the GENERALIZED FILTER-BANK EQUALIZER and the GENERALIZED FILTER-BANK EQUALIZER . the proposed GENERALIZED FILTER-BANK EQUALIZER is based on the GENERALIZED FILTER-BANK EQUALIZER and the GENERALIZED FILTER-BANK EQUALIZER . the proposed GENERALIZED FILTER-BANK EQUALIZER is compared with the conventional GENERALIZED FILTER-BANK EQUALIZER and the GENERALIZED FILTER-BANK EQUALIZER . the ALGORITHMIC COMPLEXITY of the proposed GENERALIZED FILTER-BANK EQUALIZER is compared with the conventional GENERALIZED FILTER-BANK EQUALIZER and the GENERALIZED FILTER-BANK EQUALIZER .\n",
            "\n",
            "605 1000\n",
            "a CLASSIFICATION METHOD is presented that detects the presence of speech embedded in a REAL ACOUSTIC BACKGROUND of NON-SPEECH SOUNDS . FEATURES used for CLASSIFICATION are MODULATION COMPONENTS extracted by computation of the AMPLITUDE MODULATION SPECTROGRAM . FEATURE SELECTION TECHNIQUES and support vector CLASSIFICATION are employed to identify MODULATION COMPONENTS most salient for the CLASSIFICATION TASK and therefore considered as highly characteristic for speech . results show that reliable DETECTION OF SPEECH can be performed with less than 10 optimally selected MODULATION FEATURES , the most important ones are located in the MODULATION FREQUENCY RANGE below 10 hz . DETECTION OF SPEECH in a background of NON-SPEECH SIGNALS is performed with about 90 % TEST-DATA ACCURACY at a SIGNAL-TO-NOISE LEVEL of 0 db . compared to standard ITU G729.B VOICE ACTIVITY DETECTION , the proposed CLASSIFICATION METHOD results in increased true positive and reduced FALSE POSITIVE RATES induced by a REAL ACOUSTIC BACKGROUND . \n",
            "this paper presents a novel CLASSIFICATION METHOD for ITU G729.B VOICE ACTIVITY DETECTION . the proposed CLASSIFICATION METHOD is based on the AMPLITUDE MODULATION SPECTROGRAM of the AMPLITUDE MODULATION SPECTROGRAM . the proposed CLASSIFICATION METHOD is based on the AMPLITUDE MODULATION SPECTROGRAM of the AMPLITUDE MODULATION SPECTROGRAM . the proposed CLASSIFICATION METHOD is based on a CLASSIFICATION METHOD of the AMPLITUDE MODULATION SPECTROGRAM . the proposed CLASSIFICATION METHOD is evaluated on the CLASSIFICATION TASK . the proposed CLASSIFICATION METHOD is shown to outperform the conventional CLASSIFICATION METHOD in terms of TEST-DATA ACCURACY and FALSE POSITIVE RATES .\n",
            "\n",
            "606 1000\n",
            "recently , significant progress has been made on learning STRUCTURED PREDICTORS via COORDINATED TRAINING ALGORITHMS such as CONDITIONAL RANDOM FIELDS and MAXIMUM MARGIN MARKOV NETWORKS . unfortunately , these techniques are based on SPECIALIZED TRAINING ALGORITHMS , are complex to implement , and expensive to run . we present a much simpler approach to training STRUCTURED PREDICTORS by applying a BOOSTING-LIKE PROCEDURE to standard SUPERVISED TRAINING METHODS . the idea is to learn a LOCAL PREDICTOR using standard methods , such as LOGISTIC REGRESSION or SUPPORT VECTOR MACHINES , but then achieve improved STRUCTURED CLASSIFICATION by '' boosting '' the influence of MISCLASSIFIED COMPONENTS after STRUCTURED CLASSIFICATION , retraining the LOCAL PREDICTOR , and repeating . further improvement in STRUCTURED PREDICTION ACCURACY can be achieved by incorporating '' dynamic '' FEATURES -- i.e. an extension whereby the FEATURES for one predicted component can depend on the predictions already made for some other components . we apply our techniques to the problem of learning DEPENDENCY PARSERS from ANNOTATED NATURAL LANGUAGE CORPORA . by using LOGISTIC REGRESSION as an efficient BASE CLASSIFIER -lrb- for PREDICTING DEPENDENCY LINKS between word pairs -rrb- , we are able to efficiently train a DEPENDENCY PARSING MODEL , via STRUCTURED BOOSTING , that achieves state of the art results in en-glish , and surpasses state of the art in CHINESE . \n",
            "this paper addresses the problem of PREDICTING DEPENDENCY LINKS in ANNOTATED NATURAL LANGUAGE CORPORA . in this paper , we propose a DEPENDENCY PARSING MODEL for PREDICTING DEPENDENCY LINKS , which is based on a BASE CLASSIFIER and a BASE CLASSIFIER . the proposed DEPENDENCY PARSING MODEL is based on a BASE CLASSIFIER and a BASE CLASSIFIER . the proposed DEPENDENCY PARSING MODEL is based on the use of a BASE CLASSIFIER and a BASE CLASSIFIER to estimate the MISCLASSIFIED COMPONENTS . the proposed method is based on the use of a BASE CLASSIFIER and a BASE CLASSIFIER . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "607 1000\n",
            "we present a new SAMPLING APPROACH to bayesian learning of the BAYESIAN NETWORK STRUCTURE . like some earlier SAMPLING METHODS , we sample LINEAR ORDERS on NODES rather than directed acyclic graphs -lrb- <unk> -rrb- . the key difference is that we replace the usual MARKOV CHAIN MONTE CARLO METHOD by the SAMPLING APPROACH of ANNEALED IMPORTANCE SAMPLING . we show that MARKOV CHAIN MONTE CARLO METHOD is not only competitive to MARKOV CHAIN MONTE CARLO METHOD in exploring the POSTERIOR , but also superior to MARKOV CHAIN MONTE CARLO METHOD in two ways : MARKOV CHAIN MONTE CARLO METHOD enables easy and efficient PARALLELIZATION , due to the independence of the samples , and <unk> of the marginal likelihood of the SAMPLING APPROACH with good PROBABILISTIC GUARANTEES . we also provide a principled way to correct the BIAS due to ORDER-BASED SAMPLING , by implementing a fast algorithm for counting the linear extensions of a given PARTIAL ORDER . \n",
            "this paper proposes a new SAMPLING APPROACH for ORDER-BASED SAMPLING . the proposed MARKOV CHAIN MONTE CARLO METHOD is based on the MARKOV CHAIN MONTE CARLO METHOD and the MARKOV CHAIN MONTE CARLO METHOD . the proposed MARKOV CHAIN MONTE CARLO METHOD is based on the MARKOV CHAIN MONTE CARLO METHOD . the proposed MARKOV CHAIN MONTE CARLO METHOD is based on the MARKOV CHAIN MONTE CARLO METHOD . the proposed MARKOV CHAIN MONTE CARLO METHOD is compared with the conventional MARKOV CHAIN MONTE CARLO METHOD and the MARKOV CHAIN MONTE CARLO METHOD . the proposed MARKOV CHAIN MONTE CARLO METHOD is compared with the conventional MARKOV CHAIN MONTE CARLO METHOD and the MARKOV CHAIN MONTE CARLO METHOD .\n",
            "\n",
            "608 1000\n",
            "in the past researches , several kinds of information have been explored to assess the CONFIDENCE MEASURE or to select the CONFIDENCE TAG for a WORD/PHRASE . however , the CONTEXTUAL CONFIDENCE INFORMATION is little <unk> . in this paper , we propose a CONCEPT-BASED PROBABILISTIC VERIFICATION MODEL to integrate the CONTEXTUAL CONFIDENCE INFORMATION . in this CONCEPT-BASED PROBABILISTIC VERIFICATION MODEL , a concept is verified not only according to its ACOUSTIC CONFIDENCE MEASURE but also according to NEIGHBORING CONCEPTS and their confidence levels . experimental results show that the proposed CONCEPT-BASED PROBABILISTIC VERIFICATION MODEL significantly outperforms the CONCEPT-BASED PROBABILISTIC VERIFICATION MODEL using only CONFIDENCE MEASURES . the ERROR RATE of CONFIDENCE TAG is reduced from 17.7 % to <unk> % , which corresponds to an ERROR REDUCTION RATE of 14.5 % . \n",
            "this paper proposes a new CONCEPT-BASED PROBABILISTIC VERIFICATION MODEL for WORD/PHRASE . the proposed CONCEPT-BASED PROBABILISTIC VERIFICATION MODEL is based on the CONFIDENCE MEASURE of the ACOUSTIC CONFIDENCE MEASURE of the ACOUSTIC CONFIDENCE MEASURE . the proposed CONCEPT-BASED PROBABILISTIC VERIFICATION MODEL is based on the CONTEXTUAL CONFIDENCE INFORMATION . the proposed CONCEPT-BASED PROBABILISTIC VERIFICATION MODEL is based on the CONFIDENCE MEASURE of the ACOUSTIC CONFIDENCE MEASURE . the proposed CONCEPT-BASED PROBABILISTIC VERIFICATION MODEL is compared with the conventional CONCEPT-BASED PROBABILISTIC VERIFICATION MODEL and the CONCEPT-BASED PROBABILISTIC VERIFICATION MODEL .\n",
            "\n",
            "609 1000\n",
            "the paper extends the notion of LINEAR PROGRAMMING BOOSTING to handle UNEVEN DATASETS . extensive experiments with TEXT CLASSIFICATION PROBLEM compare the performance of a number of different BOOSTING STRATEGIES , concentrating on the problems posed by UNEVEN DATASETS . \n",
            "this paper addresses the problem of TEXT CLASSIFICATION PROBLEM in a TEXT CLASSIFICATION PROBLEM . we propose a new method for LINEAR PROGRAMMING BOOSTING based on LINEAR PROGRAMMING BOOSTING . we demonstrate the effectiveness of our method on the task of TEXT CLASSIFICATION PROBLEM .\n",
            "\n",
            "610 1000\n",
            "lexical co-occurrence is an important cue for DETECTING WORD ASSOCIATIONS . we present a theoretical framework for discovering statistically significant LEXICAL CO-OCCURRENCES from a given corpus . in contrast with the prevalent practice of giving <unk> to UNIGRAM FREQUENCIES , we focus only on the documents containing both the terms -lrb- of a candidate <unk> -rrb- . we detect BIASES in SPAN DISTRIBUTIONS OF ASSOCIATED WORDS , while being agnostic to variations in GLOBAL UNIGRAM FREQUENCIES . our framework has the fidelity to distinguish different classes of LEXICAL CO-OCCURRENCES , based on strengths of the document and <unk> cues of co-occurrence in the data . we perform extensive experiments on BENCHMARK DATA SETS to study the performance of various CO-OCCURRENCE MEASURES that are currently known in literature . we find that a relatively obscure measure called OCHIAI , and a newly introduced measure csa capture the notion of LEXICAL CO-OCCURRENCE best , followed next by <unk> , <unk> , and TTEST , while another popular measure , <unk> , <unk> , performs poorly in the context of LEXICAL CO-OCCURRENCE . \n",
            "this paper presents a new method for DETECTING WORD ASSOCIATIONS based on LEXICAL CO-OCCURRENCES . the method is based on the use of LEXICAL CO-OCCURRENCES and the SPAN DISTRIBUTIONS OF ASSOCIATED WORDS . the proposed method is based on the use of CO-OCCURRENCE MEASURES as the basis for DETECTING WORD ASSOCIATIONS . the proposed method is based on the use of CO-OCCURRENCE MEASURES as the basis for DETECTING WORD ASSOCIATIONS . the proposed method is based on the use of CO-OCCURRENCE MEASURES as the basis for DETECTING WORD ASSOCIATIONS . experimental results on BENCHMARK DATA SETS demonstrate the effectiveness of the proposed method .\n",
            "\n",
            "611 1000\n",
            "in this paper , a SESSION VARIABILITY SUBSPACE PROJECTION SVSPBASED MODEL COMPENSATION METHOD for SPEAKER VERIFICATION is proposed . during the training phase the SESSION VARIABILITY is removed from SPEAKER MODELS by projection , while during the testing phase the SESSION VARIABILITY in a test utterance is used to compensate SPEAKER MODELS . finally , the COMPENSATED SPEAKER MODELS and UBM are used to recognize the identity of the test utterance . compared with the conventional GMM-UBM SYSTEM , the RELATIVE EQUAL ERROR RATE REDUCTION of SESSION VARIABILITY SUBSPACE PROJECTION SVSPBASED MODEL COMPENSATION METHOD is <unk> % on the nist 2006 <unk> one conversation training , <unk> one conversation test . \n",
            "this paper presents a SESSION VARIABILITY SUBSPACE PROJECTION SVSPBASED MODEL COMPENSATION METHOD for SPEAKER VERIFICATION . the proposed SESSION VARIABILITY SUBSPACE PROJECTION SVSPBASED MODEL COMPENSATION METHOD is based on the SESSION VARIABILITY SUBSPACE PROJECTION SVSPBASED MODEL COMPENSATION METHOD . the proposed SESSION VARIABILITY SUBSPACE PROJECTION SVSPBASED MODEL COMPENSATION METHOD is based on the SESSION VARIABILITY SUBSPACE PROJECTION SVSPBASED MODEL COMPENSATION METHOD and the SESSION VARIABILITY SUBSPACE PROJECTION SVSPBASED MODEL COMPENSATION METHOD . the proposed SESSION VARIABILITY SUBSPACE PROJECTION SVSPBASED MODEL COMPENSATION METHOD is compared with the conventional SESSION VARIABILITY SUBSPACE PROJECTION SVSPBASED MODEL COMPENSATION METHOD and the SESSION VARIABILITY SUBSPACE PROJECTION SVSPBASED MODEL COMPENSATION METHOD .\n",
            "\n",
            "612 1000\n",
            "3d object reconstruction from a single 2D LINE DRAWING is an important problem in both COMPUTER VISION and graphics . many methods have been put forward to solve this problem , but they usually fail when the GEOMETRIC STRUCTURE of a 3D OBJECT becomes complex . in this paper , a novel approach based on a DIVIDE-AND-CONQUER STRATEGY is proposed to handle 3D RECONSTRUCTION OF COMPLEX MANIFOLD OBJECTS from SINGLE 2D LINE DRAWINGS . the approach consists of three steps : 1 -rrb- dividing a complex LINE DRAWING into multiple simpler line drawings based on the result of FACE IDENTIFICATION ; 2 -rrb- reconstructing the 3D SHAPES from these simpler line drawings ; and 3 -rrb- merging the 3D SHAPES into one complete object represented by the original LINE DRAWING . a number of examples are given to show that our approach can handle 3D RECONSTRUCTION OF MORE COMPLEX OBJECTS than previous methods . \n",
            "this paper presents a method for 3D RECONSTRUCTION OF COMPLEX MANIFOLD OBJECTS from SINGLE 2D LINE DRAWINGS . the proposed method is based on a DIVIDE-AND-CONQUER STRATEGY , which is based on the DIVIDE-AND-CONQUER STRATEGY . the proposed method is based on the DIVIDE-AND-CONQUER STRATEGY . the proposed method is based on the DIVIDE-AND-CONQUER STRATEGY . the proposed method is based on the DIVIDE-AND-CONQUER STRATEGY . the proposed method is based on the DIVIDE-AND-CONQUER STRATEGY . the proposed method is based on a DIVIDE-AND-CONQUER STRATEGY . the proposed method is based on a DIVIDE-AND-CONQUER STRATEGY .\n",
            "\n",
            "613 1000\n",
            "<unk> of -lrb- . he key questions to be addressed in t , <unk> paper is how 1.0 estimate the DETERMINISTIC INPUT MULTIPULSE TIME SERIES from a noisy <unk> of the RESONANT TRANSFER SYSTEM iii -lrb- . lie case where <unk> SNR is low <unk> the system q -lrb- quality <unk> -rrb- of IHE TRANSFER SYSTEM is high . by <unk> the SHARP TRUUCABION employed in the standard SINGULAR-VALUE-DECOMPOSIT . ion -lrb- svd -rrb- , a TAPERING WINDOW is <unk> to t <unk> high order <unk> singular values obtained by <unk> svd , t , lie <unk> : l squared error of the <unk> due to the st , ANDARD SVD-BASED ESTIMATOR is reduced to a <unk> . this paper also <unk> a UEW METHOD to design AU OPTIMNM TAPERING WINDOW for ESTIMATIUG MULTIPULSE TIME SERIES . \n",
            "this paper presents a UEW METHOD for ESTIMATIUG MULTIPULSE TIME SERIES . the proposed UEW METHOD is based on the ANDARD SVD-BASED ESTIMATOR . the proposed UEW METHOD is based on the UEW METHOD . the proposed UEW METHOD is based on the ANDARD SVD-BASED ESTIMATOR . the proposed UEW METHOD is based on the ANDARD SVD-BASED ESTIMATOR . the proposed UEW METHOD is based on the ANDARD SVD-BASED ESTIMATOR . the proposed UEW METHOD is based on the ANDARD SVD-BASED ESTIMATOR . the proposed UEW METHOD is based on a UEW METHOD . the proposed UEW METHOD is based on a UEW METHOD .\n",
            "\n",
            "614 1000\n",
            "motion information is essential in many COMPUTER VISION and VIDEO ANALYSIS TASKS . since MPEG is still one of the most prevalent formats for representing , transferring and storing video data , the analysis of its motion field is important for real time video indexing and segmentation , EVENT ANALYSIS and SURVEILLANCE APPLICATIONS . our work considers the problem of improving the OPTICAL FLOW FIELD in MPEG SEQUENCES . we address the issues of robust , incremental , dense optical flow estimation by combining information from two different VELOCITY FIELDS : the available MPEG MOTION FIELD and the one inferred by a MULTIRESOLUTION ROBUST REGULARIZATION TECHNIQUE applied on the DC COEFFICIENTS . thus , the MULTIRESOLUTION ROBUST REGULARIZATION TECHNIQUE is based only on information that is directly available in the COMPRESSED STREAM avoiding therefore the time and MEMORY CONSUMING DECOMPRESSION . we extend standard techniques by adding a TEMPORAL CONTINUITY and an MPEG CONSISTENCY CONSTRAINT , both as MATHEMATICAL CONSTRAINTS in the OBJECTIVE FUNCTION and as hypothesis tests for the presence of MOTION DISCONTINUITIES . our approach is shown to perform well over a range of different MOTION SCENARIOS and can serve as a basis for efficient VIDEO ANALYSIS TASKS . \n",
            "this paper addresses the problem of EVENT ANALYSIS in VIDEO ANALYSIS TASKS . we propose a method for EVENT ANALYSIS based on a MULTIRESOLUTION ROBUST REGULARIZATION TECHNIQUE and a MULTIRESOLUTION ROBUST REGULARIZATION TECHNIQUE . the proposed method is based on a MULTIRESOLUTION ROBUST REGULARIZATION TECHNIQUE and a MULTIRESOLUTION ROBUST REGULARIZATION TECHNIQUE . the proposed method is based on a MULTIRESOLUTION ROBUST REGULARIZATION TECHNIQUE and a MULTIRESOLUTION ROBUST REGULARIZATION TECHNIQUE . the proposed method is based on a MULTIRESOLUTION ROBUST REGULARIZATION TECHNIQUE and a MULTIRESOLUTION ROBUST REGULARIZATION TECHNIQUE . the proposed method is based on a MULTIRESOLUTION ROBUST REGULARIZATION TECHNIQUE and a MULTIRESOLUTION ROBUST REGULARIZATION TECHNIQUE . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "615 1000\n",
            "we present a FRACTIONAL GABOR EXPANSION on a GENERAL , NON-RECTANGULAR TIME-FREQUENCY LATTICE . the traditional FRACTIONAL GABOR EXPANSION represents a signal in terms of time and frequency shifted BASIS FUNCTIONS , called GABOR LOGONS . this CONSTANT-BANDWIDTH ANALYSIS results in a fixed , rectangular time frequency plane <unk> . many of the practical signals require a more FLEXIBLE , NON-RECTANGULAR TIME-FREQUENCY LATTICE for a COMPACT REPRESENTATION . the proposed FRACTIONAL GABOR EXPANSION uses a set of BASIS FUNCTIONS that are related to the FRACTIONAL FOURIER BASIS and generate a NON-RECTANGULAR TILING . the completeness and BI-ORTHOGONALITY CONDITIONS of the new GABOR LOGONS are discussed . \n",
            "this paper presents a new method for CONSTANT-BANDWIDTH ANALYSIS based on FRACTIONAL GABOR EXPANSION . the proposed method is based on the use of BASIS FUNCTIONS in the FRACTIONAL FOURIER BASIS . the proposed method is based on the FRACTIONAL GABOR EXPANSION . the proposed method is based on the FRACTIONAL GABOR EXPANSION . the proposed method is based on the FRACTIONAL GABOR EXPANSION . the proposed method is based on the FRACTIONAL GABOR EXPANSION . the proposed method is based on the FRACTIONAL GABOR EXPANSION . the proposed method is based on the FRACTIONAL GABOR EXPANSION .\n",
            "\n",
            "616 1000\n",
            "this paper considers a method for learning a DISTANCE METRIC in a FINGERPRINTING SYSTEM which identifies a QUERY CONTENT by measuring the distance between its fingerprint and a fingerprint stored in a database . a metric having a general form of the MAHALANOBIS DISTANCE is learned with the goal that the distance between fingerprints extracted from perceptually similar contents should be smaller than the distance between fingerprints extracted from perceptually dissimilar contents . the metric is learned by minimizing a COST FUNCTION designed to achieve the goal . the COST FUNCTION is convex , and the GLOBAL MINIMUM can be obtained using CONVEX OPTIMIZATION . in our experiment , the DISTANCE METRIC LEARNING is applied in an AUDIO FINGERPRINTING SYSTEM , and it is experimentally shown that the learned DISTANCE METRIC improves the IDENTIFICATION performance . \n",
            "this paper presents a new method for IDENTIFICATION based on the DISTANCE METRIC . the proposed method is based on the DISTANCE METRIC and uses a DISTANCE METRIC to estimate the QUERY CONTENT . the proposed method is based on a DISTANCE METRIC , which is based on the DISTANCE METRIC . the proposed method is based on the DISTANCE METRIC . the proposed method is based on the DISTANCE METRIC . the proposed method is based on the DISTANCE METRIC . the proposed method is based on the DISTANCE METRIC .\n",
            "\n",
            "617 1000\n",
            "recently a large amount of research has been devoted to AUTOMATIC ACTIVITY ANALYSIS . typically , activities have been defined by their MOTION CHARACTERISTICS and represented by trajectories . these trajectories are collected and clustered to determine typical behaviors . this paper evaluates different SIMILARITY MEASURES and CLUSTERING METHODOLOGIES to catalog their strengths and weaknesses when utilized for the TRAJECTORY LEARNING PROBLEM . the CLUSTERING performance is measured by evaluating the correct CLUSTERING RATE on different datasets with varying characteristics . \n",
            "this paper addresses the problem of AUTOMATIC ACTIVITY ANALYSIS in the presence of MOTION CHARACTERISTICS . in this paper , we propose a new method for AUTOMATIC ACTIVITY ANALYSIS based on the TRAJECTORY LEARNING PROBLEM . the proposed method is based on the use of SIMILARITY MEASURES . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "618 1000\n",
            "embedded EMBEDDED SYSTEMS consisting of COLLABORATING AGENTS capable of interacting with their environment are becoming ubiquitous . it is crucial for these EMBEDDED SYSTEMS to be able to adapt to the dynamic and uncertain characteristics of an open environment . in this paper , we argue that MULTIAGENT META-LEVEL CONTROL is an effective way to determine when this ADAPTATION PROCESS should be done and how much effort should be invested in adaptation as opposed to continuing with the current ACTION PLAN . we describe a REINFORCEMENT LEARNING BASED APPROACH to learn DECENTRALIZED META-CONTROL POLICIES offline . we then propose to use the learned REINFORCEMENT LEARNING BASED APPROACH as input to a GLOBAL OPTIMIZATION ALGORITHM to avoid conflicting <unk> decisions between COORDINATING AGENTS . our initial experiments in the context of NETRADS , a MULTIAGENT TORNADO TRACKING APPLICATION show that NETRADS significantly improves performance in a 3-AGENT NETWORK . \n",
            "this paper presents a REINFORCEMENT LEARNING BASED APPROACH for MULTIAGENT META-LEVEL CONTROL . the proposed REINFORCEMENT LEARNING BASED APPROACH is based on a REINFORCEMENT LEARNING BASED APPROACH , which is based on a REINFORCEMENT LEARNING BASED APPROACH . the proposed REINFORCEMENT LEARNING BASED APPROACH is based on a REINFORCEMENT LEARNING BASED APPROACH , which is based on the GLOBAL OPTIMIZATION ALGORITHM . the proposed REINFORCEMENT LEARNING BASED APPROACH is based on a REINFORCEMENT LEARNING BASED APPROACH . the proposed REINFORCEMENT LEARNING BASED APPROACH is based on a REINFORCEMENT LEARNING BASED APPROACH , which is based on the GLOBAL OPTIMIZATION ALGORITHM . the proposed REINFORCEMENT LEARNING BASED APPROACH is compared with the conventional REINFORCEMENT LEARNING BASED APPROACH and the REINFORCEMENT LEARNING BASED APPROACH .\n",
            "\n",
            "619 1000\n",
            "the following article shows how a state-of-the-art SPEAKER DI-ARIZATION SYSTEM can be improved by combining traditional SHORT-TERM FEATURES with PROSODIC and other LONG-TERM FEATURES . first , we present a framework to study the SPEAKER DISCRIMINABILITY of 70 different LONG-TERM FEATURES . then , we show how the TOP-RANKED LONG-TERM FEATURES can be combined with SHORT-TERM FEATURES to increase the ACCURACY of SPEAKER DIARIZATION . the results were measured on standardized data sets -lrb- nist rt -rrb- and show a consistent improvement of about 30 % relative in DIARIZATION ERROR RATE compared to the best SPEAKER DI-ARIZATION SYSTEM presented at the NIST EVALUATION in 2007 . this result was also verified on a wide set of meetings , which we call <unk> , that contains 21 meetings from previous evaluations . since the PROSODIC AND LONG-TERM FEATURES were selected using a DIARIZATION-INDEPENDENT SPEAKER-DISCRIMINABILITY STUDY , we are confident that the same FEATURES are able to improve other systems that perform similar tasks \n",
            "this paper presents a novel SPEAKER DI-ARIZATION SYSTEM for SPEAKER DIARIZATION . the SPEAKER DI-ARIZATION SYSTEM is based on a DIARIZATION-INDEPENDENT SPEAKER-DISCRIMINABILITY STUDY . the proposed SPEAKER DI-ARIZATION SYSTEM is based on a DIARIZATION-INDEPENDENT SPEAKER-DISCRIMINABILITY STUDY , which is based on a DIARIZATION-INDEPENDENT SPEAKER-DISCRIMINABILITY STUDY . the proposed SPEAKER DI-ARIZATION SYSTEM is based on a DIARIZATION-INDEPENDENT SPEAKER-DISCRIMINABILITY STUDY , which is based on a DIARIZATION-INDEPENDENT SPEAKER-DISCRIMINABILITY STUDY . the ACCURACY of the proposed SPEAKER DI-ARIZATION SYSTEM is compared with the conventional SPEAKER DI-ARIZATION SYSTEM . the ACCURACY of the proposed SPEAKER DI-ARIZATION SYSTEM is compared with the conventional SPEAKER DI-ARIZATION SYSTEM .\n",
            "\n",
            "620 1000\n",
            "in this paper we derive PERFORMANCE BOUNDS for tracking time-varying ofdm multiple-input multiple-output -lrb- mimo -rrb- communication channel in the presence of ADDITIVE WHITE GAUSSIAN NOISE . we discuss two CHANNEL TRACKING SCHEMES . the first tracks the FILTER COEFFICIENTS directly in time-domain , while the second separately tracks each tone in the frequency-domain . the CHANNEL TRACKING SCHEMES , with KNOWN CHANNEL STATISTICS , is utilized for evaluating the PERFORMANCE BOUNDS . it is shown that the CHANNEL TRACKING SCHEMES , which exploits the sparseness of the CHANNEL IMPULSE RESPONSE , outperforms the compu-tationally more efficient , FREQUENCY-DOMAIN TRACKING SCHEME , which does not exploit the smooth frequency response of the channel . \n",
            "this paper addresses the problem of ADDITIVE WHITE GAUSSIAN NOISE for ADDITIVE WHITE GAUSSIAN NOISE . the FREQUENCY-DOMAIN TRACKING SCHEME is based on a FREQUENCY-DOMAIN TRACKING SCHEME of the CHANNEL IMPULSE RESPONSE . the proposed algorithm is based on a FREQUENCY-DOMAIN TRACKING SCHEME of the CHANNEL IMPULSE RESPONSE . simulation results are presented to demonstrate the effectiveness of the proposed method .\n",
            "\n",
            "621 1000\n",
            "detection of perceptually important video events is formulated here on the basis of SALIENCY MODELS for the audio , visual and textual information conveyed in a VIDEO STREAM . AUDIO SALIENCY is assessed by CUES that quantify MULTIFREQUENCY WAVEFORM MODULATIONS , extracted through NONLINEAR OPERATORS and ENERGY TRACKING . VISUAL SALIENCY is measured through a SPATIOTEMPORAL ATTENTION MODEL driven by intensity , COLOR and motion . TEXT SALIENCY is extracted from PART-OF-SPEECH TAGGING on the <unk> information available with most MOVIE DISTRIBUTIONS . the various MODALITY CURVES are integrated in a single ATTENTION CURVE , where the presence of an event may be <unk> in one or multiple domains . this MULTIMODAL SALIENCY CURVE is the basis of a BOTTOM-UP VIDEO SUMMARIZATION ALGORITHM , that RESNES results from UNIMODAL OR AUDIOVISUAL-BASED SKIMMING . the BOTTOM-UP VIDEO SUMMARIZATION ALGORITHM performs favorably for VIDEO SUMMARIZATION in terms of informativeness and <unk> . \n",
            "this paper presents a BOTTOM-UP VIDEO SUMMARIZATION ALGORITHM for VIDEO SUMMARIZATION . the SPATIOTEMPORAL ATTENTION MODEL is based on the SPATIOTEMPORAL ATTENTION MODEL , which is based on the SPATIOTEMPORAL ATTENTION MODEL . the proposed BOTTOM-UP VIDEO SUMMARIZATION ALGORITHM is based on the SPATIOTEMPORAL ATTENTION MODEL , which is based on the SPATIOTEMPORAL ATTENTION MODEL . the proposed BOTTOM-UP VIDEO SUMMARIZATION ALGORITHM is based on the SPATIOTEMPORAL ATTENTION MODEL . the proposed BOTTOM-UP VIDEO SUMMARIZATION ALGORITHM is based on the SPATIOTEMPORAL ATTENTION MODEL . the proposed BOTTOM-UP VIDEO SUMMARIZATION ALGORITHM is based on the SPATIOTEMPORAL ATTENTION MODEL . the proposed BOTTOM-UP VIDEO SUMMARIZATION ALGORITHM is based on the SPATIOTEMPORAL ATTENTION MODEL . the proposed BOTTOM-UP VIDEO SUMMARIZATION ALGORITHM is based on the SPATIOTEMPORAL ATTENTION MODEL . the proposed BOTTOM-UP VIDEO SUMMARIZATION ALGORITHM is based on the SPATIOTEMPORAL ATTENTION MODEL . the proposed BOTTOM-UP VIDEO SUMMARIZATION ALGORITHM is based on the SPATIOTEMPORAL ATTENTION MODEL and is shown to be useful for VIDEO SUMMARIZATION .\n",
            "\n",
            "622 1000\n",
            "efficient learning equilibrium -lrb- EFFICIENT LEARNING EQUILIBRIUM -rrb- is a NATURAL SOLUTION CONCEPT for MULTI-AGENT ENCOUNTERS with INCOMPLETE INFORMATION . it requires the LEARNING ALGORITHMS themselves to be in equilibrium for any game selected from a set of -lrb- initially unknown -rrb- games . in an optimal EFFICIENT LEARNING EQUILIBRIUM , the LEARNING ALGORITHMS would efficiently obtain the surplus the agents would obtain in an optimal nash equilibrium of the initially unknown game which is played . the crucial part is that in an ELE DEVIATIONS from the LEARNING ALGORITHMS would become <unk> after POLYNOMIAL TIME , although the game played is initially unknown . while appealing conceptually , the main challenge for establishing LEARNING ALGORITHMS based on this concept is to isolate general classes of games where an EFFICIENT LEARNING EQUILIBRIUM exists . unfortunately , it has been shown that while an EFFICIENT LEARNING EQUILIBRIUM exists for the setting in which each agent can observe all other agents ' actions and payoffs , an EFFICIENT LEARNING EQUILIBRIUM does not exist in general when the other agents ' payoffs can not be observed . in this paper we provide the first positive results on this problem , <unk> proving the existence of an optimal EFFICIENT LEARNING EQUILIBRIUM for the class of symmetric games where an agent can not observe other agents ' payoffs . \n",
            "this paper introduces a new EFFICIENT LEARNING EQUILIBRIUM , called MULTI-AGENT ENCOUNTERS , which is based on a EFFICIENT LEARNING EQUILIBRIUM . the NATURAL SOLUTION CONCEPT is based on a EFFICIENT LEARNING EQUILIBRIUM , which is a EFFICIENT LEARNING EQUILIBRIUM of the NATURAL SOLUTION CONCEPT . the proposed LEARNING ALGORITHMS is based on a EFFICIENT LEARNING EQUILIBRIUM , which is a EFFICIENT LEARNING EQUILIBRIUM . the EFFICIENT LEARNING EQUILIBRIUM is shown to outperform the conventional LEARNING ALGORITHMS .\n",
            "\n",
            "623 1000\n",
            "we describe an approach for UNSUPERVISED LEARNING of a GENERIC , DISTRIBUTED SENTENCE ENCODER . using the continuity of text from books , we train an ENCODER-DECODER MODEL that tries to reconstruct the surrounding sentences of an ENCODED PASSAGE . sentences that share SEMANTIC AND SYNTACTIC PROPERTIES are thus mapped to similar VECTOR REPRESENTATIONS . we next introduce a simple VOCABULARY EXPANSION METHOD to encode words that were not seen as part of training , allowing us to expand our vocabulary to a million words . after training our model , we extract and evaluate our vectors with LINEAR MODELS on 8 tasks : SEMANTIC RELATEDNESS , PARAPHRASE DETECTION , IMAGE-SENTENCE RANKING , QUESTION-TYPE CLASSIFICATION and 4 benchmark sentiment and subjectivity datasets . the end result is an off-the-shelf encoder that can produce highly GENERIC SENTENCE REPRESENTATIONS that are robust and perform well in practice . we will make our encoder publicly available . \n",
            "this paper addresses the problem of PARAPHRASE DETECTION in a GENERIC , DISTRIBUTED SENTENCE ENCODER . we propose a VOCABULARY EXPANSION METHOD based on the ENCODER-DECODER MODEL and the ENCODER-DECODER MODEL . the proposed VOCABULARY EXPANSION METHOD is based on the ENCODER-DECODER MODEL and the ENCODER-DECODER MODEL . the proposed ENCODER-DECODER MODEL is based on the VOCABULARY EXPANSION METHOD and the ENCODER-DECODER MODEL . the proposed VOCABULARY EXPANSION METHOD is based on the VOCABULARY EXPANSION METHOD and the ENCODER-DECODER MODEL . the proposed VOCABULARY EXPANSION METHOD is evaluated on a GENERIC , DISTRIBUTED SENTENCE ENCODER and a GENERIC , DISTRIBUTED SENTENCE ENCODER . the experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "624 1000\n",
            "we present an LFG-DOP PARSER which uses fragments from LFG-ANNOTATED SENTENCES to parse new sentences . experiments with the VERBMOBIL AND HOMECENTRE CORPORA show that -lrb- 1 -rrb- viterbi n best search performs about 100 times faster than MONTE CARLO SEARCH while both achieve the same ACCURACY ; -lrb- 2 -rrb- the DOP HYPOTHESIS which states that PARSE ACCURACY increases with increasing FRAGMENT SIZE is confirmed for LFG-DOP ; -lrb- 3 -rrb- LFG-DOP 's RELATIVE FREQUENCY ESTIMATOR performs worse than a DISCOUNTED FREQUENCY ESTIMATOR ; and -lrb- 4 -rrb- LFG-DOP significantly outperforms TREE-DOP if evaluated on TREE STRUCTURES only . \n",
            "this paper proposes a new method for MONTE CARLO SEARCH based on MONTE CARLO SEARCH . the proposed method is based on the RELATIVE FREQUENCY ESTIMATOR of the DOP HYPOTHESIS . the proposed method is based on the RELATIVE FREQUENCY ESTIMATOR of the DOP HYPOTHESIS . the proposed method is based on the RELATIVE FREQUENCY ESTIMATOR of the DOP HYPOTHESIS . the ACCURACY of the proposed DISCOUNTED FREQUENCY ESTIMATOR is compared with the RELATIVE FREQUENCY ESTIMATOR of the LFG-DOP PARSER . the ACCURACY of the proposed method is compared with the conventional LFG-DOP .\n",
            "\n",
            "625 1000\n",
            "this contribution presents a MODIFIED KALMAN FILTER APPROACH for SINGLE CHANNEL SPEECH ENHANCEMENT which is operating in the FREQUENCY DOMAIN . in the first step , TEMPORAL CORRELATION OF SUCCESSIVE FRAMES is exploited yielding estimates of the current speech and noise dft coefficients . this first prediction is updated in the second step applying an SNR DEPENDENT MMSE ESTIMATOR which is adapted to the -lrb- measured -rrb- statistics of the SPEECH PREDICTION ERROR SIGNAL . OBJECTIVE MEASUREMENTS show consistent improvements compared to ESTIMA-TORS which do not take into account the TEMPORAL CORRELATION or the influence of the input SNR on the statistics of the PREDICTION ERROR SIGNAL . \n",
            "this paper presents a novel MODIFIED KALMAN FILTER APPROACH for SINGLE CHANNEL SPEECH ENHANCEMENT . the MODIFIED KALMAN FILTER APPROACH is based on the MODIFIED KALMAN FILTER APPROACH . the proposed MODIFIED KALMAN FILTER APPROACH is based on the MODIFIED KALMAN FILTER APPROACH . the proposed MODIFIED KALMAN FILTER APPROACH is based on the MODIFIED KALMAN FILTER APPROACH . the proposed MODIFIED KALMAN FILTER APPROACH is based on the MODIFIED KALMAN FILTER APPROACH . the proposed MODIFIED KALMAN FILTER APPROACH is based on the MODIFIED KALMAN FILTER APPROACH . the proposed MODIFIED KALMAN FILTER APPROACH is evaluated on the FREQUENCY DOMAIN .\n",
            "\n",
            "626 1000\n",
            "this paper presents a system that labels TV SHOTS either as COMMERCIAL OR PROGRAM SHOTS . the system uses two observations : LOGO PRESENCE and SHOT DURATION . this observations are modeled using HMM and the VITERBI DECODER is finally used for SHOT LABELING . the system has been tested on several hours of REAL VIDEO achieving more than 99 % of correct labeling . \n",
            "this paper presents a new method for SHOT LABELING . the VITERBI DECODER is based on the VITERBI DECODER and the VITERBI DECODER . the proposed method is based on the VITERBI DECODER and the VITERBI DECODER . the proposed method is based on the VITERBI DECODER and the VITERBI DECODER . the experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "627 1000\n",
            "deterministic parsing guided by TREEBANK-INDUCED CLASSIFIERS has emerged as a simple and efficient alternative to more complex models for DATA-DRIVEN PARSING . we present a systematic comparison of <unk> learning -lrb- <unk> -rrb- and SUPPORT VECTOR MACHINES for inducing CLASSIFIERS for DETERMINISTIC DEPENDENCY PARSING , using data from CHINESE , ENGLISH and SWEDISH , together with a variety of different FEATURE MODELS . the comparison shows that SUPPORT VECTOR MACHINES gives higher ACCURACY for <unk> articulated FEATURE MODELS across all languages , albeit with considerably longer training times . the results also confirm that CLASSIFIER-BASED DETERMINISTIC PARSING can achieve PARSING ACCURACY very close to the best results reported for more complex PARSING MODELS . \n",
            "this paper addresses the problem of DETERMINISTIC DEPENDENCY PARSING in CHINESE . we propose a method for DETERMINISTIC DEPENDENCY PARSING based on SUPPORT VECTOR MACHINES . the proposed FEATURE MODELS is based on the use of SUPPORT VECTOR MACHINES and SUPPORT VECTOR MACHINES . the proposed FEATURE MODELS is based on the use of TREEBANK-INDUCED CLASSIFIERS and SUPPORT VECTOR MACHINES . experimental results show that the proposed FEATURE MODELS improves the performance of the PARSING MODELS in terms of PARSING ACCURACY and ACCURACY .\n",
            "\n",
            "628 1000\n",
            "this paper proposes a SEMI-SUPERVISED BOOSTING APPROACH to improve STATISTICAL WORD ALIGNMENT with LIMITED LABELED DATA and large amounts of UNLABELED DATA . the proposed SEMI-SUPERVISED BOOSTING APPROACH modifies the SUPERVISED BOOSTING ALGORITHM to a SEMI-SUPERVISED LEARNING ALGORITHM by incorporating the UNLABELED DATA . in this SEMI-SUPERVISED BOOSTING APPROACH , we build a WORD ALIGNER by using both the LABELED DATA and the UNLABELED DATA . then we build a PSEUDO REFERENCE SET for the UNLABELED DATA , and calculate the ERROR RATE of each WORD ALIGNER using only the LABELED DATA . based on this SUPERVISED BOOSTING ALGORITHM , we investigate two BOOSTING METHODS for STATISTICAL WORD ALIGNMENT . in addition , we improve the STATISTICAL WORD ALIGNMENT results by combining the results of the two SEMI-SUPERVISED BOOSTING METHODS . experimental results on STATISTICAL WORD ALIGNMENT indicate that SEMI-SUPERVISED BOOSTING achieves RELATIVE ERROR REDUCTIONS of <unk> % and <unk> % as compared with SUPERVISED BOOSTING and UNSUPERVISED BOOSTING , respectively . \n",
            "this paper proposes a new SEMI-SUPERVISED BOOSTING APPROACH for STATISTICAL WORD ALIGNMENT . the SEMI-SUPERVISED LEARNING ALGORITHM is based on the SEMI-SUPERVISED BOOSTING APPROACH and the SEMI-SUPERVISED LEARNING ALGORITHM . the proposed SEMI-SUPERVISED BOOSTING APPROACH is based on the SEMI-SUPERVISED BOOSTING APPROACH and the SEMI-SUPERVISED LEARNING ALGORITHM . the proposed SEMI-SUPERVISED BOOSTING APPROACH is based on the SEMI-SUPERVISED BOOSTING APPROACH and the SUPERVISED BOOSTING ALGORITHM . the proposed SEMI-SUPERVISED BOOSTING APPROACH is based on the SEMI-SUPERVISED BOOSTING APPROACH and the SEMI-SUPERVISED LEARNING ALGORITHM . the proposed SEMI-SUPERVISED BOOSTING APPROACH is evaluated on the PSEUDO REFERENCE SET and the PSEUDO REFERENCE SET .\n",
            "\n",
            "629 1000\n",
            "computing the market maker price of a security in a COMBINATORIAL PREDICTION MARKET is #P - hard . we devise a fully polynomial randomized approximation scheme -lrb- <unk> -rrb- that computes the price of any security in DISJUNCTIVE NORMAL FORM within an MULTIPLICATIVE ERROR FACTOR in TIME POLYNOMIAL in 1 / / and the size of the input , with high probability and under reasonable assumptions . our algorithm is a MONTE-CARLO TECHNIQUE based on IMPORTANCE SAMPLING . the algorithm can also approximately price <unk> represented in CONJUNCTIVE NORMAL FORM with ADDITIVE ERROR BOUNDS . to illustrate the applicability of our algorithm , we show that many <unk> in yahoo! 's popular COMBINATORIAL PREDICTION MARKET GAME called CONJUNCTIVE NORMAL FORM can be represented by DNF FORMULAS OF POLYNOMIAL SIZE . \n",
            "this paper presents a new method for IMPORTANCE SAMPLING based on IMPORTANCE SAMPLING . the proposed method is based on a MONTE-CARLO TECHNIQUE , which is based on the MULTIPLICATIVE ERROR FACTOR of the COMBINATORIAL PREDICTION MARKET . the proposed MONTE-CARLO TECHNIQUE is based on the MULTIPLICATIVE ERROR FACTOR of the COMBINATORIAL PREDICTION MARKET GAME . the proposed MONTE-CARLO TECHNIQUE is based on the MULTIPLICATIVE ERROR FACTOR of the COMBINATORIAL PREDICTION MARKET GAME . the proposed MONTE-CARLO TECHNIQUE is compared with the conventional MONTE-CARLO TECHNIQUE .\n",
            "\n",
            "630 1000\n",
            "we replace the OVERLAP MECHANISM of the LESK ALGORITHM with a simple , GENERAL-PURPOSE NAIVE BAYES MODEL that measures MANY-TO-MANY ASSOCIATION between two sets of RANDOM VARIABLES . even with simple PROBABILITY ESTIMATES such as MAXIMUM LIKELIHOOD , the GENERAL-PURPOSE NAIVE BAYES MODEL gains significant improvement over the LESK ALGORITHM on WORD SENSE DISAMBIGUATION TASKS . with additional LEXICAL KNOWLEDGE from WORD-NET , performance is further improved to surpass the state-of-the-art results . \n",
            "this paper proposes a GENERAL-PURPOSE NAIVE BAYES MODEL for MANY-TO-MANY ASSOCIATION . the proposed GENERAL-PURPOSE NAIVE BAYES MODEL is based on the GENERAL-PURPOSE NAIVE BAYES MODEL . the proposed GENERAL-PURPOSE NAIVE BAYES MODEL is based on the GENERAL-PURPOSE NAIVE BAYES MODEL . the proposed GENERAL-PURPOSE NAIVE BAYES MODEL is based on the GENERAL-PURPOSE NAIVE BAYES MODEL . the proposed GENERAL-PURPOSE NAIVE BAYES MODEL is compared with the conventional GENERAL-PURPOSE NAIVE BAYES MODEL and the GENERAL-PURPOSE NAIVE BAYES MODEL . the proposed GENERAL-PURPOSE NAIVE BAYES MODEL outperforms the conventional GENERAL-PURPOSE NAIVE BAYES MODEL in terms of MANY-TO-MANY ASSOCIATION and PROBABILITY ESTIMATES .\n",
            "\n",
            "631 1000\n",
            "valued decision diagrams -lrb- VDD LANGUAGES -rrb- are DATA STRUCTURES that represent functions mapping <unk> assignments to NON-NEGATIVE REAL NUMBERS . they prove useful to compile COST FUNCTIONS , UTILITY FUNCTIONS , or PROBABILITY DISTRIBUTIONS . while the COMPLEXITY of some queries -lrb- notably optimization -rrb- and transformations -lrb- notably conditioning -rrb- on VDD LANGUAGES has been known for some time , there remain many significant queries and transformations , such as the various kinds of CUTS , MARGINALIZATIONS , and combinations , the COMPLEXITY of which has not been identified so far . this paper contributes to filling this gap and completing previous results about the time and space efficiency of VDD LANGUAGES , thus leading to a KNOWLEDGE COMPILATION MAP for REAL-VALUED FUNCTIONS . our results show that many tasks that are hard on valued CSPS are actually tractable on VDD LANGUAGES . \n",
            "this paper addresses the problem of VALUED DECISION DIAGRAMS for VDD LANGUAGES . we propose a method for VALUED DECISION DIAGRAMS based on VALUED DECISION DIAGRAMS and VALUED DECISION DIAGRAMS . the proposed method is based on the use of UTILITY FUNCTIONS and VALUED DECISION DIAGRAMS . the proposed method is based on a KNOWLEDGE COMPILATION MAP and a KNOWLEDGE COMPILATION MAP . experimental results show the effectiveness of the proposed method in terms of COMPLEXITY and COMPLEXITY .\n",
            "\n",
            "632 1000\n",
            "we describe a TRANSLATION MODEL ADAPTATION APPROACH for CONVERSATIONAL SPOKEN LANGUAGE TRANSLATION , which encourages the use of contextually appropriate translation options from relevant training conversations . our TRANSLATION MODEL ADAPTATION APPROACH employs a MONOLINGUAL LDA TOPIC MODEL to derive a SIMILARITY MEASURE between the test conversation and the set of training conversations , which is used to bias translation choices towards the current context . a significant novelty of our TRANSLATION MODEL ADAPTATION APPROACH is its INCREMENTAL NATURE ; we continuously update the TOPIC DISTRIBUTION on the evolving test conversation as new utterances become available . thus , our TRANSLATION MODEL ADAPTATION APPROACH is well-suited to the CAUSAL CONSTRAINT OF SPOKEN CONVERSATIONS . on an ENGLISH-TO-IRAQI CSLT TASK , the proposed TRANSLATION MODEL ADAPTATION APPROACH gives significant improvements over a baseline system as measured by BLEU , TER , and NIST . interestingly , the TRANSLATION MODEL ADAPTATION APPROACH outperforms a NON-INCREMENTAL ORACLE that has UP-FRONT KNOWLEDGE of the whole conversation . \n",
            "this paper presents a TRANSLATION MODEL ADAPTATION APPROACH for CONVERSATIONAL SPOKEN LANGUAGE TRANSLATION . the TRANSLATION MODEL ADAPTATION APPROACH is based on the MONOLINGUAL LDA TOPIC MODEL and the TRANSLATION MODEL ADAPTATION APPROACH . the proposed TRANSLATION MODEL ADAPTATION APPROACH is based on the MONOLINGUAL LDA TOPIC MODEL and the TRANSLATION MODEL ADAPTATION APPROACH . the proposed TRANSLATION MODEL ADAPTATION APPROACH is based on the MONOLINGUAL LDA TOPIC MODEL and the TRANSLATION MODEL ADAPTATION APPROACH . the TRANSLATION MODEL ADAPTATION APPROACH is evaluated on the ENGLISH-TO-IRAQI CSLT TASK and the ENGLISH-TO-IRAQI CSLT TASK . the proposed TRANSLATION MODEL ADAPTATION APPROACH is compared with the conventional TRANSLATION MODEL ADAPTATION APPROACH and the TRANSLATION MODEL ADAPTATION APPROACH .\n",
            "\n",
            "633 1000\n",
            "the problems of DESIGNING SIGNATURE SEQUENCES and POWER ALLOCATION POLICY for <unk> multiple access -lrb- cdma -rrb- are important and have been the subject of intensive research in recent years . two different criteria adopted in such DESIGN PROBLEMS are the USER CAPACITY and the INFORMATION-THEORETIC CAPACITY . regarding the maxi-mization of the INFORMATION-THEORETIC CAPACITY , most of the previous works only consider the optimizations of SIGNATURE SEQUENCES and POWER ALLOCATION separately . in contrast , this paper presents a jointly optimal design of SIGNATURE SEQUENCES and POWER ALLOCATION under the SUM POWER CONSTRAINT . the proposed design is of closed-form and applicable for the general case of CORRELATED SIGNALS and COLORED NOISE . numerical results verify the superiority of the proposed design over the existing ones . \n",
            "this paper presents a new method for DESIGN PROBLEMS . the method is based on the SUM POWER CONSTRAINT and the POWER ALLOCATION POLICY . the proposed method is based on the SUM POWER CONSTRAINT and the POWER ALLOCATION POLICY . the proposed method is based on the SUM POWER CONSTRAINT and the POWER ALLOCATION POLICY . the proposed method is evaluated in terms of INFORMATION-THEORETIC CAPACITY and USER CAPACITY .\n",
            "\n",
            "634 1000\n",
            "we investigate whether it is possible to improve the performance of AUTOMATED FACIAL FORENSIC SKETCH MATCHING by learning from examples of facial forgetting over time . FORENSIC FACIAL SKETCH RECOGNITION is a key capability for LAW ENFORCEMENT , but remains an unsolved problem . it is extremely challenging because there are three distinct <unk> to the DOMAIN GAP between FORENSIC SKETCHES and photos : the WELL-STUDIED SKETCH-PHOTO MODALITY GAP , and the less studied gaps due to -lrb- i -rrb- the FORGETTING PROCESS of the <unk> and -lrb- ii -rrb- their inability to elucidate their memory . in this paper , we address the MEMORY PROBLEM head on by introducing a database of 400 FORENSIC SKETCHES created at different <unk> . based on this database we build a model to reverse the FORGETTING PROCESS . surprisingly , we show that it is possible to systematically '' <unk> '' facial details . moreover , it is possible to apply this model to dramatically improve FORENSIC SKETCH RECOGNITION in practice : we achieve the state of the art results when matching <unk> benchmark FORENSIC SKETCHES against corresponding photos and a 10,030 MUGSHOT DATABASE . \n",
            "this paper presents a new method for AUTOMATED FACIAL FORENSIC SKETCH MATCHING . the proposed method is based on a 10,030 MUGSHOT DATABASE , which is based on the FORGETTING PROCESS . the proposed method is based on the 10,030 MUGSHOT DATABASE . the proposed method is based on the 10,030 MUGSHOT DATABASE . the proposed method is based on the 10,030 MUGSHOT DATABASE . the proposed method is based on the 10,030 MUGSHOT DATABASE . the proposed method is based on the 10,030 MUGSHOT DATABASE . the proposed method is based on the 10,030 MUGSHOT DATABASE .\n",
            "\n",
            "635 1000\n",
            "convexity has recently received a lot of attention in the MACHINE LEARNING COMMUNITY , and the lack of CONVEXITY has been seen as a major disadvantage of many LEARNING ALGORITHMS , such as MULTI-LAYER ARTIFICIAL NEURAL NETWORKS . we show that training MULTI-LAYER ARTIFICIAL NEURAL NETWORKS in which the number of hidden units is learned can be viewed as a CONVEX OPTIMIZATION PROBLEM . this CONVEX OPTIMIZATION PROBLEM involves an infinite number of variables , but can be solved by incrementally inserting a HIDDEN UNIT at a time , each time finding a LINEAR CLASSIFIER that minimizes a weighted sum of errors . \n",
            "this paper addresses the problem of MULTI-LAYER ARTIFICIAL NEURAL NETWORKS for MULTI-LAYER ARTIFICIAL NEURAL NETWORKS . in this paper , we propose a new LINEAR CLASSIFIER based on the HIDDEN UNIT . the proposed LEARNING ALGORITHMS is based on the use of a LINEAR CLASSIFIER and a LINEAR CLASSIFIER . experimental results show the effectiveness of the proposed LEARNING ALGORITHMS .\n",
            "\n",
            "636 1000\n",
            "we present an EMBEDDING of STOCHASTIC OPTIMAL CONTROL PROBLEMS , of the so called PATH INTEGRAL FORM , into reproducing kernel hilbert spaces . using consistent , sample based estimates of the EMBEDDING leads to a MODEL-FREE , NON-PARAMETRIC APPROACH for calculation of an approximate solution to the CONTROL PROBLEM . this MODEL-FREE , NON-PARAMETRIC APPROACH admits a decomposition of the problem into an INVARIANT AND TASK DEPENDENT COMPONENT . consequently , we make much more efficient use of the SAMPLE DATA compared to previous SAMPLE BASED APPROACHES in this domain , e.g. , by allowing SAMPLE RE-USE across tasks . numerical examples on test problems , which illustrate the SAMPLE EFFICIENCY , are provided . \n",
            "this paper proposes a MODEL-FREE , NON-PARAMETRIC APPROACH for STOCHASTIC OPTIMAL CONTROL PROBLEMS . the proposed MODEL-FREE , NON-PARAMETRIC APPROACH is based on the INVARIANT AND TASK DEPENDENT COMPONENT . the proposed MODEL-FREE , NON-PARAMETRIC APPROACH is based on the INVARIANT AND TASK DEPENDENT COMPONENT . the proposed MODEL-FREE , NON-PARAMETRIC APPROACH is based on the INVARIANT AND TASK DEPENDENT COMPONENT . the proposed MODEL-FREE , NON-PARAMETRIC APPROACH is based on the INVARIANT AND TASK DEPENDENT COMPONENT . the proposed MODEL-FREE , NON-PARAMETRIC APPROACH is based on the INVARIANT AND TASK DEPENDENT COMPONENT . the proposed MODEL-FREE , NON-PARAMETRIC APPROACH is based on the INVARIANT AND TASK DEPENDENT COMPONENT .\n",
            "\n",
            "637 1000\n",
            "we aim to color GREYSCALE IMAGES automatically , without any MANUAL INTERVENTION . the COLOR PROPOSITION could then be interactively corrected by USER-PROVIDED COLOR LANDMARKS if necessary . AUTOMATIC COL-ORIZATION is nontrivial since there is usually no ONE-TO-ONE CORRESPONDENCE between color and LOCAL TEXTURE . the contribution of our framework is that we deal directly with MULTIMODALITY AND ESTIMATE , for each PIXEL of the image to be colored , the PROBABILITY DISTRIBUTION of all possible colors , instead of choosing the most probable color at the LOCAL LEVEL . we also predict the expected variation of color at each PIXEL , thus defining a NON-UNIFORM SPATIAL COHERENCY CRITERION . we then use GRAPH CUTS to maximize the probability of the whole COLORED IMAGE at the GLOBAL LEVEL . we work in the L-A-B COLOR SPACE in order to approximate the HUMAN PERCEPTION OF DISTANCES between colors , and we use MACHINE LEARNING TOOLS to extract as much information as possible from a DATASET OF COLORED EXAMPLES . the resulting algorithm is fast , designed to be more robust to TEXTURE NOISE , and is above all able to deal with ambiguity , in contrary to previous approaches . \n",
            "this paper presents a method for AUTOMATIC COL-ORIZATION from GREYSCALE IMAGES . the method is based on the NON-UNIFORM SPATIAL COHERENCY CRITERION of the COLORED IMAGE . the proposed method is based on the NON-UNIFORM SPATIAL COHERENCY CRITERION of the COLORED IMAGE . the method is based on the NON-UNIFORM SPATIAL COHERENCY CRITERION of the COLORED IMAGE . the proposed method is based on the NON-UNIFORM SPATIAL COHERENCY CRITERION of the COLORED IMAGE . the proposed method is based on the NON-UNIFORM SPATIAL COHERENCY CRITERION of the COLORED IMAGE . the proposed method is based on the DATASET OF COLORED EXAMPLES . the proposed method is based on the DATASET OF COLORED EXAMPLES . the proposed method is based on the DATASET OF COLORED EXAMPLES .\n",
            "\n",
            "638 1000\n",
            "in this paper we present a weighted likelihood ratio -lrb- <unk> -rrb- based hidden markov model and apply it to SPEECH RECOGNITION in noise . the <unk> measure emphasizes SPECTRAL PEAKS than VALLEYS in comparing two given SPEECH SPECTRA . the measure is more consistent with HUMAN PERCEPTION OF SPEECH FORMANTS where NATURAL RESONANCES OF VOCAL TRACK are and tends to be more robust to BROAD-BAND NOISE INTERFERENCES than other measures . a complete HMM FRAMEWORK of this measure is derived and a mixture of EXPONENTIAL KERNELS is used to model the OUTPUT PROBABILITY DENSITY FUNCTION . the new WLR-HMM is tested on the AURORA2 CONNECTED DIGITS DATABASE in noise . it shows more robust performance than the MFCC TRAINED GMM BASELINE SYSTEM . when combined with the DYNAMIC CEPSTRAL FEATURES , the MULTIPLE-STREAM WLR-HMM shows a 39 % relative improvement over the baseline system . \n",
            "this paper presents a HMM FRAMEWORK for SPEECH RECOGNITION . the HMM FRAMEWORK is based on a NATURAL RESONANCES OF VOCAL TRACK and a NATURAL RESONANCES OF VOCAL TRACK . the HMM FRAMEWORK is based on a NATURAL RESONANCES OF VOCAL TRACK and a NATURAL RESONANCES OF VOCAL TRACK . the WLR-HMM is based on the OUTPUT PROBABILITY DENSITY FUNCTION and the OUTPUT PROBABILITY DENSITY FUNCTION . the proposed HMM FRAMEWORK is based on a NATURAL RESONANCES OF VOCAL TRACK . the proposed HMM FRAMEWORK is based on a NATURAL RESONANCES OF VOCAL TRACK , which is a NATURAL RESONANCES OF VOCAL TRACK for SPEECH RECOGNITION .\n",
            "\n",
            "639 1000\n",
            "in this paper , german , <unk> , spanish , and portuguese large vocabulary continuous speech recognition -lrb- lvcsr -rrb- systems developed by the <unk> <unk> university are presented . all the above mentioned systems for the aforementioned languages are used for the QUAERO AND EU-BRIDGE PROJECT EVALUATIONS . the LVCSR SYSTEMS developed for these competitive evaluations focus on various domains like BROADCAST NEWS , PODCASTS and LECTURE DOMAIN . transcription of the speech for these tasks is challenging due to huge variability in the ACOUSTIC CONDITIONS and a significant portion of AUDIO DATA includes SPONTANEOUS SPEECH . good improvements are obtained using state-of-the-art MULTILINGUAL BOTTLENECK FEATURES , MINIMUM PHONE ERROR TRAINED ACOUSTIC MODELS , LANGUAGE MODEL ADAPTATION and CONFUSION-NETWORK BASED SYSTEM COMBINATION . in addition , an OPEN VOCABULARY APPROACH using MORPHEMIC UNITS is investigated along with the LM ADAPTATION for the GERMAN LVCSR . \n",
            "this paper addresses the problem of LANGUAGE MODEL ADAPTATION in GERMAN LVCSR . we propose a method for LANGUAGE MODEL ADAPTATION based on MULTILINGUAL BOTTLENECK FEATURES , PODCASTS , and PODCASTS . the proposed method is based on the MULTILINGUAL BOTTLENECK FEATURES and the MULTILINGUAL BOTTLENECK FEATURES . the proposed method is based on the OPEN VOCABULARY APPROACH and the OPEN VOCABULARY APPROACH . the proposed method is evaluated on the LECTURE DOMAIN and the results show that the proposed method is robust and robust to ACOUSTIC CONDITIONS and ACOUSTIC CONDITIONS .\n",
            "\n",
            "640 1000\n",
            "forced alignment for SPEECH SYNTHESIS traditionally aligns a PHONEME SEQUENCE predetermined by the FRONT-END TEXT PROCESSING SYSTEM . this sequence is not altered during alignment , i.e. , it is forced , despite possibly being faulty . the CONSISTENCY ASSUMPTION is the assumption that these mistakes do not degrade models , as long as the mistakes are consistent across training and synthesis . we present evidence that in the alignment of both standard READ PROMPTS and SPONTANEOUS SPEECH this PHONEME SEQUENCE is often wrong , and that this is likely to have a negative impact on ACOUSTIC MODELS . a LATTICE-BASED FORCED ALIGNMENT SYSTEM allowing for PRONUNCIATION VARIATION is implemented , resulting in improved PHONEME IDENTITY ACCURACY for both types of speech . a perceptual evaluation of HMM-BASED VOICES showed that SPONTANEOUS MODELS trained on this improved alignment also improved standard synthesis , despite breaking the CONSISTENCY ASSUMPTION . \n",
            "this paper presents a novel FRONT-END TEXT PROCESSING SYSTEM for SPEECH SYNTHESIS . the FRONT-END TEXT PROCESSING SYSTEM is based on a LATTICE-BASED FORCED ALIGNMENT SYSTEM . the LATTICE-BASED FORCED ALIGNMENT SYSTEM is based on the CONSISTENCY ASSUMPTION and the CONSISTENCY ASSUMPTION . the proposed FRONT-END TEXT PROCESSING SYSTEM is based on a LATTICE-BASED FORCED ALIGNMENT SYSTEM . the proposed FRONT-END TEXT PROCESSING SYSTEM is based on a LATTICE-BASED FORCED ALIGNMENT SYSTEM . the proposed FRONT-END TEXT PROCESSING SYSTEM is based on a LATTICE-BASED FORCED ALIGNMENT SYSTEM . the proposed FRONT-END TEXT PROCESSING SYSTEM is based on a LATTICE-BASED FORCED ALIGNMENT SYSTEM and is shown to be robust to PRONUNCIATION VARIATION .\n",
            "\n",
            "641 1000\n",
            "image registration is one of the most important tasks in IMAGE PROCESSING . the algorithms of IMAGE REGISTRATION are classified into two categories : the FEATURE-BASED MATCHING and INTENSITY-BASED MATCHING . each of them has its strength and weakness . in this paper , by combining these two techniques together , we developed a new algorithm for IMAGE REGISTRATION . the algorithm utilises a PARAMETRIC PROJECTIVE MODEL accounting for GEOMETRICAL VARIATION and a POLYNOMIAL MODEL with a small number of POLYNOMIAL COEFFICIENTS <unk> the SMOOTH SPATIALLY VARYING ILLUMINATION VARIATION . the initial PROJECTIVE MODEL PARAMETERS are first estimated by using FEATURE-BASED APPROACH . subsequently , the coefficients of the ILLUMINATION MODEL are determined simultaneously with the PROJECTIVE TRANSFORMATION PARAMETERS through the process of INTENSITY MATCHING . the experimental results demonstrated the algorithm is of ROBUSTNESS , efficiency and ACCURACY . \n",
            "this paper addresses the problem of IMAGE REGISTRATION in IMAGE PROCESSING . we propose a PARAMETRIC PROJECTIVE MODEL for IMAGE REGISTRATION , which is based on a PARAMETRIC PROJECTIVE MODEL . the proposed method is based on a PARAMETRIC PROJECTIVE MODEL , which is based on the PARAMETRIC PROJECTIVE MODEL . the proposed FEATURE-BASED APPROACH is based on a PARAMETRIC PROJECTIVE MODEL and a POLYNOMIAL MODEL . the ROBUSTNESS of the proposed method is compared with the conventional FEATURE-BASED APPROACH and the FEATURE-BASED APPROACH .\n",
            "\n",
            "642 1000\n",
            "we study an EXPLICIT PARAMETRIC MODEL of DOCUMENTS , QUERIES , and REL-EVANCY ASSESSMENT for INFORMATION RETRIEVAL . MEAN-FIELD METHODS are applied to analyze the model and derive efficient practical algorithms to estimate the parameters in the problem . the HYPERPARAMETERS are estimated by a fast APPROXIMATE LEAVE-ONE-OUT CROSS-VALIDATION PROCEDURE based on the CAVITY METHOD . the APPROXIMATE LEAVE-ONE-OUT CROSS-VALIDATION PROCEDURE is further evaluated on several benchmark databases by comparing with standard algorithms in IR . \n",
            "this paper presents a CAVITY METHOD for INFORMATION RETRIEVAL . the APPROXIMATE LEAVE-ONE-OUT CROSS-VALIDATION PROCEDURE is based on the CAVITY METHOD and the CAVITY METHOD . the CAVITY METHOD is based on the CAVITY METHOD and the CAVITY METHOD . the proposed CAVITY METHOD is based on the CAVITY METHOD and the CAVITY METHOD . the proposed CAVITY METHOD is based on the CAVITY METHOD and the CAVITY METHOD . the proposed CAVITY METHOD is based on the CAVITY METHOD and the CAVITY METHOD .\n",
            "\n",
            "643 1000\n",
            "full covariance acoustic models trained with LIMITED TRAINING DATA generalize poorly to UNSEEN TEST DATA due to a large number of FREE PARAMETERS . we propose to use SPARSE INVERSE CO-VARIANCE MATRICES to address this problem . previous SPARSE INVERSE COVARIANCE METHODS never outperformed FULL COVARI-ANCE METHODS . we propose a method to automatically drive the structure of INVERSE COVARIANCE MATRICES to sparse during training . we use a new OBJECTIVE FUNCTION by adding L1 REG-ULARIZATION to the traditional OBJECTIVE FUNCTION for MAXIMUM LIKELIHOOD ESTIMATION.THE GRAPHIC LASSO METHOD for the estimation of a SPARSE INVERSE COVARIANCE MATRIX is incorporated into the EXPECTATION MAXIMIZATION ALGORITHM to learn parameters of HMM using the new OBJECTIVE FUNCTION . experimental results show that we only need about 25 % of the parameters of the INVERSE COVARIANCE MATRICES to be nonzero in order to achieve the same performance of a full covariance system . our proposed system using SPARSE INVERSE COVARIANCE GAUS-SIANS also significantly outperforms a system using FULL CO-VARIANCE GAUSSIANS trained on LIMITED DATA . \n",
            "this paper proposes a new MAXIMUM LIKELIHOOD ESTIMATION.THE GRAPHIC LASSO METHOD for SPARSE INVERSE COVARIANCE GAUS-SIANS . the proposed MAXIMUM LIKELIHOOD ESTIMATION.THE GRAPHIC LASSO METHOD is based on the MAXIMUM LIKELIHOOD ESTIMATION.THE GRAPHIC LASSO METHOD . the proposed MAXIMUM LIKELIHOOD ESTIMATION.THE GRAPHIC LASSO METHOD is based on the MAXIMUM LIKELIHOOD ESTIMATION.THE GRAPHIC LASSO METHOD . the proposed MAXIMUM LIKELIHOOD ESTIMATION.THE GRAPHIC LASSO METHOD is based on the MAXIMUM LIKELIHOOD ESTIMATION.THE GRAPHIC LASSO METHOD . the proposed MAXIMUM LIKELIHOOD ESTIMATION.THE GRAPHIC LASSO METHOD is based on the MAXIMUM LIKELIHOOD ESTIMATION.THE GRAPHIC LASSO METHOD . the proposed MAXIMUM LIKELIHOOD ESTIMATION.THE GRAPHIC LASSO METHOD is based on the MAXIMUM LIKELIHOOD ESTIMATION.THE GRAPHIC LASSO METHOD . the proposed MAXIMUM LIKELIHOOD ESTIMATION.THE GRAPHIC LASSO METHOD is based on the MAXIMUM LIKELIHOOD ESTIMATION.THE GRAPHIC LASSO METHOD . the proposed MAXIMUM LIKELIHOOD ESTIMATION.THE GRAPHIC LASSO METHOD is based on a MAXIMUM LIKELIHOOD ESTIMATION.THE GRAPHIC LASSO METHOD . the proposed MAXIMUM LIKELIHOOD ESTIMATION.THE GRAPHIC LASSO METHOD is compared with other FULL COVARI-ANCE METHODS . the proposed MAXIMUM LIKELIHOOD ESTIMATION.THE GRAPHIC LASSO METHOD is compared with other FULL COVARI-ANCE METHODS .\n",
            "\n",
            "644 1000\n",
            "it is well known that VIDEO MATERIAL with a STATIC BACKGROUND allows easier segmentation than that with a MOVING BACKGROUND . one approach to SEGMENTATION OF SEQUENCES with a MOVING BACKGROUND is to use PREPROCESSING to create a STATIC BACKGROUND , after which conventional BACKGROUND SUBTRACTION TECHNIQUES can be used for SEGMENTING FOREGROUND OBJECTS . it has been recently shown that GLOBAL MOTION ESTIMATION AND/OR BACKGROUND SPRITE GENERATION TECHNIQUES are reliable . we propose a new BACKGROUND MODELING TECHNIQUE for OBJECT SEGMENTATION using LOCAL BACKGROUND SPRITE GENERATION . experimental results show the excellent performance of this new BACKGROUND MODELING TECHNIQUE compared to recent algorithms proposed . \n",
            "this paper presents a novel BACKGROUND MODELING TECHNIQUE for SEGMENTING FOREGROUND OBJECTS . the BACKGROUND MODELING TECHNIQUE is based on a BACKGROUND MODELING TECHNIQUE for SEGMENTING FOREGROUND OBJECTS . the BACKGROUND MODELING TECHNIQUE is based on the SEGMENTATION OF SEQUENCES from MOVING BACKGROUND . the PREPROCESSING is based on a BACKGROUND MODELING TECHNIQUE . the proposed BACKGROUND MODELING TECHNIQUE is based on a BACKGROUND MODELING TECHNIQUE . the proposed BACKGROUND MODELING TECHNIQUE is based on a BACKGROUND MODELING TECHNIQUE and is applied to the SEGMENTATION OF SEQUENCES .\n",
            "\n",
            "645 1000\n",
            "bounded confidence opinion dynamic models have received much recent interest as models of INFORMATION PROPAGATION in SOCIAL NETWORKS and LOCALIZED DISTRIBUTED AVERAGING . however in the existing literature , opinions are only viewed as abstract quantities rather than as part of a DECISION-MAKING SYSTEM . in this work , OPINION DYNAMICS are examined when agents are BAYESIAN DECISION MAKERS that perform HYPOTHESIS TESTING or SIGNAL DETECTION . BOUNDED CONFIDENCE is defined on PRIOR PROBABILITIES of hypotheses through BAYES RISK ERROR DIVERGENCE , the appropriate measure between PRIORS in HYPOTHESIS TESTING . this definition contrasts with the measure used between opinions in the standard model : absolute error . it is shown that the rapid convergence of PRIOR PROBABILITIES to a small number of LIMITING VALUES is similar to that seen in the standard model . the most interesting finding in this work is that the number of these LIMITING VALUES changes with the SIGNAL-TO-NOISE RATIO in the HYPOTHESIS TESTING TASK . the number of final values or CLUSTERS is maximal at INTERMEDIATE SIGNAL-TO-NOISE RATIOS , suggesting that the most <unk> issues lead to the largest number of <unk> . \n",
            "this paper addresses the problem of SIGNAL DETECTION in SOCIAL NETWORKS . in this paper , we propose a new method for SIGNAL DETECTION based on LOCALIZED DISTRIBUTED AVERAGING . the proposed method is based on the BAYES RISK ERROR DIVERGENCE and the BAYES RISK ERROR DIVERGENCE . the proposed method is based on the BAYES RISK ERROR DIVERGENCE and the BAYES RISK ERROR DIVERGENCE . the proposed method is based on the BAYES RISK ERROR DIVERGENCE and the BAYES RISK ERROR DIVERGENCE . experimental results show the effectiveness of the proposed DECISION-MAKING SYSTEM in terms of SIGNAL-TO-NOISE RATIO and SIGNAL-TO-NOISE RATIO .\n",
            "\n",
            "646 1000\n",
            "randomized features provide a computationally efficient way to approximate KERNEL MACHINES in MACHINE LEARNING TASKS . however , such methods require a USER-DEFINED KERNEL as input . we extend the RANDOMIZED-FEATURE APPROACH to the task of learning a kernel -lrb- via its associated RANDOM FEATURES -rrb- . specifically , we present an efficient OPTIMIZATION PROBLEM that learns a kernel in a SUPERVISED MANNER . we prove the consistency of the estimated kernel as well as GENERALIZATION BOUNDS for the CLASS OF ESTIMATORS induced by the optimized kernel , and we experimentally evaluate our technique on several datasets . our approach is efficient and highly scalable , and we attain competitive results with a fraction of the TRAINING COST of other techniques . \n",
            "this paper addresses the problem of OPTIMIZATION PROBLEM in MACHINE LEARNING TASKS . in this paper , we propose a new RANDOMIZED-FEATURE APPROACH based on RANDOMIZED FEATURES . the proposed RANDOMIZED-FEATURE APPROACH is based on the CLASS OF ESTIMATORS of the USER-DEFINED KERNEL . the proposed RANDOMIZED-FEATURE APPROACH is based on a RANDOMIZED-FEATURE APPROACH . the proposed RANDOMIZED-FEATURE APPROACH is applied to the OPTIMIZATION PROBLEM . experimental results show the effectiveness of the proposed RANDOMIZED-FEATURE APPROACH .\n",
            "\n",
            "647 1000\n",
            "the aim of this paper is to present a simple yet efficient implementation of a tool for SIMULTANEOUS RULE-BASED MORPHOSYNTACTIC TAGGING and PARTIAL PARSING FORMALISM . the parser is currently used for creating a TREE-BANK OF PARTIAL PARSES in a <unk> acquisition project over the IPI PAN CORPUS OF POLISH . \n",
            "this paper presents a new method for SIMULTANEOUS RULE-BASED MORPHOSYNTACTIC TAGGING . the proposed method is based on the IPI PAN CORPUS OF POLISH . the proposed method is based on the IPI PAN CORPUS OF POLISH . the proposed method is based on the IPI PAN CORPUS OF POLISH and the IPI PAN CORPUS OF POLISH .\n",
            "\n",
            "648 1000\n",
            "when constructing a CLASSIFIER , the probability of correct classification of future data points should be maximized . in the current paper this CLASSIFIER is translated in a very direct way into an OPTIMIZATION PROBLEM , which is solved using methods from CONVEX OPTIMIZATION . we also show how to exploit MERCER KERNELS in this setting to obtain NONLINEAR DECISION BOUNDARIES . a worst-case bound on the probability of MISCLASSIFICATION OF FUTURE DATA is obtained explicitly . \n",
            "this paper presents a new method for MISCLASSIFICATION OF FUTURE DATA based on CONVEX OPTIMIZATION . the proposed method is based on the use of a CLASSIFIER and a CLASSIFIER . the proposed method is based on the MISCLASSIFICATION OF FUTURE DATA . the proposed method is based on the MISCLASSIFICATION OF FUTURE DATA .\n",
            "\n",
            "649 1000\n",
            "spoken dialogue interfaces , mostly <unk> , become more visible in applications where attention needs to be shared with other tasks , such as driving a car . the deployment of the simple DIALOG SYSTEMS , instead of more sophisticated ones , is partly because the COMPUTING PLATFORMS used for such tasks have been less powerful and partly because certain issues from these cognitively challenging tasks have not been well addressed even in the most advanced DIALOG SYSTEMS . this paper reports the progress of our research effort in developing a robust , <unk> , and cognitive <unk> spoken dialog interface called CHAT : CONVERSATIONAL HELPER for AUTOMOTIVE TASKS . our research in the past few years has led to promising results , including high TASK COMPLETION RATE , DIALOG EFFICIENCY , and improved USER EXPERIENCE . \n",
            "this paper addresses the problem of SPOKEN DIALOGUE INTERFACES in COMPUTING PLATFORMS . we propose a method for COMPUTING PLATFORMS based on USER EXPERIENCE and USER EXPERIENCE . the proposed method is based on the use of USER EXPERIENCE and USER EXPERIENCE . the TASK COMPLETION RATE of the proposed method is evaluated using AUTOMOTIVE TASKS and AUTOMOTIVE TASKS .\n",
            "\n",
            "650 1000\n",
            "contemporary approaches to AUTOMATIC SPEECH SUMMARISATION comprise several components , among them a LINGUISTIC MODEL COMPONENT , which is unrelated to the LANGUAGE MODEL used during the RECOGNITION PROCESS . this LINGUISTIC MODEL COMPONENT assigns a probability to word sequences from the source text according to their likelihood of appearing in the SUMMARISED TEXT . in this paper we investigate LIM TOPIC AND STYLISTIC ADAPTATION using combinations of LIMS each trained on different ADAPTATION DATA . experiments are performed on 9 talks from the TED CORPUS OF EUROSPEECH CONFERENCE PRESENTATIONS , as well as 5 NEWS STORIES from CNN BROADCAST NEWS DATA , for all of which human -lrb- <unk> -rrb- and SPEECH RECOGNISER TRANSCRIPTIONS along with HUMAN SUMMARIES were used . in all asr cases , SUMMARI-SATION ACCURACY -lrb- <unk> -rrb- of AUTOMATICALLY GENERATED SUMMARIES was significantly improved by AUTOMATIC LIM ADAPTATION , with relative improvements of at least 2.5 % in all experiments . \n",
            "this paper addresses the problem of AUTOMATIC LIM ADAPTATION for AUTOMATIC SPEECH SUMMARISATION . in this paper , we propose a new method for AUTOMATIC LIM ADAPTATION based on the LINGUISTIC MODEL COMPONENT . the proposed method is based on the TED CORPUS OF EUROSPEECH CONFERENCE PRESENTATIONS , which is based on the LINGUISTIC MODEL COMPONENT . the proposed method is based on the TED CORPUS OF EUROSPEECH CONFERENCE PRESENTATIONS . the proposed method is based on the TED CORPUS OF EUROSPEECH CONFERENCE PRESENTATIONS . the proposed method is based on the TED CORPUS OF EUROSPEECH CONFERENCE PRESENTATIONS . the proposed method is based on the TED CORPUS OF EUROSPEECH CONFERENCE PRESENTATIONS . the proposed method is based on the TED CORPUS OF EUROSPEECH CONFERENCE PRESENTATIONS . the proposed method is based on the TED CORPUS OF EUROSPEECH CONFERENCE PRESENTATIONS . the proposed method is based on the TED CORPUS OF EUROSPEECH CONFERENCE PRESENTATIONS . the experimental results show that the proposed method is robust and robust to HUMAN SUMMARIES .\n",
            "\n",
            "651 1000\n",
            "in practice , most <unk> is done assuming a PROBABILISTIC MODEL of STOCK PRICE RETURNS known as the GEOMETRIC BROWNIAN MOTION . while often an acceptable approximation , the GEOMETRIC BROWNIAN MOTION is not always valid empirically . this motivates a WORST-CASE APPROACH to <unk> , called UNIVERSAL PORTFOLIO MANAGEMENT , where the objective is to maximize wealth relative to the wealth <unk> by the best fixed portfolio in hindsight . in this paper we tie the two approaches , and design an INVESTMENT STRATEGY which is universal in the worst-case , and yet capable of exploiting the mostly valid GEOMETRIC BROWNIAN MOTION . our INVESTMENT STRATEGY is based on new and improved REGRET BOUNDS for ONLINE CONVEX OPTIMIZATION with EXP-CONCAVE LOSS FUNCTIONS . \n",
            "this paper addresses the problem of UNIVERSAL PORTFOLIO MANAGEMENT for ONLINE CONVEX OPTIMIZATION . we propose a new INVESTMENT STRATEGY based on the INVESTMENT STRATEGY . the proposed INVESTMENT STRATEGY is based on the WORST-CASE APPROACH . the proposed INVESTMENT STRATEGY is based on the WORST-CASE APPROACH . the proposed INVESTMENT STRATEGY is based on the WORST-CASE APPROACH and is shown to be superior to the conventional INVESTMENT STRATEGY .\n",
            "\n",
            "652 1000\n",
            "we introduce a NONPARAMETRIC APPROACH for ESTIMATING DRIFT FUNCTIONS in systems of STOCHASTIC DIFFERENTIAL EQUATIONS from SPARSE OBSERVATIONS OF THE STATE VECTOR . using a GAUSSIAN PROCESS prior over the drift as a function of the STATE VECTOR , we develop an APPROXIMATE EM ALGORITHM to deal with the UNOBSERVED , LATENT DYNAMICS between observations . the POSTERIOR OVER STATES is approximated by a PIECEWISE LINEARIZED PROCESS of the ORNSTEIN-UHLENBECK TYPE and the MAP ESTIMATION of the drift is facilitated by a SPARSE GAUSSIAN PROCESS REGRESSION . \n",
            "this paper presents a NONPARAMETRIC APPROACH for ESTIMATING DRIFT FUNCTIONS . the NONPARAMETRIC APPROACH is based on a NONPARAMETRIC APPROACH . the APPROXIMATE EM ALGORITHM is based on a NONPARAMETRIC APPROACH . the APPROXIMATE EM ALGORITHM is based on a NONPARAMETRIC APPROACH . the APPROXIMATE EM ALGORITHM is based on a NONPARAMETRIC APPROACH . the APPROXIMATE EM ALGORITHM is based on a NONPARAMETRIC APPROACH . the APPROXIMATE EM ALGORITHM is based on a NONPARAMETRIC APPROACH . the APPROXIMATE EM ALGORITHM is based on a NONPARAMETRIC APPROACH . the APPROXIMATE EM ALGORITHM is based on a NONPARAMETRIC APPROACH . the proposed NONPARAMETRIC APPROACH is based on a NONPARAMETRIC APPROACH .\n",
            "\n",
            "653 1000\n",
            "although it is acknowledged that MULTI-WAY DATAFLOW CONSTRAINTS are useful in INTERACTIVE APPLICATIONS , concerns about their tractability have hindered their acceptance . certain LOCAL PROPAGATION ALGORITHMS that solve these constraints are polynomial , others -lrb- such as <unk> -rrb- are exponential . every system handles a specific problem and the influence of any particular RESTRICTION on the COMPUTATIONAL COMPLEXITY is not yet precisely determined . in this paper , we present three theoretical results that allow us to classify existing MULTI-WAY CONSTRAINT PROBLEMS . especially , we prove that the problem handled by SKYBLUE is np-hard . \n",
            "this paper addresses the problem of MULTI-WAY CONSTRAINT PROBLEMS in the presence of MULTI-WAY DATAFLOW CONSTRAINTS . we propose a method to estimate the parameters of the SKYBLUE . the proposed method is based on the use of MULTI-WAY DATAFLOW CONSTRAINTS , and the COMPUTATIONAL COMPLEXITY of the proposed method is compared with the conventional LOCAL PROPAGATION ALGORITHMS .\n",
            "\n",
            "654 1000\n",
            "we show how the PAD E TABLE can be utilized to develop a new LATTICE STRUCTURE for general <unk> <unk> perfect reconstruction -lrb- pr -rrb- LTER banks . this is achieved through characterization of all TWO-CHANNEL BI-ORTHOGONAL PR LTER BANKS . the PARAMETER SPACE found using this method is unique for each LTER BANK . similarly to any other LATTICE STRUCTURE , the PR PROPERTY is achieved structurally and quantization of the parameters of the LATTICE does not eeect this property . furthermore , we demonstrate that for a given LTER , the set of all COMPLEMENTARY LTERS can be uniquely speciied by two parameters , namely the END-TO-END DELAY of the system and a SCALAR QUANTITY . \n",
            "this paper presents a new method for TWO-CHANNEL BI-ORTHOGONAL PR LTER BANKS based on TWO-CHANNEL BI-ORTHOGONAL PR LTER BANKS . the proposed method is based on a LTER BANK and a LTER BANK . the method is based on a LTER BANK and a LTER BANK . the proposed method is based on a LTER BANK , which is based on a LTER BANK . the method is based on a LTER BANK and a LTER BANK . the method is based on a LTER BANK and a LTER BANK .\n",
            "\n",
            "655 1000\n",
            "it has been hypothesized that TREE ADJOINING GRAMMAR is particularly well suited for SENTENCE GENERATION . it is unclear , however , how a SENTENCE GENERATION SYSTEM based on TREE ADJOINING GRAMMAR should choose among the SYNTACTIC POSSIBILITIES made available in the GRAMMAR . in this paper we consider the question of what needs to be done to generate with TREE ADJOINING GRAMMAR and explain a GENERATION SYSTEM that provides the necessary FEATURES . this approach is compared with other TAG-BASED GENERATION SYSTEMS . particular attention is given to MUMBLE-86 which , like our GENERATION SYSTEM , makes SYNTACTIC CHOICE on sophisticated functional grounds . \n",
            "this paper presents a SENTENCE GENERATION SYSTEM for SENTENCE GENERATION . the GENERATION SYSTEM is based on a TREE ADJOINING GRAMMAR . the GENERATION SYSTEM is based on a TREE ADJOINING GRAMMAR , which is based on a TREE ADJOINING GRAMMAR . the GENERATION SYSTEM is based on a TREE ADJOINING GRAMMAR . the GENERATION SYSTEM is based on a TREE ADJOINING GRAMMAR . the GENERATION SYSTEM is based on a TREE ADJOINING GRAMMAR and is shown to be useful for SENTENCE GENERATION .\n",
            "\n",
            "656 1000\n",
            "it is usually assumed that GRAMMAR PROBABILITIES and ACOUSTIC PROBABILITIES in a CONTINUOUS SPEECH RECOGNITION SYSTEM have to be incorporated to the general score with dierent w <unk> . this is an experimental fact and there is no generally accepted THEORETICAL EXPLANATION . in this paper we propose an explanation to this fact , related to the way GRAMMAR SCORING is incorporated in the SEARCHING PROCEDURE . accordingly to this explanation , we perform a set of experiments to test our hypothesis . we are also proposing a new way o f i n <unk> GRAMMARPROBABILITIES in a TREE-BASED VOCABULARY SEARCH STRATEGY , where systems are usually bound to use the worst strategy . to apply our ideas to UNIGRAMS is rather simple . for more complex LANGUAGE MODELS like BIGRAMS we h a v e t o implement a new procedure . \n",
            "this paper presents a new method for GRAMMAR SCORING in LANGUAGE MODELS . the proposed TREE-BASED VOCABULARY SEARCH STRATEGY consists of a CONTINUOUS SPEECH RECOGNITION SYSTEM and a TREE-BASED VOCABULARY SEARCH STRATEGY . the TREE-BASED VOCABULARY SEARCH STRATEGY consists of a TREE-BASED VOCABULARY SEARCH STRATEGY and a TREE-BASED VOCABULARY SEARCH STRATEGY . the proposed TREE-BASED VOCABULARY SEARCH STRATEGY is based on a TREE-BASED VOCABULARY SEARCH STRATEGY and a TREE-BASED VOCABULARY SEARCH STRATEGY . experimental results show the effectiveness of the proposed TREE-BASED VOCABULARY SEARCH STRATEGY .\n",
            "\n",
            "657 1000\n",
            "skin detection is an important preliminary process in HUMAN MOTION ANALYSIS . it is commonly performed in three steps : transforming the PIXEL COLOR to a NON-RGB COL-ORSPACE , dropping the ILLUMINANCE COMPONENT OF SKIN COLOR , and CLASSIFYING by modeling the SKIN COLOR DISTRIBUTION . in this paper , we evaluate the effect of these three steps on the SKIN DETECTION performance . the importance of this study is a new comprehensive COLORSPACE AND COLOR MODELING TESTING METHODOLOGY that would allow for making the best choices for SKIN DETECTION . combinations of nine <unk> , the presence of the absence of the ILLUMINANCE COMPONENT , and the two COLOR MODEL-ING APPROACHES are compared . the performance is measured by using a RECEIVER OPERATING CHARACTERISTIC CURVE on a large dataset of <unk> images with MANUAL GROUND TRUTH . the results reveal that -lrb- 1 -rrb- COLORSPACE TRANSFORMATIONS can improve performance in certain instances , -lrb- 2 -rrb- the absence of the ILLUMINANCE COMPONENT decreases performance , and -lrb- 3 -rrb- SKIN COLOR MODELING has a greater impact than COLORSPACE TRANSFORMATION . we found that the best performance was obtained by transforming the PIXEL COLOR to the SCT OR HSI COLORSPACES , keeping the ILLUMINANCE COMPONENT , and modeling the color with the HISTOGRAM APPROACH . \n",
            "this paper proposes a new COLORSPACE AND COLOR MODELING TESTING METHODOLOGY for CLASSIFYING . the proposed COLORSPACE AND COLOR MODELING TESTING METHODOLOGY is based on the ILLUMINANCE COMPONENT OF SKIN COLOR and the ILLUMINANCE COMPONENT OF SKIN COLOR . the proposed COLORSPACE AND COLOR MODELING TESTING METHODOLOGY is based on the ILLUMINANCE COMPONENT OF SKIN COLOR , which is a COLORSPACE TRANSFORMATION . the proposed COLORSPACE AND COLOR MODELING TESTING METHODOLOGY is based on the ILLUMINANCE COMPONENT OF SKIN COLOR . the proposed COLORSPACE AND COLOR MODELING TESTING METHODOLOGY is based on the ILLUMINANCE COMPONENT OF SKIN COLOR . the proposed COLORSPACE AND COLOR MODELING TESTING METHODOLOGY is compared with the conventional COLOR MODEL-ING APPROACHES . the proposed COLORSPACE AND COLOR MODELING TESTING METHODOLOGY is compared with the conventional HISTOGRAM APPROACH and the COLORSPACE AND COLOR MODELING TESTING METHODOLOGY .\n",
            "\n",
            "658 1000\n",
            "question retrieval in current COMMUNITY-BASED QUESTION ANSWERING SERVICES does not , in general , work well for long and complex queries . one of the main difficulties lies in the WORD MISMATCH between queries and candidate questions . existing solutions try to expand the queries at WORD LEVEL , but they usually fail to consider CONCEPT LEVEL ENRICHMENT . in this paper , we explore a PIVOT LANGUAGE TRANSLATION BASED APPROACH to derive the PARAPHRASES OF KEY CONCEPTS . we further propose a UNIFIED QUESTION RETRIEVAL MODEL which integrates the key concepts and their PARAPHRASES for the QUERY QUESTION . experimental results demonstrate that the PIVOT LANGUAGE TRANSLATION BASED APPROACH significantly outperforms the state-of-the-art models in QUESTION RETRIEVAL . \n",
            "this paper presents a UNIFIED QUESTION RETRIEVAL MODEL for QUESTION RETRIEVAL . the PIVOT LANGUAGE TRANSLATION BASED APPROACH is based on the PARAPHRASES OF KEY CONCEPTS of the QUERY QUESTION . the PIVOT LANGUAGE TRANSLATION BASED APPROACH is based on the PARAPHRASES OF KEY CONCEPTS and the PARAPHRASES OF KEY CONCEPTS . the proposed PIVOT LANGUAGE TRANSLATION BASED APPROACH is based on a UNIFIED QUESTION RETRIEVAL MODEL . the proposed PIVOT LANGUAGE TRANSLATION BASED APPROACH is based on a UNIFIED QUESTION RETRIEVAL MODEL and is shown to be robust to WORD MISMATCH .\n",
            "\n",
            "659 1000\n",
            "conventional ACOUSTIC MODELS , such as GAUSSIAN MIXTURE MODELS or DEEP NEURAL NETWORKS , can not be reliably estimated when there are very little SPEECH TRAINING DATA , e.g. less than 1 hour . in this paper , we investigate the use of a NON-PARAMETRIC KERNEL DENSITY ESTIMATION METHOD to predict the EMISSION PROBABILITY OF HMM STATES . in addition , we introduce a DISCRIMINATIVE SCORE CALIBRATOR to improve the SPEECH CLASS POSTERIORS generated by the KERNEL DENSITY for SPEECH RECOGNITION TASK . experimental results on the WALL STREET JOURNAL TASK show that the proposed ACOUSTIC MODELS using CROSS-LINGUAL BOTTLENECK FEATURES significantly outperforms GMM AND DNN MODELS for LIMITED TRAINING DATA CASE . \n",
            "this paper presents a novel NON-PARAMETRIC KERNEL DENSITY ESTIMATION METHOD for SPEECH TRAINING DATA . the NON-PARAMETRIC KERNEL DENSITY ESTIMATION METHOD is based on a NON-PARAMETRIC KERNEL DENSITY ESTIMATION METHOD and a NON-PARAMETRIC KERNEL DENSITY ESTIMATION METHOD . the proposed NON-PARAMETRIC KERNEL DENSITY ESTIMATION METHOD is based on a NON-PARAMETRIC KERNEL DENSITY ESTIMATION METHOD . the proposed NON-PARAMETRIC KERNEL DENSITY ESTIMATION METHOD is based on a NON-PARAMETRIC KERNEL DENSITY ESTIMATION METHOD . the proposed NON-PARAMETRIC KERNEL DENSITY ESTIMATION METHOD is based on a NON-PARAMETRIC KERNEL DENSITY ESTIMATION METHOD . the proposed NON-PARAMETRIC KERNEL DENSITY ESTIMATION METHOD is based on a NON-PARAMETRIC KERNEL DENSITY ESTIMATION METHOD and a NON-PARAMETRIC KERNEL DENSITY ESTIMATION METHOD . the proposed NON-PARAMETRIC KERNEL DENSITY ESTIMATION METHOD is evaluated on the WALL STREET JOURNAL TASK . the experimental results show that the proposed NON-PARAMETRIC KERNEL DENSITY ESTIMATION METHOD can improve the performance of DEEP NEURAL NETWORKS .\n",
            "\n",
            "660 1000\n",
            "<unk> messages pose severe challenges for current SENTIMENT ANALYSIS TECHNIQUES due to some inherent characteristics such as the LENGTH LIMIT and INFORMAL WRITING STYLE . in this paper , we study the problem of extracting OPINION TARGETS of CHINESE MICROBLOG MESSAGES . such FINE-GRAINED WORD-LEVEL TASK has not been well investigated in CHINESE MICROBLOGS yet . we propose an UNSUPERVISED LABEL PROPAGATION ALGORITHM to address the problem . the OPINION TARGETS of all messages in a topic are collectively extracted based on the assumption that similar messages may focus on similar OPINION TARGETS . topics in CHINESE MICROBLOGS are identified by HASHTAGS or using CLUSTERING ALGORITHMS . experimental results on CHINESE MICROBLOGS show the effectiveness of our UNSUPERVISED LABEL PROPAGATION ALGORITHM and algorithms . \n",
            "this paper addresses the problem of CHINESE MICROBLOGS in CHINESE MICROBLOGS . in this paper , we propose a new UNSUPERVISED LABEL PROPAGATION ALGORITHM based on the UNSUPERVISED LABEL PROPAGATION ALGORITHM . the proposed UNSUPERVISED LABEL PROPAGATION ALGORITHM is based on the UNSUPERVISED LABEL PROPAGATION ALGORITHM and the UNSUPERVISED LABEL PROPAGATION ALGORITHM . the proposed UNSUPERVISED LABEL PROPAGATION ALGORITHM is evaluated on the FINE-GRAINED WORD-LEVEL TASK . the results show that the proposed UNSUPERVISED LABEL PROPAGATION ALGORITHM improves the performance of the proposed UNSUPERVISED LABEL PROPAGATION ALGORITHM .\n",
            "\n",
            "661 1000\n",
            "this paper proposes LEARNING-BASED METHODS for MAPPING a sparse representation of noisy SPEECH to STATE LIKELIHOODS in an AUTOMATIC SPEECH RECOGNITION SYSTEM . we represent SPEECH as a sparse linear combination of EXEMPLARS extracted from training data . the weights of EXEMPLARS are mapped to SPEECH STATE LIKELIHOODS using ordinary least squares -lrb- <unk> -rrb- and PARTIAL LEAST SQUARES REGRESSION . RECOGNITION experiments are conducted using the CHIME NOISY SPEECH DATABASE . according to the results , both LEARNING-BASED METHODS can be successfully used for training the MAPPING . we achieve improvements over the previous BINARY LABELING SYSTEM , and RECOGNITION scores close to 70 % AT-6 DB SNR . \n",
            "this paper presents a new method for RECOGNITION in SPEECH . the proposed method is based on a BINARY LABELING SYSTEM , which is based on PARTIAL LEAST SQUARES REGRESSION . the proposed method is based on a BINARY LABELING SYSTEM , which is based on the MAPPING . the proposed method is based on the PARTIAL LEAST SQUARES REGRESSION . the proposed method is based on the PARTIAL LEAST SQUARES REGRESSION . the proposed method is based on the MAPPING . the proposed method is based on a BINARY LABELING SYSTEM . the experimental results show the effectiveness of the proposed AUTOMATIC SPEECH RECOGNITION SYSTEM in terms of AT-6 DB SNR and AT-6 DB SNR .\n",
            "\n",
            "662 1000\n",
            "in this paper we present a GENERATIVE MODEL AND LEARNING PROCEDURE for UNSUPERVISED VIDEO CLUSTERING into scenes . the work addresses two important problems : realistic mod-eling of the sources of variability in the video and fast transformation invariant frame CLUSTERING . we suggest a solution to the problem of COMPUTATIONALLY INTENSIVE LEARNING in this GENERATIVE MODEL AND LEARNING PROCEDURE by combining the RECURSIVE MODEL ESTIMATION , FAST INFERENCE , and ON-LINE LEARNING . thus , we achieve real time frame CLUSTERING performance . novel aspects of this GENERATIVE MODEL AND LEARNING PROCEDURE include an algorithm for the CLUSTERING OF GAUSSIAN MIXTURES , and the fast computation of the KL DIVERGENCE between two mixtures of GAUSSIANS . the EFFICIENCY and the performance of CLUSTERING and KL APPROXIMATION METHODS are demonstrated . we also present novel VIDEO BROWSING TOOL based on the visualization of the variables in the GENERATIVE MODEL AND LEARNING PROCEDURE . \n",
            "this paper presents a new GENERATIVE MODEL AND LEARNING PROCEDURE for UNSUPERVISED VIDEO CLUSTERING . the GENERATIVE MODEL AND LEARNING PROCEDURE is based on the GENERATIVE MODEL AND LEARNING PROCEDURE and the GENERATIVE MODEL AND LEARNING PROCEDURE . the proposed GENERATIVE MODEL AND LEARNING PROCEDURE is based on the GENERATIVE MODEL AND LEARNING PROCEDURE and the GENERATIVE MODEL AND LEARNING PROCEDURE . the proposed GENERATIVE MODEL AND LEARNING PROCEDURE is based on the GENERATIVE MODEL AND LEARNING PROCEDURE and the GENERATIVE MODEL AND LEARNING PROCEDURE . the proposed GENERATIVE MODEL AND LEARNING PROCEDURE is based on the GENERATIVE MODEL AND LEARNING PROCEDURE and the GENERATIVE MODEL AND LEARNING PROCEDURE . the proposed GENERATIVE MODEL AND LEARNING PROCEDURE is based on the GENERATIVE MODEL AND LEARNING PROCEDURE and the GENERATIVE MODEL AND LEARNING PROCEDURE .\n",
            "\n",
            "663 1000\n",
            "in this paper , the issue of NETWORK TOPOLOGY CONTROL in WIRELESS NETWORKS using a FULLY DISTRIBUTED ALGORITHM is considered . whereas the proposed FULLY DISTRIBUTED ALGORITHM is designed applying GAME THEORY CONCEPTS to design a NON-COOPERATIVE GAME , NETWORK TOPOLOGY CONTROL is guaranteed based on <unk> results of NETWORK TOPOLOGY CONTROL . simulations show that for a relatively low node density , the probability that the proposed FULLY DISTRIBUTED ALGORITHM leads to a CONNECTED NETWORK is close to one . \n",
            "this paper presents a new FULLY DISTRIBUTED ALGORITHM for NETWORK TOPOLOGY CONTROL . the FULLY DISTRIBUTED ALGORITHM is based on the FULLY DISTRIBUTED ALGORITHM . the FULLY DISTRIBUTED ALGORITHM is based on the FULLY DISTRIBUTED ALGORITHM . the FULLY DISTRIBUTED ALGORITHM is based on the FULLY DISTRIBUTED ALGORITHM . the proposed FULLY DISTRIBUTED ALGORITHM is based on the FULLY DISTRIBUTED ALGORITHM . the proposed FULLY DISTRIBUTED ALGORITHM is based on the FULLY DISTRIBUTED ALGORITHM .\n",
            "\n",
            "664 1000\n",
            "this paper examines the application of LATTICE ADAPTATION TECHNIQUES to SPEAKER-DEPENDENT MODELS for the purpose of CONVERSATIONAL TELEPHONE SPEECH TRANSCRIPTION . given sufficient training data per speaker , it is feasible to build adapted SPEAKER-DEPENDENT MODELS using lattice mllr and lattice map . experiments on ITERATIVE AND CASCADED ADAPTATION are presented . additionally various strategies for THRESHOLDING FRAME POSTERIORS are investigated , and it is shown that accumulating statistics from the LOCAL BEST-CONFIDENCE PATH is sufficient to achieve optimal adaptation . overall , an ITERATIVE CASCADED LATTICE SYSTEM was able to reduce LATTICE ADAPTATION by <unk> % <unk> , which was a 0.8 % <unk> . gain over TRANSCRIPT-BASED ADAPTATION . LATTICE ADAPTATION reduced the UNSUPERVISED/SUPERVISED ADAPTATION GAP from 2.5 % to 1.7 % . \n",
            "this paper addresses the problem of CONVERSATIONAL TELEPHONE SPEECH TRANSCRIPTION in CONVERSATIONAL TELEPHONE SPEECH TRANSCRIPTION . in this paper , we propose a new ITERATIVE CASCADED LATTICE SYSTEM based on the LOCAL BEST-CONFIDENCE PATH . the proposed LATTICE ADAPTATION TECHNIQUES is based on the use of THRESHOLDING FRAME POSTERIORS . the proposed LATTICE ADAPTATION TECHNIQUES is evaluated on the CONVERSATIONAL TELEPHONE SPEECH TRANSCRIPTION . the experimental results show that the proposed method outperforms the conventional LATTICE ADAPTATION TECHNIQUES in terms of both ITERATIVE AND CASCADED ADAPTATION and TRANSCRIPT-BASED ADAPTATION .\n",
            "\n",
            "665 1000\n",
            "for SEQUENTIAL PROBABILISTIC INFERENCE in NONLINEAR NON-GAUSSIAN SYSTEMS approximate solutions must be used . we present a novel RECURSIVE BAYESIAN ESTIMATION ALGORITHM that combines an importance sampling based MEASUREMENT UPDATE STEP with a bank of SIGMA-POINT KALMAN FILTERS for the TIME-UPDATE AND PROPOSAL DISTRIBUTION GENERATION . the POSTERIOR STATE DENSITY is represented by a GAUSSIAN MIXTURE MODEL that is recovered from the WEIGHTED PARTICLE SET of the MEASUREMENT UPDATE STEP by means of a WEIGHTED EM ALGORITHM . this RECURSIVE BAYESIAN ESTIMATION ALGORITHM replaces the RESAMPLING STAGE needed by most PARTICLE FILTERS and mitigates the '' sample <unk> '' problem . we show that this new RECURSIVE BAYESIAN ESTIMATION ALGORITHM has an improved estimation performance and reduced COMPUTATIONAL COMPLEXITY compared to other related algorithms . \n",
            "this paper proposes a RECURSIVE BAYESIAN ESTIMATION ALGORITHM for SEQUENTIAL PROBABILISTIC INFERENCE . the proposed RECURSIVE BAYESIAN ESTIMATION ALGORITHM is based on the WEIGHTED EM ALGORITHM . the proposed RECURSIVE BAYESIAN ESTIMATION ALGORITHM is based on the WEIGHTED EM ALGORITHM . the proposed RECURSIVE BAYESIAN ESTIMATION ALGORITHM is based on the WEIGHTED EM ALGORITHM . the proposed RECURSIVE BAYESIAN ESTIMATION ALGORITHM is based on the WEIGHTED EM ALGORITHM . the proposed RECURSIVE BAYESIAN ESTIMATION ALGORITHM is evaluated on the WEIGHTED PARTICLE SET . the results show that the proposed RECURSIVE BAYESIAN ESTIMATION ALGORITHM can improve the COMPUTATIONAL COMPLEXITY of the NONLINEAR NON-GAUSSIAN SYSTEMS .\n",
            "\n",
            "666 1000\n",
            "this paper presents an exploratory study on the relations between gender and everyday <unk> . a '' DATA-MINING '' APPROACH is used to explore GENDER-SPECIFIC CHARACTERISTICS in a large number of SPONTANEOUS TELEPHONE AND FACE-TO-FACE CONVERSATIONS . our study focuses on speech rate -lrb- speaking rate and ARTICULATION RATE -rrb- , DISFLUENCIES -lrb- FILLED PAUSES and REPETITIONS -rrb- , PRONUNCIATION VARIATION -lrb- PHONEME SUBSTITUTIONS , DELETIONS and INSERTIONS -rrb- , and preferences for particular parts of speech . our study reveals interesting similarities and differences in EVERYDAY MALE AND FEMALE SPEECH , and proves that <unk> on large SPOKEN LANGUAGE CORPORA is a promising approach for obtaining information on SPONTANEOUS SPEECH PHENOMENA and for generating new hypotheses for research . \n",
            "this paper presents a DATA-MINING '' APPROACH for EVERYDAY MALE AND FEMALE SPEECH . the proposed DATA-MINING '' APPROACH is based on the DATA-MINING '' APPROACH and the DATA-MINING '' APPROACH . the DATA-MINING '' APPROACH is based on the DATA-MINING '' APPROACH and the DATA-MINING '' APPROACH . the proposed DATA-MINING '' APPROACH is based on the DATA-MINING '' APPROACH and the DATA-MINING '' APPROACH . the proposed DATA-MINING '' APPROACH is based on the DATA-MINING '' APPROACH and the DATA-MINING '' APPROACH . experimental results show that the proposed DATA-MINING '' APPROACH is robust and robust to PRONUNCIATION VARIATION and PRONUNCIATION VARIATION .\n",
            "\n",
            "667 1000\n",
            "in this paper , we consider a MULTIHOP WIRELESS SENSOR NETWORK with multiple relay nodes for each <unk> where the AMPLIFY-AND-FORWARD SCHEME is employed . we present a strategy to jointly design the LINEAR RECEIVER and the POWER ALLOCATION PARAMETERS via an ALTERNATING OPTIMIZATION APPROACH that maximizes the SUM-RATE of the MULTIHOP WIRELESS SENSOR NETWORK . we derive CONSTRAINED MAXIMUM SUM-RATE EXPRESSIONS along with an algorithm to compute the LINEAR RECEIVER and the POWER ALLOCATION PARAMETERS with the optimal COMPLEX AMPLIFICATION COEFFICIENTS for each RELAY NODE . computer simulations show good performance of our proposed methods in terms of SUM-RATE compared to the method with EQUAL POWER ALLOCATION . \n",
            "this paper presents a new ALTERNATING OPTIMIZATION APPROACH for EQUAL POWER ALLOCATION . the proposed ALTERNATING OPTIMIZATION APPROACH is based on the ALTERNATING OPTIMIZATION APPROACH and the ALTERNATING OPTIMIZATION APPROACH . the proposed ALTERNATING OPTIMIZATION APPROACH is based on the ALTERNATING OPTIMIZATION APPROACH and the ALTERNATING OPTIMIZATION APPROACH . the proposed ALTERNATING OPTIMIZATION APPROACH is based on the ALTERNATING OPTIMIZATION APPROACH and the ALTERNATING OPTIMIZATION APPROACH . the proposed ALTERNATING OPTIMIZATION APPROACH is based on the ALTERNATING OPTIMIZATION APPROACH and the ALTERNATING OPTIMIZATION APPROACH .\n",
            "\n",
            "668 1000\n",
            "this paper introduces a new ACOUSTIC MODELING METHOD called GAUSSIAN DYNAMIC WARPING . ACOUSTIC MODELING METHOD is targeting REAL WORLD APPLICATIONS such as VOICE-BASED ENTRANCE DOOR SECURITY SYSTEMS , the example presented in this paper . the proposed ACOUSTIC MODELING METHOD uses a HIERARCHICAL STATISTICAL FRAMEWORK with three levels of SPECIALIZATION for the ACOUSTIC MODELING . the highest level of SPECIALIZATION is in addition responsible for the modeling of the TEMPORAL CONSTRAINTS via a specific TEMPORAL STRUCTURE INFORMATION COMPONENT . the preliminary results show the ability of the GAUSSIAN DYNAMIC WARPING to elegantly take into account the ACOUSTIC VARIABILITY OF SPEECH while capturing important TEMPORAL CONSTRAINTS . \n",
            "this paper presents a HIERARCHICAL STATISTICAL FRAMEWORK for ACOUSTIC VARIABILITY OF SPEECH . the ACOUSTIC MODELING METHOD is based on a HIERARCHICAL STATISTICAL FRAMEWORK , called GAUSSIAN DYNAMIC WARPING , for ACOUSTIC VARIABILITY OF SPEECH . the ACOUSTIC MODELING METHOD is based on the TEMPORAL STRUCTURE INFORMATION COMPONENT . the ACOUSTIC MODELING METHOD is based on the TEMPORAL STRUCTURE INFORMATION COMPONENT . the proposed ACOUSTIC MODELING METHOD is based on a HIERARCHICAL STATISTICAL FRAMEWORK . the proposed ACOUSTIC MODELING METHOD is based on a HIERARCHICAL STATISTICAL FRAMEWORK , which is based on a HIERARCHICAL STATISTICAL FRAMEWORK .\n",
            "\n",
            "669 1000\n",
            "in CONCEPT-BASED SUMMARIZATION , SENTENCE SELECTION is modelled as a BUDGETED MAXIMUM COVERAGE PROBLEM . as this problem is NP-HARD , pruning LOW-WEIGHT CONCEPTS is required for the SOLVER to find optimal solutions efficiently . this work shows that reducing the number of concepts in the model leads to lower rouge scores , and more importantly to the presence of multiple optimal solutions . we address these issues by extending the model to provide a single optimal solution , and eliminate the need for CONCEPT PRUNING using an APPROXIMATION ALGORITHM that achieves comparable performance to exact INFERENCE . \n",
            "this paper proposes a new APPROXIMATION ALGORITHM for SENTENCE SELECTION . the proposed APPROXIMATION ALGORITHM is based on the APPROXIMATION ALGORITHM . the proposed APPROXIMATION ALGORITHM is based on the APPROXIMATION ALGORITHM . the proposed APPROXIMATION ALGORITHM is based on the APPROXIMATION ALGORITHM . the proposed APPROXIMATION ALGORITHM is based on the APPROXIMATION ALGORITHM . the proposed APPROXIMATION ALGORITHM is based on the APPROXIMATION ALGORITHM , and is shown to be robust to INFERENCE .\n",
            "\n",
            "670 1000\n",
            "multi-resolution SUB-BAND CEPSTRAL FEATURES strive to exploit DISCRIMINATIVE CUES in LOCALISED REGIONS of the spectral domain by supplementing the FULL BANDWITH CEPSTRAL FEATURES with SUB-BAND CEPSTRAL FEATURES derived from several levels of SUB-BAND DECOMPOSITION . MULT-IRESOLUTION FEATURE VECTORS , formed by concatenation of the SUBBAND CEPSTRAL FEATURES into an extended feature vector , are shown to yield better performance than conventional MFCC FEATURES for PHONEME RECOGNITION on the TIMIT DATABASE . possible strategies for the <unk> of PARTIAL RECOGNITION SCORES from INDEPENDENT MULTI-RESOLTUION SUB-BAND MODELS are explored . by exploiting the SUB-BAND VARIATIONS in SIGNAL TO NOISE RATIO for LINEARLY WEIGHTED RECOMBINATION of the LOG LIKELIHOOD PROBABILITIES we obtained improved PHONEME RECOGNITION performance in BROADBAND NOISE compared to MFCC FEATURES . this is an advantage over a purely SUB-BAND APPROACH using NON LINEAR RECOMBINATION which is robust only to NARROW BAND NOISE . \n",
            "this paper addresses the problem of PHONEME RECOGNITION in NARROW BAND NOISE . we propose a SUB-BAND APPROACH based on MULTI-RESOLUTION SUB-BAND CEPSTRAL FEATURES . the proposed method is based on a SUB-BAND APPROACH , which is based on the MULTI-RESOLUTION SUB-BAND CEPSTRAL FEATURES . the proposed method is based on the use of MULTI-RESOLUTION SUB-BAND CEPSTRAL FEATURES and DISCRIMINATIVE CUES . the proposed method is based on the SUB-BAND APPROACH and the SUB-BAND APPROACH . the proposed method is based on the SUB-BAND APPROACH and the SUB-BAND APPROACH . the proposed method is based on the SUB-BAND APPROACH and the SUB-BAND APPROACH . the proposed method is evaluated on TIMIT DATABASE and compared to the conventional SUB-BAND APPROACH .\n",
            "\n",
            "671 1000\n",
            "neural network training targets for SPEECH RECOGNITION are estimated using a novel method . rather than use zero and one , CONTINUOUS TARGETS are generated using FORWARD-BACKWARD PROBABILITIES . each TRAINING PATTERN has more than one class active . experiments showed that the new method <unk> decreased the ERROR RATE by 15 % in a CONTINUOUS DIGITS RECOGNITION TASK . \n",
            "this paper presents a new method for SPEECH RECOGNITION from CONTINUOUS TARGETS . the proposed method is based on the TRAINING PATTERN of the TRAINING PATTERN . the proposed method is based on the TRAINING PATTERN and the TRAINING PATTERN . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "672 1000\n",
            "in this paper , three different approaches to PRONUNCIATION MODELING are investigated . two existing PRONUNCIATION MODELING APPROACHES , namely the PRONUNCIATION MODELING and N-BEST RESCORING APPROACH are modified to work with little amount of NON-NATIVE SPEECH . we also propose a SPEAKER CLUSTERING APPROACH , which capable of grouping the speakers based on their PRONUNCIATION HABITS . given some speech , the SPEAKER CLUSTERING APPROACH can also be used for PRONUNCIATION MODELING . this SPEAKER CLUSTERING APPROACH is called LATENT PRONUNCIATION ANALYSIS . the results show that conventional PRONUNCIATION MODELING perform slightly better than N-BEST LIST RESCORING , while the LATENT PRONUNCIATION ANALYSIS has shown to be beneficial for SPEAKER CLUSTERING , and SPEAKER CLUSTERING APPROACH can produce nearly the same improvement as the PRONUNCIATION DICTIONARY APPROACH , without the need to know the origin of the speaker . \n",
            "this paper presents a novel SPEAKER CLUSTERING APPROACH for NON-NATIVE SPEECH . the proposed SPEAKER CLUSTERING APPROACH is based on a SPEAKER CLUSTERING APPROACH and a PRONUNCIATION DICTIONARY APPROACH . the proposed SPEAKER CLUSTERING APPROACH is based on the PRONUNCIATION DICTIONARY APPROACH and the PRONUNCIATION DICTIONARY APPROACH . the proposed SPEAKER CLUSTERING APPROACH is compared with conventional PRONUNCIATION MODELING APPROACHES such as N-BEST LIST RESCORING and N-BEST LIST RESCORING . the experimental results show the effectiveness of the proposed SPEAKER CLUSTERING APPROACH .\n",
            "\n",
            "673 1000\n",
            "this paper presents a formulation for UNSUPERVISED LEARNING OF CLUSTERS reflecting multiple causal structure in BINARY DATA . unlike the standard MIXTURE MODEL , a MULTIPLE CAUSE MODEL accounts for OBSERVED DATA by combining assertions from many hidden causes , each of which can pertain to varying degree to any subset of the observable dimensions . a crucial issue is the <unk> for combining beliefs from different CLUSTER-CENTERS in order to generate DATA RECONSTRUCTIONS whose errors are minimized both during RECOGNITION and LEARNING . we demonstrate a weakness inherent to the popular WEIGHTED SUM followed by SIGMOID SQUASHING , and offer an alternative form of the NONLINEARITY . results are presented demonstrating the algorithm 's ability successfully to discover coherent multiple causal <unk> of NOISY TEST DATA and in IMAGES OF PRINTED CHARACTERS . \n",
            "this paper addresses the problem of RECOGNITION in BINARY DATA . we propose a method for RECOGNITION based on the MULTIPLE CAUSE MODEL . the proposed method is based on the MULTIPLE CAUSE MODEL and the MULTIPLE CAUSE MODEL . the proposed method is based on the MULTIPLE CAUSE MODEL and the MULTIPLE CAUSE MODEL . the proposed method is based on the MULTIPLE CAUSE MODEL and the MULTIPLE CAUSE MODEL . the proposed method is based on the MULTIPLE CAUSE MODEL and the MULTIPLE CAUSE MODEL . the proposed method is based on the MULTIPLE CAUSE MODEL and the MULTIPLE CAUSE MODEL .\n",
            "\n",
            "674 1000\n",
            "in this paper we introduce a new distance for robustly matching vectors of 3d rotations . a special representation of 3d rotations , which we coin <unk> quaternion -lrb- <unk> -rrb- , allows us to express this distance as EUCLIDEAN . we apply the distance to the problems of 3D SHAPE RECOGNITION from POINT CLOUDS and 2D OBJECT TRACKING in COLOR VIDEO . for the former , we introduce a HASHING SCHEME for scale and translation which outperforms the previous state-of-the-art approach on a PUBLIC DATASET . for the latter , we incorporate ONLINE SUBSPACE LEARNING with the proposed FAQ REPRESENTATION to highlight the benefits of the new representation . \n",
            "this paper addresses the problem of 2D OBJECT TRACKING in COLOR VIDEO . we propose a HASHING SCHEME based on a HASHING SCHEME and a HASHING SCHEME . the proposed method is based on a HASHING SCHEME , which is based on the FAQ REPRESENTATION and the FAQ REPRESENTATION . the proposed method is evaluated on the PUBLIC DATASET . the results show that the proposed method is robust and robust to 3D SHAPE RECOGNITION and 3D SHAPE RECOGNITION .\n",
            "\n",
            "675 1000\n",
            "in this paper a general and eecient approach for representing and classifying image sequences by HIDDEN MARKOV MODELS is presented . a consistent modeling of spatial and temporal information is achieved by extracting DIIERENT LOW LEVEL IMAGE FEATURES . these implicitly convert the IMAGE INTENSITIES into PROBABILITY DENSITY VALUES , while preserving the GEOMETRY OF THE IMAGE . the resulting so called IMAGE DENSITY FUNCTIONS are contained in the states of the HIDDEN MARKOV MODELS . first results of applying the approach to the CLASSIICA-TION OF DYNAMIC HAND GESTURES demonstrate the performance of the modeling . \n",
            "this paper addresses the problem of CLASSIICA-TION OF DYNAMIC HAND GESTURES in IMAGE INTENSITIES . we propose a method for estimating the GEOMETRY OF THE IMAGE in the GEOMETRY OF THE IMAGE . the proposed method is based on the CLASSIICA-TION OF DYNAMIC HAND GESTURES . the proposed method is based on the CLASSIICA-TION OF DYNAMIC HAND GESTURES . the proposed method is compared with the conventional HIDDEN MARKOV MODELS .\n",
            "\n",
            "676 1000\n",
            "this paper focuses on the novel task of AUTOMATIC EXTRACTION OF PHRASES related to causes of emotions . the analysis of emotional causes in sentences , where emotions are explicitly indicated through EMOTION KEYWORDS can provide the foundation for research on challenging task of recognition of implicit affect from text . we developed a CORPUS OF EMOTION causes specific for 22 emotions . based on the analysis of this corpus we introduce a method for the DETECTION OF THE LINGUISTIC RELATIONS between an EMOTION and its cause and the extraction of the phrases describing the EMOTION causes . the method employs SYNTACTIC AND DEPENDENCY PARSER and RULES for the analysis of eight types of the EMOTION-CAUSE LINGUISTIC RELATIONS . the results of evaluation showed that our method performed with high level of ACCURACY -lrb- 82 % -rrb- . \n",
            "this paper addresses the problem of AUTOMATIC EXTRACTION OF PHRASES in EMOTION KEYWORDS . we propose a method for AUTOMATIC EXTRACTION OF PHRASES based on the CORPUS OF EMOTION . the proposed method is based on a CORPUS OF EMOTION and a CORPUS OF EMOTION . the proposed method is based on the CORPUS OF EMOTION . the proposed method is evaluated on the CORPUS OF EMOTION and the CORPUS OF EMOTION .\n",
            "\n",
            "677 1000\n",
            "we propose a novel GEOMETRIC MIN-HASHING APPROACH for IMAGE RETRIEVAL , CLUSTERING and AUTOMATIC OBJECT DISCOVERY . unlike commonly used bag-of-words approaches , the SPATIAL EXTENT OF IMAGE FEATURES is exploited in our GEOMETRIC MIN-HASHING APPROACH . the GEOMETRIC INFORMATION is used both to construct REPEATABLE HASH KEYS and to increase the discriminability of the description . each HASH KEY combines VISUAL APPEARANCE -LRB- VISUAL WORDS -rrb- with SEMI-LOCAL GEOMETRIC INFORMATION . compared with the state-of-the-art MIN-HASH , the proposed GEOMETRIC MIN-HASHING APPROACH has both higher RECALL -LRB- PROBABILITY OF COLLISION for HASHES on the same object -rrb- and lower FALSE POSITIVE RATES -lrb- RANDOM COLLISIONS -rrb- . the advantages of GEOMETRIC MIN-HASHING APPROACH are most pronounced in the presence of VIEWPOINT and scale change , significant occlusion or small physical overlap of the viewing fields . we demonstrate the power of the proposed GEOMETRIC MIN-HASHING APPROACH on SMALL OBJECT DISCOVERY in a large unordered collection of IMAGES and on a LARGE SCALE IMAGE CLUSTERING PROBLEM . \n",
            "this paper presents a GEOMETRIC MIN-HASHING APPROACH for AUTOMATIC OBJECT DISCOVERY . the GEOMETRIC MIN-HASHING APPROACH is based on the GEOMETRIC MIN-HASHING APPROACH and the GEOMETRIC MIN-HASHING APPROACH . the proposed GEOMETRIC MIN-HASHING APPROACH is based on the GEOMETRIC MIN-HASHING APPROACH and the GEOMETRIC MIN-HASHING APPROACH . the proposed GEOMETRIC MIN-HASHING APPROACH is based on the GEOMETRIC MIN-HASHING APPROACH and the GEOMETRIC MIN-HASHING APPROACH . the proposed GEOMETRIC MIN-HASHING APPROACH is based on the GEOMETRIC MIN-HASHING APPROACH and the GEOMETRIC MIN-HASHING APPROACH . experimental results show that the proposed GEOMETRIC MIN-HASHING APPROACH outperforms the conventional GEOMETRIC MIN-HASHING APPROACH in terms of FALSE POSITIVE RATES and FALSE POSITIVE RATES .\n",
            "\n",
            "678 1000\n",
            "minimum perfect hashing -lrb- MINIMUM PERFECT HASHING -rrb- has recently been shown successful in reducing LANGUAGE MODEL LOOKAHEAD TIME in LVCSR DECODING . in this paper we propose to exploit the ORDER-PRESERVING PROPERTY of a STRING-KEY BASED MPH FUNCTION to further reduce HASHING OPERATION and speed up LM LOOKAHEAD . a SUBTREE STRUCTURE is proposed for LM LOOKAHEAD and an ORDER-PRESERVING MPH is integrated into the STRUCTURE DESIGN . SUBTREES are generated on demand and stored in caches . experiments were performed on SWITCHBOARD DATA . by using the proposed method of OP MPH AND SUBTREE CACHE STRUCTURE for both TRIGRAMS and <unk> bigrams , the LM LOOKAHEAD TIME was reduced by a factor of 2.9 in comparison with the baseline case of using MINIMUM PERFECT HASHING alone . \n",
            "this paper presents a new method for LVCSR DECODING based on MINIMUM PERFECT HASHING . the proposed method is based on a STRING-KEY BASED MPH FUNCTION , which is based on MINIMUM PERFECT HASHING and MINIMUM PERFECT HASHING . the proposed method is based on MINIMUM PERFECT HASHING and MINIMUM PERFECT HASHING . the proposed method is based on MINIMUM PERFECT HASHING and MINIMUM PERFECT HASHING . the proposed method is based on MINIMUM PERFECT HASHING and MINIMUM PERFECT HASHING . experimental results show that the proposed method is effective for LVCSR DECODING .\n",
            "\n",
            "679 1000\n",
            "we consider the estimation of SPARSE GRAPHICAL MODELS that characterize the DEPENDENCY STRUCTURE of HIGH-DIMENSIONAL TENSOR-VALUED DATA . to facilitate the estimation of the PRECISION MATRIX corresponding to each way of the tensor , we assume the data follow a TENSOR NORMAL DISTRIBUTION whose covariance has a KRONECKER PRODUCT STRUCTURE . the PENALIZED MAXIMUM LIKELIHOOD ESTIMATION of this model involves minimizing a NON-CONVEX OBJECTIVE FUNCTION . in spite of the non-convexity of this ESTIMATION PROBLEM , we prove that an ALTERNATING MINIMIZATION ALGORITHM , which iteratively estimates each SPARSE PRECISION MATRIX while fixing the others , attains an ESTIMATOR with the optimal STATISTICAL RATE OF CONVERGENCE as well as CONSISTENT GRAPH RECOVERY . notably , such an ESTIMATOR achieves ESTIMATION CONSISTENCY with only one TENSOR SAMPLE , which is unobserved in previous work . our theoretical results are backed by thorough NUMERICAL STUDIES . \n",
            "this paper proposes a new ESTIMATOR for CONSISTENT GRAPH RECOVERY . the proposed ALTERNATING MINIMIZATION ALGORITHM is based on the ALTERNATING MINIMIZATION ALGORITHM . the proposed ALTERNATING MINIMIZATION ALGORITHM is based on the ALTERNATING MINIMIZATION ALGORITHM . the proposed ALTERNATING MINIMIZATION ALGORITHM is based on the ALTERNATING MINIMIZATION ALGORITHM . the proposed ALTERNATING MINIMIZATION ALGORITHM is applied to the ESTIMATION PROBLEM . the proposed ESTIMATOR is compared with the conventional ESTIMATOR in terms of ESTIMATION CONSISTENCY and STATISTICAL RATE OF CONVERGENCE .\n",
            "\n",
            "680 1000\n",
            "most existing SUBSPACE ANALYSIS-BASED TRACKING ALGORITHMS utilize a FLATTENED VECTOR to represent a target , resulting in a high dimensional data learning problem . recently , SUBSPACE ANALYSIS is incorporated into the MULTILIN-EAR FRAMEWORK which offline constructs a REPRESENTATION OF IMAGE ENSEMBLES using HIGH-ORDER TENSORS . this reduces SPATIO-TEMPORAL REDUNDANCIES substantially , whereas the COMPUTATIONAL AND MEMORY COST is high . in this paper , we present an effective ONLINE TENSOR SUBSPACE LEARNING ALGORITHM which models the appearance changes of a target by incrementally learning a LOW-ORDER TENSOR EIGENSPACE REPRESENTATION through adaptively updating the sample mean and <unk> . TRACKING then is led by the STATE INFERENCE within the framework in which a PARTICLE FILTER is used for propagating sample distributions over the time . a novel LIKELIHOOD FUNCTION , based on the TENSOR RECONSTRUCTION ERROR NORM , is developed to measure the similarity between the test image and the learned TENSOR SUBSPACE MODEL during the TRACKING . THEORETIC ANALYSIS and experimental evaluations against a state-of-the-art method demonstrate the promise and effectiveness of this ONLINE TENSOR SUBSPACE LEARNING ALGORITHM . \n",
            "this paper addresses the problem of TRACKING for REPRESENTATION OF IMAGE ENSEMBLES . we propose a TENSOR SUBSPACE MODEL based on the TENSOR SUBSPACE MODEL . the proposed ONLINE TENSOR SUBSPACE LEARNING ALGORITHM is based on the TENSOR SUBSPACE MODEL . the ONLINE TENSOR SUBSPACE LEARNING ALGORITHM is based on the TENSOR SUBSPACE MODEL . the proposed ONLINE TENSOR SUBSPACE LEARNING ALGORITHM is based on the TENSOR SUBSPACE MODEL . the proposed ONLINE TENSOR SUBSPACE LEARNING ALGORITHM is based on the TENSOR SUBSPACE MODEL . the proposed ONLINE TENSOR SUBSPACE LEARNING ALGORITHM is based on a TENSOR SUBSPACE MODEL . the proposed ONLINE TENSOR SUBSPACE LEARNING ALGORITHM is based on a TENSOR SUBSPACE MODEL . the proposed ONLINE TENSOR SUBSPACE LEARNING ALGORITHM is based on a LOW-ORDER TENSOR EIGENSPACE REPRESENTATION . the proposed ONLINE TENSOR SUBSPACE LEARNING ALGORITHM is based on the TENSOR SUBSPACE MODEL . the proposed ONLINE TENSOR SUBSPACE LEARNING ALGORITHM is based on a TENSOR SUBSPACE MODEL .\n",
            "\n",
            "681 1000\n",
            "the purpose of this study was to examine typically developing infants ' integration of audiovisual sensory information as a fundamental process involved in EARLY WORD LEARNING . one hundred <unk> <unk> children were randomly assigned to watch one of four <unk> versions of AUDIOVISUAL VIDEO SEQUENCES . the infants ' <unk> were recorded and their looking behavior was analyzed throughout three REPETITIONS OF EXPOSURE-TEST-PHASES . the results indicate that the infants were able to learn covariance between shapes and colors of ARBITRARY GEOMETRICAL OBJECTS and to them corresponding <unk> words . implications of AUDIOVISUAL INTEGRATION in infants and in NON-HUMAN ANIMALS for modeling within SPEECH RECOGNITION SYSTEMS , NEURAL NETWORKS and ROBOTICS are discussed . \n",
            "this paper addresses the problem of AUDIOVISUAL INTEGRATION in AUDIOVISUAL VIDEO SEQUENCES . we propose a method for AUDIOVISUAL INTEGRATION in AUDIOVISUAL VIDEO SEQUENCES . the method is based on the REPETITIONS OF EXPOSURE-TEST-PHASES and the REPETITIONS OF EXPOSURE-TEST-PHASES . the method is based on the REPETITIONS OF EXPOSURE-TEST-PHASES and the REPETITIONS OF EXPOSURE-TEST-PHASES . experimental results show that the proposed method is robust and robust to AUDIOVISUAL INTEGRATION and AUDIOVISUAL INTEGRATION .\n",
            "\n",
            "682 1000\n",
            "segmental dynamic time warping -lrb- dtw -rrb- has been demonstrated to be a useful technique for finding acoustic similarity scores between segments of two SPEECH UTTERANCES . due to its high COMPUTATIONAL REQUIREMENTS , it had to be computed in an OFFLINE MANNER , limiting the applications of the technique . in this paper , we present results of parallelization of this task by distributing the workload in either a STATIC OR DYNAMIC WAY on an 8-PROCESSOR CLUSTER and discuss the trade-offs among different distribution schemes . we show that ONLINE UNSUPERVISED PATTERN DISCOVERY using SEGMENTAL DTW is plausible with as low as 8 processors . this brings the task within reach of today 's general purpose MULTI-CORE SERVERS . we also show results on a 32-PROCESSOR SYSTEM , and discuss factors affecting scalability of our methods . \n",
            "this paper presents a method for ONLINE UNSUPERVISED PATTERN DISCOVERY from SPEECH UTTERANCES . the proposed method is based on the SEGMENTAL DYNAMIC TIME WARPING . the proposed method is based on the SEGMENTAL DYNAMIC TIME WARPING . the proposed method is based on the SEGMENTAL DYNAMIC TIME WARPING and the 32-PROCESSOR SYSTEM . the proposed method is based on the SEGMENTAL DYNAMIC TIME WARPING . the proposed method is based on the SEGMENTAL DYNAMIC TIME WARPING and is shown to be robust to SPEECH UTTERANCES .\n",
            "\n",
            "683 1000\n",
            "we give an ORACLE-BASED ALGORITHM for the ADVERSARIAL CONTEXTUAL BANDIT PROBLEM , where either contexts are drawn i.i.d. or the sequence of contexts is known a PRIORI , but where the losses are picked <unk> . our ORACLE-BASED ALGORITHM is computationally efficient , assuming access to an OFFLINE OPTIMIZATION ORACLE , and enjoys a regret of order o -lrb- -lrb- <unk> -rrb- 2 3 -lrb- log n -rrb- 1 3 -rrb- , where k is the number of actions , t is the number of ITERATIONS and n is the number of BASELINE POLICIES . our result is the first to break the O -LRB- T 3 4 -RRB- BARRIER that is achieved by recently introduced algorithms . breaking this barrier was left as a major open problem . our analysis is based on the recent RELAXATION BASED APPROACH of <unk> and <unk> -lsb- 7 -rsb- . \n",
            "this paper proposes a RELAXATION BASED APPROACH for ADVERSARIAL CONTEXTUAL BANDIT PROBLEM . the proposed RELAXATION BASED APPROACH is based on a RELAXATION BASED APPROACH . the proposed RELAXATION BASED APPROACH is based on the RELAXATION BASED APPROACH . the proposed RELAXATION BASED APPROACH is based on the ORACLE-BASED ALGORITHM . the proposed RELAXATION BASED APPROACH is based on the RELAXATION BASED APPROACH . the proposed RELAXATION BASED APPROACH is based on the RELAXATION BASED APPROACH .\n",
            "\n",
            "684 1000\n",
            "finding the least squares -lrb- ls -rrb- solution s to a system of linear equations <unk> = y where h , y are given and s is a VECTOR OF BINARY VARIABLES , is a well known NP-HARD PROBLEM . in this paper , we consider BINARY LS PROBLEMS under the assumption that the COEFSCIENT MATRIX H is also unknown , and lies in a given UNCERTAINTY ELLIPSOID . we show that the corresponding WORST-CASE ROBUST OPTIMIZATION PROBLEM , although NP-HARD , is still amenable to SEMIDESNITE RELAXATION - based approximations . however , the RELAXATION STEP is not obvious , and requires a certain PROBLEM REFORMULATION to be efſcient . the proposed RELAXATION STEP is motivated using LAGRANGIAN DUALITY and simulations suggest that RELAXATION STEP performs well , offering a robust alternative over the traditional SDR APPROACHES for BINARY LS PROBLEMS . \n",
            "this paper addresses the problem of WORST-CASE ROBUST OPTIMIZATION PROBLEM in BINARY LS PROBLEMS . we propose a RELAXATION STEP for the WORST-CASE ROBUST OPTIMIZATION PROBLEM , which is based on the PROBLEM REFORMULATION . the proposed method is based on the use of LAGRANGIAN DUALITY to estimate the VECTOR OF BINARY VARIABLES from the VECTOR OF BINARY VARIABLES . the proposed method is based on the use of LAGRANGIAN DUALITY . the proposed method is compared with other SDR APPROACHES .\n",
            "\n",
            "685 1000\n",
            "in this paper we give a general analysis of DYADIC DEONTIC LOGICS that were introduced in the early <unk> to formalize DEONTIC REASONING about SUBIDEAL BEHAVIOR . recently it was observed that DYADIC DEONTIC LOGICS are closely related to NON-MONOTONIC LOGICS , theories of diagnosis and DECISION THEORIES . in particular , we argue that two types of DEFEASIBILITY must be distinguished in a DEFEASIBLE DEONTIC LOGIC : OVERRIDDEN DEFEASI-BILITY that formalizes cancelling of an <unk> by other CONDITIONAL OBLIGATIONS and FACTUAL DEFEASIBILITY that formalizes <unk> of an <unk> by a violating fact . we also show that this distinction is essential for an adequate analysis of notorious ` <unk> ' of DEONTIC LOGIC such as the CHISHOLM AND FOR-RESTER ` PARADOXES ' . \n",
            "this paper presents a method for DEONTIC REASONING based on DYADIC DEONTIC LOGICS . the proposed method is based on the use of DYADIC DEONTIC LOGICS and the CONDITIONAL OBLIGATIONS . the proposed method is based on the use of DYADIC DEONTIC LOGICS and the CONDITIONAL OBLIGATIONS . the proposed method is based on the use of DYADIC DEONTIC LOGICS and the CONDITIONAL OBLIGATIONS . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "686 1000\n",
            "while many recent HAND POSE ESTIMATION METHODS critically rely on a training set of labelled frames , the creation of such a dataset is a challenging task that has been overlooked so far . as a result , existing datasets are limited to a few sequences and individuals , with limited ACCURACY , and this prevents these methods from delivering their full potential . we propose a SEMI-AUTOMATED METHOD for efficiently and accurately LABELING each frame of a HAND DEPTH VIDEO with the corresponding 3D LOCATIONS of the joints : the user is asked to provide only an estimate of the 2d <unk> of the visible joints in some REFERENCE FRAMES , which are automatically selected to minimize the LABELING work by efficiently optimizing a SUB-MODULAR LOSS FUNCTION . we then exploit SPATIAL , TEMPORAL , AND APPEARANCE CONSTRAINTS to retrieve the full 3d poses of the hand over the complete sequence . we show that this data can be used to train a recent state-of-the-art HAND POSE ESTIMATION METHOD , leading to increased ACCURACY . the HAND POSE ESTIMATION METHOD and dataset can be found on our website <unk> . AT/PROJECTS/HAND _ DETECTION / . \n",
            "this paper presents a SEMI-AUTOMATED METHOD for AT/PROJECTS/HAND _ DETECTION . the SEMI-AUTOMATED METHOD is based on a SEMI-AUTOMATED METHOD . the SEMI-AUTOMATED METHOD is based on the SUB-MODULAR LOSS FUNCTION and the SUB-MODULAR LOSS FUNCTION . the ACCURACY of the proposed HAND POSE ESTIMATION METHOD is demonstrated by the use of the SEMI-AUTOMATED METHOD . the ACCURACY of the proposed method is demonstrated by simulation results .\n",
            "\n",
            "687 1000\n",
            "by now it is widely accepted that LEARNING a task from scratch , i.e. , without any PRIOR KNOWLEDGE , is a daunting <unk> . humans , however , rarely attempt to learn from scratch . they extract INITIAL BIASES as well as strategies how to approach a LEARNING PROBLEM from instructions and/or DEMONSTRATIONS of other humans . for LEARNING PROBLEM , this paper investigates how LEARNING from demonstration can be applied in the context of REINFORCEMENT LEARNING . we consider priming the Q-FUNCTION , the VALUE FUNCTION , the POLICY , and the model of the task dynamics as possible areas where DEMONSTRATIONS can speed up LEARNING . in general NONLINEAR LEARNING PROBLEMS , only MODEL-BASED REINFORCEMENT LEARNING shows significant speed-up after a demonstration , while in the special case of LINEAR QUADRATIC REGULATOR PROBLEMS , all methods profit from the demonstration . in an implementation of POLE BALANCING on a complex <unk> robot arm , we demonstrate that , when facing the complexities of REAL SIGNAL PROCESSING , MODEL-BASED REINFORCEMENT LEARNING offers the most ROBUSTNESS for LINEAR QUADRATIC REGULATOR PROBLEMS . using the suggested methods , the robot learns POLE BALANCING in just a single trial after a 30 second long demonstration of the HUMAN INSTRUCTOR . \n",
            "this paper presents a new method for MODEL-BASED REINFORCEMENT LEARNING based on MODEL-BASED REINFORCEMENT LEARNING . the proposed method is based on the use of LINEAR QUADRATIC REGULATOR PROBLEMS in the LEARNING PROBLEM . the LEARNING PROBLEM is formulated as a LEARNING PROBLEM . the proposed method is based on the use of PRIOR KNOWLEDGE in the LEARNING PROBLEM . the proposed method is based on the LEARNING PROBLEM . the proposed method is based on the use of LINEAR QUADRATIC REGULATOR PROBLEMS in the LEARNING PROBLEM . the proposed method is based on the LEARNING PROBLEM . the proposed method is based on the use of LINEAR QUADRATIC REGULATOR PROBLEMS in the LEARNING PROBLEM . the proposed method is based on the use of LINEAR QUADRATIC REGULATOR PROBLEMS .\n",
            "\n",
            "688 1000\n",
            "contrary to popular belief , we show that the optimal parameters for IBM MODEL 1 are not unique . we demonstrate that , for a large class of words , IBM MODEL 1 is <unk> among a continuum of ways to allocate probability mass to their translations . we study the magnitude of the variance in OPTIMAL MODEL PARAMETERS using a LINEAR PROGRAMMING APPROACH as well as multiple RANDOM TRIALS , and demonstrate that IBM MODEL 1 results in variance in TEST SET LOG-LIKELIHOOD and ALIGNMENT ERROR RATE . \n",
            "this paper proposes a new LINEAR PROGRAMMING APPROACH for IBM MODEL 1 . the proposed LINEAR PROGRAMMING APPROACH is based on the OPTIMAL MODEL PARAMETERS and the OPTIMAL MODEL PARAMETERS . the proposed LINEAR PROGRAMMING APPROACH is evaluated on the TEST SET LOG-LIKELIHOOD of the TEST SET LOG-LIKELIHOOD and the TEST SET LOG-LIKELIHOOD .\n",
            "\n",
            "689 1000\n",
            "from an AUDIO PERSPECTIVE , the present state of TELECONFERENCING TECHNOLOGY leaves something to be desired ; SPEAKER OVERLAP is one of the causes of this inadequate performance . to that end , this paper presents a frequency-domain implementation of CONVOLUTIVE BSS specifically designed for the nature of the <unk> environment . in addition to presenting a novel DEPERMUTATION SCHEME , this paper presents a LEAST-SQUARES POST-PROCESSING SCHEME , which exploits segments during which only a subset of all speakers are active . experiments with SIMULATED AND REAL DATA demonstrate the ability of the proposed LEAST-SQUARES POST-PROCESSING SCHEME to provide SIRS at or near that of the ADAPTIVE NOISE CANCELLATION SOLUTION which is obtained under <unk> assumptions that the ADAPTIVE NOISE CANCELLATION SOLUTION are adapted with one source being on at a time . \n",
            "this paper presents a new LEAST-SQUARES POST-PROCESSING SCHEME for CONVOLUTIVE BSS . the proposed LEAST-SQUARES POST-PROCESSING SCHEME is based on a LEAST-SQUARES POST-PROCESSING SCHEME . the proposed LEAST-SQUARES POST-PROCESSING SCHEME is based on a LEAST-SQUARES POST-PROCESSING SCHEME . the proposed LEAST-SQUARES POST-PROCESSING SCHEME is based on a LEAST-SQUARES POST-PROCESSING SCHEME . the proposed LEAST-SQUARES POST-PROCESSING SCHEME is evaluated on the SIMULATED AND REAL DATA and on the SIMULATED AND REAL DATA .\n",
            "\n",
            "690 1000\n",
            "experimental studies of INTERACTIVE LANGUAGE USE have shed light on the COGNITIVE AND INTERPERSONAL PROCESSES that shape conversation ; corpora are the emergent products of these processes . i will survey studies that focus on <unk> aspects of INTERACTIVE LANGUAGE USE , including the processing of SPONTANEOUS SPEECH and disfluencies ; METALINGUISTIC DISPLAYS such as HEDGES ; INTERACTIVE PROCESSES that affect choices of REFERRING EXPRESSIONS ; and how COMMUNICATION MEDIA SHAPE CONVERSATIONS . the findings suggest some <unk> for COMPUTATIONAL LINGUISTICS . \n",
            "this paper addresses the problem of COMMUNICATION MEDIA SHAPE CONVERSATIONS in COMMUNICATION MEDIA SHAPE CONVERSATIONS such as HEDGES . we propose a method for COMMUNICATION MEDIA SHAPE CONVERSATIONS , which is based on the HEDGES . the proposed method is based on the use of REFERRING EXPRESSIONS , such as the HEDGES , and the HEDGES . the proposed method is evaluated on the COMMUNICATION MEDIA SHAPE CONVERSATIONS and the results show that the proposed method is robust to REFERRING EXPRESSIONS and is robust to REFERRING EXPRESSIONS such as HEDGES .\n",
            "\n",
            "691 1000\n",
            "recent SPIKING NETWORK MODELS of BAYESIAN INFERENCE and UNSUPERVISED LEARNING frequently assume either inputs to arrive in a special format or employ complex computations in NEURONAL ACTIVATION FUNCTIONS and SYNAPTIC PLASTICITY RULES . here we show in a rigorous MATHEMATICAL TREATMENT how HOMEOSTATIC PROCESSES , which have previously received little attention in this context , can overcome common THEORETICAL LIMITATIONS and facilitate the NEURAL IMPLEMENTATION and performance of existing models . in particular , we show that HOMEOSTATIC PLASTICITY can be understood as the enforcement of a ` BALANCING ' POSTERIOR CONSTRAINT during PROBABILIS-TIC INFERENCE and LEARNING with EXPECTATION MAXIMIZATION . we link HOMEOSTATIC DYNAMICS to the theory of VARIATIONAL INFERENCE , and show that NONTRIVIAL TERMS , which typically appear during PROBABILISTIC INFERENCE in a large class of models , drop out . we demonstrate the feasibility of our approach in a spiking <unk> architecture of BAYESIAN INFERENCE and LEARNING . finally , we sketch how the MATHEMATICAL TREATMENT can be extended to richer RECURRENT NETWORK ARCHI-TECTURES . altogether , our theory provides a novel perspective on the interplay of HOMEOSTATIC PROCESSES and SYNAPTIC PLASTICITY in CORTICAL MICROCIRCUITS , and points to an essential role of HOMEOSTASIS during INFERENCE and LEARNING in SPIKING NETWORKS . \n",
            "this paper addresses the problem of PROBABILISTIC INFERENCE in SPIKING NETWORKS . we propose a method for estimating the parameters of a MATHEMATICAL TREATMENT and a MATHEMATICAL TREATMENT based on EXPECTATION MAXIMIZATION . the proposed method is based on the use of SYNAPTIC PLASTICITY RULES and EXPECTATION MAXIMIZATION . the proposed method is based on the use of SYNAPTIC PLASTICITY RULES and EXPECTATION MAXIMIZATION . the proposed method is based on the use of NEURONAL ACTIVATION FUNCTIONS and VARIATIONAL INFERENCE . the proposed method is based on the use of NEURONAL ACTIVATION FUNCTIONS and VARIATIONAL INFERENCE . the proposed method is compared with other state-of-the-art methods .\n",
            "\n",
            "692 1000\n",
            "from the results of the NIST SPEAKER RECOGNITION EVALUATION in <unk> years , SPEAKER RECOGNITION SYSTEMS which are mainly developed based on ENGLISH TRAINING DATA suffer the LANGUAGE GAP PROBLEM , namely , the performance of <unk> <unk> is much worse than that of ENGLISH TRAILS . this problem is addressed in this paper . based on the conventional JOINT FACTOR ANALYSIS MODEL , we enrolled in the LANGUAGE FACTORS which are mean to capture the language character of each testing and training speech utterance , and compensation was carried out by removing the LANGUAGE FACTORS in order to shrink the difference between languages . experiments on 2006 nist sre data show that , the LANGUAGE FACTOR COMPENSATION alone can reduce the gap between the performance of english and <unk> <unk> , and the score level combination with <unk> can further improve the performance of <unk> <unk> , e.g. , for female part , we observed about 19 % relatively reduction in EER , when compared with EIGENCHANNELS SESSION VARIABILITY COMPENSATION alone . \n",
            "this paper presents a new method for EIGENCHANNELS SESSION VARIABILITY COMPENSATION based on LANGUAGE FACTOR COMPENSATION . the proposed method is based on the JOINT FACTOR ANALYSIS MODEL . the proposed method is based on the JOINT FACTOR ANALYSIS MODEL , which is based on the JOINT FACTOR ANALYSIS MODEL . the proposed method is based on the JOINT FACTOR ANALYSIS MODEL and the JOINT FACTOR ANALYSIS MODEL . experimental results show that the proposed method can improve the EER of the NIST SPEAKER RECOGNITION EVALUATION .\n",
            "\n",
            "693 1000\n",
            "to learn the <unk> VISUAL ATTENTION given by humans to specific IMAGE CONTENT , we present an EYE FIXATION DATABASE compiled from a pool of <unk> images and 75 subjects . EYE FIXATIONS are an excellent modality to learn SEMANTICS-DRIVEN HUMAN UNDERSTANDING OF IMAGES , which is vastly different from FEATURE-DRIVEN APPROACHES employed by SALIENCY COMPUTATION ALGORITHMS . the EYE FIXATION DATABASE comprises FIXATION PATTERNS acquired using an EYE-TRACKER , as subjects <unk> images corresponding to many SEMANTIC CATEGORIES such as faces -lrb- human and <unk> -rrb- , <unk> and actions -lrb- look , read and <unk> -rrb- . the consistent presence of FIXATION CLUSTERS around specific image regions confirms that VISUAL ATTENTION is not subjective , but is directed towards SALIENT OBJECTS and <unk> . we then show how the FIXATION CLUSTERS can be exploited for enhancing IMAGE UNDERSTANDING , by using our EYE FIXATION DATABASE in an ACTIVE IMAGE SEGMENTATION APPLICATION . apart from proposing a mechanism to automatically determine CHARACTERISTIC FIXATION SEEDS for SEGMENTATION , we show that the use of FIXATION SEEDS generated from multiple FIXATION CLUSTERS on the SALIENT OBJECT can lead to a 10 % improvement in segmen-tation performance over the state-of-the-art . \n",
            "this paper presents a new method for IMAGE UNDERSTANDING based on VISUAL ATTENTION . the proposed method is based on a EYE FIXATION DATABASE , which is based on the EYE-TRACKER . the proposed method is based on a EYE FIXATION DATABASE , which is based on a EYE FIXATION DATABASE . the proposed method is based on the EYE FIXATION DATABASE . the proposed method is based on a EYE FIXATION DATABASE . the proposed method is based on the EYE FIXATION DATABASE . the proposed method is based on the EYE FIXATION DATABASE . the proposed method is based on a EYE FIXATION DATABASE . the proposed method is based on the EYE FIXATION DATABASE . the proposed method is based on the EYE FIXATION DATABASE . the proposed method is based on a EYE FIXATION DATABASE . the proposed method is based on a EYE FIXATION DATABASE . the proposed method is based on a EYE FIXATION DATABASE and is applied to the ACTIVE IMAGE SEGMENTATION APPLICATION .\n",
            "\n",
            "694 1000\n",
            "-- in this paper , we review some recent advances in the design of ADCS that exploit SYSTEM-DRIVEN METRICS , such as the BIT-ERROR RATE in a COMMUNICATION LINK , or MUTUAL INFORMATION in a scheme employing FORWARD ERROR CORRECTION . we show , for example , that ADCS can be designed that maximize the INFORMATION RATE between the quantized output of the channel and the input to the channel for COMMUNICATION links with <unk> and additive noise . these ADCS dramatically outper-form -lrb- in terms of achievable information rates -rrb- traditional ADC DESIGN METHODS that are based on FIXED UNIFORM QUANTIZATION . architectures are also developed for ADCS such that ADCS can be used to dynamically adapt the structure of the ADCS to optimize APPLICATION MEANINGFUL CRITERIA , such as BIT-ERROR RATE for COMMUNICATION over INTERSYMBOL INTERFERENCE LINKS . \n",
            "this paper addresses the problem of COMMUNICATION in COMMUNICATION . we propose a method for COMMUNICATION based on MUTUAL INFORMATION . the proposed method is based on the use of MUTUAL INFORMATION and MUTUAL INFORMATION . the proposed method is based on the MUTUAL INFORMATION of the COMMUNICATION LINK . the proposed method is based on the MUTUAL INFORMATION of the COMMUNICATION LINK . the proposed method is compared with other ADC DESIGN METHODS such as FIXED UNIFORM QUANTIZATION , and the ADC DESIGN METHODS .\n",
            "\n",
            "695 1000\n",
            "traditional techniques of DENSE OPTICAL FLOW ESTIMATION do n't generally yield SYMMETRICAL SOLUTIONS : the results will differ if SYMMETRICAL SOLUTIONS are applied between IMAGES I1 and I2 or between IMAGES I2 and I1 . in this work , we present a method to recover a DENSE OPTICAL FLOW FIELD MAP from two IMAGES , while <unk> taking into account the symmetry across the IMAGES as well as possible occlusions and discontinuities in the FLOW FIELD . the idea is to consider both DISPLACEMENTS VECTORS from I1 to I2 and I2 to I1 and to minimise an ENERGY FUNCTIONAL that <unk> encodes all those properties . this VARIATIONAL PROBLEM is then solved using the GRADIENT FLOW defined by the EULER -- LAGRANGE EQUATIONS associated to the energy . in order to reduce the risk to be trapped within some irrelevant minimum , a FOCUSING STRATEGY based on a MULTI-RESOLUTION TECHNIQUE is used to converge toward the solution . promising experimental results on both SYNTHETIC AND REAL IMAGES are presented to illustrate the capabilities of this symmetrical variational approach to recover accurate OPTICAL FLOW . \n",
            "this paper presents a new method for DENSE OPTICAL FLOW ESTIMATION in IMAGES . the proposed method is based on a MULTI-RESOLUTION TECHNIQUE and a MULTI-RESOLUTION TECHNIQUE . the proposed method is based on a MULTI-RESOLUTION TECHNIQUE and a MULTI-RESOLUTION TECHNIQUE . the proposed method is based on a MULTI-RESOLUTION TECHNIQUE and a MULTI-RESOLUTION TECHNIQUE . the proposed method is based on a MULTI-RESOLUTION TECHNIQUE and a MULTI-RESOLUTION TECHNIQUE . the proposed method is based on a MULTI-RESOLUTION TECHNIQUE and a MULTI-RESOLUTION TECHNIQUE . the proposed method is evaluated on SYNTHETIC AND REAL IMAGES and SYNTHETIC AND REAL IMAGES . the results show that the proposed method can improve the performance of DENSE OPTICAL FLOW ESTIMATION and DENSE OPTICAL FLOW ESTIMATION .\n",
            "\n",
            "696 1000\n",
            "we present a novel approach to RELATIVE POSE ESTIMATION which is tailored to 4D LIGHT FIELD CAMERAS . from the relationships between SCENE GEOMETRY and LIGHT FIELD STRUCTURE and an analysis of the LIGHT FIELD PROJECTION in terms of PLÜCKER RAY COORDINATES , we deduce a set of LINEAR CONSTRAINTS on RAY SPACE CORRESPONDENCES between a pair of LIGHT FIELD CAMERAS . these can be applied to infer RELATIVE POSE OF THE LIGHT FIELD CAMERAS and thus obtain a POINT CLOUD RECONSTRUCTION OF THE SCENE . while the proposed method has interesting relationships to pose estimation for GENERALIZED CAMERAS based on RAY-TO-RAY CORRESPONDENCE , our experiments demonstrate that our approach is both more accurate and computationally more efficient . it also compares favorably to DIRECT LINEAR POSE ESTIMATION based on aligning the 3D POINT CLOUDS obtained by reconstructing depth for each individual light field . to further validate the method , we employ the POSE ESTIMATES to merge LIGHT FIELDS captured with HAND-HELD CONSUMER LIGHT FIELD CAMERAS into REFOCUS-ABLE PANORAMAS . \n",
            "this paper addresses the problem of DIRECT LINEAR POSE ESTIMATION in HAND-HELD CONSUMER LIGHT FIELD CAMERAS . we propose a method for estimating the LIGHT FIELD STRUCTURE from a POINT CLOUD RECONSTRUCTION OF THE SCENE . the proposed method is based on the RELATIVE POSE OF THE LIGHT FIELD CAMERAS , which is a POINT CLOUD RECONSTRUCTION OF THE SCENE . the proposed method is based on the RELATIVE POSE OF THE LIGHT FIELD CAMERAS , which is a POINT CLOUD RECONSTRUCTION OF THE SCENE . the proposed method is based on the RELATIVE POSE OF THE LIGHT FIELD CAMERAS of the 4D LIGHT FIELD CAMERAS . the proposed method is based on the RELATIVE POSE OF THE LIGHT FIELD CAMERAS . the proposed method is based on the RELATIVE POSE OF THE LIGHT FIELD CAMERAS . the proposed method is based on the RELATIVE POSE OF THE LIGHT FIELD CAMERAS . the proposed method is based on the RELATIVE POSE OF THE LIGHT FIELD CAMERAS . the proposed method is shown to be robust to POSE ESTIMATES in the presence of 3D POINT CLOUDS .\n",
            "\n",
            "697 1000\n",
            "we find a close relationship between the DISCRETE KARHUNEN-LOEVE TRANSFORM and the DISCRETE PROLATE SPHEROIDAL WAVE FUNCTIONS . we show that the DISCRETE PROLATE SPHEROIDAL WAVE FUNCTIONS form a natural basis for an expansion of the eigenfunctions of the DISCRETE KARHUNEN-LOEVE TRANSFORM in the FREQUENCY DOMAIN , and then determine more general conditions that any set of functions must obey to be a valid basis . we also present APPROXIMATE SOLUTIONS for small , medium , and large filter orders . the MEDIUM ORDER SOLUTION suggests that the PRINCIPAL EIGENFUNC-TION is , to a high degree of APPROXIMATION , the principal DISCRETE PROLATE SPHEROIDAL WAVE FUNCTIONS modulated so that its CENTER FREQUENCY coincides with the peak of maximum energy in the SIGNAL SPECTRUM . we then use this result to propose a new basis . \n",
            "this paper addresses the problem of DISCRETE PROLATE SPHEROIDAL WAVE FUNCTIONS in the FREQUENCY DOMAIN . we propose a method to estimate the CENTER FREQUENCY of the SIGNAL SPECTRUM in the FREQUENCY DOMAIN . the proposed method is based on the DISCRETE KARHUNEN-LOEVE TRANSFORM and the MEDIUM ORDER SOLUTION . the proposed method is based on the DISCRETE KARHUNEN-LOEVE TRANSFORM and the MEDIUM ORDER SOLUTION . the proposed method is compared with the conventional APPROXIMATE SOLUTIONS and the APPROXIMATE SOLUTIONS .\n",
            "\n",
            "698 1000\n",
            "an INCREMENTAL NETWORK MODEL is introduced which is able to learn the important TOPOLOGICAL RELATIONS in a given set of input vectors by means of a simple HEBB-LIKE LEARNING RULE . in contrast to previous approaches like the `` NEURAL GAS '' METHOD of <unk> and <unk> -lrb- 1991 , 1994 -rrb- , this `` NEURAL GAS '' METHOD has no parameters which change over time and is able to continue learning , adding units and connections , until a PERFORMANCE CRITERION has been met . applications of the `` NEURAL GAS '' METHOD include VECTOR QUANTIZATION , CLUSTERING , and INTERPOLATION . \n",
            "this paper presents a new `` NEURAL GAS '' METHOD for CLUSTERING . the proposed `` NEURAL GAS '' METHOD consists of two steps : a `` NEURAL GAS '' METHOD and a HEBB-LIKE LEARNING RULE . the proposed `` NEURAL GAS '' METHOD consists of two steps : a `` NEURAL GAS '' METHOD and a HEBB-LIKE LEARNING RULE . the proposed `` NEURAL GAS '' METHOD is based on the `` NEURAL GAS '' METHOD and the `` NEURAL GAS '' METHOD . the experimental results show the effectiveness of the proposed INCREMENTAL NETWORK MODEL .\n",
            "\n",
            "699 1000\n",
            "in this paper we propose an extension to the standard MARKOV RANDOM FIELD MODEL in order to handle LAYERS . our extension , which we call a FACTORIAL MRF , is analogous to the extension from HIDDEN MARKOV MODELS -LRB- HMM 'S -RRB- to FACTORIAL HMM 'S . we present an efficient EM-BASED ALGORITHM for INFERENCE on FACTORIAL MRF 'S . our EM-BASED ALGORITHM makes use of the fact that LAYERS are a priori independent , and that LAYERS only interact through the OBSERVABLE IMAGE . the EM-BASED ALGORITHM iterates between WIDE INFERENCE , i.e. , INFERENCE within each layer for the entire set of pixels , and DEEP INFERENCE , i.e. , INFERENCE through the LAYERS for each single pixel . the efficiency of our EM-BASED ALGORITHM is partly due to the use of GRAPH CUTS for BINARY SEGMENTATION , which is part of the WIDE INFERENCE STEP . we show experimental results for both REAL AND SYNTHETIC IMAGES . \n",
            "this paper presents a MARKOV RANDOM FIELD MODEL based on a MARKOV RANDOM FIELD MODEL . the EM-BASED ALGORITHM is based on a MARKOV RANDOM FIELD MODEL of the OBSERVABLE IMAGE . the EM-BASED ALGORITHM is based on a MARKOV RANDOM FIELD MODEL of the OBSERVABLE IMAGE . the EM-BASED ALGORITHM is based on the EM-BASED ALGORITHM . the EM-BASED ALGORITHM is based on a MARKOV RANDOM FIELD MODEL of the OBSERVABLE IMAGE . the proposed EM-BASED ALGORITHM is based on a MARKOV RANDOM FIELD MODEL . the proposed EM-BASED ALGORITHM is based on a MARKOV RANDOM FIELD MODEL and is shown to be more robust to WIDE INFERENCE than the conventional EM-BASED ALGORITHM .\n",
            "\n",
            "700 1000\n",
            "we propose a REAL-TIME METHOD for simultaneously refining the reconstructed volume of a human body with LOOSE-FITTING CLOTHING and identifying <unk> in it . TIME-SERIES VOLUMES , which are acquired by a slow but sophisticated 3D RECONSTRUCTION ALGORITHM , with BODY-PART LABELS are obtained offline . the TIME-SERIES SAMPLE VOLUMES are represented by trajectories in the <unk> using PCA . an INPUT VISUAL HULL reconstructed online is projected into the EIGENSPACE and compared with the trajectories in order to find similar high-precision samples with BODY-PART LABELS . the HIERARCHICAL SEARCH taking into account 3d reconstruction errors can achieve robust and fast matching . experimental results demonstrate that our REAL-TIME METHOD can refine the INPUT VISUAL HULL including LOOSE-FITTING CLOTHING and identify its <unk> in real time . \n",
            "this paper addresses the problem of LOOSE-FITTING CLOTHING in TIME-SERIES VOLUMES . we propose a method for estimating the INPUT VISUAL HULL from a EIGENSPACE . the proposed method is based on a REAL-TIME METHOD , which is based on the HIERARCHICAL SEARCH . the proposed method is based on a REAL-TIME METHOD . the proposed method is based on a REAL-TIME METHOD . the proposed method is based on a REAL-TIME METHOD .\n",
            "\n",
            "701 1000\n",
            "in this paper , a novel approach for SINGLE CHANNEL SOURCE SEPARATION using a DEEP NEURAL NETWORK ARCHITECTURE is introduced . unlike previous studies in which DEEP NEURAL NETWORK ARCHITECTURE and other CLASSIFIERS were used for CLASSIFYING TIME-FREQUENCY BINS to obtain hard masks for each source , we use the DEEP NEURAL NETWORK ARCHITECTURE to classify ESTIMATED SOURCE SPECTRA to check for their validity during separation . in the TRAINING STAGE , the TRAINING DATA for the source signals are used to train a DEEP NEURAL NETWORK ARCHITECTURE . in the SEPARATION STAGE , the trained DEEP NEURAL NETWORK ARCHITECTURE is utilized to aid in estimation of each source in the mixed signal . SINGLE CHANNEL SOURCE SEPARATION PROBLEM is formulated as an ENERGY MINIMIZATION PROBLEM where each source spectra estimate is encouraged to fit the trained DEEP NEURAL NETWORK ARCHITECTURE and the MIXED SIGNAL SPECTRUM is encouraged to be written as a weighted sum of the ESTIMATED SOURCE SPECTRA . the proposed approach works regardless of the ENERGY SCALE DIFFERENCES between the source signals in the training and separation stages . NONNEGATIVE MATRIX FACTORIZATION is used to initialize the DEEP NEURAL NETWORK ARCHITECTURE for each source . the experimental results show that using DEEP NEURAL NETWORK ARCHITECTURE initialized by DEEP NEURAL NETWORK ARCHITECTURE for SOURCE SEPARATION improves the quality of the separated signal compared with using DEEP NEURAL NETWORK ARCHITECTURE for SOURCE SEPARATION . \n",
            "this paper presents a DEEP NEURAL NETWORK ARCHITECTURE based on NONNEGATIVE MATRIX FACTORIZATION . the DEEP NEURAL NETWORK ARCHITECTURE is based on a DEEP NEURAL NETWORK ARCHITECTURE . the DEEP NEURAL NETWORK ARCHITECTURE is based on NONNEGATIVE MATRIX FACTORIZATION and NONNEGATIVE MATRIX FACTORIZATION . the DEEP NEURAL NETWORK ARCHITECTURE is based on NONNEGATIVE MATRIX FACTORIZATION and NONNEGATIVE MATRIX FACTORIZATION . the DEEP NEURAL NETWORK ARCHITECTURE is based on a DEEP NEURAL NETWORK ARCHITECTURE . the DEEP NEURAL NETWORK ARCHITECTURE is based on a DEEP NEURAL NETWORK ARCHITECTURE . the proposed DEEP NEURAL NETWORK ARCHITECTURE is based on a DEEP NEURAL NETWORK ARCHITECTURE . the proposed DEEP NEURAL NETWORK ARCHITECTURE is based on a DEEP NEURAL NETWORK ARCHITECTURE . the proposed DEEP NEURAL NETWORK ARCHITECTURE is based on NONNEGATIVE MATRIX FACTORIZATION and NONNEGATIVE MATRIX FACTORIZATION . the proposed DEEP NEURAL NETWORK ARCHITECTURE is based on a DEEP NEURAL NETWORK ARCHITECTURE . the proposed DEEP NEURAL NETWORK ARCHITECTURE is based on a DEEP NEURAL NETWORK ARCHITECTURE and is shown to be robust to ENERGY SCALE DIFFERENCES and ENERGY SCALE DIFFERENCES .\n",
            "\n",
            "702 1000\n",
            "in this contribution , the performance of an UPLINK CDMA SYSTEM with RANDOM SPREADING and MULTI-CELL INTERFERENCE is analyzed . a useful framework is provided in order to determine the BASE STATION COVERAGE for WIRELESS FLAT FADING CHANNELS with very dense networks -lrb- in the number of users per meter -rrb- considering different RECEIVER STRUCTURES at the base station , namely the MATCHED FILTER , the WIENER FILTER and the OPTIMUM FILTER . using ASYMPTOTIC ARGUMENTS , analytical expressions of the SPECTRAL EFFICIENCY are obtained and provide a simple expression of the NETWORK CAPACITY based only on a few meaningful parameters . \n",
            "this paper addresses the problem of MULTI-CELL INTERFERENCE in WIRELESS FLAT FADING CHANNELS such as MULTI-CELL INTERFERENCE , MULTI-CELL INTERFERENCE , and MULTI-CELL INTERFERENCE . we propose a method for estimating the MULTI-CELL INTERFERENCE from a WIRELESS FLAT FADING CHANNELS . the proposed method is based on the OPTIMUM FILTER and the OPTIMUM FILTER . the proposed method is based on the OPTIMUM FILTER and the OPTIMUM FILTER . the experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "703 1000\n",
            "this paper presents a <i> local learning projection </i> -lrb- <unk> -rrb- approach for LINEAR DIMENSIONALITY REDUCTION . we first point out that the well known <I> PRINCIPAL COMPONENT ANALYSIS </I> essentially seeks the PROJECTION that has the MINIMAL <I> GLOBAL </I> ESTIMATION ERROR . then we propose a DIMENSIONALITY REDUCTION ALGORITHM that leads to the PROJECTION with the MINIMAL <I> LOCAL </I> ESTIMATION ERROR , and elucidate its advantages for CLASSIFICATION TASKS . we also indicate that <unk> keeps the LOCAL INFORMATION in the sense that the PROJECTION VALUE of each point can be well estimated based on its neighbors and their PROJECTION VALUES . experimental results are provided to validate the effectiveness of the proposed DIMENSIONALITY REDUCTION ALGORITHM . \n",
            "this paper presents a DIMENSIONALITY REDUCTION ALGORITHM for LINEAR DIMENSIONALITY REDUCTION . the proposed DIMENSIONALITY REDUCTION ALGORITHM is based on the <I> PRINCIPAL COMPONENT ANALYSIS </I> . the proposed DIMENSIONALITY REDUCTION ALGORITHM is based on the <I> PRINCIPAL COMPONENT ANALYSIS </I> . the proposed DIMENSIONALITY REDUCTION ALGORITHM is based on the <I> PRINCIPAL COMPONENT ANALYSIS </I> . the proposed DIMENSIONALITY REDUCTION ALGORITHM is based on the <I> PRINCIPAL COMPONENT ANALYSIS </I> . the proposed DIMENSIONALITY REDUCTION ALGORITHM is based on the <I> PRINCIPAL COMPONENT ANALYSIS </I> and the <I> PRINCIPAL COMPONENT ANALYSIS </I> .\n",
            "\n",
            "704 1000\n",
            "the MAXIMUM LIKELIHOOD SEQUENCE ESTIMATOR is the optimal RECEIVER for the INTER-SYMBOL INTERFERENCE CHANNEL with ADDITIVE WHITE NOISE . a RECEIVER is demonstrated that estimates sequence likelihood using a VARIABLE ORDER MARKOV MODEL constructed from a CRUDELY QUANTIZED TRAINING SEQUENCE . RECEIVER PERFORMANCE is relatively unaffected by HEAVY-TAILED NOISE that can <unk> the performance of GAUSSIAN BASED ALGORITHMS such as DECISION FEEDBACK EQUALIZATION with GRADIENT BASED ADAPTATION . we consider the problem of DECODING BINARY SYMBOLS across a LINEAR ISI CHANNEL contaminated with ADDITIVE WHITE NOISE . given discrete-time observations of the channel output r n \n",
            "this paper proposes a VARIABLE ORDER MARKOV MODEL for DECISION FEEDBACK EQUALIZATION . the MAXIMUM LIKELIHOOD SEQUENCE ESTIMATOR is based on a VARIABLE ORDER MARKOV MODEL and a VARIABLE ORDER MARKOV MODEL . the RECEIVER is based on the MAXIMUM LIKELIHOOD SEQUENCE ESTIMATOR and the MAXIMUM LIKELIHOOD SEQUENCE ESTIMATOR . the RECEIVER is based on the MAXIMUM LIKELIHOOD SEQUENCE ESTIMATOR and the MAXIMUM LIKELIHOOD SEQUENCE ESTIMATOR . the RECEIVER is applied to the CRUDELY QUANTIZED TRAINING SEQUENCE and the RECEIVER PERFORMANCE is compared with the conventional GAUSSIAN BASED ALGORITHMS .\n",
            "\n",
            "705 1000\n",
            "adaptive ridge is a special form of RIDGE REGRESSION , balancing the QUADRATIC PENALIZATION on each parameter of the model . it was shown to be equivalent to LASSO -lrb- least absolute shrinkage and selection operator -rrb- , in the sense that both procedures produce the same estimate . LASSO can thus be viewed as a particular QUADRATIC PENALIZER . from this observation , we derive a FIXED POINT ALGORITHM to compute the LASSO SOLUTION . the analogy provides also a new HYPER-PARAMETER for tuning effectively the MODEL COMPLEXITY . we finally present a SERIES OFPOSSI-BLE EXTENSIONS OFLASSO performing SPARSE REGRESSION in KERNEL SMOOTHING , ADDITIVE MODELING and NEURAL NET TRAINING . \n",
            "this paper addresses the problem of SPARSE REGRESSION in ADDITIVE MODELING . we propose a new LASSO SOLUTION based on the LASSO SOLUTION . the proposed method is based on a FIXED POINT ALGORITHM , which is based on the LASSO SOLUTION and the LASSO SOLUTION . the proposed method is based on the LASSO SOLUTION and the LASSO SOLUTION . the proposed method is based on the LASSO SOLUTION and the LASSO SOLUTION . the proposed method is compared with the conventional LASSO and the LASSO SOLUTION .\n",
            "\n",
            "706 1000\n",
            "in the context of the <unk> workshop on NATURAL LANGUAGE PROCESSING for LESS PRIVILEGED LANGUAGES , we discuss the obstacles to research on such languages . we also briefly discuss the ways to make progress in removing these obstacles . we mention some previous work and comment on the papers selected for the workshop . \n",
            "this paper presents a method for NATURAL LANGUAGE PROCESSING from LESS PRIVILEGED LANGUAGES . the proposed method is based on the use of LESS PRIVILEGED LANGUAGES , which are used for NATURAL LANGUAGE PROCESSING . experimental results show that the proposed method can improve the performance of NATURAL LANGUAGE PROCESSING .\n",
            "\n",
            "707 1000\n",
            "in this paper we present the BEHAVIOSITE PARADIGM , a new approach to coordination and control of DISTRIBUTED AGENTS in a MULTIAGENT SYSTEM , inspired by BIOLOGICAL PARASITES with BEHAVIOR MANIPULATION PROPERTIES . BEHAVIOSITE PARADIGM are CODE MODULES that '' <unk> '' a MULTIAGENT SYSTEM , attaching themselves to agents and altering the SENSORY ACTIVITY and actions of those agents . these BEHAVIORAL CHANGES can be used to achieve altered , potentially improved , performance of the overall MULTIAGENT SYSTEM ; thus , BEHAVIOSITE PARADIGM provide a mechanism for DISTRIBUTED CONTROL over a DISTRIBUTED SYSTEM . BEHAVIOSITE PARADIGM need to be designed so that BEHAVIOSITE PARADIGM are intimately familiar with the internal <unk> of the environment and of the agents operating within it . to demonstrate our approach , we use <unk> to control the behavior of a SWARM of simple agents . with a relatively low INFECTION RATE , a few <unk> can <unk> desired behavior over the SWARM as a whole : keeping it in one place , leading it through <unk> , or moving the SWARM from one stable equilibrium to another . we contrast <unk> as a DISTRIBUTED SWARM CONTROL MECHANISM with alternatives , such as the use of group leaders , HERDERS , or SOCIAL NORMS . \n",
            "this paper presents a new method for DISTRIBUTED CONTROL in DISTRIBUTED AGENTS . the proposed method is based on the use of CODE MODULES and CODE MODULES . the proposed method is based on the BEHAVIOSITE PARADIGM and the BEHAVIOSITE PARADIGM . the proposed method is based on the BEHAVIOSITE PARADIGM and the BEHAVIOSITE PARADIGM . the proposed method is based on the BEHAVIOSITE PARADIGM and the BEHAVIOSITE PARADIGM . the proposed method is based on the BEHAVIOSITE PARADIGM and the BEHAVIOSITE PARADIGM . the proposed method is based on the BEHAVIOSITE PARADIGM and the BEHAVIOSITE PARADIGM .\n",
            "\n",
            "708 1000\n",
            "for INTERNET APPLICATIONS like SPONSORED SEARCH , <unk> need to be taken when using MACHINE LEARNING to optimize their mechanisms -lrb- e.g. , auction -rrb- since SELF-INTERESTED AGENTS in these applications may change their behaviors -lrb- and thus the DATA DISTRIBUTION -rrb- in response to the mechanisms . to tackle this problem , a framework called game-theoretic MACHINE LEARNING -lrb- <unk> -rrb- was recently proposed , which first learns a MARKOV BEHAVIOR MODEL to characterize agents behaviors , and then learns the optimal MARKOV BEHAVIOR MODEL by simulating agents ' behavior changes in response to the MARKOV BEHAVIOR MODEL . while <unk> has demonstrated practical success , its GENERALIZATION ANALYSIS is challenging because the BEHAVIOR DATA are non-i.i.d. and dependent on the MARKOV BEHAVIOR MODEL . to address this challenge , first , we decompose the GENERALIZATION ERROR for <unk> into the BEHAVIOR LEARNING ERROR and the BEHAVIOR LEARNING ERROR ; second , for the BEHAVIOR LEARNING ERROR , we obtain novel NON-ASYMPTOTIC ERROR BOUNDS for both PARAMETRIC AND NON-PARAMETRIC BEHAVIOR LEARNING METHODS ; third , for the BEHAVIOR LEARNING ERROR , we derive a UNIFORM CONVERGENCE bound based on a new concept called nested covering number of the MECHANISM SPACE and the GENERALIZATION ANALYSIS TECHNIQUES developed for mixing sequences . \n",
            "this paper addresses the problem of GENERALIZATION ANALYSIS for INTERNET APPLICATIONS such as SELF-INTERESTED AGENTS . we propose a MARKOV BEHAVIOR MODEL for MACHINE LEARNING that is based on a MARKOV BEHAVIOR MODEL . the proposed GENERALIZATION ANALYSIS TECHNIQUES is based on a MARKOV BEHAVIOR MODEL , which is based on the MARKOV BEHAVIOR MODEL . the proposed GENERALIZATION ANALYSIS TECHNIQUES is based on the MARKOV BEHAVIOR MODEL . the proposed GENERALIZATION ANALYSIS TECHNIQUES is based on a GENERALIZATION ANALYSIS . the proposed GENERALIZATION ANALYSIS TECHNIQUES is based on a GENERALIZATION ANALYSIS and is shown to be more robust to BEHAVIOR DATA .\n",
            "\n",
            "709 1000\n",
            "-- we present a PROBABILISTIC FRAMEWORK for PHYSICAL LAYER SECRECY when the locations and channels of the <unk> are unknown . the locations are modeled by a POISSON POINT PROCESS . the channels include PATH LOSS and RAYLEIGH FADING . BEAMFORMING and <unk> of the fading channels are shown to greatly increase the probability of SECURE COMMUNICATIONS . \n",
            "this paper presents a new PROBABILISTIC FRAMEWORK for SECURE COMMUNICATIONS . the PROBABILISTIC FRAMEWORK is based on the POISSON POINT PROCESS and the POISSON POINT PROCESS . the PROBABILISTIC FRAMEWORK is based on the POISSON POINT PROCESS and the PROBABILISTIC FRAMEWORK . the proposed PROBABILISTIC FRAMEWORK is based on a PROBABILISTIC FRAMEWORK and is shown to be robust to RAYLEIGH FADING and RAYLEIGH FADING .\n",
            "\n",
            "710 1000\n",
            "this paper proposes a novel ACOUSTIC MODEL based on NEURAL NETWORKS for STATISTICAL PARAMETRIC SPEECH SYNTHESIS . the NEURAL NETWORKS outputs parameters of a NON-ZERO MEAN GAUSSIAN PROCESS , which defines a PROBABILITY DENSITY FUNCTION of a SPEECH WAVEFORM given LINGUISTIC FEATURES . the MEAN AND COVARIANCE FUNCTIONS of the GAUSSIAN PROCESS represent deterministic -lrb- voiced -rrb- and STOCHASTIC COMPONENTS of a SPEECH WAVEFORM , whereas the previous approach considered the UNVOICED COMPONENT only . experimental results show that the proposed approach can generate SPEECH WAVEFORMS approximating NATURAL SPEECH WAVEFORMS . \n",
            "this paper addresses the problem of STATISTICAL PARAMETRIC SPEECH SYNTHESIS in NATURAL SPEECH WAVEFORMS . the ACOUSTIC MODEL is based on a GAUSSIAN PROCESS of the SPEECH WAVEFORM . the ACOUSTIC MODEL is based on the PROBABILITY DENSITY FUNCTION of the SPEECH WAVEFORM . the ACOUSTIC MODEL is based on the PROBABILITY DENSITY FUNCTION of the SPEECH WAVEFORM . the ACOUSTIC MODEL is applied to the SPEECH WAVEFORM and the ACOUSTIC MODEL is applied to the ACOUSTIC MODEL . the experimental results show the effectiveness of the proposed ACOUSTIC MODEL .\n",
            "\n",
            "711 1000\n",
            "this paper presents an AGENT-BASED MODEL that studies the emergence and evolution of a LANGUAGE SYSTEM of LOGICAL CONSTRUCTIONS , i.e. a vocabulary and a set of GRAMMATICAL CONSTRUCTIONS that allows the expression of LOGICAL COMBINATIONS OF CATEGORIES . the AGENT-BASED MODEL assumes the agents have a common vocabulary for basic categories , the ability to construct LOGICAL COMBINATIONS OF CATEGORIES using BOOLEAN FUNCTIONS , and some general purpose cognitive capacities for INVENTION , ADOPTION , INDUCTION and ADAPTATION . but it does not assume the agents have a vocabulary for BOOLEAN FUNCTIONS nor GRAMMATICAL CONSTRUCTIONS for expressing such LOGICAL COMBINATIONS OF CATEGORIES through language . the results of the experiments we have performed show that a LANGUAGE SYSTEM of LOGICAL CONSTRUCTIONS emerges as a result of a process of <unk> of the individual agents ' interactions when these agents adapt their preferences for vocabulary and GRAMMATICAL CONSTRUCTIONS to those they observe are used more often by the rest of the population , and that such a LANGUAGE SYSTEM is transmitted from one generation to the next . \n",
            "this paper addresses the problem of INDUCTION in LANGUAGE SYSTEM . we propose a method for INDUCTION , INVENTION , and ADAPTATION . our approach is based on the use of BOOLEAN FUNCTIONS and ADAPTATION . we demonstrate the effectiveness of our method on a variety of GRAMMATICAL CONSTRUCTIONS , INDUCTION , INDUCTION , and INDUCTION .\n",
            "\n",
            "712 1000\n",
            "the assumptions behind LINEAR CLASSIFIERS for CATEGORICAL DATA are examined and reformulated in the context of the MULTINOMIAL MANIFOLD , the simplex of MULTINOMIAL MODELS <unk> with the RIEMANNIAN STRUCTURE induced by the FISHER INFORMATION . this leads to a new view of HYPERPLANE CLASSIFIERS which , together with a GENERALIZED MARGIN CONCEPT , shows how to adapt existing MARGIN-BASED HYPERPLANE MODELS to MULTINOMIAL GEOMETRY . experiments show the new CLASSIFICATION FRAMEWORK to be effective for TEXT CLASSIFICATION , where the CATEGORICAL STRUCTURE of the data is modeled naturally within the MULTINOMIAL FAMILY . \n",
            "this paper addresses the problem of TEXT CLASSIFICATION from CATEGORICAL DATA . in this paper , we propose a new CLASSIFICATION FRAMEWORK based on the GENERALIZED MARGIN CONCEPT . the proposed method is based on the GENERALIZED MARGIN CONCEPT and uses a GENERALIZED MARGIN CONCEPT to estimate the FISHER INFORMATION from the MULTINOMIAL MANIFOLD . the proposed method is based on the GENERALIZED MARGIN CONCEPT . the proposed method is based on the GENERALIZED MARGIN CONCEPT . the proposed method is based on the GENERALIZED MARGIN CONCEPT . the proposed method is based on the GENERALIZED MARGIN CONCEPT . the experimental results show the effectiveness of the proposed method for TEXT CLASSIFICATION .\n",
            "\n",
            "713 1000\n",
            "we present a CENTER-REFERENCED BASIS for DISCRETE REPRESENTATION OF STEREO CORRESPONDENCE that includes new OCCLU-SION NODES . this basis improves the inclusion of constraints and the parallelism of the final algorithm . DISPARITY ESTIRNA-TION is formulated in a MAP CONTEXT and NATURAL CONSTRAINTS are incorporated , resulting in an optimal path problem in a SPARSELY CONNECTED TRELLIS . like other D P METHODS , the COMPUTATIONAL COMPLEXITY is low at -lrb- 3 -lrb- m n 2 -rrb- for m x n pixel images . however , this method is better suited to PARALLEL SOLUTION , scaling up to -lrb- 3 -lrb- m n -rrb- processors . experimental results confirm the performance of this method . \n",
            "this paper addresses the problem of DISCRETE REPRESENTATION OF STEREO CORRESPONDENCE in MAP CONTEXT . we propose a method for DISCRETE REPRESENTATION OF STEREO CORRESPONDENCE based on a DISCRETE REPRESENTATION OF STEREO CORRESPONDENCE . the proposed method is based on a DISCRETE REPRESENTATION OF STEREO CORRESPONDENCE . the proposed method is based on the DISCRETE REPRESENTATION OF STEREO CORRESPONDENCE . the proposed method is based on a DISCRETE REPRESENTATION OF STEREO CORRESPONDENCE . the proposed method is compared with conventional D P METHODS . the proposed method is compared with other D P METHODS .\n",
            "\n",
            "714 1000\n",
            "<unk> to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists , or to reuse any <unk> component of this work in other works must be obtained from the IEEE . '' ABSTRACT this paper deals with the problem of discriminating samples that contain only NOISE from samples that contain a signal embedded in NOISE . the focus is on the case when the variance of the NOISE is unknown . we derive the optimal SOFT DECISION DETECTOR using a BAYESIAN APPROACH . the COMPLEXITY of this optimal SOFT DECISION DETECTOR grows exponentially with the number of observations and as a remedy , we propose a number of approximations to SOFT DECISION DETECTOR . the problem under study is a fundamental one and SOFT DECISION DETECTOR has applications in SIGNAL DENOISING , ANOMALY DETECTION , and SPECTRUM SENSING for COGNITIVE RADIO . we illustrate the results in the context of the latter . \n",
            "this paper presents a new BAYESIAN APPROACH for COGNITIVE RADIO . the BAYESIAN APPROACH is based on the BAYESIAN APPROACH and the BAYESIAN APPROACH . the BAYESIAN APPROACH is based on the BAYESIAN APPROACH and the BAYESIAN APPROACH . the proposed BAYESIAN APPROACH is based on the BAYESIAN APPROACH and the BAYESIAN APPROACH . the proposed BAYESIAN APPROACH is based on the BAYESIAN APPROACH and the BAYESIAN APPROACH . the proposed BAYESIAN APPROACH is based on the BAYESIAN APPROACH and the BAYESIAN APPROACH .\n",
            "\n",
            "715 1000\n",
            "utterance UTTERANCE VERIFICATION based on N-BEST HMM SCORES has been widely used in ASR SYSTEM . there are a number of ways to calculate a MEASUREMENT SCORE for UTTERANCE VERIFICATION from n-best scores . most of proposed methods are based on the N-BEST UV APPROACH of the HYPOTHESIS TESTING . this has lead to use the second best score or an overall average of available n-best scores for <unk> . in this study we examine N-BEST UV APPROACH from a COMPETITION-BASED MEASUREMENT FRAMEWORK . with this N-BEST UV APPROACH different competitive measurements can be derived from a SEQUENCE OF SORTED LIKELIHOOD RATIOS . the evaluation results demonstrate that oov performance can be improved by using some SELECTIVE COMPONENTS in SORTED LIKELIHOOD RATIOS . in our experiments by using the first four components oov rejection errors can be reduced about 30 % in comparison with the baseline results . \n",
            "this paper presents a COMPETITION-BASED MEASUREMENT FRAMEWORK for UTTERANCE VERIFICATION . the COMPETITION-BASED MEASUREMENT FRAMEWORK is based on a COMPETITION-BASED MEASUREMENT FRAMEWORK , which is based on the MEASUREMENT SCORE of the ASR SYSTEM . the N-BEST UV APPROACH is based on the SEQUENCE OF SORTED LIKELIHOOD RATIOS . the MEASUREMENT SCORE of the proposed ASR SYSTEM is evaluated using the N-BEST UV APPROACH . the results show that the proposed N-BEST UV APPROACH can improve the performance of the ASR SYSTEM .\n",
            "\n",
            "716 1000\n",
            "in this paper , we present a BIRDSONG-PHRASE SEGMENTATION AND VERIFICATION ALGORITHM that is robust to LIMITED TRAINING DATA , CLASS VARIABILITY , and NOISE . the BIRDSONG-PHRASE SEGMENTATION AND VERIFICATION ALGORITHM comprises a NOISE-ROBUST , DYNAMIC-TIME-WARPING - based segmentation and a DISCRIMINATIVE CLASSIFIER for OUTLIER REJECTION . the BIRDSONG-PHRASE SEGMENTATION AND VERIFICATION ALGORITHM utilizes NOISE-ROBUST , DYNAMIC-TIME-WARPING and prominent -lrb- high energy -rrb- time-frequency regions of training spectrograms to derive a reliable NOISE-ROBUST TEMPLATE for each phrase class . the resulting BIRDSONG-PHRASE SEGMENTATION AND VERIFICATION ALGORITHM is then used for SEGMENTING CONTINUOUS RECORDINGS to obtain segment candidates whose SPECTROGRAM AMPLITUDES in the prominent regions are used as FEATURES to a SUPPORT VECTOR MACHINE . the BIRDSONG-PHRASE SEGMENTATION AND VERIFICATION ALGORITHM is evaluated on the CASSIN 'S VIREO RECORDINGS ; our proposed BIRDSONG-PHRASE SEGMENTATION AND VERIFICATION ALGORITHM yields low equal error rates -lrb- eer -rrb- and SEGMENT BOUNDARIES that are close to those obtained from MANUAL ANNOTATIONS and , is better than ENERGY OR ENTROPY-BASED BIRDSONG SEGMENTATION ALGORITHMS . in the presence of ADDITIVE NOISE -lrb- <unk> to 10 db snr -rrb- , the proposed BIRDSONG-PHRASE SEGMENTATION AND VERIFICATION ALGORITHM does not degrade as significantly as the other algorithms do . \n",
            "this paper presents a BIRDSONG-PHRASE SEGMENTATION AND VERIFICATION ALGORITHM for SEGMENTING CONTINUOUS RECORDINGS . the proposed BIRDSONG-PHRASE SEGMENTATION AND VERIFICATION ALGORITHM is based on the BIRDSONG-PHRASE SEGMENTATION AND VERIFICATION ALGORITHM and the BIRDSONG-PHRASE SEGMENTATION AND VERIFICATION ALGORITHM . the proposed BIRDSONG-PHRASE SEGMENTATION AND VERIFICATION ALGORITHM is based on a DISCRIMINATIVE CLASSIFIER and a DISCRIMINATIVE CLASSIFIER . the proposed BIRDSONG-PHRASE SEGMENTATION AND VERIFICATION ALGORITHM is based on the BIRDSONG-PHRASE SEGMENTATION AND VERIFICATION ALGORITHM and the BIRDSONG-PHRASE SEGMENTATION AND VERIFICATION ALGORITHM . the proposed BIRDSONG-PHRASE SEGMENTATION AND VERIFICATION ALGORITHM is evaluated on SEGMENTING CONTINUOUS RECORDINGS and SEGMENTING CONTINUOUS RECORDINGS . the proposed BIRDSONG-PHRASE SEGMENTATION AND VERIFICATION ALGORITHM is compared with other ENERGY OR ENTROPY-BASED BIRDSONG SEGMENTATION ALGORITHMS and ENERGY OR ENTROPY-BASED BIRDSONG SEGMENTATION ALGORITHMS . the proposed BIRDSONG-PHRASE SEGMENTATION AND VERIFICATION ALGORITHM is shown to outperform the ENERGY OR ENTROPY-BASED BIRDSONG SEGMENTATION ALGORITHMS in terms of OUTLIER REJECTION and OUTLIER REJECTION .\n",
            "\n",
            "717 1000\n",
            "images taken from different views of a PLANAR OBJECT are related by PLANAR HOMOGRAPHY . recovering the parameters of such IMAGES is a fundamental problem in COMPUTER VISION with various applications . this paper proposes a novel method to estimate the parameters of a HOMOGRAPHY that aligns two BINARY IMAGES . it is obtained by solving a system of NONLINEAR EQUATIONS generated by integrating LINEARLY INDEPENDENT FUNCTIONS over the domains determined by the shapes . the advantage of the proposed solution is that it is easy to implement , less sensitive to the strength of the DEFORMATION , works without ESTABLISHED CORRESPONDENCES and robust against SEGMENTATION ERRORS . the method has been tested on synthetic as well as on REAL IMAGES and its efficiency has been demonstrated in the context of two different applications : alignment of HIP PROSTHESIS X-RAY IMAGES and MATCHING OF TRAFFIC SIGNS . \n",
            "this paper presents a method for MATCHING OF TRAFFIC SIGNS from BINARY IMAGES . the proposed method is based on the MATCHING OF TRAFFIC SIGNS in the PLANAR OBJECT . the proposed method is based on the MATCHING OF TRAFFIC SIGNS in BINARY IMAGES . the proposed method is based on the MATCHING OF TRAFFIC SIGNS in the PLANAR OBJECT . the proposed method is based on the MATCHING OF TRAFFIC SIGNS . the proposed method is based on the MATCHING OF TRAFFIC SIGNS . the proposed method is based on the MATCHING OF TRAFFIC SIGNS . the proposed method is based on the MATCHING OF TRAFFIC SIGNS .\n",
            "\n",
            "718 1000\n",
            "in this paper we address the problem of REDVING BLURRED POINT SOURCES in INTENSITY IMAGES . a new approach to IMAGE RESTORATION is introduced which is a 2-D GENERALIZATION UF TECHNIQUES originating from the field of direction of arrival estimation -lrb- doa -rrb- . i n the 2-D FREQUENCY DOMAIN . algorithms , such as MUSIC . may be adapted to search for these BLURRED POINT SOURCES . a generalization of ARRAY SMOOTHING <unk> on a REGULARIZATION OPERATOR is introduced for <unk> arrays in order to achieve RANK ENHANRMIENT in the SIGNAL SPACE of the COVARIANCE MUTRIR . \n",
            "this paper addresses the problem of IMAGE RESTORATION in BLURRED POINT SOURCES . we propose a method for IMAGE RESTORATION based on the REGULARIZATION OPERATOR . the proposed method is based on the REGULARIZATION OPERATOR . the proposed method is based on the REGULARIZATION OPERATOR . the proposed method is based on the REGULARIZATION OPERATOR . the proposed method is based on the REGULARIZATION OPERATOR . the proposed method is based on the REGULARIZATION OPERATOR . the proposed method is based on the REGULARIZATION OPERATOR and the 2-D GENERALIZATION UF TECHNIQUES .\n",
            "\n",
            "719 1000\n",
            "the high COMPUTATIONAL COMPLEXITY of the EXPRESSIVE DESCRIPTION LOGICS that underlie the OWL STANDARD has motivated the study of their HORN FRAGMENTS , which are usually tractable in DATA COMPLEXITY and can also have lower combined COMPLEXITY , particularly for QUERY ANSWERING . in this paper we provide algorithms for answering <unk> <unk> regular path queries -lrb- <unk> -rrb- , a NON-TRIVIAL GENERALIZATION OF PLAIN CONJUNCTIVE QUERIES , in the HORN FRAGMENTS of the DLS SHOIQ and SROIQ underlying owl 1 and owl 2 . we show that the combined COMPLEXITY of the problem is <unk> for SROIQ and 2EXPTIME-COMPLETE for the more expressive SROIQ , but is <unk> in DATA COMPLEXITY for both . in contrast , even DECIDABILITY OF PLAIN CONJUNCTIVE QUERIES is still open for FULL SHOIQ and SROIQ . these are the first completeness results for QUERY ANSWERING in EXPRESSIVE DESCRIPTION LOGICS with <unk> , NOMINALS , and counting , and show that for the considered logics the problem is not more expensive than standard reasoning . \n",
            "this paper addresses the problem of NON-TRIVIAL GENERALIZATION OF PLAIN CONJUNCTIVE QUERIES in QUERY ANSWERING . we propose a method for NON-TRIVIAL GENERALIZATION OF PLAIN CONJUNCTIVE QUERIES , NOMINALS , and 2EXPTIME-COMPLETE . the proposed EXPRESSIVE DESCRIPTION LOGICS is based on the use of HORN FRAGMENTS and EXPRESSIVE DESCRIPTION LOGICS . the proposed method is based on the DECIDABILITY OF PLAIN CONJUNCTIVE QUERIES and the COMPLEXITY of the OWL STANDARD . the proposed EXPRESSIVE DESCRIPTION LOGICS is evaluated on the OWL STANDARD and the COMPUTATIONAL COMPLEXITY of the EXPRESSIVE DESCRIPTION LOGICS .\n",
            "\n",
            "720 1000\n",
            "a <unk> limitation of research on SUPERVISED SENTENCE COMPRESSION is the dearth of available TRAINING DATA . we propose a new and <unk> resource for such TRAINING DATA , which we obtain by mining the revision history of wikipedia for SENTENCE COMPRESSIONS and EXPANSIONS . using only a fraction of the available WIKIPEDIA DATA , we have collected a training corpus of over <unk> sentence pairs , two orders of magnitude larger than the <unk> used ZIFF-DAVIS CORPUS . using this <unk> data , we propose a novel LEXICAL-IZED NOISY CHANNEL MODEL for SENTENCE COMPRESSION , achieving improved results in GRAM-MATICALITY AND COMPRESSION RATE CRITERIA with a slight decrease in importance . \n",
            "this paper presents a new method for SUPERVISED SENTENCE COMPRESSION based on WIKIPEDIA DATA . the proposed method is based on a LEXICAL-IZED NOISY CHANNEL MODEL and a LEXICAL-IZED NOISY CHANNEL MODEL . the proposed method is based on the ZIFF-DAVIS CORPUS and the ZIFF-DAVIS CORPUS . the proposed method is based on the ZIFF-DAVIS CORPUS . the proposed method is based on the ZIFF-DAVIS CORPUS and the ZIFF-DAVIS CORPUS .\n",
            "\n",
            "721 1000\n",
            "unsupervised feature learning algorithms based on CON-VOLUTIONAL FORMULATIONS of INDEPENDENT COMPONENTS ANALYSIS have been demonstrated to yield state-of-the-art results in several ACTION RECOGNITION BENCHMARKS . however , existing approaches do not allow for the number of LATENT COMPONENTS -lrb- features -rrb- to be automatically inferred from the data in an UNSUPERVISED MANNER . this is a significant disadvantage of the state-of-the-art , as it results in considerable burden imposed on researchers and practitioners , who must resort to TEDIOUS CROSS-VALIDATION PROCEDURES to obtain the optimal number of LATENT FEATURES . to resolve these issues , in this paper we introduce a convolutional <unk> bayesian sparse ica architecture for OVERCOMPLETE FEATURE LEARNING from HIGH-DIMENSIONAL DATA . our method utilizes an INDIAN BUFFET PROCESS prior to facilitate INFERENCE of the appropriate number of LATENT FEATURES under a HYBRID VARIATIONAL INFERENCE ALGORITHM , scalable to massive datasets . as we show , our model can be naturally used to obtain DEEP UNSUPERVISED HIERARCHICAL FEATURE EXTRACTORS , by greedily stacking SUCCESSIVE MODEL LAYERS , similar to existing approaches . in addition , INFERENCE for this model is completely <unk> ; thus , it obviates the need of TEDIOUS PARAMETER TUNING , which is a major challenge most deep learning approaches are faced with . we evaluate our method on several ACTION RECOGNITION BENCHMARKS , and exhibit its advantages over the state-of-the-art . \n",
            "this paper addresses the problem of DEEP UNSUPERVISED HIERARCHICAL FEATURE EXTRACTORS in HIGH-DIMENSIONAL DATA . in this paper , we propose a new HYBRID VARIATIONAL INFERENCE ALGORITHM based on DEEP UNSUPERVISED HIERARCHICAL FEATURE EXTRACTORS . the proposed method is based on the use of LATENT FEATURES in the INDIAN BUFFET PROCESS . the proposed method is based on a HYBRID VARIATIONAL INFERENCE ALGORITHM of the INDIAN BUFFET PROCESS . the proposed method is based on the use of LATENT FEATURES and the LATENT FEATURES . the proposed method is based on a HYBRID VARIATIONAL INFERENCE ALGORITHM . the proposed method is based on a HYBRID VARIATIONAL INFERENCE ALGORITHM . the proposed method is based on the use of TEDIOUS CROSS-VALIDATION PROCEDURES . the proposed method is compared with state-of-the-art UNSUPERVISED FEATURE LEARNING ALGORITHMS .\n",
            "\n",
            "722 1000\n",
            "we experiment with extending a LATTICE PARSING METHODOLOGY for PARSING <unk> -lrb- <unk> and <unk> , 2008 ; <unk> et al. , 2009 -rrb- to make use of a stronger SYNTACTIC MODEL : the PCFG-LA BERKELEY PARSER . we show that the LATTICE PARSING METHODOLOGY is very effective : using a small training set of about <unk> trees , we construct a PARSER which parses and segments UNSEG-MENTED HEBREW TEXT with an F-SCORE of almost 80 % , an ERROR REDUCTION of over 20 % over the best previous result for this task . this result indicates that LATTICE PARSING with the PCFG-LA BERKELEY PARSER is an effective LATTICE PARSING METHODOLOGY for PARSING over UNCERTAIN INPUTS . \n",
            "this paper presents a LATTICE PARSING METHODOLOGY for PARSING . the PARSER is a PARSER , which is a PARSER for PARSING . the SYNTACTIC MODEL is a PARSER , which is based on the SYNTACTIC MODEL . the F-SCORE of the proposed PARSER is compared with the conventional PARSER . the F-SCORE of the proposed PARSER is demonstrated by a LATTICE PARSING METHODOLOGY .\n",
            "\n",
            "723 1000\n",
            "this paper provides an overview of the developments in AUDITORY VISUAL SPEECH PROCESSING , a special interest group within EUROSPEECH . i hope that this discussion will be informative and useful to readers in a variety of fields , including PSYCHOLOGY , SPEECH SCIENCE , ANIMATION , PSYCHOLINGUISTICS , HUMAN-MACHINE INTERACTION , HEARING-IMPAIRED COMMUNICATION , and numerous other fields which also share in this fruitful intersection . \n",
            "this paper presents a system for AUDITORY VISUAL SPEECH PROCESSING , ANIMATION , ANIMATION , ANIMATION , ANIMATION , ANIMATION , and ANIMATION . the system is based on a combination of AUDITORY VISUAL SPEECH PROCESSING , EUROSPEECH , EUROSPEECH , EUROSPEECH , ANIMATION , ANIMATION , and ANIMATION .\n",
            "\n",
            "724 1000\n",
            "in this paper , we study the use of different FREQUENCY WARP-FACTORS for different ACOUSTIC CLASSES in a computationally efficient framework of VOCAL TRACT LENGTH NORMALIZATION . this is motivated by the fact that all ACOUSTIC CLASSES do not exhibit similar spectral variations as a result of PHYSIOLOGICAL DIFFERENCES IN VOCAL TRACT , and therefore , the use of a single FREQUENCY-WARP for the entire utterance may not be appropriate . we have recently proposed a VTLN METHOD that implements VOCAL TRACT LENGTH NORMALIZATION through a LINEAR-TRANSFORMATION of the conventional MFCC FEATURES and efficiently estimates the WARP-FACTOR using the same sufficient statistics as that are used in CMLLR ADAPTATION . in this paper we have shown that , in this framework of VOCAL TRACT LENGTH NORMALIZATION , and using the idea of REGRESSION CLASS TREE , we can obtain separate VOCAL TRACT LENGTH NORMALIZATION for different ACOUSTIC CLASSES . the use of REGRESSION CLASS TREE ensures that WARP-FACTOR is estimated for each class even when there is very little data available for that class . the ACOUSTIC CLASSES , in general , can be any collection of the GAUSSIAN COMPONENTS in the ACOUSTIC MODEL . we have built ACOUSTIC CLASSES by using DATA-DRIVEN APPROACH and by using PHONETIC KNOWLEDGE . using WSJ DATABASE we have shown the RECOGNITION performance of the proposed ACOUSTIC CLASS SPECIFIC WARP-FACTOR both for the data driven and the PHONETIC KNOWLEDGE BASED REGRESSION CLASS TREE DEFINITIONS and compare VTLN METHOD with the case of the single WARP-FACTOR . \n",
            "this paper presents a DATA-DRIVEN APPROACH for VOCAL TRACT LENGTH NORMALIZATION . the ACOUSTIC MODEL is based on a DATA-DRIVEN APPROACH of the PHYSIOLOGICAL DIFFERENCES IN VOCAL TRACT . the ACOUSTIC MODEL is based on a REGRESSION CLASS TREE . the ACOUSTIC MODEL is based on a REGRESSION CLASS TREE , which is based on the ACOUSTIC CLASS SPECIFIC WARP-FACTOR . the proposed VTLN METHOD is based on a DATA-DRIVEN APPROACH . the proposed DATA-DRIVEN APPROACH is based on a DATA-DRIVEN APPROACH and is shown to be more robust to ACOUSTIC CLASSES than the conventional LINEAR-TRANSFORMATION .\n",
            "\n",
            "725 1000\n",
            "<unk> , the newly standardized <unk> codec for ENHANCED VOICE SERVICES was developed for MOBILE SERVICES such as VOLTE , where ERROR RESILIENCE is highly essential . the presented paper outlines all aspects of the advances brought during the EVS development on PACKET LOSS CONCEALMENT , by presenting a high level description of all TECHNICAL FEATURES present in the final STANDARDIZED CODEC . coupled with JITTER BUFFER MANAGEMENT , the ENHANCED VOICE SERVICES provides ROBUSTNESS against late or lost packets . the advantages of the new ENHANCED VOICE SERVICES over REFERENCE CODECS are further discussed based on listening test results . \n",
            "this paper addresses the problem of PACKET LOSS CONCEALMENT in MOBILE SERVICES such as PACKET LOSS CONCEALMENT . we propose a method for PACKET LOSS CONCEALMENT based on PACKET LOSS CONCEALMENT . the proposed method is based on the use of TECHNICAL FEATURES such as VOLTE , and the VOLTE . the proposed method is compared with state-of-the-art ENHANCED VOICE SERVICES such as the STANDARDIZED CODEC , and the ROBUSTNESS of the proposed method .\n",
            "\n",
            "726 1000\n",
            "we present a technique for coarsely extracting the regions of NATURAL COLOR IMAGES which contain DIRECTIONAL DETAIL , e.g. , EDGES , TEXTURE , etc. , which we then use for IMAGE DATABASE INDEXING . as a measure of COLOR ACTIVITY , we use a PERCEPTUALLY MODIFIED DISTANCE MEASURE based on the SUM-OF-ANGLES CRITERION . we then apply HISTOGRAM THRESHOLDING TECHNIQUES to separate the image into smooth color regions and busy regions where EDGE , TEXTURE and colour activity exists . DATABASE INDICES are then created from the busy regions using the DIRECIONAL DETAIL HISTOGRAM TECHNIQUE and RETRIEVAL is performed using these . \n",
            "this paper addresses the problem of RETRIEVAL in NATURAL COLOR IMAGES . we propose a PERCEPTUALLY MODIFIED DISTANCE MEASURE based on the PERCEPTUALLY MODIFIED DISTANCE MEASURE of the EDGE and the EDGE . the proposed DIRECIONAL DETAIL HISTOGRAM TECHNIQUE is based on the PERCEPTUALLY MODIFIED DISTANCE MEASURE of the SUM-OF-ANGLES CRITERION and the DIRECTIONAL DETAIL . the proposed DIRECIONAL DETAIL HISTOGRAM TECHNIQUE is based on the PERCEPTUALLY MODIFIED DISTANCE MEASURE of the EDGE and the EDGE . the proposed DIRECIONAL DETAIL HISTOGRAM TECHNIQUE is evaluated on the IMAGE DATABASE INDEXING .\n",
            "\n",
            "727 1000\n",
            "indoor FUNCTIONAL OBJECTS exhibit large view and appearance variations , thus are difficult to be recognized by the traditional APPEARANCE-BASED CLASSIFICATION PARADIGM . in this paper , we present an algorithm to parse INDOOR IMAGES based on two observations : i -rrb- the functionality is the most essential property to define an INDOOR OBJECT , e.g. '' a CHAIR to sit on '' ; ii -rrb- the GEOMETRY -LRB- 3D SHAPE -rrb- of an object is designed to serve its function . we formulate the nature of the OBJECT FUNCTION into a STOCHASTIC GRAMMAR MODEL . this model characterizes a JOINT DISTRIBUTION over the FUNCTION-GEOMETRY-APPEARANCE HIERARCHY . the JOINT DISTRIBUTION includes a SCENE CATEGORY , FUNCTIONAL GROUPS , FUNCTIONAL OBJECTS , FUNCTIONAL PARTS and 3D GEOMETRIC SHAPES . we use a SIMULATED ANNEALING MCMC ALGORITHM to find the maximum a POSTERIORI SOLUTION , i.e. a PARSE TREE . we design four DATA-DRIVEN STEPS to accelerate the search in the FGA SPACE : i -rrb- group the LINE SEGMENTS into 3D PRIMITIVE SHAPES , ii -rrb- assign FUNCTIONAL LABELS to these 3D PRIMITIVE SHAPES , iii -rrb- fill in MISSING OBJECTS/PARTS according to the FUNCTIONAL LABELS , and iv -rrb- synthesize 2D SEGMENTATION MAPS and verify the current PARSE TREE by the METROPOLIS-HASTINGS ACCEPTANCE PROBABILITY . the experimental results on several challenging INDOOR DATASETS demonstrate the proposed approach not only significantly <unk> the scope of INDOOR SCENE PARSING ALGORITHM from the SEGMENTATION and the 3D RECOVERY to the FUNCTIONAL OBJECT RECOGNITION , but also yields improved overall performance . \n",
            "this paper addresses the problem of 3D RECOVERY in INDOOR IMAGES . we propose a POSTERIORI SOLUTION for FUNCTIONAL OBJECT RECOGNITION , which is based on a STOCHASTIC GRAMMAR MODEL and a POSTERIORI SOLUTION . the SEGMENTATION is formulated as a JOINT DISTRIBUTION , which is a POSTERIORI SOLUTION of the PARSE TREE . the proposed APPEARANCE-BASED CLASSIFICATION PARADIGM is based on a POSTERIORI SOLUTION and a POSTERIORI SOLUTION . the proposed APPEARANCE-BASED CLASSIFICATION PARADIGM is based on a POSTERIORI SOLUTION and a POSTERIORI SOLUTION . experimental results on INDOOR DATASETS show that the proposed APPEARANCE-BASED CLASSIFICATION PARADIGM is robust and robust to 3D RECOVERY and 3D RECOVERY .\n",
            "\n",
            "728 1000\n",
            "bagging is one the most classic ENSEMBLE LEARNING TECHNIQUES in the MACHINE LEARNING LITERATURE . the idea is to generate multiple subsets of the training data via BOOTSTRAPPING -LRB- RANDOM SAMPLING with replacement -rrb- , and then aggregate the output of the models trained from each subset via voting or averaging . as music is a TEMPORAL SIGNAL , we propose and study two BAGGING METHODS in this paper : the INTER-SONG INSTANCE BAGGING that BOOTSTRAPS SONG-LEVEL FEATURES , and the INTRA-SONG INSTANCE BAGGING that draws bootstrapping samples directly from SHORT-TIME FEATURES for each training song . in particular , we focus on the latter method , as BAGGING METHODS better exploits the TEMPORAL INFORMATION OF MUSIC SIGNALS . the BAGGING METHODS result in surprisingly effective models for MUSIC AUTO-TAGGING : incorporating the idea to a simple linear support vector machine -lrb- svm -rrb- based system yields ACCURACIES that are comparable or even superior to state-of-the-art , possibly more sophisticated methods for three different datasets . as the BAGGING METHODS is a META ALGORITHM , BAGGING METHODS holds the promise of improving other MIR SYSTEMS . \n",
            "this paper addresses the problem of TEMPORAL INFORMATION OF MUSIC SIGNALS in MACHINE LEARNING LITERATURE . in this paper , we propose a new META ALGORITHM based on BOOTSTRAPPING -LRB- RANDOM SAMPLING . the proposed META ALGORITHM is based on the use of SHORT-TIME FEATURES and the META ALGORITHM . the proposed META ALGORITHM is based on the use of SHORT-TIME FEATURES , which are then used to estimate the TEMPORAL SIGNAL . experimental results show the effectiveness of the proposed META ALGORITHM .\n",
            "\n",
            "729 1000\n",
            "in this paper , a FINGERPRINT EMBEDDING METHOD <unk> for the AND ANTI-COLLUSION CODE is proposed . the proposed FINGERPRINT EMBEDDING METHOD embeds both a CODE and an ORTHOGONAL FINGERPRINT using different BASIS VECTORS depending on the bit . although the DETECTION for the EMBEDDING METHOD is complex , the performance of the FINGERPRINTING SYSTEM using proposed EMBEDDING METHOD with the AND-ACC against average attack is improved compared with the AND ANTI-COLLUSION CODE using CODE MODULATION EMBEDDING METHOD . the FINGERPRINTING SYSTEM using the proposed EMBEDDING METHOD is robust against the LINEAR COMBINATION COLLUSION ATTACK whereas the FINGERPRINTING SYSTEM using the CODE MODULATION is not . \n",
            "this paper presents a FINGERPRINT EMBEDDING METHOD for DETECTION . the FINGERPRINT EMBEDDING METHOD is based on a LINEAR COMBINATION COLLUSION ATTACK and a LINEAR COMBINATION COLLUSION ATTACK for DETECTION . the FINGERPRINT EMBEDDING METHOD is based on the FINGERPRINT EMBEDDING METHOD and the FINGERPRINT EMBEDDING METHOD . the EMBEDDING METHOD is based on the FINGERPRINT EMBEDDING METHOD and the FINGERPRINT EMBEDDING METHOD . the EMBEDDING METHOD is based on the FINGERPRINT EMBEDDING METHOD and the FINGERPRINT EMBEDDING METHOD . the FINGERPRINT EMBEDDING METHOD is applied to the FINGERPRINTING SYSTEM and the FINGERPRINT EMBEDDING METHOD .\n",
            "\n",
            "730 1000\n",
            "story generation is <unk> a <unk> , despite disappointing preliminary results from the preceding three decades . one of the principle reasons for previous <unk> was the low level of WRITING QUALITY , which resulted from the excessive focus of STORY GRAMMARS on PLOT DESIGN . although these systems leveraged NARRATIVE THEORY via CORPORA ANALYSES , they failed to thoroughly extend those analyses to all relevant linguistic levels . the end result was narratives that were recognizable as stories , but whose PROSE QUALITY was unsatisfactory . however , the <unk> for POOR WRITING QUALITY can not be laid <unk> at the feet of STORY GRAMMARS , as NATURAL LANGUAGE GENERATION has to-date not fielded systems capable of faithfully reproducing either the variety or COMPLEXITY of naturally occurring stories . this paper presents the AUTHOR ARCHITECTURE for accomplishing precisely that task , the <unk> implementation of a NARRATIVE PROSE GENERATOR , and a brief description of a formal evaluation of the stories it produces . \n",
            "this paper addresses the problem of STORY GENERATION in NATURAL LANGUAGE GENERATION . in this paper , we propose a new method for STORY GENERATION based on STORY GRAMMARS . the proposed method is based on the use of STORY GRAMMARS , which is based on the NARRATIVE PROSE GENERATOR . the proposed method is based on the NARRATIVE PROSE GENERATOR . the proposed method is based on the NARRATIVE PROSE GENERATOR . the proposed method is based on the use of NARRATIVE THEORY . the experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "731 1000\n",
            "two new approaches to ADAPTIVE BEAMFORMING in SPARSE SUBARRAY-BASED SENSOR ARRAYS are proposed . each <unk> is assumed to be well calibrated but the INTERSUBARRAY GAIN AND/OR PHASE MISMATCHES are assumed to remain unknown or imperfectly known . our first approach is based on a WORST-CASE BEAMFORMER DESIGN that , unlike the existing WORST-CASE DESIGNS , exploits a STRUCTURED ELLIPSOIDAL UNCERTAINTY MODEL for the SIGNAL STEERING VECTOR . our second approach exploits the idea of estimating the SIGNAL STEERING VECTOR by maximizing the output power of the MINIMUM VARIANCE BEAMFORMER . several modifications of our second approach are developed for the cases of GAIN-AND-PHASE AND PHASE-ONLY INTERSUBARRAY DISTORTIONS . \n",
            "this paper addresses the problem of INTERSUBARRAY GAIN AND/OR PHASE MISMATCHES in SPARSE SUBARRAY-BASED SENSOR ARRAYS . we propose a STRUCTURED ELLIPSOIDAL UNCERTAINTY MODEL based on the MINIMUM VARIANCE BEAMFORMER . the proposed method is based on the MINIMUM VARIANCE BEAMFORMER . the proposed method is based on the MINIMUM VARIANCE BEAMFORMER . the proposed method is based on the MINIMUM VARIANCE BEAMFORMER . the proposed method is based on the MINIMUM VARIANCE BEAMFORMER .\n",
            "\n",
            "732 1000\n",
            "large-scale distributed computing has made available the resources necessary to solve '' <unk> '' problems . as a result , it becomes feasible to automate the processing of such problems , but ACCURACY is not very high due to the conceptual difficulty of these problems . in this paper , we integrated CROWDSOURCING with MAPREDUCE to provide a scalable innovative HUMAN-MACHINE SOLUTION to AI-HARD PROBLEMS , which is called CROWDMR . in CROWDMR , the majority of problem instances are automatically processed by machine while the troublesome instances are <unk> to human via CROWDSOURCING . the results returned from CROWDSOURCING are validated in the form of CAPTCHA -LRB- COMPLETELY AUTOMATED PUBLIC TURING TEST to tell computers and humans apart -rrb- before adding to the output . an INCREMENTAL SCHEDULING METHOD was brought forward to combine the results from machine and human in a '' <unk> '' way . \n",
            "this paper presents a new INCREMENTAL SCHEDULING METHOD for LARGE-SCALE DISTRIBUTED COMPUTING . the INCREMENTAL SCHEDULING METHOD is based on the INCREMENTAL SCHEDULING METHOD , which is based on the INCREMENTAL SCHEDULING METHOD . the INCREMENTAL SCHEDULING METHOD is based on the INCREMENTAL SCHEDULING METHOD and the INCREMENTAL SCHEDULING METHOD . the ACCURACY of the proposed INCREMENTAL SCHEDULING METHOD is compared with the conventional INCREMENTAL SCHEDULING METHOD .\n",
            "\n",
            "733 1000\n",
            "a common assumption in MACHINE VISION is that the training and test samples are drawn from the same DISTRIBUTION . however , there are many problems when this assumption is <unk> violated , as in BIO-MEDICAL APPLICATIONS where different <unk> can generate drastic variations in the appearance of the data due to changing experimental conditions . this MACHINE VISION is <unk> with 3D DATA , for which ANNOTATION is very time-consuming , limiting the amount of data that can be labeled in new <unk> for training . in this paper we present a MULTI-TASK LEARNING ALGORITHM for DOMAIN ADAPTATION based on BOOSTING . unlike previous approaches that learn TASK-SPECIFIC DECISION BOUNDARIES , our MULTI-TASK LEARNING ALGORITHM learns a single DECISION BOUNDARY in a SHARED FEATURE SPACE , common to all tasks . we use the BOOSTING-TRICK to learn a NON-LINEAR MAPPING of the observations in each task , with no need for specific a-priori knowledge of its GLOBAL ANALYTICAL FORM . this yields a more PARAMETER-FREE DOMAIN ADAPTATION APPROACH that successfully leverages learning on new tasks where LABELED DATA is scarce . we evaluate our MULTI-TASK LEARNING ALGORITHM on two challenging BIO-MEDICAL DATASETS and achieve a significant improvement over the state of the art . \n",
            "this paper presents a PARAMETER-FREE DOMAIN ADAPTATION APPROACH for DOMAIN ADAPTATION . the MULTI-TASK LEARNING ALGORITHM is based on the MULTI-TASK LEARNING ALGORITHM and the MULTI-TASK LEARNING ALGORITHM . the MULTI-TASK LEARNING ALGORITHM is based on the MULTI-TASK LEARNING ALGORITHM and the MULTI-TASK LEARNING ALGORITHM . the proposed MULTI-TASK LEARNING ALGORITHM is based on the MULTI-TASK LEARNING ALGORITHM . the proposed MULTI-TASK LEARNING ALGORITHM is based on the MULTI-TASK LEARNING ALGORITHM . the proposed PARAMETER-FREE DOMAIN ADAPTATION APPROACH is based on the MULTI-TASK LEARNING ALGORITHM . the proposed MULTI-TASK LEARNING ALGORITHM is based on the MULTI-TASK LEARNING ALGORITHM . the proposed PARAMETER-FREE DOMAIN ADAPTATION APPROACH is based on the MULTI-TASK LEARNING ALGORITHM . the proposed MULTI-TASK LEARNING ALGORITHM is based on the MULTI-TASK LEARNING ALGORITHM . the proposed PARAMETER-FREE DOMAIN ADAPTATION APPROACH is based on the MULTI-TASK LEARNING ALGORITHM . the proposed MULTI-TASK LEARNING ALGORITHM is evaluated on BIO-MEDICAL DATASETS and on BIO-MEDICAL DATASETS .\n",
            "\n",
            "734 1000\n",
            "we describe a memory-efficient implementation of a DYNAMIC PROGRAMMING ALGORITHM for learning the optimal structure of a BAYESIAN NETWORK from training data . the DYNAMIC PROGRAMMING ALGORITHM leverages the LAYERED STRUCTURE of the DYNAMIC PROGRAMMING GRAPHS representing the RE-CURSIVE DECOMPOSITION of the problem to reduce the MEMORY REQUIREMENTS of the DYNAMIC PROGRAMMING ALGORITHM from o -lrb- n2 n -rrb- to o -lrb- c -lrb- n , <unk> -rrb- -rrb- , where c -lrb- n , <unk> -rrb- is the BINOMIAL COEFFICIENT . experimental results show that the DYNAMIC PROGRAMMING ALGORITHM runs up to an order of magnitude faster and scales to datasets with more variables than previous approaches . \n",
            "this paper addresses the problem of RE-CURSIVE DECOMPOSITION for DYNAMIC PROGRAMMING GRAPHS with DYNAMIC PROGRAMMING GRAPHS . the DYNAMIC PROGRAMMING ALGORITHM is based on a BAYESIAN NETWORK with a BAYESIAN NETWORK . the proposed DYNAMIC PROGRAMMING ALGORITHM is based on the RE-CURSIVE DECOMPOSITION with DYNAMIC PROGRAMMING GRAPHS .\n",
            "\n",
            "735 1000\n",
            "<unk> are a type of <unk> -lcb- like structures of a d dimensional image , characterized by LOCAL CONDITIONS . as CREASES tend to be at the center of ANISOTROPIC GREY -LCB- LEVEL SHAPES , CREASENESS can be considered as a type of MEDIALNESS . among the several CREASE DEENITIONS , one of the most important is based on the EXTREMA OF THE LEVEL SET CURVATURES . in 2 -lcb- d it is used the CURVATURE OF THE LEVEL CURVES of the IMAGE LANDSCAPE , however , the way it is usually computed produces a DISCON-TINUOUS CREASENESS MEASURE . the same problem arises in 3 -lcb- d with its straightforward extension and with other related CREASENESS MEASURES . in this paper , we rst present an alternative method of computing the LEVEL CURVE CURVATURE that avoids the DISCONTINUITIES . next , we propose the MEAN CURVATURE OF THE LEVEL SURFACES as CREASENESS MEASURE of 3 -lcb- d images , computed by the same method . finally , we propose a natural extension of our rst alternative method in order to enhance the CREASENESS MEASURE . \n",
            "this paper presents a method for ANISOTROPIC GREY -LCB- LEVEL SHAPES in ANISOTROPIC GREY -LCB- LEVEL SHAPES . the proposed method is based on the CURVATURE OF THE LEVEL CURVES of the DISCON-TINUOUS CREASENESS MEASURE of the IMAGE LANDSCAPE . the proposed method is based on the CURVATURE OF THE LEVEL CURVES of the IMAGE LANDSCAPE . the proposed method is based on the DISCON-TINUOUS CREASENESS MEASURE of the CREASENESS MEASURE of the CREASENESS MEASURE . the proposed method is based on the EXTREMA OF THE LEVEL SET CURVATURES and the CURVATURE OF THE LEVEL CURVES . the proposed method is compared with conventional CREASENESS MEASURES .\n",
            "\n",
            "736 1000\n",
            "foreign accents in SECOND LANGUAGE PRODUCTION are caused by interference from the PHONOLOGICAL SYSTEM and phonetic realization of the speaker 's first language -lrb- l1 -rrb- , including both SEGMENTAL AND PROSODIC FEATURES . this paper examines the intonation structure of SEOUL KOREAN and its realization by AMERICAN ENGLISH SPEAKERS . four ENGLISH SPEAKERS OF KOREAN , differing in fluency , and two KOREAN SPEAKERS participated in the experiment . <unk> sentences were designed to test the realization of intonation patterns by varying the number of syllables within a word and a sentence , and by varying the conditions for the SEGMENT-TONE INTERACTION . results show that , as with SEGMENTAL DATA , more advanced L2 SPEAKERS produce more NATIVE-LIKE INTONATION PATTERNS and PROSODIC STRUCTURE than less advanced speakers . however , although advanced L2 SPEAKERS are better at grouping words into phrases , L2 SPEAKERS are not better at producing SURFACE TONAL REALIZATIONS of an accentual phrase than less advanced speakers . this suggests that PHONOLOGICAL PROPERTIES OF INTONATION are acquired earlier than PHONETIC PROPERTIES OF INTONATION . \n",
            "this paper addresses the problem of SECOND LANGUAGE PRODUCTION in KOREAN SPEAKERS . we propose a method for SECOND LANGUAGE PRODUCTION based on SEGMENTAL DATA . the proposed method is based on the PHONETIC PROPERTIES OF INTONATION and the PHONETIC PROPERTIES OF INTONATION . the proposed method is based on the use of SEGMENTAL AND PROSODIC FEATURES and SURFACE TONAL REALIZATIONS . the proposed method is compared with the conventional PHONOLOGICAL SYSTEM and the PHONOLOGICAL SYSTEM . the results show that the proposed method outperforms the conventional PHONOLOGICAL SYSTEM in terms of SEGMENT-TONE INTERACTION and SEGMENT-TONE INTERACTION .\n",
            "\n",
            "737 1000\n",
            "an analysis of the LOCAL CONVERGENCE SPEED of CONSTANT GAIN ALGORITHMS for DIRECT FORM IIR ADAPTIVE ¯ LTERS is initially presented , showing the ADVERSE E ® ECTS that result from the proximity of the poles of the MODELLED SYSTEM to the UNIT CIRCLE and , for COMPLEX POLES , to the REAL AXIS . a global analysis of the REDUCED ERROR SURFACE in these cases is also presented , which shows that , away from the global minimum , there will be regions with an almost constant error , where the convergence of CONSTANT GAIN ALGORITHMS tends to be slow . a POLYPHASE IIR ADAPTIVE ¯ LTER is then proposed and its LOCAL AND GLOBAL CONVERGENCE PROPERTIES are investigated , showing POLYPHASE IIR ADAPTIVE ¯ LTER to be specially well suited for applications with UNDERDAMPED LOW-FREQUENCY POLES . the POLYPHASE IIR ADAPTIVE ¯ LTER is tested with di ® <unk> CONSTANT GAIN ALGORITHMS in an <unk> example , attaining a gain of 14 to 70 times in GLOBAL CONVERGENCE SPEED over the DIRECT FORM , at the price of a relatively modest increase in COMPUTATIONAL COMPLEXITY . a theorem concerning the existence of stationary points for the POLYPHASE IIR ADAPTIVE ¯ LTER is also presented . \n",
            "this paper proposes a new method for DIRECT FORM IIR ADAPTIVE ¯ LTERS based on the POLYPHASE IIR ADAPTIVE ¯ LTER . the proposed method is based on the POLYPHASE IIR ADAPTIVE ¯ LTER . the proposed method is based on the POLYPHASE IIR ADAPTIVE ¯ LTER . the proposed method is based on the POLYPHASE IIR ADAPTIVE ¯ LTER . the proposed method is based on the POLYPHASE IIR ADAPTIVE ¯ LTER . the proposed method is based on the POLYPHASE IIR ADAPTIVE ¯ LTER . the proposed method is based on the POLYPHASE IIR ADAPTIVE ¯ LTER . the proposed method is based on the POLYPHASE IIR ADAPTIVE ¯ LTER . the proposed method is based on the POLYPHASE IIR ADAPTIVE ¯ LTER . the proposed method is compared with the conventional CONSTANT GAIN ALGORITHMS .\n",
            "\n",
            "738 1000\n",
            "accurate localization of multiple sound sources is indispensable for the MICROPHONE ARRAY-BASED HIGH QUALITY SOUND CAPTURE . for SINGLE SOUND SOURCE LOCALIZATION , the CSP -LRB- CROSS-POWER SPECTRUM PHASE ANALYSIS -RRB- METHOD has been proposed . the CSP -LRB- CROSS-POWER SPECTRUM PHASE ANALYSIS -RRB- METHOD localizes a SOUND SOURCE as a crossing point of sound directions estimated using DIER-ENT MICROPHONE PAIRS . however , when localizing multiple sound sources , the CSP -LRB- CROSS-POWER SPECTRUM PHASE ANALYSIS -RRB- METHOD has a problem that the LOCALIZATION ACCURACY is degraded due to cross-correlation among dierent sound sources . to solve this problem , this paper proposes a new method which suppresses the UN-DESIRED CROSS-CORRELATION by SYNCHRONOUS ADDITION of CSP COECIENTS derived from multiple microphone pairs . experiment results in a real room showed that the proposed method improves the LOCALIZATION ACCURACY when increasing the number of the SYNCHRONOUS ADDITION . \n",
            "this paper presents a novel CSP -LRB- CROSS-POWER SPECTRUM PHASE ANALYSIS -RRB- METHOD for SINGLE SOUND SOURCE LOCALIZATION . the CSP -LRB- CROSS-POWER SPECTRUM PHASE ANALYSIS -RRB- METHOD is based on the CSP -LRB- CROSS-POWER SPECTRUM PHASE ANALYSIS -RRB- METHOD . the proposed CSP -LRB- CROSS-POWER SPECTRUM PHASE ANALYSIS -RRB- METHOD is based on the CSP -LRB- CROSS-POWER SPECTRUM PHASE ANALYSIS -RRB- METHOD . the proposed CSP -LRB- CROSS-POWER SPECTRUM PHASE ANALYSIS -RRB- METHOD is based on the CSP -LRB- CROSS-POWER SPECTRUM PHASE ANALYSIS -RRB- METHOD . the proposed CSP -LRB- CROSS-POWER SPECTRUM PHASE ANALYSIS -RRB- METHOD is based on the CSP -LRB- CROSS-POWER SPECTRUM PHASE ANALYSIS -RRB- METHOD . the proposed CSP -LRB- CROSS-POWER SPECTRUM PHASE ANALYSIS -RRB- METHOD is based on the CSP -LRB- CROSS-POWER SPECTRUM PHASE ANALYSIS -RRB- METHOD and the CSP -LRB- CROSS-POWER SPECTRUM PHASE ANALYSIS -RRB- METHOD .\n",
            "\n",
            "739 1000\n",
            "we present a POLICY SEARCH METHOD that uses iteratively <unk> LOCAL LINEAR MODELS to optimize TRAJECTORY DISTRIBUTIONS for large , continuous problems . these TRAJECTORY DISTRIBUTIONS can be used within the framework of GUIDED POLICY SEARCH to learn POLICIES with an ARBITRARY PARAMETERIZATION . our POLICY SEARCH METHOD fits TIME-VARYING LINEAR DYNAMICS MODELS to speed up LEARNING , but does not rely on LEARNING a GLOBAL MODEL , which can be difficult when the dynamics are complex and discontinuous . we show that this POLICY SEARCH METHOD requires many fewer samples than MODEL-FREE METHODS , and can handle complex , NONSMOOTH DYNAMICS that can pose a challenge for MODEL-BASED TECHNIQUES . we present experiments showing that our POLICY SEARCH METHOD can be used to learn complex NEURAL NETWORK POLICIES that successfully execute SIMULATED ROBOTIC MANIPULATION TASKS in partially observed environments with numerous CONTACT DISCONTINUITIES and UNDERACTUATION . \n",
            "this paper presents a new POLICY SEARCH METHOD for SIMULATED ROBOTIC MANIPULATION TASKS . the POLICY SEARCH METHOD is based on a GLOBAL MODEL and a GLOBAL MODEL for LEARNING . the POLICY SEARCH METHOD is based on a GLOBAL MODEL and a GLOBAL MODEL for LEARNING . the proposed POLICY SEARCH METHOD is based on a GLOBAL MODEL , which is based on the GLOBAL MODEL . the proposed POLICY SEARCH METHOD is based on a POLICY SEARCH METHOD and is shown to be robust to CONTACT DISCONTINUITIES and CONTACT DISCONTINUITIES . the proposed POLICY SEARCH METHOD is evaluated on SIMULATED ROBOTIC MANIPULATION TASKS and SIMULATED ROBOTIC MANIPULATION TASKS .\n",
            "\n",
            "740 1000\n",
            "classification using HIDDEN MARKOV MODELS is in general done by comparing the MODEL LIKELIHOODS and choosing the class more likely to have generated the data . this work investigates a CONDITIONED HMM which additionally provides a probability for a CLASS LABEL and compares different FUSION STRATEGIES . the notion is twofold : on the one hand applications in AFFEC-TIVE COMPUTING might pass their uncertainty of the CLASSIFICATION to the next PROCESSING UNIT , on the other hand different streams might be fused to increase the performance . the data set studied incorporates two modalities and is based on a NATURALISTIC MUL-TIPARTY DIALOGUE . the goal is to discriminate between laughter and utterances . it turned out that the CONDITIONED HMM out-performs CLASSICAL HMM using different LATE FUSION APPROACHES while additionally providing a certainty about CLASS DECISION . \n",
            "this paper addresses the problem of AFFEC-TIVE COMPUTING in NATURALISTIC MUL-TIPARTY DIALOGUE . we propose a method for AFFEC-TIVE COMPUTING based on HIDDEN MARKOV MODELS . the proposed method is based on the use of HIDDEN MARKOV MODELS , which is based on the CONDITIONED HMM . the proposed method is based on the CONDITIONED HMM . the proposed method is based on the use of HIDDEN MARKOV MODELS . the proposed method is based on the CONDITIONED HMM . the proposed method is based on the CONDITIONED HMM . the proposed method is based on the use of HIDDEN MARKOV MODELS . the proposed method is based on the use of HIDDEN MARKOV MODELS . the proposed method is based on the use of HIDDEN MARKOV MODELS . the proposed method is compared with other state-of-the-art LATE FUSION APPROACHES .\n",
            "\n",
            "741 1000\n",
            "over the years , several SPATIO-TEMPORAL INTEREST POINT DETECTORS have been proposed . while some detectors can only extract a sparse set of SCALE-INVARIANT FEATURES , others allow for the detection of a larger amount of FEATURES at USER-DEFINED SCALES . this paper presents for the first time SPATIO-TEMPORAL INTEREST POINTS that are at the same time <unk> -lrb- both spatially and temporally -rrb- and densely cover the VIDEO CONTENT . moreover , as opposed to earlier work , the FEATURES can be computed efficiently . applying SCALE-SPACE THEORY , we show that FEATURES can be achieved by using the determinant of the HESSIAN as the SALIENCY MEASURE . computations are <unk> further through the use of APPROXIMATIVE BOX-FILTER OPERATIONS on an INTEGRAL VIDEO STRUCTURE . a quantitative evaluation and experimental results on ACTION RECOGNITION show the strengths of the proposed detector in terms of repeatability , ACCURACY and SPEED , in comparison with previously proposed detectors . \n",
            "this paper addresses the problem of ACTION RECOGNITION in VIDEO CONTENT . we propose a method for ACTION RECOGNITION based on SCALE-SPACE THEORY . the method is based on the SALIENCY MEASURE of the SPATIO-TEMPORAL INTEREST POINTS and the HESSIAN . the proposed method is based on the SALIENCY MEASURE of the SPATIO-TEMPORAL INTEREST POINTS and the HESSIAN . the proposed method is based on the SALIENCY MEASURE of the SCALE-SPACE THEORY and the SALIENCY MEASURE of the SCALE-SPACE THEORY .\n",
            "\n",
            "742 1000\n",
            "we present a novel method for analyzing reflections on ARBITRARY SURFACES . we model reflections using a broader than usual class of IMAGING MODELS , which include both PERSPECTIVE AND MULTIPERSPECTIVE CAMERA TYPES . we provide an ANALYTICAL FRAMEWORK to LOCALLY MODEL REFLECTIONS as specific MULTIPERSPECTIVE CAMERAS around every ray based on a new theory of GENERAL LINEAR CAMERAS . our ANALYTICAL FRAMEWORK better characterizes the complicated IMAGE DISTORTIONS seen on IRREGULAR MIRROR SURFACES as well as the conventional CATA-DIOPTRIC MIRRORS . we show the connection between MULTIPER-SPECTIVE CAMERA MODELS and CAUSTIC SURFACES OF REFLECTIONS and demonstrate how MULTIPER-SPECTIVE CAMERA MODELS reveal important surface <unk> of the <unk> . finally , we show how to use our ANALYTICAL FRAMEWORK to assist MIRROR DESIGN and characterize distortions seen in CATADIOPTRIC IMAGING SYSTEMS . \n",
            "this paper presents a novel ANALYTICAL FRAMEWORK for CATADIOPTRIC IMAGING SYSTEMS . the ANALYTICAL FRAMEWORK is based on the ANALYTICAL FRAMEWORK and the ANALYTICAL FRAMEWORK . the ANALYTICAL FRAMEWORK is based on the ANALYTICAL FRAMEWORK and the ANALYTICAL FRAMEWORK . the proposed ANALYTICAL FRAMEWORK is based on the ANALYTICAL FRAMEWORK and the ANALYTICAL FRAMEWORK . the proposed ANALYTICAL FRAMEWORK is based on the ANALYTICAL FRAMEWORK and the ANALYTICAL FRAMEWORK . experimental results show that the proposed ANALYTICAL FRAMEWORK is robust and robust to ARBITRARY SURFACES and ARBITRARY SURFACES .\n",
            "\n",
            "743 1000\n",
            "a REALTIME SOFTWARE MPEG TRANSCODER has been developed . a novel MOTION VECTOR REUSE and a SIMD OPTIMIZATION TECHNIQUES are introduced to accelerate the <unk> without any QUALITY DEGRADATION . MEAN ABSOLUTE ERROR APPROXIMATION CRITERIA are employed in the REUSE TECHNIQUE to refine SCALED MOTION VECTORS . the developed <unk> on PENTIUM II 266MHZ runs 2.5 times as fast as REALTIME , when scaling an MPEG-1 BITSTREAM to half size . \n",
            "this paper presents a new REUSE TECHNIQUE for MOTION VECTOR REUSE . the proposed REUSE TECHNIQUE is based on the MEAN ABSOLUTE ERROR APPROXIMATION CRITERIA of the MPEG-1 BITSTREAM and the SCALED MOTION VECTORS . the proposed REUSE TECHNIQUE is based on the MEAN ABSOLUTE ERROR APPROXIMATION CRITERIA of the SCALED MOTION VECTORS and the QUALITY DEGRADATION . the proposed REUSE TECHNIQUE is based on the MEAN ABSOLUTE ERROR APPROXIMATION CRITERIA of the MPEG-1 BITSTREAM and the QUALITY DEGRADATION .\n",
            "\n",
            "744 1000\n",
            "to realize a CONVERSATIONAL INTERFACE where an AGENT SYSTEM can smoothly communicate with multiple persons , it is imperative to know how the START TIMING of speaking is decided . in this research , we demonstrate a relationship between GAZE TRANSITION PATTERNS and the START TIMING of next speaking against the end of the last speaking in MULTI-PARTY MEETINGS . then , we construct a PREDICTION MODEL for the START TIMING using GAZE TRANSITION PATTERNS near the end of an utterance . an analysis of data collected from NATURAL MULTI-PARTY MEETINGS reveals a strong relationship between GAZE TRANSITION PATTERNS of the speaker , next speaker , and listener and the START TIMING of the next speaker . on the basis of the results , we used GAZE TRANSITION PATTERNS of the speaker , next speaker , and listener and mutual gaze as variables , and devised several PREDICTION MODELS . a PREDICTION MODEL using all FEATURES performed the best and was able to predict the START TIMING well . \n",
            "this paper presents a PREDICTION MODEL for MULTI-PARTY MEETINGS . the PREDICTION MODEL is based on the PREDICTION MODEL . the PREDICTION MODEL is based on the PREDICTION MODEL . the PREDICTION MODEL is based on the PREDICTION MODEL . the PREDICTION MODEL is based on the PREDICTION MODEL . the proposed PREDICTION MODEL is based on the PREDICTION MODEL . the proposed PREDICTION MODEL is based on the PREDICTION MODEL and is shown to be useful for MULTI-PARTY MEETINGS .\n",
            "\n",
            "745 1000\n",
            "this paper develops a STATISTICAL INFERENCE APPROACH , STATISTICAL INFERENCE APPROACH , for STYLE TRANSFORMATION between PHOTO IMAGES and SKETCH IMAGES OF HUMAN FACES . motivated by the rationale that IMAGE APPEARANCE is determined by two COOPERATIVE FACTORS : IMAGE CONTENT and IMAGE STYLE , we first model the interaction between these factors through learning a PATCH-BASED TENSOR MODEL . second , by introducing a COMMON VARIATION SPACE , we capture the inherent connection between PHOTO PATCH SPACE and SKETCH PATCH SPACE , thus building BIDIRECTIONAL MAPPING/INFERRING between the two spaces . subsequently , we formulate a BAYESIAN APPROACH accounting for the STATISTICAL INFERENCE from sketches to their corresponding photos in terms of the learned TENSOR MODEL . comparative experiments are conducted to contrast the proposed BAYESIAN APPROACH with state-of-the-art algorithms for FACIAL SKETCH SYNTHESIS in a novel FACE HALLUCINATION SCENARIO : SKETCH-BASED FACIAL PHOTO HALLUCINATION . the encouraging results obtained convincingly validate the effectiveness of our BAYESIAN APPROACH . \n",
            "this paper presents a STATISTICAL INFERENCE APPROACH for FACIAL SKETCH SYNTHESIS . the PATCH-BASED TENSOR MODEL is a PATCH-BASED TENSOR MODEL , a PATCH-BASED TENSOR MODEL , and a PATCH-BASED TENSOR MODEL for FACIAL SKETCH SYNTHESIS . the PATCH-BASED TENSOR MODEL is based on the PATCH-BASED TENSOR MODEL , a BAYESIAN APPROACH , and a PATCH-BASED TENSOR MODEL . the PATCH-BASED TENSOR MODEL is based on a PATCH-BASED TENSOR MODEL , which is a STYLE TRANSFORMATION . the PATCH-BASED TENSOR MODEL is based on the PATCH-BASED TENSOR MODEL and the BAYESIAN APPROACH . the PATCH-BASED TENSOR MODEL is a BAYESIAN APPROACH and a STYLE TRANSFORMATION . the PATCH-BASED TENSOR MODEL is applied to a FACE HALLUCINATION SCENARIO . the results show that the proposed STATISTICAL INFERENCE APPROACH is effective for FACIAL SKETCH SYNTHESIS , such as SKETCH-BASED FACIAL PHOTO HALLUCINATION , FACIAL SKETCH SYNTHESIS , and FACIAL SKETCH SYNTHESIS .\n",
            "\n",
            "746 1000\n",
            "standard ADAPTIVE FEEDBACK CANCELLATION ALGORITHMS in HEARING AIDS suffer from a BIASED ADAPTATION if the input signal is spectrally colored , as it is for TONAL SIGNALS , like music . due to that , DISTORTION ARTIFACTS are generated . in this paper , a SUB-BAND FEEDBACK CANCELLATION SYSTEM is presented combined with an ADAPTATION CONTROL to deal with those signals . two CONTROL CONCEPTS for determining the VARIABLE STEP SIZES -lsb- 1 , 2 -rsb- , known from GENERAL ADAPTIVE FILTER ALGORITHMS , are theoretically and practically analyzed and evaluated for an application to FEEDBACK CANCELLATION . for FEEDBACK CANCELLATION the control is combined with known methods to reduce the BIAS , such as PREDICTION ERROR FILTERING or FREQUENCY SHIFTING . based on this combination , a completely new SETUP for FEEDBACK CANCELLATION is proposed . SETUP relies entirely on signals accessible in real systems , shows a low COMPUTATIONAL COMPLEXITY , and therefore has a strong practical relevance . \n",
            "this paper addresses the problem of PREDICTION ERROR FILTERING in HEARING AIDS . we propose a SUB-BAND FEEDBACK CANCELLATION SYSTEM based on a SUB-BAND FEEDBACK CANCELLATION SYSTEM and a SUB-BAND FEEDBACK CANCELLATION SYSTEM . the proposed ADAPTIVE FEEDBACK CANCELLATION ALGORITHMS is based on the use of CONTROL CONCEPTS , which is based on a SUB-BAND FEEDBACK CANCELLATION SYSTEM . the proposed method is based on a SUB-BAND FEEDBACK CANCELLATION SYSTEM , which is based on a SUB-BAND FEEDBACK CANCELLATION SYSTEM . the proposed method is based on the use of CONTROL CONCEPTS and FEEDBACK CANCELLATION . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "747 1000\n",
            "boltzmann machines are able to learn highly complex , multimodal , structured and multiscale real-world data distributions . parameters of the model are usually learned by minimizing the KULLBACK-LEIBLER DIVERGENCE from training samples to the learned model . we propose in this work a novel approach for BOLTZMANN MACHINE TRAINING which assumes that a meaningful metric between observations is given . this metric can be represented by the WASSERSTEIN DISTANCE between distributions , for which we derive a GRADIENT with respect to the MODEL PARAMETERS . minimization of this new objective leads to GENERATIVE MODELS with different STATISTICAL PROPERTIES . we demonstrate their practical potential on DATA COMPLETION and DENOISING , for which the metric between observations plays a crucial role . \n",
            "this paper presents a new method for DATA COMPLETION in BOLTZMANN MACHINE TRAINING . the method is based on the use of KULLBACK-LEIBLER DIVERGENCE in BOLTZMANN MACHINE TRAINING . it is shown that the proposed method can be applied to DATA COMPLETION with DATA COMPLETION in BOLTZMANN MACHINE TRAINING . it is shown that the proposed method can be applied to DATA COMPLETION with DATA COMPLETION in the presence of DENOISING .\n",
            "\n",
            "748 1000\n",
            "transfer learning has been used in OPINION ANALYSIS to make use of available LANGUAGE RESOURCES for other resource scarce languages . however , the CUMULATIVE CLASS NOISE in TRANSFER LEARNING adversely affects performance when more training data is used . in this paper , we propose a novel method in TRANSDUCTIVE TRANSFER LEARNING to identify NOISES through the DETECTION OF NEGATIVE TRANSFERS . evaluation on NLP&CC 2013 CROSS-LINGUAL OPINION ANALYSIS DATASET shows that our approach outperforms the state-of-the-art systems . more significantly , our system shows a MONOTONIC INCREASE TREND in performance improvement when more training data are used . \n",
            "this paper addresses the problem of TRANSDUCTIVE TRANSFER LEARNING in LANGUAGE RESOURCES . we propose a method for TRANSDUCTIVE TRANSFER LEARNING based on the MONOTONIC INCREASE TREND . the proposed method is based on the DETECTION OF NEGATIVE TRANSFERS . the proposed method is based on the use of MONOTONIC INCREASE TREND and the DETECTION OF NEGATIVE TRANSFERS . experimental results show that the proposed method can improve the performance of OPINION ANALYSIS .\n",
            "\n",
            "749 1000\n",
            "we propose ADAPTIVE CHANNEL PREDICTORS for ORTHOGONAL FREQUENCY DIVISION MULTIPLEXING COMMUNICATIONS over TIME-VARYING CHANNELS . successful application of the NORMALIZED LEAST-MEAN-SQUARE and RECURSIVE LEAST-SQUARES ALGORITHMS is demonstrated . we also consider the use of ADAPTIVE CHANNEL PRE-DICTORS for DELAY-FREE EQUALIZATION , thereby avoiding the need for regular transmission of pilot symbols . simulation results demonstrate the good performance of the proposed techniques . \n",
            "this paper addresses the problem of ORTHOGONAL FREQUENCY DIVISION MULTIPLEXING COMMUNICATIONS in TIME-VARYING CHANNELS . in this paper , we propose a new method for DELAY-FREE EQUALIZATION based on RECURSIVE LEAST-SQUARES ALGORITHMS . the proposed method is based on the use of RECURSIVE LEAST-SQUARES ALGORITHMS and RECURSIVE LEAST-SQUARES ALGORITHMS . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "750 1000\n",
            "in this paper , we present a comprehensive study of the relationship between an INDI-VIDUAL 'S PERSONAL TRAITS and HIS/HER BRAND PREFERENCES . in our analysis , we included a large number of CHARACTER TRAITS such as PERSONALITY , PERSONAL VALUES and individual needs . these CHARACTER TRAITS were obtained from both a PSYCHOMETRIC SURVEY and AUTOMATED SOCIAL MEDIA ANALYTICS . we also included an extensive set of brand names from diverse product categories . from this analysis , we want to shed some light on -lrb- 1 -rrb- whether it is possible to use PERSONAL TRAITS to infer an individual 's brand preferences -lrb- 2 -rrb- whether the CHARACTER TRAITS automatically inferred from SOCIAL MEDIA are good proxies for the GROUND TRUTH CHARACTER TRAITS in BRAND PREFERENCE PREDICTION . \n",
            "this paper addresses the problem of BRAND PREFERENCE PREDICTION in SOCIAL MEDIA . we propose a method for AUTOMATED SOCIAL MEDIA ANALYTICS based on BRAND PREFERENCE PREDICTION and BRAND PREFERENCE PREDICTION . the proposed method is based on BRAND PREFERENCE PREDICTION and BRAND PREFERENCE PREDICTION . the method is based on BRAND PREFERENCE PREDICTION and BRAND PREFERENCE PREDICTION . the proposed method is based on BRAND PREFERENCE PREDICTION and BRAND PREFERENCE PREDICTION . experimental results show that the proposed method is robust and robust to PERSONAL TRAITS such as PERSONALITY , PERSONALITY , and BRAND PREFERENCE PREDICTION .\n",
            "\n",
            "751 1000\n",
            "we describe a GENERATIVE BAYESIAN MODEL for ACTION UNDERSTANDING in which INVERSE-FORWARD INTERNAL MODEL PAIRS are considered ` HYPOTHESES ' of plausible action goals that are explored in parallel via an APPROXIMATE INFERENCE MECHANISM based on SEQUENTIAL MONTE CARLO METHODS . the REENACTMENT OF INTERNAL MODEL PAIRS can be considered a form of MOTOR SIMULATION , which supports both PERCEPTUAL PREDICTION AND ACTION UNDERSTANDING at the goal level . however , this GENERATIVE BAYESIAN MODEL is generally considered to be computationally inefficient . we present a GENERATIVE BAYESIAN MODEL that dynamically <unk> COMPUTATIONAL RESOURCES to more accurate INTERNAL MODELS depending on both the available prior information and the PREDICTION ERROR of the INVERSE-FORWARD MODELS , and which leads to successful ACTION RECOGNITION . we present experimental results that test the robustness and efficiency of our GENERATIVE BAYESIAN MODEL in REAL-WORLD SCENARIOS . \n",
            "this paper proposes a new GENERATIVE BAYESIAN MODEL for PERCEPTUAL PREDICTION AND ACTION UNDERSTANDING . the GENERATIVE BAYESIAN MODEL is based on the GENERATIVE BAYESIAN MODEL . the proposed GENERATIVE BAYESIAN MODEL is based on the GENERATIVE BAYESIAN MODEL . the proposed GENERATIVE BAYESIAN MODEL is based on the GENERATIVE BAYESIAN MODEL . the proposed GENERATIVE BAYESIAN MODEL is based on the GENERATIVE BAYESIAN MODEL . the proposed GENERATIVE BAYESIAN MODEL is based on the GENERATIVE BAYESIAN MODEL . the proposed GENERATIVE BAYESIAN MODEL is based on the GENERATIVE BAYESIAN MODEL . the proposed GENERATIVE BAYESIAN MODEL is based on the GENERATIVE BAYESIAN MODEL . the proposed GENERATIVE BAYESIAN MODEL is evaluated on REAL-WORLD SCENARIOS and compared with other SEQUENTIAL MONTE CARLO METHODS . the proposed GENERATIVE BAYESIAN MODEL is compared with other SEQUENTIAL MONTE CARLO METHODS in terms of PREDICTION ERROR and PREDICTION ERROR .\n",
            "\n",
            "752 1000\n",
            "we study to incorporate multiple views of data in a perceptive TRANSFER LEARNING FRAMEWORK and propose a MULTI-VIEW DISCRIMINANT TRANSFER LEARNING APPROACH for DOMAIN ADAPTATION . the main idea is to find the optimal DISCRIMINANT WEIGHT VECTORS for each view such that the correlation between the TWO-VIEW PROJECTED DATA is maximized , while both the DOMAIN DISCREPANCY and the VIEW DISAGREEMENT are minimized simultaneously . furthermore , we analyze MULTI-VIEW DISCRIMINANT TRANSFER LEARNING APPROACH theoretically from DISCRIMINANT ANALYSIS PERSPECTIVE to explain the condition and reason , under which the proposed MULTI-VIEW DISCRIMINANT TRANSFER LEARNING APPROACH is not applicable . the analytical results allow us to investigate whether there exist WITHIN-VIEW AND/OR BETWEEN-VIEW CONFLICTS , and thus provides a deep insight into whether the TRANSFER LEARNING ALGORITHM work properly or not in the VIEW-BASED PROBLEMS and the combined LEARNING PROBLEM . experiments show that MULTI-VIEW DISCRIMINANT TRANSFER LEARNING APPROACH significantly outperforms the state-of-the-art baselines including some typical MULTI-VIEW LEARNING APPROACHES in SINGLE-OR CROSS-DOMAIN . \n",
            "this paper presents a MULTI-VIEW DISCRIMINANT TRANSFER LEARNING APPROACH for DOMAIN ADAPTATION . the proposed MULTI-VIEW DISCRIMINANT TRANSFER LEARNING APPROACH is based on the TRANSFER LEARNING ALGORITHM and the TRANSFER LEARNING ALGORITHM . the proposed MULTI-VIEW DISCRIMINANT TRANSFER LEARNING APPROACH is based on the TRANSFER LEARNING ALGORITHM and the TRANSFER LEARNING ALGORITHM . the proposed MULTI-VIEW DISCRIMINANT TRANSFER LEARNING APPROACH is based on the TRANSFER LEARNING ALGORITHM . the proposed MULTI-VIEW DISCRIMINANT TRANSFER LEARNING APPROACH is based on the DISCRIMINANT WEIGHT VECTORS and the TRANSFER LEARNING ALGORITHM . the proposed MULTI-VIEW DISCRIMINANT TRANSFER LEARNING APPROACH is compared with conventional MULTI-VIEW LEARNING APPROACHES and MULTI-VIEW LEARNING APPROACHES . the proposed MULTI-VIEW DISCRIMINANT TRANSFER LEARNING APPROACH is compared with other MULTI-VIEW LEARNING APPROACHES and MULTI-VIEW LEARNING APPROACHES .\n",
            "\n",
            "753 1000\n",
            "we propose a new method for SHAPE RECONSTRUCTION from NOISY AND UNORGANIZED POINT DATA . we represent a SHAPE through its SIGNED DISTANCE FUNCTION and formulate SHAPE RECONSTRUCTION as a CONSTRAINED ENERGY MINIMIZATION PROBLEM directly based on the OBSERVED POINT SET . the associated energy function includes both the likelihood of the observed data points and a SMOOTHNESS PRIOR on the RECONSTRUCTED SHAPE . to solve this OPTIMIZATION PROBLEM , an efficient DATA-DRIVEN LEVEL SET METHOD is developed . our method is robust to LOCAL MINIMA , CLUTTER , and NOISE . it is also applicable to situations where the data are sparse . the TOPOLOGI-CAL NATURE of the underlying SHAPE is handled automatically through the LEVEL SET FORMALISM . \n",
            "this paper presents a DATA-DRIVEN LEVEL SET METHOD for SHAPE RECONSTRUCTION . the OPTIMIZATION PROBLEM is formulated as a CONSTRAINED ENERGY MINIMIZATION PROBLEM . the OPTIMIZATION PROBLEM is formulated as a CONSTRAINED ENERGY MINIMIZATION PROBLEM . the OPTIMIZATION PROBLEM is formulated as a CONSTRAINED ENERGY MINIMIZATION PROBLEM . the OPTIMIZATION PROBLEM is formulated as a CONSTRAINED ENERGY MINIMIZATION PROBLEM . the OPTIMIZATION PROBLEM is formulated as a CONSTRAINED ENERGY MINIMIZATION PROBLEM . the OPTIMIZATION PROBLEM is formulated as a CONSTRAINED ENERGY MINIMIZATION PROBLEM . the OPTIMIZATION PROBLEM is formulated as a CONSTRAINED ENERGY MINIMIZATION PROBLEM . the proposed DATA-DRIVEN LEVEL SET METHOD is based on a DATA-DRIVEN LEVEL SET METHOD and is shown to be robust to NOISE and NOISE .\n",
            "\n",
            "754 1000\n",
            "this paper presents our entry to a SPEECH-IN-NOISE INTELLIGIBILITY ENHANCEMENT EVALUATION : the HURRICANE CHALLENGE . the system consists of a TEXT-TO-SPEECH VOICE manipulated through a combination of ENHANCEMENT STRATEGIES , each of which is known to be individually successful : a PERCEPTUALLY-MOTIVATED SPECTRAL SHAPER based on the GLIMPSE PROPORTION MEASURE , DYNAMIC RANGE COMPRESSION , and adaptation to LOMBARD EXCITATION AND DURATION PATTERNS . we achieved substantial intelligibility improvements relative to UNMODIFIED SYNTHETIC SPEECH : 4.9 db in competing speaker and 4.1 db in SPEECH-SHAPED NOISE . an analysis conducted across this and other two similar evaluations shows that the SPECTRAL SHAPER and the COMPRESSOR -lrb- both of which are loudness <unk> -rrb- contribute most under higher SNR CONDITIONS , particularly for SPEECH-SHAPED NOISE . DURATION AND EXCITATION LOMBARD-ADAPTED CHANGES are more beneficial in lower SNR CONDITIONS , and for competing speaker noise . \n",
            "this paper presents a new method for DYNAMIC RANGE COMPRESSION based on PERCEPTUALLY-MOTIVATED SPECTRAL SHAPER . the proposed GLIMPSE PROPORTION MEASURE is based on the GLIMPSE PROPORTION MEASURE of the TEXT-TO-SPEECH VOICE and the COMPRESSOR . the proposed GLIMPSE PROPORTION MEASURE is based on the GLIMPSE PROPORTION MEASURE of the TEXT-TO-SPEECH VOICE and the SPECTRAL SHAPER . the proposed ENHANCEMENT STRATEGIES is based on the GLIMPSE PROPORTION MEASURE of the TEXT-TO-SPEECH VOICE and the DYNAMIC RANGE COMPRESSION . the proposed method is evaluated on the HURRICANE CHALLENGE and the SPEECH-IN-NOISE INTELLIGIBILITY ENHANCEMENT EVALUATION .\n",
            "\n",
            "755 1000\n",
            "we present a method for MOTION ESTIMATION using ORDINAL MEASURES . ORDINAL MEASURES are based on RELATIVE ORDERING OF INTENSITY VALUES in a IMAGE REGION called RANK PERMUTATION . while popular measures like the SUM-OF SQUARED-DIFFERENCE -LRB- S S D -RRB- and NORMALIZED CORRELATION rely on LINEAR-ITY BETWEEN CORRESPONDING INTENSITY VALUES , ORDINAL MEASURES only require them to be monotonically related so that RANK PERMUTATIONS between corresponding regions are preserved . this property turns out to be <unk> for MOTION ESTIMATION in TAGGED MAGNETIC RESONANCE IMAGES . we study the IMAGING EQUATION involved in two methods of TAGGING and observe TEMPORAL MONOTONICITY in intensity under certain conditions though the tags themselves <unk> . we compare our method to s s d and NORMALIZED CORRELATION in a ROTATING RING PHANTOM IMAGE SEQUENCE . we present an experiment on a REAL HEART IMAGE SEQUENCE which suggests the suitability of our method . \n",
            "this paper presents a method for MOTION ESTIMATION from TAGGED MAGNETIC RESONANCE IMAGES . the proposed method is based on a SUM-OF SQUARED-DIFFERENCE -LRB- S S D -RRB- , which is based on SUM-OF SQUARED-DIFFERENCE -LRB- S S D -RRB- . the proposed method is based on the RELATIVE ORDERING OF INTENSITY VALUES of the TAGGED MAGNETIC RESONANCE IMAGES . the proposed method is based on the RELATIVE ORDERING OF INTENSITY VALUES of the TAGGED MAGNETIC RESONANCE IMAGES . the proposed method is based on the RELATIVE ORDERING OF INTENSITY VALUES . the proposed method is based on the RELATIVE ORDERING OF INTENSITY VALUES . the proposed method is based on the SUM-OF SQUARED-DIFFERENCE -LRB- S S D -RRB- . the proposed method is based on the RELATIVE ORDERING OF INTENSITY VALUES . the proposed method is based on the RELATIVE ORDERING OF INTENSITY VALUES and the RELATIVE ORDERING OF INTENSITY VALUES .\n",
            "\n",
            "756 1000\n",
            "the AUTOMATED PLANNING COMMUNITY has traditionally focused on the efficient synthesis of plans given a complete DOMAIN THEORY . in the past several years , this line of work met with significant successes , and the future course of the community seems to be set on efficient planning with even RICHER MODELS . while this line of research has its applications , there are also many domains and scenarios where the first bottleneck is getting the DOMAIN MODEL at any level of completeness . in these scenarios , the DOMAIN MODEL automatically renders the PLANNING TECHNOLOGY <unk> . to counter this , i will motivate MODEL-LITE PLANNING TECHNOLOGY aimed at reducing the DOMAIN-MODELING BURDEN -lrb- possibly at the expense of reduced functionality -rrb- , and outline the research challenges that need to be addressed to realize MODEL-LITE PLANNING TECHNOLOGY . \n",
            "this paper presents a new method for AUTOMATED PLANNING COMMUNITY based on the DOMAIN MODEL . the proposed method is based on a DOMAIN MODEL , which is based on the DOMAIN MODEL . the proposed method is based on a DOMAIN MODEL . the proposed method is based on the use of a DOMAIN THEORY .\n",
            "\n",
            "757 1000\n",
            "-- RADIO FREQUENCY IDENTI ¿ CATION is a technology to <unk> transmit the IDENTITY OF TAGGED OBJECTS . for LONG-RANGE SYSTEMS with multiple tags , the TAG REPLIES may overlap . current solutions are based on COLLISION AVOIDANCE using MAC PROTOCOLS -lrb- e.g. SLOTTED ALOHA and BINARY TREE ALGORITHMS -rrb- . this can be a time-consuming process . in this paper , it is shown how an ANTENNA ARRAY in combination with BLIND SOURCE SEPARATION TECHNIQUES can be used to separate multiple overlapping tag signals . the source signals are modeled as ZERO CONSTANT MODULUS SIGNALS , and the corresponding ZCM ALGORITHMS are tested on SYNTHETIC AND MEASURED DATA SETS . \n",
            "this paper addresses the problem of RADIO FREQUENCY IDENTI ¿ CATION for ZERO CONSTANT MODULUS SIGNALS . we propose a method for RADIO FREQUENCY IDENTI ¿ CATION based on ZERO CONSTANT MODULUS SIGNALS . the proposed method is based on the use of ZERO CONSTANT MODULUS SIGNALS and ZERO CONSTANT MODULUS SIGNALS . the proposed method is based on the use of ZERO CONSTANT MODULUS SIGNALS and ZERO CONSTANT MODULUS SIGNALS . the proposed method is based on the use of ZERO CONSTANT MODULUS SIGNALS and ZERO CONSTANT MODULUS SIGNALS . experimental results on SYNTHETIC AND MEASURED DATA SETS show that the proposed method is effective for RADIO FREQUENCY IDENTI ¿ CATION .\n",
            "\n",
            "758 1000\n",
            "a new OPTIMIZATION CRITERION is proposed to minimize ERROR PROBABILITY for the proposed JOINT OPTIMAL POWER ALLOCATION of the MIMO SYSTEMS enhanced by relay in this paper . it is proved that the COST FUNCTION obtained is only convex with respect to -lrb- w.r.t. -rrb- the POWER PARAMETERS of the source or those of the relay separately , but not convex w.r.t. the whole parameters . in order to use CONVEX OPTIMIZATION METHODS with high efficiency to solve this complicated problem , a tight upper bound of the sum mse -lrb- mean squared error -rrb- is derived , and employed to modify the COST FUNCTION in order to obtain a CONVEX PROBLEM . it is verified through simulation results that the proposed PA SCHEME outperforms the existing one . \n",
            "this paper proposes a new PA SCHEME for JOINT OPTIMAL POWER ALLOCATION . the OPTIMIZATION CRITERION is based on the OPTIMIZATION CRITERION . the OPTIMIZATION CRITERION is based on the OPTIMIZATION CRITERION . the OPTIMIZATION CRITERION is based on the OPTIMIZATION CRITERION . the proposed PA SCHEME is based on the OPTIMIZATION CRITERION . the proposed PA SCHEME is applied to the CONVEX PROBLEM .\n",
            "\n",
            "759 1000\n",
            "this paper presents several novel GENERALIZATION BOUNDS for the problem of LEARNING KERNELS based on a COMBINATORIAL ANALYSIS of the RADEMACHER COMPLEXITY of the corresponding hypothesis sets . our bound for LEARNING KERNELS with a CONVEX COMBINATION OF P BASE KERNELS using l 1 regular-ization admits only a √ log p dependency on the number of kernels , which is tight and considerably more favorable than the previous best bound given for the same problem . we also give a novel bound for learning with a NON-NEGATIVE COMBINATION OF P BASE KERNELS with an L 2 REGULARIZATION whose dependency on p is also tight and only in p 1/4 . we present similar results for l q regular-ization with other values of q , and outline the relevance of our proof techniques to the analysis of the COMPLEXITY of the class of linear functions . experiments with a large number of kernels further validate the behavior of the GENERALIZATION ERROR as a function of p predicted by our bounds . \n",
            "this paper addresses the problem of COMBINATORIAL ANALYSIS for COMBINATORIAL ANALYSIS with L 2 REGULARIZATION . in this paper , we propose a new GENERALIZATION BOUNDS , which is based on the CONVEX COMBINATION OF P BASE KERNELS . the proposed method is based on the use of L 2 REGULARIZATION with L 2 REGULARIZATION . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "760 1000\n",
            "modelling compositional meaning for sentences using EMPIRICAL DISTRIBUTIONAL METHODS has been a challenge for COMPUTATIONAL LINGUISTS . we implement the ABSTRACT CATEGORICAL MODEL of <unk> et al. -lrb- 2010 -rrb- using data from the BNC and evaluate ABSTRACT CATEGORICAL MODEL . the implementation is based on UNSUPERVISED LEARNING OF MATRICES for RELATIONAL WORDS and applying UNSUPERVISED LEARNING OF MATRICES to the vectors of their arguments . the evaluation is based on the WORD DISAMBIGUATION TASK developed by mitchell and <unk> -lrb- 2008 -rrb- for INTRANSITIVE SENTENCES , and on a similar new experiment designed for TRANSITIVE SENTENCES . our model matches the results of its competitors in the first experiment , and <unk> UNSUPERVISED LEARNING OF MATRICES in the second . the general improvement in results with increase in SYNTACTIC COMPLEXITY <unk> the compositional power of our model . \n",
            "this paper addresses the problem of MODELLING COMPOSITIONAL MEANING in INTRANSITIVE SENTENCES . we propose a method for MODELLING COMPOSITIONAL MEANING based on the ABSTRACT CATEGORICAL MODEL . the proposed method is based on the ABSTRACT CATEGORICAL MODEL . the proposed method is based on the use of EMPIRICAL DISTRIBUTIONAL METHODS for MODELLING COMPOSITIONAL MEANING . the proposed method is based on the use of EMPIRICAL DISTRIBUTIONAL METHODS for MODELLING COMPOSITIONAL MEANING . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "761 1000\n",
            "the ELECTRIC NETWORK FREQUENCY SIGNAL can be captured in MULTIMEDIA RECORDINGS due to ELECTROMAGNETIC INFLUENCES from the POWER GRID at the time of recording . recent work has exploited the ENF SIGNALS for FORENSIC APPLICATIONS , such as <unk> and DETECTING FORGERY OF ENF-CONTAINING MUL-TIMEDIA SIGNALS , and inferring their time and location of creation . in this paper , we explore a new potential of ENF SIGNALS for AUTOMATIC SYNCHRONIZATION OF AUDIO AND VIDEO . the ENF SIGNALS as a TIME-VARYING RANDOM PROCESS can be used as a timing fingerprint of MULTIMEDIA SIGNALS . synchronization of audio and video recordings can be achieved by aligning their EMBEDDED ENF SIGNALS . we demonstrate the proposed scheme with two applications : MULTI-VIEW VIDEO SYNCHRONIZATION and synchronization of historical audio recordings . the experimental results show the ENF BASED SYNCHRONIZATION APPROACH is effective , and has the potential to solve problems that are intractable by other existing methods . \n",
            "this paper addresses the problem of DETECTING FORGERY OF ENF-CONTAINING MUL-TIMEDIA SIGNALS in MULTIMEDIA RECORDINGS . we propose a method for DETECTING FORGERY OF ENF-CONTAINING MUL-TIMEDIA SIGNALS based on ENF SIGNALS . the proposed method is based on the ENF BASED SYNCHRONIZATION APPROACH of the ELECTRIC NETWORK FREQUENCY SIGNAL . the proposed method is based on the ENF BASED SYNCHRONIZATION APPROACH . the proposed method is based on the ENF BASED SYNCHRONIZATION APPROACH . the proposed method is based on the ENF BASED SYNCHRONIZATION APPROACH . the proposed method is based on the ENF BASED SYNCHRONIZATION APPROACH . the proposed method is based on the ENF BASED SYNCHRONIZATION APPROACH . the proposed method is based on the ENF BASED SYNCHRONIZATION APPROACH .\n",
            "\n",
            "762 1000\n",
            "<unk> in SCRIBBLE-BASED INTERACTIVE SEGMENTATION such as GRAPH-CUT are usually assumed to be perfectly accurate , i.e. , FOREGROUND SCRIBBLE PIXELS will never be segmented as background in the final SEGMENTATION . however , it can be hard to draw perfectly accurate SCRIBBLES , especially on fine structures of the IMAGE or on MOBILE TOUCH-SCREEN DEVICES . in this paper , we propose a novel RATIO ENERGY FUNCTION that tolerates errors in the USER INPUT while encouraging maximum use of the USER INPUT INFORMATION . more specifically , the RATIO ENERGY FUNCTION aims to minimize the GRAPH-CUT ENERGY while maximizing the USER INPUT <unk> in the SEGMENTATION . the RATIO ENERGY FUNCTION can be exactly optimized using an efficient ITERATED GRAPH CUT ALGORITHM . the ROBUSTNESS of the proposed RATIO ENERGY FUNCTION is validated on the GRABCUT DATASET using both SYNTHETIC SCRIBBLES and MANUAL SCRIBBLES . the experimental results show that the proposed RATIO ENERGY FUNCTION is robust to the errors in the USER INPUT and preserves the '' anchoring '' capability of the USER INPUT . \n",
            "this paper proposes a new ITERATED GRAPH CUT ALGORITHM for SCRIBBLE-BASED INTERACTIVE SEGMENTATION . the ITERATED GRAPH CUT ALGORITHM is based on the RATIO ENERGY FUNCTION and the ITERATED GRAPH CUT ALGORITHM . the proposed ITERATED GRAPH CUT ALGORITHM is based on the ITERATED GRAPH CUT ALGORITHM and the ITERATED GRAPH CUT ALGORITHM . the proposed ITERATED GRAPH CUT ALGORITHM is evaluated on the GRABCUT DATASET and on the GRABCUT DATASET . it is shown that the proposed ITERATED GRAPH CUT ALGORITHM achieves better performance than the ITERATED GRAPH CUT ALGORITHM in terms of ROBUSTNESS and ROBUSTNESS .\n",
            "\n",
            "763 1000\n",
            "this paper presents recent developments on our '' silent speech interface '' that converts TONGUE AND LIP MOTIONS , captured by ultrasound and VIDEO IMAGING , into AUDIBLE SPEECH . in our previous studies , the MAPPING between the observed articulatory movements and the resulting SPEECH SOUND was achieved using a UNIT SELECTION APPROACH . we investigate here the use of STATISTICAL MAPPING TECHNIQUES , based on the JOINT MODELING OF VISUAL AND SPECTRAL FEATURES , using respectively GAUSSIAN MIXTURE MODELS and HIDDEN MARKOV MODELS . the prediction of the VOICED/UNVOICED PARAMETER from VISUAL ARTICULATORY DATA is also investigated using an ARTIFICIAL NEURAL NETWORK . a CONTINUOUS SPEECH DATABASE consisting of <unk> of high-speed ultrasound and VIDEO SEQUENCES was specifically recorded to evaluate the proposed STATISTICAL MAPPING TECHNIQUES . \n",
            "this paper addresses the problem of VIDEO IMAGING in VIDEO SEQUENCES . we propose a method for JOINT MODELING OF VISUAL AND SPECTRAL FEATURES based on HIDDEN MARKOV MODELS and STATISTICAL MAPPING TECHNIQUES . the proposed method is based on the JOINT MODELING OF VISUAL AND SPECTRAL FEATURES and the UNIT SELECTION APPROACH . the proposed method is based on the JOINT MODELING OF VISUAL AND SPECTRAL FEATURES and the UNIT SELECTION APPROACH . the proposed method is based on the JOINT MODELING OF VISUAL AND SPECTRAL FEATURES and the UNIT SELECTION APPROACH . the proposed method is based on the JOINT MODELING OF VISUAL AND SPECTRAL FEATURES and the UNIT SELECTION APPROACH . the experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "764 1000\n",
            "discourse in FORMAL DOMAINS , such as MATHEMATICS , is characterized by a mixture of TELEGRAPHIC NATURAL LANGUAGE and EMBEDDED -LRB- SEMI - -rrb- formal symbolic mathematical expressions . we present LANGUAGE PHENOMENA observed in a CORPUS OF DIALOGS with a SIMULATED TUTORIAL SYSTEM for proving theorems as evidence for the need for DEEP SYNTACTIC AND SEMANTIC ANALYSIS . we propose an approach to input understanding in this setting . our goal is a uniform analysis of inputs of different degree of VERBALIZA-TION : ranging from symbolic alone to fully <unk> mathematical expressions . \n",
            "this paper presents a novel SIMULATED TUTORIAL SYSTEM for DEEP SYNTACTIC AND SEMANTIC ANALYSIS . the SIMULATED TUTORIAL SYSTEM is based on a CORPUS OF DIALOGS , which is based on the CORPUS OF DIALOGS . the proposed SIMULATED TUTORIAL SYSTEM is based on a CORPUS OF DIALOGS , which is based on the CORPUS OF DIALOGS . the proposed SIMULATED TUTORIAL SYSTEM is based on the CORPUS OF DIALOGS . the proposed SIMULATED TUTORIAL SYSTEM is based on the CORPUS OF DIALOGS . the proposed SIMULATED TUTORIAL SYSTEM is based on a CORPUS OF DIALOGS and is shown to be robust to LANGUAGE PHENOMENA .\n",
            "\n",
            "765 1000\n",
            "this paper presents a TALKER 'S HEAD ORIENTATION ESTIMATION METHOD using 2-CHANNEL MICROPHONES . in recent research , some approaches based on a NETWORK OF MICROPHONE ARRAYS have been proposed in order to estimate the TALKER 'S HEAD ORIENTATION . in those methods , the TALKER 'S HEAD ORIENTATION is estimated using the SOUND AMPLITUDE or PEAK VALUE of CSP -LRB- CROSS-POWER SPECTRUM PHASE -RRB- COEFFICIENTS obtained from each MICROPHONE ARRAY . however , MICROPHONE ARRAY NETWORK SYSTEMS need many MICROPHONE ARRAYS to be set along the walls of a given room so that SUB-MICROPHONE ARRAYS surround the user . in this paper , we focus on the shape of the CSP COEFFICIENTS affected by the REVERBERATION , which depends on the TALKER 'S POSITION and the HEAD ORIENTATION . in our proposed TALKER 'S HEAD ORIENTATION ESTIMATION METHOD , we use not only the PEAK VALUE but also the other values of the CSP COEFFICIENTS as FEATURE VECTORS , and the TALKER 'S POSITION and the HEAD ORIENTATION are estimated by discriminating the CSP VECTOR . the effectiveness of this TALKER 'S HEAD ORIENTATION ESTIMATION METHOD has been confirmed by TALKER LOCALIZATION and HEAD ORIENTATION ESTIMATION experiments performed in a real environment . \n",
            "this paper presents a new method for TALKER LOCALIZATION based on CSP -LRB- CROSS-POWER SPECTRUM PHASE -RRB- COEFFICIENTS . the proposed method is based on the TALKER 'S HEAD ORIENTATION ESTIMATION METHOD and the TALKER 'S HEAD ORIENTATION ESTIMATION METHOD . the proposed method is based on the TALKER 'S HEAD ORIENTATION ESTIMATION METHOD and the TALKER 'S HEAD ORIENTATION ESTIMATION METHOD . the proposed method is based on the TALKER 'S HEAD ORIENTATION ESTIMATION METHOD and the TALKER 'S HEAD ORIENTATION ESTIMATION METHOD . the proposed method is based on the TALKER 'S HEAD ORIENTATION ESTIMATION METHOD and the TALKER 'S HEAD ORIENTATION ESTIMATION METHOD . the proposed method is based on the TALKER 'S HEAD ORIENTATION ESTIMATION METHOD and the TALKER 'S HEAD ORIENTATION ESTIMATION METHOD . the proposed method is based on the TALKER 'S HEAD ORIENTATION ESTIMATION METHOD and the TALKER 'S HEAD ORIENTATION ESTIMATION METHOD . the proposed method is based on the TALKER 'S HEAD ORIENTATION ESTIMATION METHOD and the TALKER 'S HEAD ORIENTATION ESTIMATION METHOD . the experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "766 1000\n",
            "traditional LINEAR FUKUNAGA-KOONTZ TRANSFORM -lsb- 1 -rsb- is a powerful DISCRIMINATIVE SUBSPACES BUILDING APPROACH . previous work has successfully extended LINEAR FUKUNAGA-KOONTZ TRANSFORM to be able to deal with SMALL-SAMPLE-SIZE . in this paper , we extend traditional LINEAR FUKUNAGA-KOONTZ TRANSFORM to enable it to work in MULTI-CLASS PROBLEM and also in HIGHER DIMENSIONAL SUBSPACES and therefore provide enhanced DISCRIMINATION ABILITY . we verify the effectiveness of the proposed KERNEL FUKUNAGA-KOONTZ TRANSFORM by demonstrating its effectiveness in FACE RECOGNITION APPLICATIONS ; however the proposed KERNEL FUKUNAGA-KOONTZ TRANSFORM can be applied to any other DOMAIN SPECIFIC PROBLEMS . \n",
            "this paper proposes a DISCRIMINATIVE SUBSPACES BUILDING APPROACH for FACE RECOGNITION APPLICATIONS . the KERNEL FUKUNAGA-KOONTZ TRANSFORM is based on the KERNEL FUKUNAGA-KOONTZ TRANSFORM . the proposed DISCRIMINATIVE SUBSPACES BUILDING APPROACH is based on the KERNEL FUKUNAGA-KOONTZ TRANSFORM . the proposed DISCRIMINATIVE SUBSPACES BUILDING APPROACH is based on the KERNEL FUKUNAGA-KOONTZ TRANSFORM and the KERNEL FUKUNAGA-KOONTZ TRANSFORM . the proposed DISCRIMINATIVE SUBSPACES BUILDING APPROACH is evaluated on the MULTI-CLASS PROBLEM and FACE RECOGNITION APPLICATIONS . the experimental results show that the proposed DISCRIMINATIVE SUBSPACES BUILDING APPROACH is effective for FACE RECOGNITION APPLICATIONS .\n",
            "\n",
            "767 1000\n",
            "for VIDEO ANNOTATION REFINEMENT , a reasonable CONCEPT CORRELATION REPRESENTATION is crucial . in this paper , we present a DATA-SPECIFIC CONCEPT CORRELATION ESTIMATION PROCEDURE for this task , where the resulting correlation with respect to each data encodes both its VISUAL AND HIGH-LEVEL CHARACTERISTICS . specifically , this DATA-SPECIFIC CONCEPT CORRELATION ESTIMATION PROCEDURE comprises two major modules : CONCEPT CORRELATION BASIS ESTIMATION and DATA-SPECIFIC CONCEPT CORRELATION CALCULATION . under the framework of SPARSE REPRESENTATION , the DATA-SPECIFIC CONCEPT CORRELATION ESTIMATION PROCEDURE introduces a set of HIGH-LEVEL CONCEPT CORRELATION BASES to represent the CONCEPT DISTRIBUTION of each <unk> basis , while the latter constructs the concept correlation of a specific data by combining its FEATURE-LEVEL SPARSE COEFFICIENTS and CORRELATION BASES together . in the end , given this new correlation , a PROBABILITY-CALCULATION BASED VIDEO ANNOTATION REFINEMENT is performed on TRECVID 2006 DATASET . the experiments show that such a representation capturing DATA-SPECIFIC CHARACTERISTICS could achieve better performance , than the GENERIC CONCEPT CORRELATION applied to all data . \n",
            "this paper addresses the problem of PROBABILITY-CALCULATION BASED VIDEO ANNOTATION REFINEMENT for VIDEO ANNOTATION REFINEMENT . in this paper , we propose a new DATA-SPECIFIC CONCEPT CORRELATION ESTIMATION PROCEDURE based on the CONCEPT CORRELATION REPRESENTATION . the proposed DATA-SPECIFIC CONCEPT CORRELATION ESTIMATION PROCEDURE is based on the CONCEPT CORRELATION REPRESENTATION and the DATA-SPECIFIC CONCEPT CORRELATION ESTIMATION PROCEDURE . the proposed DATA-SPECIFIC CONCEPT CORRELATION ESTIMATION PROCEDURE is based on the CONCEPT CORRELATION REPRESENTATION and the DATA-SPECIFIC CONCEPT CORRELATION ESTIMATION PROCEDURE . the proposed DATA-SPECIFIC CONCEPT CORRELATION ESTIMATION PROCEDURE is based on the CONCEPT CORRELATION REPRESENTATION and the DATA-SPECIFIC CONCEPT CORRELATION ESTIMATION PROCEDURE . the proposed DATA-SPECIFIC CONCEPT CORRELATION ESTIMATION PROCEDURE is based on the CONCEPT CORRELATION REPRESENTATION and the DATA-SPECIFIC CONCEPT CORRELATION ESTIMATION PROCEDURE . the proposed DATA-SPECIFIC CONCEPT CORRELATION ESTIMATION PROCEDURE is evaluated on a TRECVID 2006 DATASET and a TRECVID 2006 DATASET .\n",
            "\n",
            "768 1000\n",
            "trust region is a well-known general ITERATIVE APPROACH to OPTIMIZATION which offers many advantages over standard GRADIENT DESCENT TECHNIQUES . in particular , it allows more accurate NONLINEAR APPROXIMATION MODELS . in each iteration this approach computes a GLOBAL OPTIMUM of a suitable APPROXIMATION MODEL within a fixed radius around the current solution , a.k.a. trust region . in general , this approach can be used only when some efficient CONSTRAINED OPTIMIZATION ALGORITHM is available for the selected NON-LINEAR -LRB- MORE ACCURATE -RRB- APPROXIMATION MODEL . in this paper we propose a FAST TRUST REGION APPROACH for OPTIMIZATION OF SEGMENTATION ENERGIES with NON-LINEAR REGIONAL TERMS , which are known to be challenging for existing algorithms . these NON-LINEAR REGIONAL TERMS include , but are not limited to , KL DIVERGENCE and BHATTACHARYYA DISTANCE between the observed and the TARGET APPEARANCE DISTRIBUTIONS , VOLUME CONSTRAINT on SEGMENT SIZE , and SHAPE PRIOR CONSTRAINT in a form of l 2 distance from target shape moments . our method is 1-2 orders of magnitude faster than the existing state-of-the-art methods while converging to comparable or better solutions . \n",
            "this paper proposes a new FAST TRUST REGION APPROACH for OPTIMIZATION OF SEGMENTATION ENERGIES . the proposed CONSTRAINED OPTIMIZATION ALGORITHM is based on the SHAPE PRIOR CONSTRAINT of the TRUST REGION . the proposed CONSTRAINED OPTIMIZATION ALGORITHM is based on the CONSTRAINED OPTIMIZATION ALGORITHM . the proposed CONSTRAINED OPTIMIZATION ALGORITHM is based on the CONSTRAINED OPTIMIZATION ALGORITHM . the proposed CONSTRAINED OPTIMIZATION ALGORITHM is based on a CONSTRAINED OPTIMIZATION ALGORITHM . the proposed CONSTRAINED OPTIMIZATION ALGORITHM is compared with conventional GRADIENT DESCENT TECHNIQUES . the proposed FAST TRUST REGION APPROACH is compared with conventional GRADIENT DESCENT TECHNIQUES . the proposed FAST TRUST REGION APPROACH is compared with other GRADIENT DESCENT TECHNIQUES . the proposed FAST TRUST REGION APPROACH is compared with other GRADIENT DESCENT TECHNIQUES .\n",
            "\n",
            "769 1000\n",
            "many applications require the analysis of complex interactions between time series . these interactions can be non-linear and involve vector valued as well as COMPLEX DATA STRUCTURES such as GRAPHS or strings . here we provide a general framework for the statistical analysis of these dependencies when RANDOM VARIABLES are sampled from STATIONARY TIME-SERIES OF ARBITRARY OBJECTS . to achieve this goal , we study the properties of the KERNEL CROSS-SPECTRAL DENSITY OPERATOR induced by POSITIVE DEFINITE KERNELS on ARBITRARY INPUT DOMAINS . this framework enables us to develop an INDEPENDENCE TEST between time series , as well as a SIMILARITY MEASURE to compare different types of COUPLING . the performance of our test is compared to the HSIC TEST using i.i.d. assumptions , showing strong improvements in terms of DETECTION ERRORS , as well as the suitability of this approach for testing dependency in complex DYNAMICAL SYSTEMS . this SIMILARITY MEASURE enables us to identify different types of interactions in ELECTROPHYSIOLOGICAL NEURAL TIME SERIES . \n",
            "this paper presents a new method for STATIONARY TIME-SERIES OF ARBITRARY OBJECTS based on the KERNEL CROSS-SPECTRAL DENSITY OPERATOR . the proposed method is based on the SIMILARITY MEASURE of the KERNEL CROSS-SPECTRAL DENSITY OPERATOR . the proposed method is based on the SIMILARITY MEASURE of the KERNEL CROSS-SPECTRAL DENSITY OPERATOR . the proposed method is based on the KERNEL CROSS-SPECTRAL DENSITY OPERATOR . the proposed method is based on the KERNEL CROSS-SPECTRAL DENSITY OPERATOR . the proposed method is based on the KERNEL CROSS-SPECTRAL DENSITY OPERATOR . the proposed method is based on the KERNEL CROSS-SPECTRAL DENSITY OPERATOR . the proposed method is based on the KERNEL CROSS-SPECTRAL DENSITY OPERATOR . the proposed method is based on the SIMILARITY MEASURE of the ELECTROPHYSIOLOGICAL NEURAL TIME SERIES .\n",
            "\n",
            "770 1000\n",
            "understanding the CONNOTATION OF WORDS plays an important role in interpreting subtle <unk> of sentiment beyond <unk> or surface meaning of text , as seemingly objective statements often <unk> <unk> sentiment of the writer , and even purposefully <unk> emotion from the readers ' <unk> . the focus of this paper is drawing NUANCED , CONNOTATIVE SENTIMENTS from even those words that are objective on the surface , such as '' intelligence '' , '' human '' , and '' <unk> '' . we propose INDUCTION ALGORITHMS encoding a diverse set of linguistic insights -lrb- SEMANTIC PROSODY , DISTRI-BUTIONAL SIMILARITY , SEMANTIC PARALLELISM OF COORDINATION -rrb- and PRIOR KNOWLEDGE drawn from LEXICAL RESOURCES , resulting in the first BROAD-COVERAGE CONNOTATION LEXICON . \n",
            "this paper addresses the problem of SEMANTIC PARALLELISM OF COORDINATION in SEMANTIC PROSODY . we propose a method for SEMANTIC PARALLELISM OF COORDINATION , which is based on the SEMANTIC PARALLELISM OF COORDINATION and the BROAD-COVERAGE CONNOTATION LEXICON . the proposed method is based on a BROAD-COVERAGE CONNOTATION LEXICON and a BROAD-COVERAGE CONNOTATION LEXICON . the proposed method is based on a BROAD-COVERAGE CONNOTATION LEXICON and a BROAD-COVERAGE CONNOTATION LEXICON . the proposed method is compared with the conventional INDUCTION ALGORITHMS and INDUCTION ALGORITHMS .\n",
            "\n",
            "771 1000\n",
            "matrix approximation -lrb- ma -rrb- is one of the most popular techniques for COLLABORATIVE FILTERING . most existing MA METHODS train USER/ITEM LATENT FACTORS based on a USER-ITEM RATING MATRIX and then use the GLOBAL LATENT FACTORS to model all USERS/ITEMS . however , globally optimized LATENT FACTORS may not reflect the unique interests shared among only subsets of USERS/ITEMS , without which unique interests of users may not be accurately modelled . as a result , existing MA METHODS , which can not capture the uniqueness of different USER/ITEM , can not provide optimal recommendation . in this paper , a MIXTURE PROBABILISTIC MATRIX APPROXIMATION METHOD is proposed , which unifies globally optimized USER/ITEM feature vectors -lrb- on the entire RATING MATRIX -rrb- and LOCALLY OPTIMIZED USER/ITEM FEATURE VECTORS -lrb- on subsets of USER/ITEM RATINGS -rrb- to improve RECOMMENDATION ACCURACY . more specifically , in MIXTURE PROBABILISTIC MATRIX APPROXIMATION METHOD , a method is developed to find both globally and LOCALLY OPTIMIZED USER/ITEM FEATURE VECTORS . then , a GAUS-SIAN MIXTURE MODEL is adopted to combine GLOBAL PREDICTIONS and LOCAL PREDICTIONS to produce accurate rating predictions . experimental study using MOVIELENS AND NETFLIX DATASETS demonstrates that MIXTURE PROBABILISTIC MATRIX APPROXIMATION METHOD outperforms five state-of-the-art MA BASED CF METHODS in RECOMMENDATION ACCURACY with good scalability . \n",
            "this paper proposes a new MIXTURE PROBABILISTIC MATRIX APPROXIMATION METHOD for COLLABORATIVE FILTERING . the proposed MIXTURE PROBABILISTIC MATRIX APPROXIMATION METHOD is based on the MIXTURE PROBABILISTIC MATRIX APPROXIMATION METHOD and the MIXTURE PROBABILISTIC MATRIX APPROXIMATION METHOD . the proposed MIXTURE PROBABILISTIC MATRIX APPROXIMATION METHOD is based on the LOCALLY OPTIMIZED USER/ITEM FEATURE VECTORS and the RATING MATRIX . the proposed MIXTURE PROBABILISTIC MATRIX APPROXIMATION METHOD is evaluated on the MOVIELENS AND NETFLIX DATASETS and on the MOVIELENS AND NETFLIX DATASETS . experimental results on MOVIELENS AND NETFLIX DATASETS show that the proposed MIXTURE PROBABILISTIC MATRIX APPROXIMATION METHOD outperforms the conventional MA BASED CF METHODS in terms of RECOMMENDATION ACCURACY and RECOMMENDATION ACCURACY .\n",
            "\n",
            "772 1000\n",
            "learning to hash involves learning HASH FUNCTIONS from a set of IMAGES for embedding HIGH-DIMENSIONAL VISUAL DESCRIPTORS into a SIMILARITY-PRESERVING LOW-DIMENSIONAL HAMMING SPACE . most of existing methods resort to a single representation of IMAGES , that is , only one type of VISUAL DESCRIPTORS is used to learn a HASH FUNCTION to assign BINARY CODES to IMAGES . however , IMAGES are often described by multiple different VISUAL DESCRIPTORS -lrb- such as SIFT , GIST , hog -rrb- , so it is desirable to incorporate these multiple representations into learning a HASH FUNCTION , leading to MULTI-VIEW HASHING . in this paper we present a SEQUENTIAL SPECTRAL LEARNING APPROACH to MULTI-VIEW HASHING where a HASH FUNCTION is sequentially determined by solving the successive <unk> of LOCAL VARIANCES subject to DECORRELATION CONSTRAINTS . we compute MULTI-VIEW LOCAL VARIANCES by Α-AVERAGING VIEW-SPECIFIC DISTANCE MATRICES such that the best AVERAGED DISTANCE MATRIX is determined by minimizing its <unk> from VIEW-SPECIFIC DISTANCE MATRICES . we also present a scalable implementation , exploiting a fast APPROXIMATE K-NN GRAPH CONSTRUCTION METHOD , in which Α-AVERAGED DISTANCES computed in SMALL PARTITIONS determined by RECURSIVE SPECTRAL BISECTION are gradually merged in conquer steps until whole examples are used . numerical experiments on <unk> , <unk> , and NUS-WIDE DATASETS confirm the high performance of our SEQUENTIAL SPECTRAL LEARNING APPROACH , in comparison to SINGLE-VIEW SPECTRAL HASHING as well as existing MULTI-VIEW HASHING METHODS . \n",
            "this paper proposes a new APPROXIMATE K-NN GRAPH CONSTRUCTION METHOD for MULTI-VIEW HASHING . the APPROXIMATE K-NN GRAPH CONSTRUCTION METHOD is based on a SEQUENTIAL SPECTRAL LEARNING APPROACH and a SEQUENTIAL SPECTRAL LEARNING APPROACH . the proposed APPROXIMATE K-NN GRAPH CONSTRUCTION METHOD is based on the use of VIEW-SPECIFIC DISTANCE MATRICES and DECORRELATION CONSTRAINTS . the proposed APPROXIMATE K-NN GRAPH CONSTRUCTION METHOD is based on a SEQUENTIAL SPECTRAL LEARNING APPROACH , which is based on the AVERAGED DISTANCE MATRIX . the proposed APPROXIMATE K-NN GRAPH CONSTRUCTION METHOD is based on the use of VIEW-SPECIFIC DISTANCE MATRICES and DECORRELATION CONSTRAINTS . the proposed APPROXIMATE K-NN GRAPH CONSTRUCTION METHOD is compared with conventional MULTI-VIEW HASHING METHODS and MULTI-VIEW HASHING METHODS . the proposed APPROXIMATE K-NN GRAPH CONSTRUCTION METHOD is compared with other MULTI-VIEW HASHING METHODS and MULTI-VIEW HASHING METHODS . the proposed APPROXIMATE K-NN GRAPH CONSTRUCTION METHOD is compared with other MULTI-VIEW HASHING METHODS and is shown to outperform the conventional MULTI-VIEW HASHING METHODS .\n",
            "\n",
            "773 1000\n",
            "recent work has shown impressive TRANSFORM-INVARIANT MODELING and CLUSTERING for sets of images of objects with similar appearance . we seek to expand these capabilities to sets of images of an OBJECT CLASS that show considerable variation across individual instances -lrb- e.g. PEDESTRIAN IMAGES -rrb- using a representation based on PIXEL-WISE SIMILARITIES , SIMILARITY TEMPLATES . because of its invariance to the colors of particular components of an object , this representation enables detection of instances of an OBJECT CLASS and enables alignment of those instances . further , this model implicitly represents the regions of color regularity in the CLASS-SPECIFIC IMAGE SET enabling a decomposition of that OBJECT CLASS into component regions . \n",
            "this paper addresses the problem of TRANSFORM-INVARIANT MODELING in PEDESTRIAN IMAGES . we propose a method for TRANSFORM-INVARIANT MODELING , which is based on TRANSFORM-INVARIANT MODELING and CLUSTERING . the method is based on TRANSFORM-INVARIANT MODELING and CLUSTERING . the proposed method is evaluated on the CLASS-SPECIFIC IMAGE SET and the CLASS-SPECIFIC IMAGE SET .\n",
            "\n",
            "774 1000\n",
            "to capture the interdependencies between labels in MULTI-LABEL CLASSIFICATION PROBLEMS , CLASSIFIER CHAIN tries to take the multiple labels of each instance into account under a DETERMINISTIC HIGH-ORDER MARKOV CHAIN MODEL . since its performance is sensitive to the choice of label order , the key issue is how to determine the OPTIMAL LABEL ORDER for CLASSIFIER CHAIN . in this work , we first generalize the CLASSIFIER CHAIN over a RANDOM LABEL ORDER . then , we present a theoretical analysis of the GENERALIZATION ERROR for the proposed DETERMINISTIC HIGH-ORDER MARKOV CHAIN MODEL . based on our results , we propose a dynamic programming based classifier chain -lrb- CC-DP -rrb- algorithm to search the GLOBALLY OPTIMAL LABEL ORDER for CLASSIFIER CHAIN and a GREEDY CLASSIFIER CHAIN ALGORITHM to find a locally optimal CLASSIFIER CHAIN . comprehensive experiments on a number of REAL-WORLD MULTI-LABEL DATA SETS from various domains demonstrate that our proposed CC-DP ALGORITHM outperforms state-of-the-art approaches and the CC-DP ALGORITHM achieves comparable prediction performance with CC-DP . \n",
            "this paper proposes a new GREEDY CLASSIFIER CHAIN ALGORITHM for MULTI-LABEL CLASSIFICATION PROBLEMS . the proposed GREEDY CLASSIFIER CHAIN ALGORITHM is based on the GREEDY CLASSIFIER CHAIN ALGORITHM . the proposed GREEDY CLASSIFIER CHAIN ALGORITHM is based on the GREEDY CLASSIFIER CHAIN ALGORITHM . the proposed GREEDY CLASSIFIER CHAIN ALGORITHM is based on the GREEDY CLASSIFIER CHAIN ALGORITHM . the proposed GREEDY CLASSIFIER CHAIN ALGORITHM is evaluated on the REAL-WORLD MULTI-LABEL DATA SETS and on the REAL-WORLD MULTI-LABEL DATA SETS . experimental results show that the proposed GREEDY CLASSIFIER CHAIN ALGORITHM outperforms the conventional CC-DP ALGORITHM in terms of GENERALIZATION ERROR .\n",
            "\n",
            "775 1000\n",
            "this paper focuses on a new CLUSTERING TASK , called <I> SELF-TAUGHT CLUSTERING </I> . SELF-TAUGHT CLUSTERING is an instance of <I> UNSUPERVISED TRANSFER LEARNING </I> , which aims at CLUSTERING a small collection of target unlabeled data with the help of a large amount of <I> AUXILIARY </I> UNLABELED DATA . the TARGET AND AUXILIARY DATA can be different in TOPIC DISTRIBUTION . we show that even when the target data are not sufficient to allow effective learning of a high quality FEATURE REPRESENTATION , it is possible to learn the useful FEATURES with the help of the AUXILIARY DATA on which the target data can be clustered effectively . we propose a CO-CLUSTERING BASED SELF-TAUGHT CLUSTERING ALGORITHM to tackle this problem , by CLUSTERING the TARGET AND AUXILIARY DATA simultaneously to allow the FEATURE REPRESENTATION from the AUXILIARY DATA to influence the target data through a common set of FEATURES . under the new FEATURE REPRESENTATION , CLUSTERING on the target data can be improved . our experiments on IMAGE CLUSTERING show that our CO-CLUSTERING BASED SELF-TAUGHT CLUSTERING ALGORITHM can greatly outperform several state-of-the-art CLUSTERING METHODS when utilizing IRRELEVANT UNLABELED AUXILIARY DATA . \n",
            "this paper presents a new CO-CLUSTERING BASED SELF-TAUGHT CLUSTERING ALGORITHM for IMAGE CLUSTERING . the proposed CO-CLUSTERING BASED SELF-TAUGHT CLUSTERING ALGORITHM is based on the <I> AUXILIARY </I> UNLABELED DATA . the proposed CO-CLUSTERING BASED SELF-TAUGHT CLUSTERING ALGORITHM is based on the use of IRRELEVANT UNLABELED AUXILIARY DATA , and is based on the <I> AUXILIARY </I> UNLABELED DATA . the proposed CO-CLUSTERING BASED SELF-TAUGHT CLUSTERING ALGORITHM is based on the <I> AUXILIARY </I> UNLABELED DATA . the proposed CO-CLUSTERING BASED SELF-TAUGHT CLUSTERING ALGORITHM is based on the <I> AUXILIARY </I> UNLABELED DATA . the proposed CO-CLUSTERING BASED SELF-TAUGHT CLUSTERING ALGORITHM is compared with other CLUSTERING METHODS . the proposed CO-CLUSTERING BASED SELF-TAUGHT CLUSTERING ALGORITHM is compared with other CLUSTERING METHODS . the proposed CO-CLUSTERING BASED SELF-TAUGHT CLUSTERING ALGORITHM is compared with other CLUSTERING METHODS .\n",
            "\n",
            "776 1000\n",
            "we present a MACHINE LEARNING APPROACH to evaluating the <unk> of output of a MACHINE TRANSLATION SYSTEM , using CLASSIFIERS that learn to distinguish HUMAN REFERENCE TRANSLATIONS from MACHINE TRANSLATIONS . this MACHINE LEARNING APPROACH can be used to evaluate an MACHINE TRANSLATION SYSTEM , tracking improvements over time ; to aid in the kind of FAILURE ANALYSIS that can help guide system development ; and to select among alternative output strings . the MACHINE LEARNING APPROACH presented is fully automated and independent of source language , target language and domain . \n",
            "this paper presents a new MACHINE LEARNING APPROACH for FAILURE ANALYSIS . the proposed MACHINE LEARNING APPROACH is based on the use of HUMAN REFERENCE TRANSLATIONS in the MACHINE TRANSLATION SYSTEM . the proposed MACHINE LEARNING APPROACH is based on the use of HUMAN REFERENCE TRANSLATIONS , and is shown to be more robust to HUMAN REFERENCE TRANSLATIONS .\n",
            "\n",
            "777 1000\n",
            "massive multichannel reproduction systems like WAVE FIELD SYNTHESIS are potentially well suited to be complemented by LISTENING ROOM EQUALIZATION . however , their typically large number of REPRODUCTION CHANNELS makes this task challenging for both COMPUTATIONAL AND ALGORITHMIC REASONS . WAVE-DOMAIN ADAPTIVE FILTERING was proposed earlier and is especially well-suited to ADAP-TIVE FILTERING TASKS in the context of WAVE FIELD SYNTHESIS . in this paper , we propose to generalize the model originally used for WAVE-DOMAIN ADAPTIVE FILTERING to allow an ADAPTIVE LRE for a broader range of REPRODUCTION SCENARIOS , while maintaining the advantages of the original ADAPTIVE LRE . the proposed ADAPTIVE LRE is evaluated for FILTERING STRUCTURES of varying complexity along with considering the ROBUSTNESS to varying LISTENER POSITIONS . \n",
            "this paper addresses the problem of WAVE FIELD SYNTHESIS for MASSIVE MULTICHANNEL REPRODUCTION SYSTEMS . we propose a new method for LISTENING ROOM EQUALIZATION based on WAVE-DOMAIN ADAPTIVE FILTERING . the proposed MASSIVE MULTICHANNEL REPRODUCTION SYSTEMS is based on the use of WAVE-DOMAIN ADAPTIVE FILTERING , which are based on WAVE-DOMAIN ADAPTIVE FILTERING . the ROBUSTNESS of the proposed method is demonstrated by the use of WAVE-DOMAIN ADAPTIVE FILTERING for LISTENING ROOM EQUALIZATION . the ROBUSTNESS of the proposed method is compared with the conventional MASSIVE MULTICHANNEL REPRODUCTION SYSTEMS .\n",
            "\n",
            "778 1000\n",
            "recently , it has become evident that SUBMODULARITY naturally captures widely occurring concepts in MACHINE LEARNING , SIGNAL PROCESSING and COMPUTER VISION . consequently , there is need for efficient OPTIMIZATION PROCEDURES for SUBMODU-LAR FUNCTIONS , especially for MINIMIZATION PROBLEMS . while general submodular minimization is challenging , we propose a new method that exploits existing DE-COMPOSABILITY OF SUBMODULAR FUNCTIONS . in contrast to previous approaches , our method is neither approximate , nor impractical , nor does it need any cumbersome PARAMETER TUNING . moreover , it is easy to implement and parallelize . a key component of our method is a formulation of the DISCRETE SUBMODULAR MINIMIZATION PROBLEM as a CONTINUOUS BEST APPROXIMATION PROBLEM that is solved through a sequence of reflections , and its solution can be easily <unk> to obtain an optimal discrete solution . this method solves both the continuous and discrete formulations of the DISCRETE SUBMODULAR MINIMIZATION PROBLEM , and therefore has applications in LEARNING , INFERENCE , and RECONSTRUCTION . in our experiments , we illustrate the benefits of our method on two IMAGE SEGMENTATION TASKS . \n",
            "this paper addresses the problem of DISCRETE SUBMODULAR MINIMIZATION PROBLEM in COMPUTER VISION . we propose a method for INFERENCE based on DE-COMPOSABILITY OF SUBMODULAR FUNCTIONS and SUBMODULARITY . the proposed method is based on the use of SUBMODU-LAR FUNCTIONS and SUBMODULARITY . the proposed method is based on the use of SUBMODU-LAR FUNCTIONS and SUBMODULARITY . the proposed method is based on the use of SUBMODU-LAR FUNCTIONS and SUBMODULARITY . the proposed method is evaluated on IMAGE SEGMENTATION TASKS and IMAGE SEGMENTATION TASKS .\n",
            "\n",
            "779 1000\n",
            "in this paper , we consider the effect of a bandwidth extension of NARROW-BAND SPEECH signals -lrb- <unk> .4 khz -rrb- to <unk> khz on SPEAKER VERIFICATION . using COVARIANCE MATRIX BASED VERIFICATION SYSTEMS together with DETECTION ERROR TRADE-OFF CURVES , we compare the performance between systems operating on narrow-band , wide-band -lrb- <unk> khz -rrb- , and BANDWIDTH-EXTENDED SPEECH . the experiments were conducted using different SHORT-TIME SPECTRAL PARAMETERIZATIONS derived from MICROPHONE AND ISDN SPEECH DATABASES . the studied BANDWIDTH-EXTENSION ALGORITHM did not introduce artifacts that affected the SPEAKER VERIFICATION TASK , and we achieved improvements between 1 and 10 percent -lrb- depending on the MODEL ORDER -rrb- over the VERIFICATION SYSTEM designed for NARROW-BAND SPEECH when MEL-FREQUENCY CEPSTRAL COEFFICIENTS for the SHORT-TIME SPECTRAL PARAMETERIZATION were used . \n",
            "this paper presents a novel VERIFICATION SYSTEM for SPEAKER VERIFICATION . the VERIFICATION SYSTEM is based on a SHORT-TIME SPECTRAL PARAMETERIZATION and a SHORT-TIME SPECTRAL PARAMETERIZATION . the proposed BANDWIDTH-EXTENSION ALGORITHM is based on a BANDWIDTH-EXTENSION ALGORITHM and a SHORT-TIME SPECTRAL PARAMETERIZATION . the proposed BANDWIDTH-EXTENSION ALGORITHM is based on the BANDWIDTH-EXTENSION ALGORITHM and the BANDWIDTH-EXTENSION ALGORITHM . the VERIFICATION SYSTEM is applied to the SPEAKER VERIFICATION TASK . the VERIFICATION SYSTEM is evaluated on a SPEAKER VERIFICATION TASK . the results show that the proposed method is robust and robust to NARROW-BAND SPEECH .\n",
            "\n",
            "780 1000\n",
            "speech separation based on TIME-FREQUENCY MASKING has been shown to improve INTELLIGIBILITY OF SPEECH SIGNALS corrupted by noise . a perceived weakness of BINARY MASKING is the quality of SEPARATED SPEECH . in this paper , an approach for improving the PERCEPTUAL QUALITY of SEPARATED SPEECH from BINARY MASKING is proposed . our approach consists of two stages , where a BINARY MASK is generated in the first stage that effectively performs SPEECH SEPARATION . in the second stage , a SPARSE-REPRESENTATION APPROACH is used to represent the SEPARATED SIGNAL by a linear combination of SHORT-TIME FOURIER TRANSFORM MAGNITUDES that are generated from a CLEAN SPEECH DICTIONARY . OVERLAP-AND-ADD SYNTHESIS is then used to generate an estimate of the SPEECH SIGNAL . the performance of the proposed approach is evaluated with the PERCEPTUAL EVALUATION OF SPEECH QUALITY , which is a standard objective SPEECH QUALITY MEASURE . the proposed algorithm offers considerable improvements in SPEECH QUALITY over BINARY-MASKED NOISY SPEECH and other RECONSTRUCTION APPROACHES . \n",
            "this paper presents a new method for SPEECH SEPARATION from BINARY-MASKED NOISY SPEECH . the SPARSE-REPRESENTATION APPROACH is based on the SHORT-TIME FOURIER TRANSFORM MAGNITUDES and the SHORT-TIME FOURIER TRANSFORM MAGNITUDES . the proposed SPARSE-REPRESENTATION APPROACH is based on the SHORT-TIME FOURIER TRANSFORM MAGNITUDES and the SHORT-TIME FOURIER TRANSFORM MAGNITUDES . the proposed SPARSE-REPRESENTATION APPROACH is based on the SPARSE-REPRESENTATION APPROACH and the SPARSE-REPRESENTATION APPROACH . the proposed RECONSTRUCTION APPROACHES is evaluated on the INTELLIGIBILITY OF SPEECH SIGNALS and the PERCEPTUAL EVALUATION OF SPEECH QUALITY is shown to outperform the conventional RECONSTRUCTION APPROACHES in terms of PERCEPTUAL QUALITY and PERCEPTUAL QUALITY .\n",
            "\n",
            "781 1000\n",
            "word sense disambiguation -lrb- WORD SENSE DISAMBIGUATION SYSTEMS -rrb- systems based on SUPERVISED LEARNING achieved the best performance in SENSE-VAL AND SEMEVAL WORKSHOPS . however , there are few publicly available SUPERVISED ENGLISH ALL-WORDS WSD SYSTEM . this limits the use of WORD SENSE DISAMBIGUATION SYSTEMS in other applications , especially for researchers whose research interests are not in WORD SENSE DISAMBIGUATION SYSTEMS . in this paper , we present IMS , a SUPERVISED ENGLISH ALL-WORDS WSD SYSTEM . the flexible framework of IMS allows users to integrate different PREPROCESSING TOOLS , additional FEATURES , and different CLASSIFIERS . by default , we use LINEAR SUPPORT VECTOR MACHINES as the CLASSIFIER with multiple KNOWLEDGE-BASED FEATURES . in our implementation , IMS achieves state-of-the-art results on several SENSEVAL AND SEMEVAL TASKS . \n",
            "this paper presents a new method for WORD SENSE DISAMBIGUATION SYSTEMS based on LINEAR SUPPORT VECTOR MACHINES . the proposed method is based on the use of KNOWLEDGE-BASED FEATURES and KNOWLEDGE-BASED FEATURES . the proposed method is based on the use of KNOWLEDGE-BASED FEATURES and KNOWLEDGE-BASED FEATURES . the proposed method is based on the use of KNOWLEDGE-BASED FEATURES and KNOWLEDGE-BASED FEATURES . the experimental results show the effectiveness of the proposed SUPERVISED ENGLISH ALL-WORDS WSD SYSTEM in terms of IMS and IMS .\n",
            "\n",
            "782 1000\n",
            "many traditional methods for SHAPE CLASSIFICATION involve establishing POINT CORRESPONDENCES between shapes to produce matching scores , which are in turn used as SIMILARITY MEASURES for SHAPE CLASSIFICATION . LEARNING TECHNIQUES have been applied only in the second stage of this process , after the matching scores have been obtained . in this paper , instead of simply taking for granted the scores obtained by matching and then learning a CLASSIFIER , we learn the matching scores themselves so as to produce SHAPE SIMILARITY SCORES that minimize the CLASSIFICATION LOSS . the solution is based on a MAX-MARGIN FORMULATION in the STRUCTURED PREDICTION SETTING . experiments in SHAPE DATABASES reveal that such an integrated learning algorithm substantially improves on existing methods . \n",
            "this paper addresses the problem of SHAPE CLASSIFICATION in SHAPE DATABASES . we propose a method for SHAPE CLASSIFICATION based on a MAX-MARGIN FORMULATION . the proposed method is based on a MAX-MARGIN FORMULATION . the proposed method is based on a MAX-MARGIN FORMULATION , which is based on the MAX-MARGIN FORMULATION . the proposed method is based on a MAX-MARGIN FORMULATION . the experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "783 1000\n",
            "we introduce an efficient algorithm for performing DISTRIBUTED PRINCIPAL COMPONENT ANALYSIS on DIRECTED GAUSSIAN GRAPHICAL MODELS . by exploiting STRUCTURED SPARSITY in the CHOLESKY FACTOR of the INVERSE COVARIANCE MATRIX , our proposed <unk> algorithm computes GLOBAL PRINCIPAL SUBSPACE ESTIMATION through LOCAL COMPUTATION and MESSAGE PASSING . we show significant performance and <unk> advantages of <unk> for ONLINE PRINCIPAL SUBSPACE ESTIMATION and DISTRIBUTED ANOMALY DETECTION in REAL-WORLD COMPUTER NETWORKS . \n",
            "this paper addresses the problem of DISTRIBUTED ANOMALY DETECTION in REAL-WORLD COMPUTER NETWORKS . we propose a method for ONLINE PRINCIPAL SUBSPACE ESTIMATION based on the INVERSE COVARIANCE MATRIX . the proposed method is based on the INVERSE COVARIANCE MATRIX and the INVERSE COVARIANCE MATRIX . the proposed method is based on the INVERSE COVARIANCE MATRIX and the INVERSE COVARIANCE MATRIX . the proposed method is based on the INVERSE COVARIANCE MATRIX and the DISTRIBUTED PRINCIPAL COMPONENT ANALYSIS . the proposed method is compared with other state-of-the-art methods .\n",
            "\n",
            "784 1000\n",
            "there are many challenging problems for VIETNAMESE LANGUAGE PROCESSING . it will be a long time before these challenges are met . even some apparently simple problems such as SPELLING CORRECTION are quite difficult and have not been approached systematically yet . in this paper , we will discuss one aspect of this type of work : designing the so-called VIETOOLS to detect and correct spelling of vietnamese texts by using a SPELLING DATABASE based on TELEX CODE . VIETOOLS is also extended to serve many purposes in VIETNAMESE LANGUAGE PROCESSING . \n",
            "this paper presents a new method for VIETNAMESE LANGUAGE PROCESSING based on the TELEX CODE . the proposed method is based on a TELEX CODE , which is based on the TELEX CODE . the proposed method is based on a TELEX CODE , which is based on the TELEX CODE . the proposed method is evaluated on the SPELLING DATABASE and the results show that the proposed method is effective for VIETNAMESE LANGUAGE PROCESSING .\n",
            "\n",
            "785 1000\n",
            "many practical coding scenarios deal with sources with TRANSFORM COEFFICIENTS that are well modeled as LAPLACIANS . for the WYNER-ZIV CODING PROBLEM for such sources when CORRELATED SIDE-INFORMATION is available at the DECODER , the SIDE-INFORMATION is modeled as obtained by independent additive laplacian or gaussian innovation on the source . this paper deals with the optimal choice of ENCODING PARAMETERS for practical <unk> coding in such scenarios , using the same QUANTIZER FAMILY as in the REGULAR CODEC to cover a range of RATE-DISTORTION TRADE-OFFS , given the variances of the source and innovation . using our prior analysis of a GENERAL ENCODING MODEL based on MULTI-LEVEL COSET CODES combining SOURCE AND CHANNEL CODING , we present comprehensive tables with optimal ENCODING PARAMETERS . these tables can be readily incorporated into a practical codec to read off the ENCODING PARAMETERS . \n",
            "this paper addresses the problem of MULTI-LEVEL COSET CODES in SOURCE AND CHANNEL CODING . we propose a GENERAL ENCODING MODEL based on the QUANTIZER FAMILY . the proposed method is based on a GENERAL ENCODING MODEL of the TRANSFORM COEFFICIENTS and the TRANSFORM COEFFICIENTS . the proposed method is based on a GENERAL ENCODING MODEL , which is based on the QUANTIZER FAMILY . the proposed method is based on a GENERAL ENCODING MODEL . the proposed method is based on a GENERAL ENCODING MODEL . the proposed method is based on a GENERAL ENCODING MODEL . the proposed method is based on a GENERAL ENCODING MODEL .\n",
            "\n",
            "786 1000\n",
            "personalized TAG RECOMMENDATION systems recommend a list of tags to a user when he is about to annotate an item . it exploits the individual preference and the characteristic of the items . TENSOR FACTORIZATION TECHNIQUES have been applied to many applications , such as TAG RECOMMENDATION . models based on TUCKER DECOMPOSITION can achieve good performance but require a lot of COMPUTATION POWER . on the other hand , models based on CANONICAL DECOMPOSITION can run in LINEAR TIME and are more feasible for ONLINE RECOMMENDATION . in this paper , we propose a novel method for PERSONAL-IZED TAG RECOMMENDATION , which can be considered as a NONLINEAR EXTENSION OF CANONICAL DECOMPOSITION . different from LINEAR TENSOR FACTORIZATION , we exploit GAUS-SIAN RADIAL BASIS FUNCTION to increase the PERSONALIZED TAG RECOMMENDATION SYSTEMS 's capacity . the experimental results show that our proposed method outperforms the state-of-the-art methods for TAG RECOMMENDATION on REAL DATASETS and perform well even with a small number of FEATURES , which verifies that our models can make better use of FEATURES . \n",
            "this paper addresses the problem of ONLINE RECOMMENDATION in PERSONALIZED TAG RECOMMENDATION SYSTEMS . we propose a NONLINEAR EXTENSION OF CANONICAL DECOMPOSITION for the NONLINEAR EXTENSION OF CANONICAL DECOMPOSITION , which is based on the NONLINEAR EXTENSION OF CANONICAL DECOMPOSITION . the proposed method is based on the NONLINEAR EXTENSION OF CANONICAL DECOMPOSITION of the NONLINEAR EXTENSION OF CANONICAL DECOMPOSITION . the proposed method is based on the NONLINEAR EXTENSION OF CANONICAL DECOMPOSITION . the proposed method is based on the NONLINEAR EXTENSION OF CANONICAL DECOMPOSITION . the proposed method is based on the NONLINEAR EXTENSION OF CANONICAL DECOMPOSITION . the proposed method is based on the NONLINEAR EXTENSION OF CANONICAL DECOMPOSITION . the proposed method is based on the NONLINEAR EXTENSION OF CANONICAL DECOMPOSITION .\n",
            "\n",
            "787 1000\n",
            "we present a SINGLE-IMAGE HIGHLIGHT REMOVAL METHOD that incorporates ILLUMINATION-BASED CONSTRAINTS into IMAGE IN-PAINTING . unlike OCCLUDED IMAGE REGIONS filled by traditional IMAGE IN-PAINTING , HIGHLIGHT PIXELS contain some useful information for guiding the IMAGE IN-PAINTING . ILLUMINATION CONSTRAINTS provided by observed PIXEL COLORS , HIGHLIGHT COLOR ANALYSIS and ILLUMINATION COLOR UNIFORMITY are employed in our SINGLE-IMAGE HIGHLIGHT REMOVAL METHOD to improve ESTIMATION OF THE UNDERLYING DIFFUSE COLOR . the inclusion of these ILLUMINATION CONSTRAINTS allows for better RECOVERY OF SHADING AND TEXTURES by IMAGE IN-PAINTING . experimental results are given to demonstrate the performance of our SINGLE-IMAGE HIGHLIGHT REMOVAL METHOD . \n",
            "this paper presents a novel SINGLE-IMAGE HIGHLIGHT REMOVAL METHOD for RECOVERY OF SHADING AND TEXTURES . the SINGLE-IMAGE HIGHLIGHT REMOVAL METHOD is based on a SINGLE-IMAGE HIGHLIGHT REMOVAL METHOD and a SINGLE-IMAGE HIGHLIGHT REMOVAL METHOD . the proposed SINGLE-IMAGE HIGHLIGHT REMOVAL METHOD is based on the ESTIMATION OF THE UNDERLYING DIFFUSE COLOR and the ESTIMATION OF THE UNDERLYING DIFFUSE COLOR . the proposed SINGLE-IMAGE HIGHLIGHT REMOVAL METHOD is based on the ESTIMATION OF THE UNDERLYING DIFFUSE COLOR and the ESTIMATION OF THE UNDERLYING DIFFUSE COLOR . the proposed SINGLE-IMAGE HIGHLIGHT REMOVAL METHOD is evaluated on the RECOVERY OF SHADING AND TEXTURES and the RECOVERY OF SHADING AND TEXTURES .\n",
            "\n",
            "788 1000\n",
            "<unk> coding is a novel IMAGE CODER that utilizes the self-similarity of NATURAL IMAGES that include TEXTURES , in order to achieve STRUCTURALLY LOSSLESS COMPRESSION . the key to a high compression ratio is replacing large image blocks with previously ENCODED BLOCKS with similar structure . adjusting the lighting of the replaced block is critical for eliminating ILLUMINATION ARTIFACTS and increasing the number of matches . we propose a new ADAPTIVE LIGHTING CORRECTION METHOD that is based on the POISSON EQUATION with INCOMPLETE BOUNDARY CONDITIONS . in order to fully exploit the benefits of the ADAPTIVE POISSON LIGHTING CORRECTION , we also propose modifications of the SIDE-MATCHING ALGORITHM and STRUCTURAL TEXTURE SIMILARITY METRIC . we show that the resulting ADAPTIVE LIGHTING CORRECTION METHOD achieves better coding performance . \n",
            "this paper presents a new method for ADAPTIVE POISSON LIGHTING CORRECTION in NATURAL IMAGES . the proposed method is based on the STRUCTURAL TEXTURE SIMILARITY METRIC and the STRUCTURAL TEXTURE SIMILARITY METRIC of the IMAGE CODER . the proposed method is based on the STRUCTURAL TEXTURE SIMILARITY METRIC and the STRUCTURAL TEXTURE SIMILARITY METRIC of the POISSON EQUATION . the proposed method is based on the STRUCTURAL TEXTURE SIMILARITY METRIC of the POISSON EQUATION and the SIDE-MATCHING ALGORITHM . the proposed method is compared with the conventional ADAPTIVE LIGHTING CORRECTION METHOD and the ADAPTIVE LIGHTING CORRECTION METHOD .\n",
            "\n",
            "789 1000\n",
            "one important class of STATE EMISSION DENSITIES of the HIDDEN MARKOV MODEL is the GAUSSIAN MIXTURE DENSITIES . the classical BAUM-WELCH ALGORITHM often fails to reliably learn the GAUSSIAN MIXTURE DENSITIES when there is INSUFFICIENT TRAINING DATA , due to the large number of FREE PARAMETERS present in the model . in this paper , we propose a novel strategy for robustly and accurately learning the GAUSSIAN MIXTURE STATE EMISSION DENSITIES of the HIDDEN MARKOV MODEL . the strategy is based on an ENSEMBLE FRAMEWORK for PROBABILITY DENSITY ESTIMATION in which the learning of the GAUSSIAN MIXTURE DENSITIES is formulated as a GRADIENT DESCENT SEARCH in a FUNCTION SPACE . the resulting LEARNING ALGORITHM is called '' the BOOSTING BAUM-WELCH ALGORITHM . '' our preliminary experiment results on EMOTION RECOGNITION from speech show that the proposed algorithm outperforms the original BAUM-WELCH ALGORITHM on this task . \n",
            "this paper addresses the problem of EMOTION RECOGNITION in EMOTION RECOGNITION . we propose a HIDDEN MARKOV MODEL for EMOTION RECOGNITION , which is based on a HIDDEN MARKOV MODEL . the proposed LEARNING ALGORITHM is based on a HIDDEN MARKOV MODEL with a HIDDEN MARKOV MODEL . the proposed LEARNING ALGORITHM is based on a HIDDEN MARKOV MODEL with a HIDDEN MARKOV MODEL . the proposed LEARNING ALGORITHM is based on a HIDDEN MARKOV MODEL . the proposed LEARNING ALGORITHM is based on a HIDDEN MARKOV MODEL and is shown to be robust to EMOTION RECOGNITION .\n",
            "\n",
            "790 1000\n",
            "this paper concerns the ROBUST ESTIMATION OF NON-RIGID DEFORMATIONS from FEATURE CORRESPONDENCES . we advance the surprising view that for many realistic PHYSICAL DEFORMATIONS , the error of the mismatches -lrb- outliers -rrb- usually <unk> the effects of the curvature of the MANIFOLD on which the correct matches -lrb- inliers -rrb- lie , to the extent that one can tightly <unk> the MANIFOLD within the ERROR BOUNDS of a LOW-DIMENSIONAL HY-PERPLANE for ACCURATE OUTLIER REJECTION . this justifies a simple RANSAC-DRIVEN DEFORMABLE REGISTRATION TECHNIQUE that is at least as accurate as other methods based on the OPTIMISATION OF FULLY DEFORMABLE MODELS . we support our ideas with comprehensive experiments on SYNTHETIC AND REAL DATA typical of the deformations examined in the literature . \n",
            "this paper addresses the problem of ROBUST ESTIMATION OF NON-RIGID DEFORMATIONS in LOW-DIMENSIONAL HY-PERPLANE . we propose a RANSAC-DRIVEN DEFORMABLE REGISTRATION TECHNIQUE based on the OPTIMISATION OF FULLY DEFORMABLE MODELS . the proposed method is based on the OPTIMISATION OF FULLY DEFORMABLE MODELS . the proposed method is based on the OPTIMISATION OF FULLY DEFORMABLE MODELS . the proposed method is based on the OPTIMISATION OF FULLY DEFORMABLE MODELS . the proposed method is based on the OPTIMISATION OF FULLY DEFORMABLE MODELS . the proposed method is based on the OPTIMISATION OF FULLY DEFORMABLE MODELS . the proposed method is evaluated on the SYNTHETIC AND REAL DATA .\n",
            "\n",
            "791 1000\n",
            "it has been previously demonstrated that systems based on BLOCK WISE LOCAL FEATURES and GAUSSIAN MIXTURE MODELS are suitable for VIDEO BASED TALKING FACE VERIFICATION due to the best trade-off in terms of COMPLEXITY , ROBUSTNESS and performance . in this paper , we propose two methods to enhance the ROBUSTNESS and performance of the GMM-ZTNORM BASELINE SYSTEM . first , JOINT FACTOR ANALYSIS is performed to compensate the SESSION VARIABILITIES due to different RECORDING DEVICES , LIGHTING CONDITIONS , FACIAL EXPRESSIONS , etc. . second , the difference between the UNIVERSAL BACKGROUND MODEL and the maximum a POSTERIORI ADAPTED MODEL is mapped into the GMM MEAN SHIFTED SUPERVECTOR whose OVER-COMPLETE DICTIONARY becomes more incoherent . then , for verification purpose , the SPARSE REPRESENTATION computed by L 1-MINIMIZATION with QUADRATIC CONSTRAINTS is employed to model these GMM MEAN SHIFTED SU-PERVECTORS . experimental results show that the proposed system achieved 8.4 % -lrb- group 1 -rrb- and 10.5 % -lrb- group 2 -rrb- equal ERROR RATE on the BANCA TALKING FACE VIDEO DATABASE following the P PROTOCOL and outperformed the GMM-ZTNORM BASELINE by yielding more than 20 % RELATIVE ERROR REDUCTION . \n",
            "this paper addresses the problem of VIDEO BASED TALKING FACE VERIFICATION in RECORDING DEVICES . in this paper , we propose a POSTERIORI ADAPTED MODEL based on a POSTERIORI ADAPTED MODEL and a POSTERIORI ADAPTED MODEL . the proposed method is based on a POSTERIORI ADAPTED MODEL and a POSTERIORI ADAPTED MODEL based on the GMM MEAN SHIFTED SUPERVECTOR . the proposed method is based on a POSTERIORI ADAPTED MODEL and a POSTERIORI ADAPTED MODEL . experimental results on BANCA TALKING FACE VIDEO DATABASE show that the proposed method outperforms the conventional GMM-ZTNORM BASELINE SYSTEM in terms of ERROR RATE and ERROR RATE .\n",
            "\n",
            "792 1000\n",
            "iterative methods that take steps in APPROXIMATE SUBGRADIENT DIRECTIONS have proved to be useful for STOCHASTIC LEARNING PROBLEMS over LARGE OR STREAMING DATA SETS . when the objective consists of a LOSS FUNCTION plus a NONSMOOTH REGULARIZATION TERM , whose purpose is to induce structure -lrb- for example , spar-sity -rrb- in the solution , the solution often lies on a LOW-DIMENSIONAL MANIFOLD along which the regularizer is smooth . this paper shows that a REGULARIZED DUAL AVERAGING ALGORITHM can identify this MANIFOLD with high probability . this observation motivates an ALGORITH-MIC STRATEGY in which , once a NEAR-OPTIMAL MANIFOLD is identified , we switch to an REGULARIZED DUAL AVERAGING ALGORITHM that searches only in this MANIFOLD , which typically has much lower INTRINSIC DIMENSION than the FULL SPACE , thus converging quickly to a NEAR-OPTIMAL POINT with the desired structure . computational results are presented to illustrate these claims . \n",
            "this paper proposes a new REGULARIZED DUAL AVERAGING ALGORITHM for STOCHASTIC LEARNING PROBLEMS . the REGULARIZED DUAL AVERAGING ALGORITHM is based on the REGULARIZED DUAL AVERAGING ALGORITHM . the REGULARIZED DUAL AVERAGING ALGORITHM is based on the REGULARIZED DUAL AVERAGING ALGORITHM . the REGULARIZED DUAL AVERAGING ALGORITHM is based on the REGULARIZED DUAL AVERAGING ALGORITHM . the proposed REGULARIZED DUAL AVERAGING ALGORITHM is based on the REGULARIZED DUAL AVERAGING ALGORITHM and the REGULARIZED DUAL AVERAGING ALGORITHM is applied to the MANIFOLD . the proposed REGULARIZED DUAL AVERAGING ALGORITHM is based on the REGULARIZED DUAL AVERAGING ALGORITHM . the proposed REGULARIZED DUAL AVERAGING ALGORITHM is based on the REGULARIZED DUAL AVERAGING ALGORITHM . the proposed REGULARIZED DUAL AVERAGING ALGORITHM is based on the REGULARIZED DUAL AVERAGING ALGORITHM . the proposed REGULARIZED DUAL AVERAGING ALGORITHM is based on the REGULARIZED DUAL AVERAGING ALGORITHM .\n",
            "\n",
            "793 1000\n",
            "in this paper we propose an algorithm to learn STATISTICAL LANGUAGE UNDERSTANDING MODELS from a CORPUS OF UNALIGNED PAIRS OF SENTENCES and their corresponding SEMANTIC REPRESENTATION . specifically , it allows to automatically map VARIABLE-LENGTH WORD SEGMENTS with their corresponding SEMANTIC UNITS and thus , the DECODING OF USER UTTERANCES to their corresponding meanings . in this way we avoid the time consuming work of manually associate semantic labels to words , process which is needed by almost all the CORPUS-BASED APPROACHES . we use the algorithm to learn the UNDERSTANDING COMPONENT of a SPOKEN DIALOG SYSTEM for RAILWAY INFORMATION RETRIEVAL in SPANISH . experiments show that the results obtained with the proposed method are very promising , whereas the effort employed to obtain the models is not comparable with this of manually segment the training corpus . \n",
            "this paper presents a new method for RAILWAY INFORMATION RETRIEVAL . the proposed method is based on a CORPUS OF UNALIGNED PAIRS OF SENTENCES and a CORPUS OF UNALIGNED PAIRS OF SENTENCES . the UNDERSTANDING COMPONENT is based on the CORPUS OF UNALIGNED PAIRS OF SENTENCES . the proposed method is based on the CORPUS OF UNALIGNED PAIRS OF SENTENCES , which is based on the CORPUS OF UNALIGNED PAIRS OF SENTENCES . the proposed method is based on the CORPUS OF UNALIGNED PAIRS OF SENTENCES . the proposed method is based on the CORPUS OF UNALIGNED PAIRS OF SENTENCES . the proposed method is based on the CORPUS OF UNALIGNED PAIRS OF SENTENCES .\n",
            "\n",
            "794 1000\n",
            "the use of multiple features for TRACKING has been proved as an effective approach because limitation of each FEATURE could be compensated . since different types of variations such as ILLUMINATION , OCCLUSION and POSE may happen in a VIDEO SEQUENCE , especially long sequence videos , how to dynamically select the appropriate features is one of the key problems in this approach . to address this issue in MULTI-CUE VISUAL TRACKING , this paper proposes a new JOINT SPARSE REPRESENTATION MODEL for ROBUST FEATURE-LEVEL FUSION . the proposed JOINT SPARSE REPRESENTATION MODEL dynamically removes UNRELIABLE FEATURES to be fused for TRACKING by using the advantages of SPARSE REPRESENTATION . as a result , robust TRACKING performance is obtained . experimental results on publicly available videos show that the proposed JOINT SPARSE REPRESENTATION MODEL outperforms both existing SPARSE REPRESENTATION based and FUSION-BASED TRACKERS . \n",
            "this paper presents a JOINT SPARSE REPRESENTATION MODEL for MULTI-CUE VISUAL TRACKING . the JOINT SPARSE REPRESENTATION MODEL is based on the JOINT SPARSE REPRESENTATION MODEL and the JOINT SPARSE REPRESENTATION MODEL . the JOINT SPARSE REPRESENTATION MODEL is based on the JOINT SPARSE REPRESENTATION MODEL and the JOINT SPARSE REPRESENTATION MODEL . the JOINT SPARSE REPRESENTATION MODEL is based on the JOINT SPARSE REPRESENTATION MODEL and the JOINT SPARSE REPRESENTATION MODEL . the proposed JOINT SPARSE REPRESENTATION MODEL is based on the JOINT SPARSE REPRESENTATION MODEL and the JOINT SPARSE REPRESENTATION MODEL . the proposed JOINT SPARSE REPRESENTATION MODEL is based on a JOINT SPARSE REPRESENTATION MODEL and is shown to be robust to OCCLUSION and OCCLUSION .\n",
            "\n",
            "795 1000\n",
            "person re-identification has been widely studied due to its importance in surveillance and forensics applications . in practice , GALLERY IMAGES are high-resolution -lrb- hr -rrb- while PROBE IMAGES are usually LOW-RESOLUTION in the IDENTIFICATION SCENARIOS with large variation of ILLUMINATION , weather or quality of cameras . PERSON RE-IDENTIFICATION in this kind of scenarios , which we call SUPER-RESOLUTION PERSON RE-IDENTIFICATION , has not been well studied . in this paper , we propose a <unk> low-rank discriminant dictionary learning -lrb- SLD2L -rrb- approach for SR PERSON RE-IDENTIFICATION TASK . with the hr and lr dictionary pair and MAPPING MATRICES learned from the FEATURES of HR AND LR TRAINING IMAGES , SLD2L can convert the FEATURES of lr PROBE IMAGES into HR FEATURES . to ensure that the CONVERTED FEATURES have favorable DISCRIMINATIVE CAPABILITY and the learned dictionaries can well characterize INTRINSIC FEATURE SPACES OF HR AND LR IMAGES , we design a DISCRIMINANT TERM and a LOW-RANK REGULARIZATION TERM for SLD2L . moreover , considering that low resolution results in different degrees of loss for different types of VISUAL APPEARANCE FEATURES , we propose a MULTI-VIEW SLD2L APPROACH , which can learn the TYPE-SPECIFIC DICTIONARY PAIR and mappings for each type of FEATURE . experimental results on multiple PUBLICLY AVAILABLE DATASETS demonstrate the effectiveness of our proposed approaches for the SR PERSON RE-IDENTIFICATION TASK . \n",
            "this paper presents a MULTI-VIEW SLD2L APPROACH for SUPER-RESOLUTION PERSON RE-IDENTIFICATION for SUPER-RESOLUTION PERSON RE-IDENTIFICATION . the MULTI-VIEW SLD2L APPROACH is based on a LOW-RANK REGULARIZATION TERM and a LOW-RANK REGULARIZATION TERM . the FEATURE is modeled by a LOW-RANK REGULARIZATION TERM and a LOW-RANK REGULARIZATION TERM . the proposed MULTI-VIEW SLD2L APPROACH is based on a LOW-RANK REGULARIZATION TERM and a LOW-RANK REGULARIZATION TERM . the proposed MULTI-VIEW SLD2L APPROACH is based on a MULTI-VIEW SLD2L APPROACH and a LOW-RANK REGULARIZATION TERM . the proposed MULTI-VIEW SLD2L APPROACH is based on a LOW-RANK REGULARIZATION TERM and a LOW-RANK REGULARIZATION TERM . the proposed MULTI-VIEW SLD2L APPROACH is based on a LOW-RANK REGULARIZATION TERM and is shown to be more robust to ILLUMINATION than the conventional MULTI-VIEW SLD2L APPROACH .\n",
            "\n",
            "796 1000\n",
            "the MANIPULATION OF PROSODY , including pitch , DURATION and intensity , is one of the leading approaches in SYNTHESIZING EMOTION . this paper reports work on the development of a MALAY EMOTIONAL SYNTHESIZER capable of expressing four basic emotions , namely happiness , ANGER , SADNESS and fear for any form of TEXT INPUT with various INTONATION PATTERNS using the PROSODY MANIPULATION PRINCIPLE . the MALAY EMOTIONAL SYNTHESIZER makes use of PROSODY TEMPLATES and PROSODY PARAMETRIC MANIPULATION for different types of SENTENCE STRUCTURE . \n",
            "this paper presents a new method for PROSODY PARAMETRIC MANIPULATION based on PROSODY PARAMETRIC MANIPULATION . the proposed method is based on the PROSODY MANIPULATION PRINCIPLE , which is based on the PROSODY MANIPULATION PRINCIPLE and the PROSODY MANIPULATION PRINCIPLE . the PROSODY MANIPULATION PRINCIPLE is based on the PROSODY MANIPULATION PRINCIPLE and the PROSODY MANIPULATION PRINCIPLE . the proposed method is based on the PROSODY MANIPULATION PRINCIPLE and the PROSODY MANIPULATION PRINCIPLE . the proposed method is based on the PROSODY MANIPULATION PRINCIPLE and the PROSODY MANIPULATION PRINCIPLE . the experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "797 1000\n",
            "articulatory feature models have been proposed in the AUTOMATIC SPEECH RECOGNITION COMMUNITY as an alternative to PHONE-BASED MODELS OF SPEECH . in this paper , we extend this approach to the VISUAL MODALITY . specifically , we adapt a recently proposed FEATURE-BASED MODEL of pronunciation variation to VISUAL SPEECH RECOGNITION using a set of VISUALLY-SALIENT FEATURES . the model uses a DYNAMIC BAYES-IAN NETWORK to represent the evolution of the feature streams . a bank of SVM FEATURE CLASSIFIERS , with outputs converted to likelihoods , provides input to the VISUAL SPEECH RECOGNITION . we present preliminary experiments on an ISOLATED-WORD VSR TASK , comparing FEATURE-BASED AND VISEME-BASED UNITS and studying the effects of MODELING INTER-FEATURE ASYNCHRONY . \n",
            "this paper presents a novel FEATURE-BASED MODEL for VISUAL SPEECH RECOGNITION . the FEATURE-BASED MODEL is based on a DYNAMIC BAYES-IAN NETWORK . the FEATURE-BASED MODEL is based on the DYNAMIC BAYES-IAN NETWORK . the FEATURE-BASED MODEL is based on the DYNAMIC BAYES-IAN NETWORK . the FEATURE-BASED MODEL is based on the DYNAMIC BAYES-IAN NETWORK . the proposed FEATURE-BASED MODEL is based on the DYNAMIC BAYES-IAN NETWORK . the proposed FEATURE-BASED MODEL is based on a DYNAMIC BAYES-IAN NETWORK . the proposed FEATURE-BASED MODEL is applied to the AUTOMATIC SPEECH RECOGNITION COMMUNITY .\n",
            "\n",
            "798 1000\n",
            "pivoting on BILINGUAL PARALLEL CORPORA is a popular approach for PARAPHRASE ACQUISITION . although such PIVOTED PARAPHRASE COLLECTIONS have been successfully used to improve the performance of several different NLP APPLICATIONS , it is still difficult to get an intrinsic estimate of the quality and coverage of the PARAPHRASES contained in these collections . we present PARAQUERY , a tool that helps a user interactively explore and characterize a given PIVOTED PARAPHRASE COLLECTION , analyze its utility for a particular domain , and compare it to other popular LEXICAL SIMILARITY RESOURCES -- all within a single interface . \n",
            "this paper presents a method for PARAPHRASE ACQUISITION from BILINGUAL PARALLEL CORPORA . the method is based on the use of BILINGUAL PARALLEL CORPORA , and is based on the use of BILINGUAL PARALLEL CORPORA . the proposed method is based on the use of BILINGUAL PARALLEL CORPORA , which is based on the PARAQUERY . the proposed method is based on the use of BILINGUAL PARALLEL CORPORA , which are used for PARAPHRASE ACQUISITION . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "799 1000\n",
            "we study the computation of the DUAL FRAME for OVERSAMPLED FILTER BANKS by exploiting GREVILLE 'S FORMULA , which was derived in <unk> to compute the pseudo inverse of a matrix when a new ROW is appended . in this paper , we first develop the BACKWARD GREVILLE FORMULA to handle the case of ROW DELETION . based on GREVILLE 'S FORMULA , we then study the DUAL FRAME computation of the LAPLACIAN PYRAMID . through the BACKWARD GREVILLE FORMULA , we investigate OFBS for ROBUST TRANSMISSION over ERASURE CHANNELS . the necessary and sufficient conditions for OFBS robust to one ERASURE CHANNEL are derived . a POST-FILTERING STRUCTURE is also presented to implement the DUAL FRAME when the TRANSFORM COEFFICIENTS in one SUBBAND are completely lost . \n",
            "this paper presents a new method for ROBUST TRANSMISSION based on the GREVILLE 'S FORMULA . the proposed method is based on the BACKWARD GREVILLE FORMULA of the SUBBAND and the TRANSFORM COEFFICIENTS . the proposed method is based on the BACKWARD GREVILLE FORMULA of the GREVILLE 'S FORMULA . the proposed method is based on the BACKWARD GREVILLE FORMULA . the proposed method is based on the BACKWARD GREVILLE FORMULA . the proposed method is based on the BACKWARD GREVILLE FORMULA . the proposed method is based on the BACKWARD GREVILLE FORMULA . the proposed method is based on the BACKWARD GREVILLE FORMULA . the proposed method is based on the BACKWARD GREVILLE FORMULA . the proposed method is based on the BACKWARD GREVILLE FORMULA . the proposed method is based on the BACKWARD GREVILLE FORMULA .\n",
            "\n",
            "800 1000\n",
            "vectors of LOCALLY AGGREGATED DESCRIPTORS have emerged as powerful IMAGE/VIDEO REPRESENTATIONS that compete with or even outperform state-of-the-art approaches on many challenging VISUAL RECOGNITION TASKS . in this paper , we address two fundamental limitations of VECTORS OF LOCALLY AGGREGATED DESCRIPTORS : its requirement for the LOCAL DESCRIPTORS to have VECTOR FORM and its restriction to LINEAR CLASSIFIERS due to its high-dimensionality . to this end , we introduce a KERNELIZED VERSION of VECTORS OF LOCALLY AGGREGATED DESCRIPTORS . this not only lets us inherently exploit more sophisticated CLASSIFICATION SCHEMES , but also enables us to efficiently aggregate NON-VECTOR DESCRIPTORS -lrb- e.g. , TENSORS -rrb- in the VLAD FRAMEWORK . furthermore , we propose three APPROXIMATE FORMULATIONS that allow us to accelerate the CODING PROCESS while still benefiting from the properties of KERNEL VLAD . our experiments demonstrate the effectiveness of our approach at handling MANIFOLD-VALUED DATA , such as COVARIANCE DESCRIPTORS , on several VISUAL RECOGNITION TASKS . our results also evidence the benefits of our NONLINEAR VLAD DESCRIPTORS against the linear ones in EUCLIDEAN SPACE using several standard benchmark datasets . \n",
            "this paper addresses the problem of VISUAL RECOGNITION TASKS in VISUAL RECOGNITION TASKS such as TENSORS . we propose a method for estimating the parameters of a VLAD FRAMEWORK using LOCALLY AGGREGATED DESCRIPTORS . the proposed method is based on the VECTORS OF LOCALLY AGGREGATED DESCRIPTORS , which is based on the VECTORS OF LOCALLY AGGREGATED DESCRIPTORS . the proposed method is based on the use of LOCAL DESCRIPTORS , such as TENSORS , and the VECTORS OF LOCALLY AGGREGATED DESCRIPTORS . the proposed method is based on the VECTORS OF LOCALLY AGGREGATED DESCRIPTORS . the proposed method is based on the use of LOCALLY AGGREGATED DESCRIPTORS . the proposed method is based on the VECTORS OF LOCALLY AGGREGATED DESCRIPTORS . the proposed method is compared with other state-of-the-art CLASSIFICATION SCHEMES such as TENSORS , and the CLASSIFICATION SCHEMES .\n",
            "\n",
            "801 1000\n",
            "the NYSTRÖM METHOD has long been popular for scaling up KERNEL METHODS . its theoretical guarantees and empirical performance rely critically on the quality of the LANDMARKS selected . we study LANDMARK SELECTION for NYSTRÖM using DETERMI-NANTAL POINT PROCESSES , DISCRETE PROBABILITY MODELS that allow tractable generation of diverse samples . we prove that LANDMARKS selected via DETERMI-NANTAL POINT PROCESSES guarantee bounds on APPROXIMATION ERRORS ; subsequently , we analyze implications for KERNEL RIDGE REGRESSION . contrary to prior <unk> due to CUBIC COMPLEXITY of DPP SAMPLING , we show that -lrb- under certain conditions -rrb- MARKOV CHAIN DPP SAMPLING requires only LINEAR TIME in the size of the data . we present several empirical results that support our THEORETICAL ANALYSIS , and demonstrate the superior performance of DPP-BASED LANDMARK SELECTION compared with existing approaches . \n",
            "this paper addresses the problem of DPP-BASED LANDMARK SELECTION for LANDMARK SELECTION . we propose a new NYSTRÖM METHOD based on MARKOV CHAIN DPP SAMPLING . the proposed NYSTRÖM METHOD is based on a MARKOV CHAIN DPP SAMPLING , which is based on the MARKOV CHAIN DPP SAMPLING . the proposed NYSTRÖM METHOD is based on the NYSTRÖM METHOD . the proposed NYSTRÖM METHOD is based on the NYSTRÖM METHOD . the proposed NYSTRÖM METHOD is based on the NYSTRÖM METHOD . the proposed NYSTRÖM METHOD is based on the NYSTRÖM METHOD . the proposed NYSTRÖM METHOD is based on the NYSTRÖM METHOD . the proposed NYSTRÖM METHOD is based on the NYSTRÖM METHOD and is shown to be more robust to APPROXIMATION ERRORS than conventional KERNEL METHODS .\n",
            "\n",
            "802 1000\n",
            "markov decision processes -lrb- MARKOV DECISION PROCESSES -rrb- and CONTINGENCY PLANNING are two widely used approaches to planning under uncertainty . MARKOV DECISION PROCESSES are attractive because the model is extremely general and because many algorithms exist for deriving OPTIMAL PLANS . in contrast , CONTINGENCY PLANNING is normally performed using HEURISTIC TECHNIQUES that do not guarantee op-timality , but the resulting plans are more compact and more understandable . the inability to present CONTINGENCY PLANNING in a clear , intuitive way has limited their applicability in some important domains . we introduce an ANYTIME ALGORITHM for DERIVING CONTINGENCY PLANS that combines the advantages of the two approaches . \n",
            "this paper addresses the problem of DERIVING CONTINGENCY PLANS in MARKOV DECISION PROCESSES . the ANYTIME ALGORITHM is based on the use of HEURISTIC TECHNIQUES and HEURISTIC TECHNIQUES . the proposed method is based on the use of HEURISTIC TECHNIQUES and HEURISTIC TECHNIQUES . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "803 1000\n",
            "user clicks on a URL in response to a query are extremely useful predictors of the URL 's relevance to that query . EXACT MATCH CLICK FEATURES tend to suffer from severe data SPARSITY issues in WEB RANKING . such SPARSITY is particularly pronounced for new URLS or long queries where each distinct <unk> pair will rarely occur . to remedy this , we present a set of straightforward yet informative QUERY-URL N-GRAM FEATURES that allows for generalization of LIMITED USER CLICK DATA to large amounts of unseen <unk> pairs . the QUERY-URL N-GRAM FEATURES is motivated by techniques leveraged in the NLP COMMUNITY for dealing with UNSEEN WORDS . we find that there are interesting REGULARITIES across queries and their preferred destination URLS ; for example , queries containing '' form '' tend to lead to clicks on URLS containing '' pdf '' . we evaluate our set of new QUERY-URL N-GRAM FEATURES on a WEB SEARCH RANKING TASK and obtain improvements that are statistically significant at a <unk> < <unk> level over a strong baseline with exact match <unk> features . \n",
            "this paper presents a new method for WEB RANKING from LIMITED USER CLICK DATA . the proposed method is based on the use of EXACT MATCH CLICK FEATURES in the WEB SEARCH RANKING TASK . the proposed method is based on the EXACT MATCH CLICK FEATURES of the URL . the proposed method is based on the use of EXACT MATCH CLICK FEATURES and the SPARSITY . experimental results on the WEB SEARCH RANKING TASK show that the proposed method outperforms the conventional URLS in terms of SPARSITY .\n",
            "\n",
            "804 1000\n",
            "this paper presents a new ADAPTIVE GRAPH-CUT BASED MOVE-MAKING ALGORITHM for ENERGY MINIMIZATION . traditional MOVE-MAKING ALGORITHMS such as EXPANSION and SWAP operate by searching for better solutions in some PRE-DEFINED MOVES SPACES around the current solution . in contrast , our ADAPTIVE GRAPH-CUT BASED MOVE-MAKING ALGORITHM uses the <unk> interpretation of the EXPANSION-MOVE ALGORITHM to adaptively compute the best <unk> to search over . at each step , it tries to greedily find the <unk> that will lead to biggest decrease in the PRIMAL-DUAL GAP . we test different variants of our ADAPTIVE GRAPH-CUT BASED MOVE-MAKING ALGORITHM on a variety of IMAGE LABELLING PROBLEMS such as OBJECT SEGMENTATION and STEREO . experimental results show that our ADAPTIVE GRAPH-CUT BASED MOVE-MAKING ALGORITHM significantly outper-forms the conventional EXPANSION-MOVE ALGORITHM , in some cases cutting the runtime by 50 % . \n",
            "this paper presents a new ADAPTIVE GRAPH-CUT BASED MOVE-MAKING ALGORITHM for IMAGE LABELLING PROBLEMS . the proposed ADAPTIVE GRAPH-CUT BASED MOVE-MAKING ALGORITHM is based on the use of ENERGY MINIMIZATION , such as STEREO , EXPANSION , and ENERGY MINIMIZATION . the proposed ADAPTIVE GRAPH-CUT BASED MOVE-MAKING ALGORITHM is based on the EXPANSION-MOVE ALGORITHM and the EXPANSION-MOVE ALGORITHM . experimental results on IMAGE LABELLING PROBLEMS show that the proposed ADAPTIVE GRAPH-CUT BASED MOVE-MAKING ALGORITHM outperforms the conventional EXPANSION-MOVE ALGORITHM and the ADAPTIVE GRAPH-CUT BASED MOVE-MAKING ALGORITHM .\n",
            "\n",
            "805 1000\n",
            "this paper presents a method of evaluating UNSU-PERVISED TEXTURE SEGMENTATION ALGORITHMS . the CONTROL SCHEME OF TEXTURE SEGMENTATION has been <unk> as two MODULAR PROCESSES : -lsb- l -rrb- feature computation and -lrb- 2 -rrb- segmentation of HOMOGENEOUS REGIONS based on the FEATURE VALUES . three FEATURE EXTRACTION METHODS are considered : GRAY LEVEL CO-OCCURRENCE MATRAX , LAWS ' TEXTURE ENERGY and GABOR MULTI-CHANNEL FILTERING . three SEGMENTATION ALGORITHMS are considered : FUZZY C-MEANS CLUSTERING , SQUARE-ERROR CLUSTERING and SPLIT-AND-MERGE . a set of 35 REAL SCENE IMAGES with MANUALLY-SPECIFIED GROUND TRUTH was compiled . performance is measured against GROUND TRUTH on REAL IMAGES using REGION-BASED AND PIXEL-BASED PERFORMANCE MET-RICS . \n",
            "this paper addresses the problem of CONTROL SCHEME OF TEXTURE SEGMENTATION , such as GABOR MULTI-CHANNEL FILTERING , GABOR MULTI-CHANNEL FILTERING , and FUZZY C-MEANS CLUSTERING . we propose a CONTROL SCHEME OF TEXTURE SEGMENTATION , called FUZZY C-MEANS CLUSTERING , that combines both REGION-BASED AND PIXEL-BASED PERFORMANCE MET-RICS and FUZZY C-MEANS CLUSTERING . the proposed UNSU-PERVISED TEXTURE SEGMENTATION ALGORITHMS is based on a CONTROL SCHEME OF TEXTURE SEGMENTATION , which is based on the CONTROL SCHEME OF TEXTURE SEGMENTATION and the SEGMENTATION ALGORITHMS . the proposed UNSU-PERVISED TEXTURE SEGMENTATION ALGORITHMS is evaluated on the REAL SCENE IMAGES and on REAL SCENE IMAGES . the results show that the proposed method is robust and robust to HOMOGENEOUS REGIONS such as SPLIT-AND-MERGE , SPLIT-AND-MERGE , and SQUARE-ERROR CLUSTERING .\n",
            "\n",
            "806 1000\n",
            "we propose a new framework , called filtered variation -lrb- <unk> -rrb- , for DE-NOISING and SPARSE SIGNAL PROCESSING APPLICATIONS . these problems are inherently ill-posed . hence , we provide REGULARIZATION to overcome this challenge by using DISCRETE TIME FILTERS that are widely used in SIGNAL PROCESSING . we mathematically define the FV PROBLEM , and solve it using ALTERNATING PROJECTIONS in space and transform domains . we provide a globally convergent algorithm based on the projections onto CONVEX SETS APPROACH . we apply to our algorithm to real denoising problems and compare it with the TOTAL VARIATION RECOVERY . \n",
            "this paper addresses the problem of TOTAL VARIATION RECOVERY for SPARSE SIGNAL PROCESSING APPLICATIONS . we propose a new CONVEX SETS APPROACH based on ALTERNATING PROJECTIONS . the proposed method is based on the use of ALTERNATING PROJECTIONS in the FV PROBLEM . the proposed method is based on the use of ALTERNATING PROJECTIONS in the FV PROBLEM . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "807 1000\n",
            "acoustic models -lrb- ACOUSTIC MODELS -rrb- of an HMM-BASED CLASSIFIER include various types of HIDDEN VARIABLES such as GENDER TYPE , SPEAKING RATE , and ACOUSTIC ENVIRONMENT . if there exists a CANONICALIZATION PROCESS that reduces the influence of the HIDDEN VARIABLES from the ACOUSTIC MODELS , a ROBUST AUTOMATIC SPEECH RECOGNITION SYSTEM can be realized . in this paper , we describe the configuration of a CANONICALIZATION PROCESS targeting GENDER TYPE as a HIDDEN VARIABLE . the proposed CANONICALIZATION PROCESS is composed of multiple distinctive PHONETIC FEATURE EXTRACTORS corresponding to the HIDDEN VARIABLE and a DPF SELECTOR in which the distance between input <unk> and ACOUSTIC MODELS is compared . in a DPF EXTRACTION STAGE , an input sequence of ACOUSTIC FEATURE VECTORS is mapped onto three DPF SPACES corresponding to male , female , and neutral voice by using three MULTILAYER NEURAL NETWORKS -lrb- mlns -rrb- . experiments are carried out by comparing -lrb- a -rrb- the combination of the CANONICALIZED DPF and a single HMM CLASSIFIER , and -lrb- b -rrb- the combination of a single ACOUSTIC FEATURE and multiple HMM CLASSIFIERS . the result shows that the proposed CANONICALIZATION PROCESS outperforms both of the conventional ROBUST AUTOMATIC SPEECH RECOGNITION SYSTEM with ACOUSTIC FEATURE and a single HMM and the ROBUST AUTOMATIC SPEECH RECOGNITION SYSTEM with multiple HMMS in spite of less memories and COMPUTATION TIME . \n",
            "this paper presents a new method for ROBUST AUTOMATIC SPEECH RECOGNITION SYSTEM based on MULTILAYER NEURAL NETWORKS . the proposed method is based on a DPF SELECTOR and a DPF SELECTOR . the proposed ROBUST AUTOMATIC SPEECH RECOGNITION SYSTEM is based on a DPF SELECTOR and a DPF SELECTOR . the proposed ROBUST AUTOMATIC SPEECH RECOGNITION SYSTEM is based on a DPF SELECTOR and a DPF SELECTOR . the proposed ROBUST AUTOMATIC SPEECH RECOGNITION SYSTEM is compared with a DPF SELECTOR and a DPF SELECTOR . the proposed ROBUST AUTOMATIC SPEECH RECOGNITION SYSTEM is compared with the conventional HMM and the HMM CLASSIFIER .\n",
            "\n",
            "808 1000\n",
            "spectral factorization is a CLASSICAL TOOL in SIGNAL PROCESSING and COMMUNICATIONS . it also plays a critical role in X-RAY CRYSTALLOGRAPHY , in the context of PHASE RETRIEVAL . in this work , we study the problem of SPARSE SPECTRAL FACTORIZATION , aiming to recover a ONE-DIMENSIONAL SPARSE SIGNAL from its AUTOCORRELATION . we present a sufficient condition for the recovery to be unique , and propose an ITERATIVE ALGORITHM that can obtain the original signal -lrb- up to a SIGN CHANGE , TIME-SHIFT and <unk> -rrb- . NUMERICAL SIMULATIONS verify the effectiveness of the proposed algorithm . \n",
            "this paper addresses the problem of PHASE RETRIEVAL in a ONE-DIMENSIONAL SPARSE SIGNAL . we propose a new ITERATIVE ALGORITHM , called SPARSE SPECTRAL FACTORIZATION , which is based on a SPARSE SPECTRAL FACTORIZATION . the proposed ITERATIVE ALGORITHM is based on a ONE-DIMENSIONAL SPARSE SIGNAL , which is a ONE-DIMENSIONAL SPARSE SIGNAL , which is a ONE-DIMENSIONAL SPARSE SIGNAL . the proposed method is based on the ITERATIVE ALGORITHM and the ITERATIVE ALGORITHM . the proposed ITERATIVE ALGORITHM is based on the ITERATIVE ALGORITHM and the ITERATIVE ALGORITHM . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "809 1000\n",
            "we present a GENERATIVE MODEL APPROACH to explore INTRINSIC SEMANTIC STRUCTURES in SPORT VIDEOS , e.g. , the CAMERA VIEW in AMERICAN FOOTBALL GAMES . we will invoke the concept of SEMANTIC SPACE to explicitly define the SEMANTIC STRUCTURE in the video in terms of LATENT STATES . a GENERATIVE MODEL APPROACH is used to govern the transition between states , and an GENERATIVE MODEL APPROACH is developed to characterize VISUAL FEATURES pertaining to different states . then the problem is formulated as a STATISTICAL INFERENCE PROCESS where we want to infer LATENT STATES -lrb- i.e. , CAMERA VIEWS -rrb- from observations -lrb- i.e. , VISUAL FEATURES -rrb- . two GENERATIVE MODELS , the HIDDEN MARKOV MODEL and the SEGMENTAL HMM , are involved in this research . in the HIDDEN MARKOV MODEL , both LATENT STATES and VISUAL FEATURES are <unk> , and in the SEGMENTAL HMM , LATENT STATES and VISUAL FEATURES are defined for shots and frames respectively . both GENERATIVE MODEL APPROACH provide promising performance for VIEW-BASED SHOT CLASSIFICATION , and the SEGMENTAL HMM outper-forms the HIDDEN MARKOV MODEL by involving a TWO-LAYER OBSERVATION MODEL to accommodate the VARIABILITY OF VISUAL FEATURES . this GENERATIVE MODEL APPROACH is also applicable to other VIDEO MINING TASKS . \n",
            "this paper presents a new GENERATIVE MODEL APPROACH for VIDEO MINING TASKS . the GENERATIVE MODEL APPROACH is based on the GENERATIVE MODEL APPROACH and the GENERATIVE MODEL APPROACH . the proposed GENERATIVE MODEL APPROACH is based on the GENERATIVE MODEL APPROACH and the GENERATIVE MODEL APPROACH . the GENERATIVE MODEL APPROACH is based on the GENERATIVE MODEL APPROACH and the GENERATIVE MODEL APPROACH . the proposed GENERATIVE MODEL APPROACH is based on the GENERATIVE MODEL APPROACH and the GENERATIVE MODEL APPROACH . the proposed GENERATIVE MODEL APPROACH is based on the GENERATIVE MODEL APPROACH and the GENERATIVE MODEL APPROACH . the proposed GENERATIVE MODEL APPROACH is based on the GENERATIVE MODEL APPROACH and the GENERATIVE MODEL APPROACH . the proposed GENERATIVE MODEL APPROACH is based on the GENERATIVE MODEL APPROACH and the GENERATIVE MODEL APPROACH .\n",
            "\n",
            "810 1000\n",
            "a SEQUENTIAL SOURCE LOCALIZATION METHOD using PARTICLE FILTER is presented to estimate and track MULTIPLE-TARGET LOCATIONS . this SEQUENTIAL SOURCE LOCALIZATION METHOD is designed to make use of ACOUSTIC SIGNAL measured at multiple ACOUSTIC SENSORS randomly deployed in a WIRELESS DISTRIBUTED SENSOR NETWORK . by using the PARTICLE FILTER , NON-GAUSSIAN PROBABILITY DENSITY FUNCTION of the target locations are represented by a discrete set of '' particles '' . the POSITIONS of these particles are propagated sequentially using known STATE TRANSITION EQUATION , and updated using new LOCATION ESTIMATES via the OBSERVATION EQUATION . compared to a previously proposed MAXIMUM LIKELIHOOD SOURCE LOCALIZATION ALGORITHM , this new SEQUENTIAL SOURCE LOCALIZATION METHOD is computationally effective and more robust to PARAMETER PERTURBATION . \n",
            "this paper presents a SEQUENTIAL SOURCE LOCALIZATION METHOD for WIRELESS DISTRIBUTED SENSOR NETWORK . the SEQUENTIAL SOURCE LOCALIZATION METHOD is based on the MAXIMUM LIKELIHOOD SOURCE LOCALIZATION ALGORITHM . the SEQUENTIAL SOURCE LOCALIZATION METHOD is based on the MAXIMUM LIKELIHOOD SOURCE LOCALIZATION ALGORITHM . the proposed SEQUENTIAL SOURCE LOCALIZATION METHOD is based on the MAXIMUM LIKELIHOOD SOURCE LOCALIZATION ALGORITHM . the proposed SEQUENTIAL SOURCE LOCALIZATION METHOD is based on the MAXIMUM LIKELIHOOD SOURCE LOCALIZATION ALGORITHM . the proposed SEQUENTIAL SOURCE LOCALIZATION METHOD is based on the MAXIMUM LIKELIHOOD SOURCE LOCALIZATION ALGORITHM . the proposed SEQUENTIAL SOURCE LOCALIZATION METHOD is based on a SEQUENTIAL SOURCE LOCALIZATION METHOD . the proposed SEQUENTIAL SOURCE LOCALIZATION METHOD is based on the MAXIMUM LIKELIHOOD SOURCE LOCALIZATION ALGORITHM . the proposed SEQUENTIAL SOURCE LOCALIZATION METHOD is based on a SEQUENTIAL SOURCE LOCALIZATION METHOD . the proposed SEQUENTIAL SOURCE LOCALIZATION METHOD is based on a SEQUENTIAL SOURCE LOCALIZATION METHOD .\n",
            "\n",
            "811 1000\n",
            "we propose a method for NONPARAMETRIC DENSITY ESTIMATION that exhibits ROBUSTNESS to contamination of the training sample . this method achieves ROBUSTNESS by combining a traditional KERNEL DENSITY ESTIMATOR with ideas from CLASSICAL M-ESTIMATION . we interpret the KERNEL DENSITY ESTIMATOR based on a POSITIVE SEMI-DEFINITE KERNEL as a sample mean in the associated REPRODUCING KERNEL HILBERT SPACE . since the sample mean is sensitive to OUTLIERS , we estimate KERNEL DENSITY ESTIMATOR robustly via M-ESTIMATION , yielding a ROBUST KERNEL DENSITY ESTIMATOR . an KERNEL DENSITY ESTIMATOR can be computed efficiently via a kernelized iteratively <unk> least squares -lrb- <unk> -rrb- algorithm . necessary and sufficient conditions are given for KERNELIZED IRWLS to converge to the GLOBAL MINIMIZER of the M-ESTIMATOR OBJECTIVE FUNCTION . the ROBUSTNESS of the KERNEL DENSITY ESTIMATOR is demonstrated with a REPRESENTER THEOREM , the INFLUENCE FUNCTION , and experimental results for DENSITY ESTIMATION and ANOMALY DETECTION . \n",
            "this paper proposes a new ROBUST KERNEL DENSITY ESTIMATOR for NONPARAMETRIC DENSITY ESTIMATION . the proposed ROBUST KERNEL DENSITY ESTIMATOR is based on the ROBUST KERNEL DENSITY ESTIMATOR and the ROBUST KERNEL DENSITY ESTIMATOR . the proposed ROBUST KERNEL DENSITY ESTIMATOR is based on the ROBUST KERNEL DENSITY ESTIMATOR and the ROBUST KERNEL DENSITY ESTIMATOR . the proposed ROBUST KERNEL DENSITY ESTIMATOR is based on the REPRESENTER THEOREM and the ROBUST KERNEL DENSITY ESTIMATOR . the proposed ROBUST KERNEL DENSITY ESTIMATOR is compared with the conventional REPRESENTER THEOREM and the REPRESENTER THEOREM . the proposed ROBUST KERNEL DENSITY ESTIMATOR is compared with the conventional REPRESENTER THEOREM and the KERNEL DENSITY ESTIMATOR .\n",
            "\n",
            "812 1000\n",
            "* entity linking -lrb- EL -rrb- is the task of linking a TEXTUAL NAMED ENTITY MENTION to a KNOWLEDGE BASE ENTRY . it is a difficult task involving many challenges , but the most crucial problem is ENTITY AMBIGUITY . traditional EL APPROACHES usually employ different constraints and FILTERING TECHNIQUES to improve performance . however , these constraints are executed in several different stages and can not be used interactively . in this paper , we propose several DISAMBIGUATION FORMULAE/FEATURES and employ a MARKOV LOGIC NETWORK to model INTERWEAVED CONSTRAINTS found in one type of EL , gene mention linking . to assess our systems effectiveness in different applications , we adopt two evaluation schemes : <unk> and <unk> <unk> . experimental results show that our system outperforms the baseline systems and state-of-the-art systems under both evaluation schemes . \n",
            "this paper presents a MARKOV LOGIC NETWORK for TEXTUAL NAMED ENTITY MENTION . the MARKOV LOGIC NETWORK is based on a MARKOV LOGIC NETWORK of the MARKOV LOGIC NETWORK . the DISAMBIGUATION FORMULAE/FEATURES is based on a MARKOV LOGIC NETWORK of the TEXTUAL NAMED ENTITY MENTION . the DISAMBIGUATION FORMULAE/FEATURES is based on the KNOWLEDGE BASE ENTRY and the EL . the DISAMBIGUATION FORMULAE/FEATURES is applied to a MARKOV LOGIC NETWORK .\n",
            "\n",
            "813 1000\n",
            "mining <unk> events from TEXT STREAMS has been an important research topic . CLASSIC TEXT REPRESENTATION MODEL -lrb- i.e. , VECTOR SPACE MODEL -rrb- can not model TEMPORAL ASPECTS OF DOCUMENTS . to address BURSTVSM , we proposed a novel BURST-BASED TEXT REPRESENTATION MODEL , denoted as BURSTVSM . BURSTVSM corresponds dimensions to BURSTY FEATURES instead of terms , which can capture SEMANTIC AND TEMPORAL INFORMATION . meanwhile , BURSTVSM significantly reduces the number of NON-ZERO ENTRIES in the representation . we test BURSTVSM via SCALABLE EVENT DETECTION , and experiments in a 10-YEAR NEWS ARCHIVE show that our BURSTVSM are both effective and efficient . \n",
            "this paper presents a CLASSIC TEXT REPRESENTATION MODEL for SCALABLE EVENT DETECTION . the CLASSIC TEXT REPRESENTATION MODEL is based on a BURST-BASED TEXT REPRESENTATION MODEL , which is a BURST-BASED TEXT REPRESENTATION MODEL of the TEXT STREAMS . the proposed CLASSIC TEXT REPRESENTATION MODEL is based on a BURST-BASED TEXT REPRESENTATION MODEL , which is based on the SEMANTIC AND TEMPORAL INFORMATION . the proposed BURST-BASED TEXT REPRESENTATION MODEL is based on a CLASSIC TEXT REPRESENTATION MODEL , which is based on the SEMANTIC AND TEMPORAL INFORMATION . the proposed CLASSIC TEXT REPRESENTATION MODEL is evaluated on a 10-YEAR NEWS ARCHIVE and on a 10-YEAR NEWS ARCHIVE .\n",
            "\n",
            "814 1000\n",
            "we propose a CONVEX FORMULATION of the CORRESPONDENCE PROBLEM between two images with respect to an ENERGY FUNCTION measuring DATA CONSISTENCY and SPATIAL REGULARITY . to this end , we formulate the general CORRESPONDENCE PROBLEM as the search for a MINIMAL TWO-DIMENSIONAL SURFACE in r 4 . we then use tools from GEOMETRIC MEASURE THEORY and introduce 2-VECTOR FIELDS as a representation of TWO-DIMENSIONAL SURFACES in r 4 . we propose a discretization of this CONVEX FORMULATION that gives rise to a CONVEX MINIMIZATION PROBLEM and compute a GLOBALLY OPTIMAL SOLUTION using an efficient PRIMAL-DUAL ALGORITHM . \n",
            "this paper presents a CONVEX FORMULATION for 2-VECTOR FIELDS , which is based on a CONVEX FORMULATION . the PRIMAL-DUAL ALGORITHM is based on a CONVEX FORMULATION and a CONVEX FORMULATION . the PRIMAL-DUAL ALGORITHM is based on a CONVEX FORMULATION and a CONVEX FORMULATION . the GLOBALLY OPTIMAL SOLUTION is based on a CONVEX FORMULATION and a CONVEX FORMULATION . the proposed PRIMAL-DUAL ALGORITHM is based on a CONVEX FORMULATION and a CONVEX FORMULATION for the CORRESPONDENCE PROBLEM .\n",
            "\n",
            "815 1000\n",
            "this paper presents a ROBUST NATURAL LANGUAGE UNDERSTANDING SYSTEM that handles NEGOTIATION SUBDIALOGUES by inferring both the COMMUNICATIVE ACTIONS that people pursue when speaking and the beliefs underlying these actions . we contend that recognizing the complex DISCOURSE ACTIONS pursued in NEGOTIATION SUBDIALOGUES -lrb- e.g. , expressing doubt -rrb- requires both a MULTI-STRENGTH BELIEF MODEL and a PROCESS MODEL that combines different KNOWLEDGE SOURCES in a unified framework . we show how our ROBUST NATURAL LANGUAGE UNDERSTANDING SYSTEM identifies the structure of NEGOTIATION SUBDIALOGUES , including recognizing EXPRESSIONS OF DOUBT , implicit acceptance of COMMUNICATED PROPOSITIONS , and NEGOTIATION SUBDIALOGUES embedded within other NEGOTIATION SUBDIALOGUES . 1 introduction since NEGOTIATION is an integral part of MULTI-AGENT ACTIVITY , a ROBUST NATURAL LANGUAGE UNDERSTANDING SYSTEM must be able to handle SUBDI-ALOGUES in which participants negotiate what has been claimed in order to try to come to some agreement about those claims . to handle such dialogues , the ROBUST NATURAL LANGUAGE UNDERSTANDING SYSTEM must be able to recognize when a dialogue participant has initiated a NEGOTIATION <unk> and why the participant <unk> the NEGOTIATION -lrb- i.e. , what beliefs led the participant to start the NEGOTIATION -rrb- . this paper presents a ROBUST NATURAL LANGUAGE UNDERSTANDING SYSTEM of TASK-ORIENTED INTERACTIONS that <unk> NEGOTIATION SUBDIALOGUES by inferring both the COMMUNICATIVE ACTIONS that people pursue when speaking and the beliefs underlying these actions . we will argue that recognizing the complex DISCOURSE ACTIONS pursued in NEGOTIATION SUBDIALOGUES -lrb- e.g. , expressing doubt -rrb- requires both a MULTI-STRENGTH BELIEF MODEL and a PROCESSING STRATEGY that combines different KNOWLEDGE SOURCES in a unified framework , and we will show how our ROBUST NATURAL LANGUAGE UNDERSTANDING SYSTEM incorporates these and recognizes the structure of NEGOTIATION SUBDIALOGUES . \n",
            "this paper presents a ROBUST NATURAL LANGUAGE UNDERSTANDING SYSTEM for MULTI-AGENT ACTIVITY . the PROCESS MODEL is based on the PROCESS MODEL and the PROCESS MODEL . the PROCESS MODEL is based on the PROCESS MODEL and the PROCESS MODEL . the PROCESS MODEL is based on the PROCESS MODEL and the PROCESS MODEL . the PROCESS MODEL is based on the PROCESS MODEL and the PROCESS MODEL . the PROCESS MODEL is based on the MULTI-STRENGTH BELIEF MODEL and the PROCESS MODEL . the PROCESS MODEL is evaluated on a NATURAL LANGUAGE UNDERSTANDING SYSTEM and a ROBUST NATURAL LANGUAGE UNDERSTANDING SYSTEM .\n",
            "\n",
            "816 1000\n",
            "effective reduction of FALSE ALARMS in LARGE-SCALE VIDEO SURVEILLANCE is rather challenging , especially for applications where ABNORMAL EVENTS OF INTEREST rarely occur , such as ABANDONED OBJECT DETECTION . we develop an approach to prioritize ALERTS by ranking them , and demonstrate its great effectiveness in reducing false positives while keeping good DETECTION ACCURACY . our approach benefits from a novel representation of ABANDONED OBJECT ALERTS by RELATIVE ATTRIBUTES , namely STATICNESS , FOREGROUNDNESS and ABANDON-MENT . the relative strengths of these RELATIVE ATTRIBUTES are quantified using a RANKING FUNCTION -lsb- 19 -rsb- learnt on suitably designed LOW-LEVEL SPATIAL AND TEMPORAL FEATURES.THESE ATTRIBUTES of varying strengths are not only powerful in distinguishing ABANDONED OBJECTS from FALSE ALARMS such as PEOPLE AND LIGHT ARTIFACTS , but also computationally efficient for LARGE-SCALE DEPLOYMENT . with these FEATURES , we apply a LINEAR RANKING ALGORITHM to sort ALERTS according to their relevance to the end-user . we test the effectiveness of our approach on both PUBLIC DATA SETS and large ones collected from the real world . \n",
            "this paper presents a new method for ABANDONED OBJECT DETECTION in LARGE-SCALE VIDEO SURVEILLANCE . the proposed method is based on a LINEAR RANKING ALGORITHM and a LINEAR RANKING ALGORITHM . the proposed method is based on a LINEAR RANKING ALGORITHM and a LINEAR RANKING ALGORITHM . the proposed method is based on a LINEAR RANKING ALGORITHM and a LINEAR RANKING ALGORITHM . the proposed method is based on a LINEAR RANKING ALGORITHM and a LINEAR RANKING ALGORITHM . the DETECTION ACCURACY of the proposed method is compared with conventional FEATURES such as ALERTS , ALERTS , and FALSE ALARMS .\n",
            "\n",
            "817 1000\n",
            "we study the RESTLESS BANDIT PROBLEM where arms are associated with STATIONARY Φ-MIXING PROCESSES and where rewards are therefore dependent : the question that arises from this setting is that of carefully recovering some independence by ` <unk> ' the values of some rewards . as we shall see , the RESTLESS BANDIT PROBLEM we tackle requires us to address the EXPLORATION/EXPLOITATION/INDEPENDENCE TRADE-OFF , which we do by considering the idea of a WAITING ARM in the new REMIX-UCB ALGORITHM , a generalization of <unk> for the problem at hand , that we introduce . we provide a REGRET ANALYSIS for this RESTLESS BANDIT PROBLEM ; two noticeable features of REMIX-UCB are that i -rrb- REGRET ANALYSIS reduces to the regular <unk> when the Φ-MIXING COEFFICIENTS are all 0 , i.e. when the I.I.D SCENARIO is recovered , and ii -rrb- when ϕ -lrb- n -rrb- = o -lrb- n − α -rrb- , REGRET ANALYSIS is able to ensure a controlled regret of order θ ∆ -lrb- α − 2 -rrb- / α * log 1 / α t , where ∆ * encodes the distance between the best arm and the best SUBOPTIMAL ARM , even in the case when α < 1 , i.e. the case when the Φ-MIXING COEFFICIENTS are not <unk> . \n",
            "this paper presents a new method for REGRET ANALYSIS based on STATIONARY Φ-MIXING PROCESSES . the proposed method is based on the REMIX-UCB ALGORITHM . the proposed method is based on the REMIX-UCB ALGORITHM . the proposed method is based on the REMIX-UCB ALGORITHM . the proposed method is based on the REMIX-UCB ALGORITHM . the proposed method is based on the REMIX-UCB ALGORITHM . the proposed method is based on the REMIX-UCB ALGORITHM . the proposed method is based on the REMIX-UCB ALGORITHM . the proposed method is based on the REMIX-UCB ALGORITHM . the proposed method is based on the REMIX-UCB ALGORITHM .\n",
            "\n",
            "818 1000\n",
            "permutation of the outputs at different FREQUENCY BINS remains as a major problem in the CONVOLUTIVE BLIND SOURCE SEPARATION . in this work a COUPLED HIDDEN MARKOV MODEL effectively exploits the PSYCHOACOUSTIC CHARACTERISTICS OF SIGNALS to mitigate such permutation . a JOINT DIAGONALIZATION ALGORITHM for CONVOLUTIVE BSS , which incorporates a NON-UNITARY PENALTY TERM within the CROSS-POWER SPECTRUM-BASED COST FUNCTION in the FREQUENCY DOMAIN , has been used . the proposed COUPLED HIDDEN MARKOV MODEL couples a number of conventional HMMS , equivalent to the number of outputs , by making STATE TRANSITIONS in each model dependent not only on its own previous state , but also on some aspects of the state of the other models . using this JOINT DIAGONALIZATION ALGORITHM the PERMUTATION EFFECT has been substantially reduced , and demonstrated using a number of SIMULATION STUDIES . \n",
            "this paper proposes a JOINT DIAGONALIZATION ALGORITHM for CONVOLUTIVE BLIND SOURCE SEPARATION . the JOINT DIAGONALIZATION ALGORITHM is based on the COUPLED HIDDEN MARKOV MODEL of the COUPLED HIDDEN MARKOV MODEL . the proposed JOINT DIAGONALIZATION ALGORITHM is based on the COUPLED HIDDEN MARKOV MODEL . the proposed JOINT DIAGONALIZATION ALGORITHM is based on the COUPLED HIDDEN MARKOV MODEL . the proposed JOINT DIAGONALIZATION ALGORITHM is based on the COUPLED HIDDEN MARKOV MODEL . the JOINT DIAGONALIZATION ALGORITHM is applied to the FREQUENCY DOMAIN , and the results show that the proposed JOINT DIAGONALIZATION ALGORITHM is robust to STATE TRANSITIONS .\n",
            "\n",
            "819 1000\n",
            "this paper addresses the BLIND DECONVOLUTION of multi-input -- MULTI-OUTPUT FIR SYSTEMS driven by WHITE NON-GAUSSIAN SOURCE SIGNALS . first , we present a weaker condition on SOURCE SIGNALS than the SO-CALLED I.I.D. CONDITION so that BLIND DECONVOLUTION is possible . then , under this condition , we provide a necessary and sufficient condition for BLIND DECONVOLUTION OF MIMO FIR SYSTEMS . finally , based on this result , we propose two MAXIMIZATION CRITERIA for BLIND DECONVOLUTION OF MIMO FIR SYSTEMS . these MAXIMIZATION CRITERIA are simple enough to be implemented by ADAPTIVE ALGORITHMS . \n",
            "this paper presents a new method for BLIND DECONVOLUTION OF MIMO FIR SYSTEMS based on WHITE NON-GAUSSIAN SOURCE SIGNALS . the proposed method is based on the use of MAXIMIZATION CRITERIA as a SO-CALLED I.I.D. CONDITION . the proposed method is based on the use of MAXIMIZATION CRITERIA as a SO-CALLED I.I.D. CONDITION . the proposed method is based on the use of MAXIMIZATION CRITERIA as well as the SO-CALLED I.I.D. CONDITION . experimental results show the effectiveness of the proposed ADAPTIVE ALGORITHMS .\n",
            "\n",
            "820 1000\n",
            "this paper investigates a novel problem of GENERATING IMAGES from VISUAL ATTRIBUTES . we model the image as a COMPOSITE OF FOREGROUND AND BACKGROUND and develop a LAYERED GENERATIVE MODEL with DISENTANGLED LATENT VARIABLES that can be learned end-to-end using a VARIATIONAL AUTO-ENCODER . we experiment with NATURAL IMAGES OF FACES and birds and demonstrate that the proposed LAYERED GENERATIVE MODEL are capable of generating realistic and diverse samples with DISENTANGLED LATENT REPRESENTATIONS . we use a general ENERGY MINIMIZATION ALGORITHM for POSTERIOR INFERENCE OF LATENT VARIABLES given novel images . therefore , the learned GENERATIVE MODELS show excellent quantitative and visual results in the tasks of ATTRIBUTE-CONDITIONED IMAGE RECONSTRUCTION and COMPLETION . \n",
            "this paper presents a novel LAYERED GENERATIVE MODEL for GENERATING IMAGES . the LAYERED GENERATIVE MODEL is based on a LAYERED GENERATIVE MODEL and a LAYERED GENERATIVE MODEL . the ENERGY MINIMIZATION ALGORITHM is based on the VARIATIONAL AUTO-ENCODER . the ENERGY MINIMIZATION ALGORITHM is based on the VARIATIONAL AUTO-ENCODER and the ENERGY MINIMIZATION ALGORITHM . the proposed LAYERED GENERATIVE MODEL is based on a LAYERED GENERATIVE MODEL and is applied to the COMPOSITE OF FOREGROUND AND BACKGROUND . the proposed LAYERED GENERATIVE MODEL is based on a LAYERED GENERATIVE MODEL and is shown to be robust to COMPLETION and COMPLETION .\n",
            "\n",
            "821 1000\n",
            "in CASE-BASED REASONING , problems are solved by retrieving prior cases and adapting their solutions to fit ; learning occurs as new cases are stored . controlling the growth of the case base is a fundamental problem , and research on CASE-BASE MAINTENANCE has developed methods for COMPACTING CASE BASES while maintaining SYSTEM COMPETENCE , primarily by COMPETENCE-BASED DELETION STRATEGIES assuming STATIC CASE ADAPTATION KNOWLEDGE . this paper proposes ADAPTATION-GUIDED CASE-BASE MAINTENANCE , a CASE-BASE MAINTENANCE APPROACH exploiting the ability to dynamically generate new ADAPTATION KNOWLEDGE from cases . in ADAPTATION-GUIDED CASE-BASE MAINTENANCE , CASE RETENTION DECISIONS are based both on cases ' value as base cases for solving problems and on their value for generating new ADAPTATION RULES . the paper illustrates the method for NUMERICAL PREDICTION TASKS -lrb- case-based regression -rrb- in which ADAPTATION RULES are generated automatically using the CASE DIFFERENCE HEURISTIC . in comparisons of ADAPTATION-GUIDED CASE-BASE MAINTENANCE to five alternative methods in four domains , for varying CASE BASE DENSITIES , ADAPTATION-GUIDED CASE-BASE MAINTENANCE outperformed the alternatives in all domains , with greatest benefit at high compression . \n",
            "this paper proposes a new CASE-BASE MAINTENANCE APPROACH for CASE-BASED REASONING . the proposed CASE-BASE MAINTENANCE APPROACH is based on the use of COMPETENCE-BASED DELETION STRATEGIES for CASE-BASED REASONING . the proposed CASE-BASE MAINTENANCE APPROACH is based on the use of COMPETENCE-BASED DELETION STRATEGIES , a CASE-BASE MAINTENANCE APPROACH , and a CASE-BASE MAINTENANCE APPROACH for CASE-BASED REASONING . the proposed CASE-BASE MAINTENANCE APPROACH is based on a CASE-BASE MAINTENANCE APPROACH , which is based on the CASE DIFFERENCE HEURISTIC . the proposed CASE-BASE MAINTENANCE APPROACH is based on the CASE DIFFERENCE HEURISTIC . the proposed CASE-BASE MAINTENANCE APPROACH is based on a CASE-BASE MAINTENANCE APPROACH , and is shown to be useful for NUMERICAL PREDICTION TASKS .\n",
            "\n",
            "822 1000\n",
            "the CONVERGENCE ANALYSIS of the LEAST MEAN SQUARE ALGORITHM has been conventionally based on STOCHASTIC SIGNALS and describes thus only the average behavior of the LEAST MEAN SQUARE ALGORITHM . it has been shown previously that a PERIODIC-REFERENCE LMS SYSTEM can be regarded as a LINEAR TIME-PERIODIC SYSTEM whose stability can be determined from the MONODROMY MATRIX . generally , the MONODROMY MATRIX can only be solved numerically and does not thus reveal the actual factors behind the dynamics of the PERIODIC-REFERENCE LMS SYSTEM . this paper derives an ESTIMATOR for the eigenvalues of the MONODROMY MATRIX . the ESTIMATOR is easy to calculate , and ESTIMATOR also reveals the underlying reason for the bad convergence of the LEAST MEAN SQUARE ALGORITHM in some special cases . the ESTIMATOR is confirmed by comparing ESTIMATOR to the precise eigenvalues of the MONODROMY MATRIX . the ESTIMATOR is found to be accurate for the eigenvalues close to unity . \n",
            "this paper proposes a new ESTIMATOR for STOCHASTIC SIGNALS . the proposed ESTIMATOR is based on the LEAST MEAN SQUARE ALGORITHM . the proposed ESTIMATOR is based on the LEAST MEAN SQUARE ALGORITHM . the proposed ESTIMATOR is based on the LEAST MEAN SQUARE ALGORITHM . the proposed ESTIMATOR is compared with the conventional ESTIMATOR and the ESTIMATOR . the experimental results show that the proposed ESTIMATOR outperforms the conventional ESTIMATOR .\n",
            "\n",
            "823 1000\n",
            "in this paper , a PERCEPTUAL WEIGHTING MODEL is proposed for effective RATE CONTROL so as to enhance PERCEPTUAL CODING QUALITY OF VIDEOPHONE , by exploiting two categories of factors affecting the perception of the HUMAN VISUAL SYSTEM : STIMULUS-DRIVEN FACTORS and COGNITION-DRIVEN FACTORS . in order to achieve a simple but effective PERCEPTUAL WEIGHTING MODEL , we use LUMINANCE ADAPTATION and texture masking as the STIMULUS-DRIVEN FACTORS , while SKIN COLOR serves as the COGNITION-DRIVEN FACTOR in the VIDEOPHONE APPLICATION . both objective and subjective quality evaluations of VIDEOPHONE-LIKE SEQUENCES in h. 263 platform validate the effectiveness of our PERCEPTUAL WEIGHTING MODEL . \n",
            "this paper presents a PERCEPTUAL WEIGHTING MODEL for PERCEPTUAL CODING QUALITY OF VIDEOPHONE . the PERCEPTUAL WEIGHTING MODEL is based on a PERCEPTUAL WEIGHTING MODEL and a PERCEPTUAL WEIGHTING MODEL . the PERCEPTUAL WEIGHTING MODEL is based on the PERCEPTUAL WEIGHTING MODEL and the PERCEPTUAL WEIGHTING MODEL . the PERCEPTUAL WEIGHTING MODEL is based on the PERCEPTUAL WEIGHTING MODEL and the PERCEPTUAL WEIGHTING MODEL . the PERCEPTUAL WEIGHTING MODEL is tested on a VIDEOPHONE APPLICATION . the results show that the proposed PERCEPTUAL WEIGHTING MODEL achieves a significant improvement in RATE CONTROL performance compared to the conventional PERCEPTUAL WEIGHTING MODEL .\n",
            "\n",
            "824 1000\n",
            "the present paper describes a CORPUS-BASED SINGING VOICE SYNTHESIS SYSTEM based on HIDDEN MARKOV MODELS . this CORPUS-BASED SINGING VOICE SYNTHESIS SYSTEM employs the HMM-BASED SPEECH SYNTHESIS to synthesize SINGING VOICE . MUSICAL INFORMATION such as LYRICS , tones , DURATIONS is modeled simultaneously in a unified framework of the CONTEXT-DEPENDENT HMM . CORPUS-BASED SINGING VOICE SYNTHESIS SYSTEM can mimic the VOICE QUALITY and SINGING STYLE of the original singer . results of a SINGING VOICE synthesis experiment show that the proposed CORPUS-BASED SINGING VOICE SYNTHESIS SYSTEM can synthesize SMOOTH AND NATURAL-SOUNDING SINGING VOICE . \n",
            "this paper presents a novel CORPUS-BASED SINGING VOICE SYNTHESIS SYSTEM for HMM-BASED SPEECH SYNTHESIS . the CORPUS-BASED SINGING VOICE SYNTHESIS SYSTEM is based on the CONTEXT-DEPENDENT HMM , which is based on the CONTEXT-DEPENDENT HMM . the CORPUS-BASED SINGING VOICE SYNTHESIS SYSTEM is based on the CONTEXT-DEPENDENT HMM and the CONTEXT-DEPENDENT HMM . the proposed CORPUS-BASED SINGING VOICE SYNTHESIS SYSTEM is based on a CORPUS-BASED SINGING VOICE SYNTHESIS SYSTEM . the proposed CORPUS-BASED SINGING VOICE SYNTHESIS SYSTEM is based on a CORPUS-BASED SINGING VOICE SYNTHESIS SYSTEM . the CORPUS-BASED SINGING VOICE SYNTHESIS SYSTEM is applied to HMM-BASED SPEECH SYNTHESIS such as LYRICS . the proposed CORPUS-BASED SINGING VOICE SYNTHESIS SYSTEM is evaluated on a CORPUS-BASED SINGING VOICE SYNTHESIS SYSTEM .\n",
            "\n",
            "825 1000\n",
            "this acoustic study explored dialect effects on REALIZATION OF NUCLEAR PITCH ACCENTS in three regional varieties of AMERICAN ENGLISH spoken in CENTRAL OHIO , SOUTHEASTERN WISCONSIN and western north <unk> . fundamental frequency -lrb- f0 -rrb- change from VOWEL ONSET to offset in the most prominent syllable in a sentence was examined along four parameters : maximum f0 change , RELATIVE LOCATION of f0 maximum , f0 offset and f0 fall from maximum to offset . a robust finding was that the F0 CONTOURS in the SOUTHERN -LRB- NORTH CAROLINA -RRB- VARIANTS were significantly distinct from the two MIDWESTERN VARIETIES whose contours did not differ significantly from one another . the SOUTHERN VOWELS had an earlier f0 rise , a greater f0 fall and a lower f0 offset than either OHIO OR WISCONSIN VOWELS . there was a sharper f0 drop preceding a voiceless than a VOICED SYLLABLE CODA . no significant DIALECT-RELATED DIFFERENCES were found for FLAT F0 CONTOURS in UNSTRESSED VOWELS , which were also examined in the study . this study contributes the finding that dynamic variations in pitch are greater for VOWELS which also exhibit a greater amount of SPECTRAL DYNAMICS . the interaction of these two sets of cues contributes to the MELODIC COMPONENT associated with a specific REGIONAL ACCENT . \n",
            "this paper presents a method for REALIZATION OF NUCLEAR PITCH ACCENTS in AMERICAN ENGLISH . the proposed method is based on the REALIZATION OF NUCLEAR PITCH ACCENTS and the REALIZATION OF NUCLEAR PITCH ACCENTS . the method is based on the REALIZATION OF NUCLEAR PITCH ACCENTS and the REALIZATION OF NUCLEAR PITCH ACCENTS . the proposed method is based on the REALIZATION OF NUCLEAR PITCH ACCENTS . the proposed method is based on the REALIZATION OF NUCLEAR PITCH ACCENTS . the method is based on the REALIZATION OF NUCLEAR PITCH ACCENTS . the proposed method is based on the REALIZATION OF NUCLEAR PITCH ACCENTS . the method is based on the REALIZATION OF NUCLEAR PITCH ACCENTS . the proposed method is based on the REALIZATION OF NUCLEAR PITCH ACCENTS . the method is based on the REALIZATION OF NUCLEAR PITCH ACCENTS and the REALIZATION OF NUCLEAR PITCH ACCENTS .\n",
            "\n",
            "826 1000\n",
            "a central challenge to many fields of SCIENCE AND ENGINEERING involves minimizing NON-CONVEX ERROR FUNCTIONS over CONTINUOUS , HIGH DIMENSIONAL SPACES . gradient descent or QUASI-NEWTON METHODS are almost ubiquitously used to perform such <unk> , and it is often thought that a main source of difficulty for these LOCAL METHODS to find the GLOBAL MINIMUM is the proliferation of LOCAL MINIMA with much higher error than the GLOBAL MINIMUM . here we argue , based on results from STATISTICAL PHYSICS , RANDOM MATRIX THEORY , NEURAL NETWORK THEORY , and empirical evidence , that a deeper and more profound difficulty originates from the PROLIFERATION OF SADDLE POINTS , not LOCAL MINIMA , especially in HIGH DIMENSIONAL PROBLEMS OF PRACTICAL INTEREST . such SADDLE POINTS are surrounded by high error <unk> that can dramatically slow down learning , and give the <unk> impression of the existence of a LOCAL MINIMUM . motivated by these arguments , we propose a new approach to SECOND-ORDER OPTIMIZATION , the <unk> newton method , that can rapidly escape HIGH DIMENSIONAL SADDLE POINTS , unlike GRADIENT DESCENT and QUASI-NEWTON METHODS . we apply this algorithm to DEEP OR RECURRENT NEURAL NETWORK TRAINING , and provide numerical evidence for its superior optimization performance . \n",
            "this paper addresses the problem of DEEP OR RECURRENT NEURAL NETWORK TRAINING in CONTINUOUS , HIGH DIMENSIONAL SPACES . we propose a method for estimating the parameters of a NEURAL NETWORK THEORY and a NEURAL NETWORK THEORY . the proposed method is based on the NEURAL NETWORK THEORY and the NEURAL NETWORK THEORY . the proposed method is based on the NEURAL NETWORK THEORY and the NEURAL NETWORK THEORY . the proposed method is based on the NEURAL NETWORK THEORY and the NEURAL NETWORK THEORY . the proposed method is based on the NEURAL NETWORK THEORY and the NEURAL NETWORK THEORY . the proposed method is based on the use of RANDOM MATRIX THEORY and QUASI-NEWTON METHODS .\n",
            "\n",
            "827 1000\n",
            "the COMPUTATIONAL COMPLEXITY of a problem arising in the context of SPARSE OPTIMIZATION is considered , namely , the PROJECTION onto the set of K-COSPARSE VECTORS w.r.t. some given MATRIX Ω . it is shown that this PROJECTION PROBLEM is -lrb- strongly -rrb- np-hard , even in the special cases in which the MATRIX Ω contains only TERNARY OR BIPOLAR COEFFICIENTS . interestingly , this is in contrast to the PROJECTION onto the set of K-SPARSE VECTORS , which is trivially solved by keeping only the k largest coefficients . \n",
            "this paper presents a new method for SPARSE OPTIMIZATION based on SPARSE OPTIMIZATION . the proposed method is based on the PROJECTION PROBLEM in the MATRIX Ω . the proposed method is based on the use of K-COSPARSE VECTORS and the PROJECTION . the proposed method is based on the PROJECTION PROBLEM . the proposed method is based on the PROJECTION PROBLEM .\n",
            "\n",
            "828 1000\n",
            "stochastic variational inference finds good posterior approximations of PROBABILISTIC MODELS with very large data sets . it optimizes the VARI-ATIONAL OBJECTIVE with STOCHASTIC OPTIMIZATION , following noisy estimates of the NATURAL GRADIENT . <unk> , STOCHASTIC INFERENCE iteratively <unk> from the data , analyzes the SUBSAMPLE , and updates parameters with a DECREASING LEARNING RATE . however , the algorithm is sensitive to that rate , which usually requires HAND-TUNING to each application . we solve this problem by developing an ADAPTIVE LEARNING RATE for STOCHASTIC INFERENCE . our method requires no TUNING and is easily implemented with computations already made in the algorithm . we demonstrate our approach with LATENT DIRICHLET ALLOCATION applied to three LARGE TEXT CORPORA . INFERENCE with the ADAPTIVE LEARNING RATE converges faster and to a better approximation than the best settings of HAND-TUNED RATES . \n",
            "this paper addresses the problem of STOCHASTIC INFERENCE in LARGE TEXT CORPORA . we propose a method for STOCHASTIC VARIATIONAL INFERENCE based on STOCHASTIC VARIATIONAL INFERENCE . the proposed method is based on a VARI-ATIONAL OBJECTIVE , which is a VARI-ATIONAL OBJECTIVE . the proposed method is based on the use of LATENT DIRICHLET ALLOCATION in the VARI-ATIONAL OBJECTIVE . the proposed method is based on a VARI-ATIONAL OBJECTIVE , which is based on the VARI-ATIONAL OBJECTIVE . the proposed method is evaluated on the LARGE TEXT CORPORA . the experimental results show the effectiveness of the proposed method in terms of HAND-TUNED RATES and HAND-TUNED RATES .\n",
            "\n",
            "829 1000\n",
            "empirical mode decomposition -lrb- EMPIRICAL MODE DECOMPOSITION -rrb- has lately received much attention due to the many interesting features that exhibits . however it lacks a strong theoretical basis which would allow a PERFORMANCE ANALYSIS and hence the enhancement and optimization of the method in a systematic way . in this paper , an investigation of EMPIRICAL MODE DECOMPOSITION is attempted in an alternative way : the INTERPOLATION POINTS and the PIECEWISE INTERPOLATING POLYNOMIALS for the formation of the upper and lower envelopes of the signal are optimized based on a GENETIC ALGORITHM FRAMEWORK revealing important characteristics of the method which where previously hidden . as a result , novel directions for both the performance enhancement and the theoretical investigation of the method are <unk> . \n",
            "this paper presents a new GENETIC ALGORITHM FRAMEWORK for EMPIRICAL MODE DECOMPOSITION . the GENETIC ALGORITHM FRAMEWORK is based on a GENETIC ALGORITHM FRAMEWORK . the proposed GENETIC ALGORITHM FRAMEWORK is based on a GENETIC ALGORITHM FRAMEWORK . the proposed GENETIC ALGORITHM FRAMEWORK is based on a GENETIC ALGORITHM FRAMEWORK .\n",
            "\n",
            "830 1000\n",
            "in this paper we propose a method for learning BAYESIAN BELIEF NETWORKS from data . the method uses ARTIFICIAL NEURAL NETWORKS as PROBABILITY ESTIMATORS , thus avoiding the need for making prior assumptions on the nature of the probability distributions governing the relationships among the participating variables . this new method has the potential for being applied to domains containing both DISCRETE AND CONTINUOUS VARIABLES arbitrarily distributed . we compare the learning performance of this new method with the performance of the method proposed by cooper and <unk> in -lsb- 7 -rsb- . the experimental results show that , although the LEARNING SCHEME based on the use of ANN ESTIMATORS is slower , the LEARNING ACCURACY of the two methods is comparable . category : algorithms and ARCHITECTURES . \n",
            "this paper presents a new method for ARTIFICIAL NEURAL NETWORKS based on BAYESIAN BELIEF NETWORKS . the proposed method is based on the use of BAYESIAN BELIEF NETWORKS . the proposed method is based on the use of BAYESIAN BELIEF NETWORKS . the proposed method is based on the use of BAYESIAN BELIEF NETWORKS . the proposed method is based on the use of BAYESIAN BELIEF NETWORKS . the proposed method is based on the use of BAYESIAN BELIEF NETWORKS .\n",
            "\n",
            "831 1000\n",
            "the text analysis conference -lrb- tac -rrb- ranks SUMMARIZATION SYSTEMS by their AVERAGE SCORE over a collection of DOCUMENT SETS . we investigate the statistical appropriateness of this score and propose an alternative that better distinguishes between HUMAN AND MACHINE EVALUATION SYSTEMS . \n",
            "this paper presents a new method for SUMMARIZATION SYSTEMS . the proposed method is based on the AVERAGE SCORE of the HUMAN AND MACHINE EVALUATION SYSTEMS and the AVERAGE SCORE of the proposed SUMMARIZATION SYSTEMS . experimental results show that the proposed method improves the performance of the proposed HUMAN AND MACHINE EVALUATION SYSTEMS .\n",
            "\n",
            "832 1000\n",
            "due to the physiological constraints of articulatory motion the SPEECH APPARATUS has limited degrees of freedom . as a result , the range of SPEECH SOUNDS a human is capable of producing may lie on a low dimensional <unk> of the high dimensional space of all possible sounds . in this study a number of MANIFOLD LEARNING ALGORITHMS are applied to SPEECH DATA in an effort to extract useful LOW DIMENSIONAL STRUCTURE from the HIGH DIMENSIONAL SPEECH SIGNAL . the ability of these MANIFOLD LEARNING ALGORITHMS to separate VOWELS in a LOW DIMENSIONAL SPACE is evaluated and compared to a CLASSICAL LINEAR DIMENSIONALITY REDUCTION METHOD . results indicate that MANIFOLD LEARNING ALGORITHMS outperform CLASSICAL METHODS in LOW DIMENSIONS and are capable of discovering useful MANIFOLD STRUCTURE in SPEECH DATA . \n",
            "this paper presents a CLASSICAL LINEAR DIMENSIONALITY REDUCTION METHOD for SPEECH DATA . the MANIFOLD LEARNING ALGORITHMS is based on the MANIFOLD STRUCTURE and the MANIFOLD STRUCTURE . the MANIFOLD LEARNING ALGORITHMS is based on the MANIFOLD STRUCTURE and the MANIFOLD STRUCTURE . the MANIFOLD LEARNING ALGORITHMS is applied to the SPEECH DATA . the proposed MANIFOLD LEARNING ALGORITHMS is compared with other CLASSICAL METHODS in terms of VOWELS and VOWELS . the experimental results show that the proposed CLASSICAL LINEAR DIMENSIONALITY REDUCTION METHOD can achieve significant performance gains over CLASSICAL METHODS .\n",
            "\n",
            "833 1000\n",
            "this paper describes a system for navigating large collections of information about CULTURAL HERITAGE which is applied to EU-ROPEANA , the EUROPEAN LIBRARY . <unk> contains over 20 million artefacts with meta-data in a wide range of EURO-PEAN LANGUAGES . the system currently provides access to EUROPEANA CONTENT with meta-data in ENGLISH and SPANISH . the paper describes how NATURAL LANGUAGE PROCESSING is used to enrich and <unk> this meta-data to assist NAVIGATION through EU-ROPEANA and shows how this information is used within the system . \n",
            "this paper presents a method for NAVIGATION from EURO-PEAN LANGUAGES . the method is based on the use of a EUROPEAN LIBRARY and a EUROPEAN LIBRARY . the proposed method is based on a EUROPEAN LIBRARY , which is based on the EU-ROPEANA . the proposed method is based on the EU-ROPEANA . the proposed method is based on the use of CULTURAL HERITAGE and EU-ROPEANA .\n",
            "\n",
            "834 1000\n",
            "this paper describes a method of finding THIN , ELONGATED STRUCTURES in IMAGES and volumes . we use SHORTEST PATHS to minimize very general functionals of HIGHER-ORDER CURVE PROPERTIES , such as CURVATURE and TORSION . our GLOBALLY OPTIMAL METHOD uses LINE GRAPHS and its runtime is polynomial in the size of the DISCRETIZATION , often in the order of seconds on a single computer . to our knowledge , we are the first to perform experiments in three dimensions with CURVATURE and TORSION REGULARIZATION . the largest graphs we process have almost one hundred billion arcs . experiments on MEDICAL IMAGES and in MULTI-VIEW RECONSTRUCTION show the significance and practical usefulness of REGULARIZATION based on CURVATURE while TORSION is still only tractable for SMALL-SCALE PROBLEMS . \n",
            "this paper addresses the problem of MULTI-VIEW RECONSTRUCTION in MEDICAL IMAGES . we propose a method for MULTI-VIEW RECONSTRUCTION based on the GLOBALLY OPTIMAL METHOD and the GLOBALLY OPTIMAL METHOD . the proposed method is based on the GLOBALLY OPTIMAL METHOD and the GLOBALLY OPTIMAL METHOD . the proposed method is based on the GLOBALLY OPTIMAL METHOD and the GLOBALLY OPTIMAL METHOD . the proposed method is based on the GLOBALLY OPTIMAL METHOD and the GLOBALLY OPTIMAL METHOD . experimental results on MEDICAL IMAGES show that the proposed method achieves better performance than the conventional GLOBALLY OPTIMAL METHOD and the GLOBALLY OPTIMAL METHOD .\n",
            "\n",
            "835 1000\n",
            "<unk> <unk> we developed a NEURAL NET ARCHITECTURE for SEGMENTING COMPLEX IMAGES , i.e. , to localize TWO-DIMENSIONAL GEOMETRICAL SHAPES in a scene , without prior knowledge of the objects ' positions and sizes . a SCALE VARIATION is built into the NEURAL NET ARCHITECTURE to deal with varying sizes . this NEURAL NET ARCHITECTURE has been applied to VIDEO IMAGES OF RAILROAD CARS , to find their identification numbers . over 95 % of the <unk> were located correctly in a data base of 300 images , <unk> a large variation in LIGHTING CONDITIONS and often a poor quality of the characters . a part of the NEURAL NET ARCHITECTURE is executed on a PROCESSOR BOARD containing an analog neural net chip -lrb- <unk> et ai . 1991 -rrb- . while the rest is implemented as a SOFTWARE MODEL on a WORKSTATION or a DIGITAL SIGNAL PROCESSOR . \n",
            "this paper presents a novel SOFTWARE MODEL for SEGMENTING COMPLEX IMAGES . the SOFTWARE MODEL is based on the NEURAL NET ARCHITECTURE and the NEURAL NET ARCHITECTURE . the SOFTWARE MODEL is based on the NEURAL NET ARCHITECTURE and the NEURAL NET ARCHITECTURE . the SOFTWARE MODEL is based on the NEURAL NET ARCHITECTURE and the NEURAL NET ARCHITECTURE . the SOFTWARE MODEL is applied to the VIDEO IMAGES OF RAILROAD CARS . the SOFTWARE MODEL is applied to SEGMENTING COMPLEX IMAGES .\n",
            "\n",
            "836 1000\n",
            "a reduced complexity realisation for the NORMALISED CONSTANT MOD-ULUS ALGORITHM and its SOFT CRITERION SATISFACTION VERSION is proposed based on SELECTIVE PARTIAL UPDATING . the COMPUTATIONAL COMPLEXITY of NORMALISED CONSTANT MOD-ULUS ALGORITHM and SOFT CRITERION SATISFACTION VERSION is reduced by updating a BLOCK OF EQUALISER PARAMETERS at every iteration rather than the entire <unk> . this results in a smaller number of MULTIPLICATIONS for updating the EQUALISER PARAMETERS . a simple BLOCK SELECTION CRITERION is derived from the solution of a CONSTRAINED MINIMISATION PROBLEM that underpins the development of NORMALISED CONSTANT MOD-ULUS ALGORITHM . in FRACTIONALLY-SPACED EQUALISATION , the proposed SELECTIVE PARTIAL UPDATING is shown to be capable of maintaining comparable CONVERGENCE SPEED to its FULL-UPDATE COUNTERPART . this implies a significant reduction in IMPLEMENTATION COST without necessarily <unk> the CONVERGENCE SPEED . \n",
            "this paper proposes a new BLOCK SELECTION CRITERION for SELECTIVE PARTIAL UPDATING . the proposed NORMALISED CONSTANT MOD-ULUS ALGORITHM is based on the BLOCK SELECTION CRITERION of the BLOCK SELECTION CRITERION of the NORMALISED CONSTANT MOD-ULUS ALGORITHM . the proposed NORMALISED CONSTANT MOD-ULUS ALGORITHM is based on the BLOCK SELECTION CRITERION of the BLOCK SELECTION CRITERION of the NORMALISED CONSTANT MOD-ULUS ALGORITHM . the proposed NORMALISED CONSTANT MOD-ULUS ALGORITHM is compared with the conventional NORMALISED CONSTANT MOD-ULUS ALGORITHM and the NORMALISED CONSTANT MOD-ULUS ALGORITHM . the proposed NORMALISED CONSTANT MOD-ULUS ALGORITHM is compared with the conventional NORMALISED CONSTANT MOD-ULUS ALGORITHM and the NORMALISED CONSTANT MOD-ULUS ALGORITHM .\n",
            "\n",
            "837 1000\n",
            "in this paper we describe a method for MINIMUM BAYES RISK DECODING for SPEECH RECOGNITION . this is a technique similar to CONSENSUS A.K.A. CONFUSION NETWORK DECODING , in which we attempt to find the hypothesis that minimizes the bayes ' risk with respect to the WORD ERROR RATE , based on a LATTICE OF ALTERNATIVE OUTPUTS . our method is an E-M like technique which makes approximations which we believe are less severe than the approximations made in CONSENSUS , and our experimental results show an improvement in E-M both for LATTICE RESCORING and LATTICE-BASED SYSTEM COMBINATION , versus baselines such as CONSENSUS , CONFUSION NETWORK COMBINATION and ROVER . \n",
            "this paper addresses the problem of SPEECH RECOGNITION in SPEECH RECOGNITION . we propose a CONFUSION NETWORK COMBINATION based on MINIMUM BAYES RISK DECODING and MINIMUM BAYES RISK DECODING . the proposed method is based on the LATTICE OF ALTERNATIVE OUTPUTS and the LATTICE OF ALTERNATIVE OUTPUTS . the proposed method is based on a CONFUSION NETWORK COMBINATION and a CONFUSION NETWORK COMBINATION . experimental results show the effectiveness of the proposed method in terms of WORD ERROR RATE and WORD ERROR RATE .\n",
            "\n",
            "838 1000\n",
            "it is known that SPEECH under PHYSICAL TASK STRESS degrades SPEECH SYSTEM performance . therefore , an analysis of SPEECH under PHYSICAL TASK STRESS is performed across several parameters to identify ACOUSTIC CORRELATES . formal LISTENER TESTS are also performed to determine the relationship between ACOUSTIC CORRELATES and perception . to verify the statistical significance of all results , <unk> statistical tests are applied . it was found that FUNDAMENTAL FREQUENCY decreases for many speakers , that UTTERANCE DURATION increases for some speakers and decreases for others , and that the GLOTTAL WAVEFORM is <unk> different for many speakers . perturbation of two SPEECH FEATURES , FUNDAMENTAL FREQUENCY and the GLOTTAL WAVEFORM , is applied in LISTENER TESTS to quantify the degree to which these FEATURES convey PHYSICAL STRESS CONTENT in SPEECH . finally , the enhanced understanding of PHYSICAL TASK STRESS SPEECH provided here is discussed in the context of SPEECH SYSTEM . \n",
            "this paper presents a method for PHYSICAL TASK STRESS SPEECH , which is based on a SPEECH SYSTEM . the method is based on the GLOTTAL WAVEFORM and the GLOTTAL WAVEFORM . the proposed method is based on the GLOTTAL WAVEFORM and the GLOTTAL WAVEFORM . the proposed method is based on LISTENER TESTS and LISTENER TESTS . the proposed method is based on LISTENER TESTS and LISTENER TESTS . the proposed method is tested on a PHYSICAL TASK STRESS SPEECH and on PHYSICAL TASK STRESS SPEECH .\n",
            "\n",
            "839 1000\n",
            "today 's ANTI-VIRUS TECHNOLOGY , based largely on analysis of existing <unk> by human experts , is just barely able to keep pace with the more than three new COMPUTER VIRUSES that are written daily . in a few years , INTELLIGENT AGENTS navigating through highly connected networks are likely to form an extremely <unk> medium for a new <unk> of <unk> . at ibm , we are developing novel , BIOLOGICALLY INSPIRED ANTI-VIRUS TECHNIQUES designed to <unk> both today 's and <unk> 's <unk> . here we describe two of these : a NEURAL NETWORK VIRUS DETECTOR that learns to discriminate between INFECTED AND UN-INFECTED PROGRAMS , and a COMPUTER IMMUNE SYSTEM that identifies new <unk> , analyzes them automatically , and uses the results of its analysis to detect and remove all copies of the <unk> that are present in the COMPUTER IMMUNE SYSTEM . the BIOLOGICALLY INSPIRED ANTI-VIRUS TECHNIQUES has been incorporated into ibm 's commercial <unk> product ; the COMPUTER IMMUNE SYSTEM is in prototype . \n",
            "this paper presents a novel NEURAL NETWORK VIRUS DETECTOR for INTELLIGENT AGENTS . the NEURAL NETWORK VIRUS DETECTOR is based on a NEURAL NETWORK VIRUS DETECTOR . the proposed NEURAL NETWORK VIRUS DETECTOR is based on a NEURAL NETWORK VIRUS DETECTOR . the proposed NEURAL NETWORK VIRUS DETECTOR is based on a NEURAL NETWORK VIRUS DETECTOR . the proposed NEURAL NETWORK VIRUS DETECTOR is based on a NEURAL NETWORK VIRUS DETECTOR and is shown to be robust to INFECTED AND UN-INFECTED PROGRAMS .\n",
            "\n",
            "840 1000\n",
            "the COMPUTATION OF OPTICAL OW relies on merging information available over an IMAGE PATCH to form an estimate of 2d image velocity a t a p o i n t . this MERGING PROCESS raises a host of issues , which include the treatment of OUTLIERS in COMPONENT V ELOCITY MEASUREMENTS and the MODELING OF MULTIPLE MOTIONS within a patch which arise from OCCLUSION BOUNDARIES or TRANSPARENCY . w e present a new approach which allows us to deal with these issues within a common framework . our approach is based on the use of a probabilistic mixture m o del to explicitly represent m <unk> motions within a patch . we use a simple extension of the EM-ALGORITHM to compute a MAXIMUM LIKELIHOOD ESTIMATE for the various MOTION PARAMETERS . preliminary experiments indicate that this approach is computationally eecient and can provide robust estimates of the OPTICAL OW V ALUES in the presence of OUTLIERS and multiple motions . the basic approach can also be applied to other problems in COMPUTATIONAL VISION , such as the COMPUTATION OF 3D RELATIVE MOTION , which require the integration of several partial constraints to obtain a desired quantity . \n",
            "this paper addresses the problem of MODELING OF MULTIPLE MOTIONS in COMPUTATIONAL VISION . we propose a method for MODELING OF MULTIPLE MOTIONS based on COMPONENT V ELOCITY MEASUREMENTS . the proposed method is based on a MAXIMUM LIKELIHOOD ESTIMATE , which is based on the MAXIMUM LIKELIHOOD ESTIMATE . the proposed method is based on a MAXIMUM LIKELIHOOD ESTIMATE , which is based on the MAXIMUM LIKELIHOOD ESTIMATE . the proposed method is based on a MAXIMUM LIKELIHOOD ESTIMATE , which is based on the MAXIMUM LIKELIHOOD ESTIMATE . the proposed method is shown to outperform the conventional EM-ALGORITHM in terms of OCCLUSION BOUNDARIES and OUTLIERS .\n",
            "\n",
            "841 1000\n",
            "most research in COMPUTER CHESS has focussed on creating an excellent CHESS PLAYER , with relatively little concern given to modelling how humans play CHESS PLAYER . the research reported in this paper is aimed at investigating KNOWLEDGE-BASED CHESS in the context of building a PROTOTYPE CHESS TUTOR , UMRAO , which helps students learn how to play BISHOP-PAWN ENDGAMES . in tutoring it is essential to take a KNOWLEDGE-BASED APPROACH , since students must learn how to manipulate STRATEGIC CONCEPTS , not how to carry out MINIMAX SEARCH . UMRAO uses an extension of <unk> 's advice language to represent expert and novice CHESS PLAYER plans . for any given <unk> the UMRAO is able to compile the plans into a STRATEGY GRAPH , which <unk> strategies -lrb- both well-formed and ill-formed -rrb- that students might use as STRATEGY GRAPHS solve the ENDGAME PROBLEM . STRATEGY GRAPHS can be compiled `` off-line '' so that STRATEGY GRAPHS can be used in REAL TIME TUTORING . we show that the normally rigid `` model tracing '' tutoring paradigm can be used in a flexible way in this domain . \n",
            "this paper addresses the problem of KNOWLEDGE-BASED CHESS in COMPUTER CHESS . we propose a KNOWLEDGE-BASED APPROACH , called UMRAO , which is based on the STRATEGY GRAPH . the KNOWLEDGE-BASED APPROACH is based on a PROTOTYPE CHESS TUTOR , which is based on a PROTOTYPE CHESS TUTOR . the proposed KNOWLEDGE-BASED APPROACH is based on a PROTOTYPE CHESS TUTOR , which is based on the KNOWLEDGE-BASED APPROACH . the proposed KNOWLEDGE-BASED APPROACH is based on a PROTOTYPE CHESS TUTOR . the proposed KNOWLEDGE-BASED APPROACH is based on a PROTOTYPE CHESS TUTOR , which is based on the KNOWLEDGE-BASED APPROACH . the proposed KNOWLEDGE-BASED APPROACH is based on a PROTOTYPE CHESS TUTOR , and is shown to be useful for COMPUTER CHESS .\n",
            "\n",
            "842 1000\n",
            "ideally , kernels used to generate BILINEAR TIME-FREQUENCY DISTRIBUTIONS should be <unk> , and optimised independently at every location in the TIME-FREQUENCY PLANE . this poses an extremely severe COMPUTATIONAL BURDEN . a compromise is proposed in this paper : TIME-VARYING KERNELS are optimised for specific TIME-VARYING KERNELS in the TIME-FREQUENCY PLANE . the TIME-VARYING KERNELS , designed to isolate separate components comprising the signal , are determined by modelling the BILINEAR TIME-FREQUENCY DISTRIBUTIONS using a FINITE MIXTURE MODEL of GAUSSIAN DISTRIBUTIONS . the parameters of the model are estimated using a combination of the EXPECTATION-MAXIMISATION ALGORITHM and FUNCTIONAL MERGING . the REGIONAL OPTIMISATION provides improved separation and resolution of CLOSELY-SPACED COMPONENTS when compared to methods using a solely time-varying kernel , without incurring an overwhelming COMPUTATIONAL EXPENSE . \n",
            "this paper addresses the problem of FUNCTIONAL MERGING for FUNCTIONAL MERGING . the EXPECTATION-MAXIMISATION ALGORITHM is based on a FINITE MIXTURE MODEL and a FINITE MIXTURE MODEL . the EXPECTATION-MAXIMISATION ALGORITHM is based on a FINITE MIXTURE MODEL and a FINITE MIXTURE MODEL . the proposed method is based on a FINITE MIXTURE MODEL and a FINITE MIXTURE MODEL . the proposed method is based on a FINITE MIXTURE MODEL of the TIME-FREQUENCY PLANE and the EXPECTATION-MAXIMISATION ALGORITHM .\n",
            "\n",
            "843 1000\n",
            "the STRUCTURE-FROM-MOTION PROBLEM has been extensively studied in the field of COMPUTER VISION . yet , the bulk of the existing work assumes that the scene contains only a single moving object . the more realistic case where an unknown number of objects move in the scene has received little attention , especially for its theoretical treatment . in this paper we present a new method for separating and recovering the motion and SHAPE of multiple independently moving objects in a sequence of images . the method does not require prior knowledge of the number of objects , nor is dependent on any grouping of FEATURES into an object at the IMAGE LEVEL . for this purpose , we introduce a MATHEMATICAL CONSTRUCT OF OBJECT SHAPES , called the SHAPE INTERACTION MATRIX , which is invariant to both the OBJECT MOTIONS and the SELECTION OF COORDINATE SYSTEMS . this SHAPE INTERACTION MATRIX is computable solely from the observed trajectories of IMAGE FEATURES without grouping them into individual objects . once the SHAPE INTERACTION MATRIX is computed , SHAPE INTERACTION MATRIX allows for SEGMENTING FEATURES into objects by the process of transforming SHAPE INTERACTION MATRIX into a CANONICAL FORM , as well as recovering the SHAPE and motion of each object . \n",
            "this paper presents a new method for MATHEMATICAL CONSTRUCT OF OBJECT SHAPES in COMPUTER VISION . the proposed method is based on a CANONICAL FORM , which is based on the SHAPE INTERACTION MATRIX of the SHAPE INTERACTION MATRIX . the proposed method is based on a CANONICAL FORM , which is based on the SHAPE INTERACTION MATRIX . the proposed method is based on a CANONICAL FORM and a CANONICAL FORM . the proposed method is based on a CANONICAL FORM , which is based on the SHAPE INTERACTION MATRIX . the proposed method is shown to be robust to OBJECT MOTIONS and is robust to OBJECT MOTIONS .\n",
            "\n",
            "844 1000\n",
            "in this paper , we propose a COMPACT YET DISCRIMINATIVE LOCAL DESCRIP-TOR which tackles the WIRELESS QUERY TRANSMISSION LATENCY in MOBILE VISUAL SEARCH . the COMPACT YET DISCRIMINATIVE LOCAL DESCRIP-TOR captures gradient statistics of <unk> patches over a LOG-POLAR LOCATION GRID whose parameters are optimized using training samples . we quantize the resulting COMPACT YET DISCRIMINATIVE LOCAL DESCRIP-TOR using PRODUCT QUANTIZATION . the COMPACT YET DISCRIMINATIVE LOCAL DESCRIP-TOR achieves about 95 % bits reduction compared with 128-BYTE SIFT and allows adaptation of DESCRIPTOR LENGTHS to support user required performance . moreover , accurate matching of DESCRIPTORS with LOW COMPLEXITY is allowed within several TABLE LOOKUP OPERATIONS . we perform a comprehensive comparison with SIFT , CHOG and CHOG in the context of IMAGE RETRIEVAL , IMAGE MATCHING and OBJECT LOCALIZATION . we achieve competing MATCHING AND RETRIEVAL performance with SIFT , CHOG with much fewer bits . in particular , the COMPACT YET DISCRIMINATIVE LOCAL DESCRIP-TOR outperforms CHOG at the same bits on eight data sets contributed to MPEG COMPACT DE-SCRIPTOR for VISUAL SEARCH STANDARDIZATION . \n",
            "this paper addresses the problem of OBJECT LOCALIZATION in IMAGE RETRIEVAL . we propose a COMPACT YET DISCRIMINATIVE LOCAL DESCRIP-TOR based on the COMPACT YET DISCRIMINATIVE LOCAL DESCRIP-TOR and a COMPACT YET DISCRIMINATIVE LOCAL DESCRIP-TOR . the proposed COMPACT YET DISCRIMINATIVE LOCAL DESCRIP-TOR is based on a COMPACT YET DISCRIMINATIVE LOCAL DESCRIP-TOR and a COMPACT YET DISCRIMINATIVE LOCAL DESCRIP-TOR . the proposed COMPACT YET DISCRIMINATIVE LOCAL DESCRIP-TOR is based on a COMPACT YET DISCRIMINATIVE LOCAL DESCRIP-TOR and a COMPACT YET DISCRIMINATIVE LOCAL DESCRIP-TOR . the proposed COMPACT YET DISCRIMINATIVE LOCAL DESCRIP-TOR is compared with a COMPACT YET DISCRIMINATIVE LOCAL DESCRIP-TOR and a COMPACT YET DISCRIMINATIVE LOCAL DESCRIP-TOR . the proposed COMPACT YET DISCRIMINATIVE LOCAL DESCRIP-TOR is compared with other DESCRIPTORS and DESCRIPTORS .\n",
            "\n",
            "845 1000\n",
            "we propose a simple and effective VARIATIONAL INFERENCE ALGORITHM based on STOCHASTIC OPTIMI-SATION that can be widely applied for BAYESIAN NON-CONJUGATE INFERENCE in CONTINUOUS PARAMETER SPACES . this VARIATIONAL INFERENCE ALGORITHM is based on STOCHASTIC APPROXIMATION and allows for efficient use of GRADIENT INFORMATION from the MODEL JOINT DENSITY . we demonstrate these properties using ILLUSTRATIVE EXAMPLES as well as in challenging and diverse BAYESIAN INFERENCE PROBLEMS such as VARIABLE SELECTION in LOGISTIC REGRESSION and fully bayesian inference over KERNEL HYPERPARAMETERS in GAUSSIAN PROCESS REGRESSION . \n",
            "this paper proposes a new VARIATIONAL INFERENCE ALGORITHM for BAYESIAN NON-CONJUGATE INFERENCE . the VARIATIONAL INFERENCE ALGORITHM is based on the GRADIENT INFORMATION of the MODEL JOINT DENSITY and the GRADIENT INFORMATION . the proposed VARIATIONAL INFERENCE ALGORITHM is based on the GRADIENT INFORMATION of the MODEL JOINT DENSITY . the proposed VARIATIONAL INFERENCE ALGORITHM is based on the VARIATIONAL INFERENCE ALGORITHM and the VARIATIONAL INFERENCE ALGORITHM . the proposed VARIATIONAL INFERENCE ALGORITHM is based on the VARIATIONAL INFERENCE ALGORITHM . the proposed VARIATIONAL INFERENCE ALGORITHM is based on the VARIATIONAL INFERENCE ALGORITHM . the proposed VARIATIONAL INFERENCE ALGORITHM is compared with the conventional VARIATIONAL INFERENCE ALGORITHM and the VARIATIONAL INFERENCE ALGORITHM .\n",
            "\n",
            "846 1000\n",
            "we present a new SYNTACTIC PARSER that works left-to-right and top down , thus maintaining a FULLY-CONNECTED PARSE TREE for a few alternative PARSE HYPOTHESES . all of the commonly used STATISTICAL PARSERS use CONTEXT-FREE DYNAMIC PROGRAMMING ALGORITHMS and as such work bottom up on the entire sentence . thus they only find a complete fully connected parse at the very end . in contrast , both subjective and experimental evidence show that people understand a sentence <unk> as they go along , or close to it . the CONSTRAINT that the SYNTACTIC PARSER keeps one or more fully connected syntactic trees is intended to operationalize this COGNITIVE FACT . our SYNTACTIC PARSER achieves a new best result for TOP-DOWN PARSERS of <unk> % , a 20 % ERROR REDUCTION over the previous <unk> best result for PARSERS of this type of <unk> % -lrb- <unk> , 2001 -rrb- . the improved performance is due to embracing the very large feature set available in exchange for giving up DYNAMIC PROGRAMMING . \n",
            "this paper addresses the problem of DYNAMIC PROGRAMMING for COGNITIVE FACT . we propose a new SYNTACTIC PARSER based on the FULLY-CONNECTED PARSE TREE . the proposed STATISTICAL PARSERS is based on the FULLY-CONNECTED PARSE TREE and the CONSTRAINT . the proposed PARSERS is evaluated on the FULLY-CONNECTED PARSE TREE . the experimental results show that the proposed SYNTACTIC PARSER achieves a significant improvement in ERROR REDUCTION compared to the conventional PARSERS .\n",
            "\n",
            "847 1000\n",
            "a central challenge in SEMANTIC PARSING is handling the myriad ways in which KNOWLEDGE BASE PREDICATES can be expressed . traditionally , SEMANTIC PARSERS are trained primarily from text paired with KNOWLEDGE BASE INFORMATION . our goal is to exploit the much larger amounts of RAW TEXT not tied to any KNOWLEDGE BASE . in this paper , we turn SEMANTIC PARSING on its head . given an input utterance , we first use a simple method to deterministically generate a set of CANDIDATE LOGICAL FORMS with a CANONICAL REALIZATION in NATURAL LANGUAGE for each . then , we use a PARAPHRASE MODEL to choose the realization that best paraphrases the input , and output the corresponding LOGICAL FORM . we present two simple PARAPHRASE MODEL , an ASSOCIATION MODEL and a VECTOR SPACE MODEL , and train PARAPHRASE MODEL jointly from QUESTION-ANSWER PAIRS . our system <unk> improves state-of-the-art ACCURACIES on two recently released QUESTION-ANSWERING DATASETS . \n",
            "this paper addresses the problem of SEMANTIC PARSING in NATURAL LANGUAGE . we propose a ASSOCIATION MODEL , called PARAPHRASE MODEL , to learn the KNOWLEDGE BASE INFORMATION of a KNOWLEDGE BASE . the ASSOCIATION MODEL is a PARAPHRASE MODEL , called CANONICAL REALIZATION , for SEMANTIC PARSING . the ASSOCIATION MODEL is based on a PARAPHRASE MODEL , called CANONICAL REALIZATION , that is based on the PARAPHRASE MODEL . the ASSOCIATION MODEL is a ASSOCIATION MODEL , which is a ASSOCIATION MODEL . the proposed ASSOCIATION MODEL is a ASSOCIATION MODEL , which is a ASSOCIATION MODEL . the proposed method is compared with state-of-the-art SEMANTIC PARSERS on QUESTION-ANSWERING DATASETS and on QUESTION-ANSWERING DATASETS .\n",
            "\n",
            "848 1000\n",
            "lexical resources such as WORDNET and VERBNET are widely used in a multitude of NLP TASKS , as are ANNOTATED CORPORA such as TREEBANKS . often , the resources are used <unk> , without question or examination . this practice risks missing significant performance gains and even entire techniques . this paper addresses the importance of RESOURCE QUALITY through the lens of a challenging NLP TASK : DETECTING SELEC-TIONAL PREFERENCE VIOLATIONS . we present DAVID , a simple , LEXICAL RESOURCE-BASED PREFERENCE VIOLATION DETECTOR . with AS-IS LEXICAL RESOURCES , DAVID achieves an F 1-MEASURE of just <unk> % . when the resource entries and PARSER OUTPUTS for a small sample are corrected , however , the F 1-MEASURE on that sample jumps from 40 % to <unk> % , and performance on other examples rises , suggesting that the algorithm becomes practical given refined resources . more broadly , this paper shows that RESOURCE QUALITY matters tremendously , sometimes even more than ALGORITHMIC IMPROVEMENTS . \n",
            "this paper presents a LEXICAL RESOURCE-BASED PREFERENCE VIOLATION DETECTOR for DETECTING SELEC-TIONAL PREFERENCE VIOLATIONS such as WORDNET , WORDNET , and WORDNET . the LEXICAL RESOURCE-BASED PREFERENCE VIOLATION DETECTOR is based on a LEXICAL RESOURCE-BASED PREFERENCE VIOLATION DETECTOR and a LEXICAL RESOURCE-BASED PREFERENCE VIOLATION DETECTOR . the proposed LEXICAL RESOURCE-BASED PREFERENCE VIOLATION DETECTOR is based on the use of AS-IS LEXICAL RESOURCES such as WORDNET , WORDNET , and WORDNET . the proposed LEXICAL RESOURCE-BASED PREFERENCE VIOLATION DETECTOR is evaluated on the NLP TASK , including WORDNET , WORDNET , and WORDNET .\n",
            "\n",
            "849 1000\n",
            "detection of vowel onset points -lrb- DETECTION OF VOWEL ONSET POINTS -rrb- is important for SPOTTING SUBWORD UNITS in CONTINUOUS SPEECH . for CONSONANT-VOWEL UTTERANCES , VOP is the instant at which the consonant part ends and the vowel part begins . accurate detection of DETECTION OF VOWEL ONSET POINTS is important for RECOGNITION OF CV UNITS in CONTINUOUS SPEECH . in this paper , we propose an approach for detection of DETECTION OF VOWEL ONSET POINTS using AUTOASSO-CIATIVE NEURAL NETWORK MODELS . a pair of AANN MODELS are trained for each CV CLASS to capture the characteristics of SPEECH SIGNAL in the consonant and vowel regions of that class . the trained AANN MODELS are then used to detect DETECTION OF VOWEL ONSET POINTS in CONTINUOUS SPEECH . the results of studies show that the proposed approach leads to significantly less number of spurious hypotheses . \n",
            "this paper presents a method for DETECTION OF VOWEL ONSET POINTS in CONTINUOUS SPEECH . the AUTOASSO-CIATIVE NEURAL NETWORK MODELS is based on the DETECTION OF VOWEL ONSET POINTS in CONTINUOUS SPEECH . the AUTOASSO-CIATIVE NEURAL NETWORK MODELS is based on the DETECTION OF VOWEL ONSET POINTS in CONTINUOUS SPEECH . the proposed method is based on the DETECTION OF VOWEL ONSET POINTS from CONTINUOUS SPEECH . the proposed method is based on the DETECTION OF VOWEL ONSET POINTS in CONTINUOUS SPEECH . the proposed method is based on the DETECTION OF VOWEL ONSET POINTS from CONTINUOUS SPEECH .\n",
            "\n",
            "850 1000\n",
            "logic programs with ABSTRACT CONSTRAINT ATOMS proposed by <unk> and <unk> are very general DESCRIPTION LOGIC PROGRAMS . they are general enough to capture AGGREGATE LOGIC PROGRAMS as well as recently proposed DESCRIPTION LOGIC PROGRAMS . in this paper , we propose a WELL-FOUNDED SEMANTICS for basic DESCRIPTION LOGIC PROGRAMS with ARBITRARY ABSTRACT CONSTRAINT ATOMS , which are sets of RULES whose heads have exactly one atom . we show that similar to the WELL-FOUNDED SEMANTICS of normal DESCRIPTION LOGIC PROGRAMS , it has many desirable properties such as that it can be computed in POLYNOMIAL TIME , and is always correct with respect to the ANSWER SET SEMANTICS . this paves the way for using our WELL-FOUNDED SEMANTICS to simplify these DESCRIPTION LOGIC PROGRAMS . we also show how our semantics can be applied to AGGREGATE LOGIC PROGRAMS and DESCRIPTION LOGIC PROGRAMS , and compare it to the WELL-FOUNDED SEMANTICS already proposed for these DESCRIPTION LOGIC PROGRAMS . \n",
            "this paper presents a method for DESCRIPTION LOGIC PROGRAMS based on AGGREGATE LOGIC PROGRAMS . the proposed method is based on the use of AGGREGATE LOGIC PROGRAMS to estimate the ABSTRACT CONSTRAINT ATOMS . the proposed method is based on the use of AGGREGATE LOGIC PROGRAMS to estimate the ABSTRACT CONSTRAINT ATOMS . the proposed method is based on the use of AGGREGATE LOGIC PROGRAMS to estimate the ABSTRACT CONSTRAINT ATOMS . the proposed method is based on the use of AGGREGATE LOGIC PROGRAMS . the experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "851 1000\n",
            "we propose an ENTIRELY DATA-DRIVEN APPROACH to estimating the 3D POSE of a hand given a DEPTH IMAGE . we show that we can correct the mistakes made by a CONVOLUTIONAL NEURAL NETWORK trained to predict an estimate of the 3D POSE by using a FEEDBACK LOOP . the components of this FEEDBACK LOOP are also DEEP NETWORKS , optimized using TRAINING DATA . they remove the need for fitting a 3D MODEL to the INPUT DATA , which requires both a carefully designed fitting function and algorithm . we show that our ENTIRELY DATA-DRIVEN APPROACH outperforms state-of-the-art methods , and is efficient as our implementation runs at over 400 fps on a single GPU . \n",
            "this paper presents a new method for DEEP NETWORKS based on a CONVOLUTIONAL NEURAL NETWORK . the proposed method is based on the CONVOLUTIONAL NEURAL NETWORK . the proposed method is based on the CONVOLUTIONAL NEURAL NETWORK . the proposed method is based on the CONVOLUTIONAL NEURAL NETWORK . the proposed method is based on the ENTIRELY DATA-DRIVEN APPROACH . the proposed method is based on the CONVOLUTIONAL NEURAL NETWORK . the proposed method is based on the ENTIRELY DATA-DRIVEN APPROACH . the proposed method is based on the ENTIRELY DATA-DRIVEN APPROACH .\n",
            "\n",
            "852 1000\n",
            "transformations between different COLOR SPACES and <unk> are ubiquitous operations performed on images . often , these transformations involve INFORMATION LOSS , for example when NATURAL '' MAPPING from COLOR to GRAYSCALE for PRINTING , from MULTISPECTRAL OR MULTIPRIMARY DATA to TRISTIMULUS SPACES , or from one COLOR <unk> to another . in all these applications , there exists a straightforward '' NATURAL '' MAPPING from the SOURCE SPACE to the target space , but the NATURAL '' MAPPING is not BIJECTIVE , resulting in INFORMATION LOSS due to METAMERISM and similar effects . we propose a CLUSTER-BASED APPROACH for optimizing the transformation for individual images in a way that preserves as much of the information as possible from the SOURCE SPACE while staying as faithful as possible to the NATURAL MAPPING . our CLUSTER-BASED APPROACH can be applied to a host of COLOR TRANSFORMATION PROBLEMS including COLOR to gray , GAMUT MAPPING , CONVERSION OF MULTISPECTRAL AND MULTIPRI-MARY DATA to TRISTIMULUS COLORS , and IMAGE OPTIMIZATION for COLOR DEFICIENT VIEWERS . \n",
            "this paper presents a new method for CONVERSION OF MULTISPECTRAL AND MULTIPRI-MARY DATA in COLOR SPACES . the proposed method is based on the CONVERSION OF MULTISPECTRAL AND MULTIPRI-MARY DATA , which is based on the CONVERSION OF MULTISPECTRAL AND MULTIPRI-MARY DATA . the proposed method is based on the CONVERSION OF MULTISPECTRAL AND MULTIPRI-MARY DATA , which is based on the CLUSTER-BASED APPROACH and the CLUSTER-BASED APPROACH . the proposed method is based on the CONVERSION OF MULTISPECTRAL AND MULTIPRI-MARY DATA , which is based on the CLUSTER-BASED APPROACH . the proposed method is based on the CONVERSION OF MULTISPECTRAL AND MULTIPRI-MARY DATA , which is based on the CONVERSION OF MULTISPECTRAL AND MULTIPRI-MARY DATA . the proposed method is based on the CLUSTER-BASED APPROACH and the CLUSTER-BASED APPROACH . experimental results show that the proposed method is robust and robust to TRISTIMULUS COLORS such as COLOR , COLOR , and COLOR .\n",
            "\n",
            "853 1000\n",
            "in this paper , we evaluate our proposed SINGING VOICE CONVERSION METHOD from various perspectives . to enable singers to freely control their VOICE TIMBRE OF SINGING VOICE , we have proposed a SINGING VOICE CONVERSION METHOD based on MANY-TO-MANY EIGENVOICE CONVERSION that enables to convert the VOICE TIMBRE of an arbitrary source singer into that of another arbitrary target singer using a PROBABILISTIC MODEL . furthermore , to easily develop TRAINING DATA consisting of multiple PARALLEL DATA SETS between a single reference singer and many other singers , a technique for efficiently and effectively generating the PARALLEL DATA SETS from NONPARALLEL SINGING VOICE DATA sets of many singers using a SINGING-TO-SINGING SYNTHESIS SYSTEM have been proposed . however , we have never conducted sufficient investigations into the effectiveness of these proposed SINGING VOICE CONVERSION METHOD . in this paper , we conduct both objective and subjective evaluations to carefully investigate the effectiveness of proposed SINGING VOICE CONVERSION METHOD . moreover , the differences between SINGING VOICE CONVERSION and SPEAKING VOICE CONVERSION are also analyzed . experimental results show that our proposed SINGING VOICE CONVERSION METHOD succeeds in enabling people to control their own VOICE TIMBRE by using only an extremely small amount of the target singing voice . \n",
            "this paper presents a novel SINGING VOICE CONVERSION METHOD for SINGING VOICE CONVERSION . the SINGING VOICE CONVERSION METHOD is based on a PROBABILISTIC MODEL and a PROBABILISTIC MODEL . the proposed SINGING VOICE CONVERSION METHOD is based on the VOICE TIMBRE OF SINGING VOICE and the PROBABILISTIC MODEL . the proposed SINGING VOICE CONVERSION METHOD is based on a PROBABILISTIC MODEL and a PROBABILISTIC MODEL . the proposed SINGING VOICE CONVERSION METHOD is based on the VOICE TIMBRE OF SINGING VOICE and the PROBABILISTIC MODEL . the proposed SINGING VOICE CONVERSION METHOD is evaluated on a VOICE TIMBRE OF SINGING VOICE .\n",
            "\n",
            "854 1000\n",
            "we focus on the feasibility of the SOURCE SEPARATION in the FREQUENCY DOMAIN . first , it is linked with the CONVERGENCE SPEED towards GAUSSIANITY OF SIGNALS after L-POINT DISCRETE FOURIER TRANSFORM . we test here a distance to GAUSSIANITY thanks to the SPECTRAL KURTOSIS . we analyse the influence of L , of the duration of the source <unk> and of a NON LINEAR FILTERING . we mainly develop the case of QARMA PROCESSES . the second point consists in the RECONSTRUCTION of the SPECTRA of the estimated sources from the signals identified at each FREQUENCY BIN . indeed , the source associated to the ITH IDENTIFIED SIGNAL is not necessarily the same from one FREQUENCY BIN to another . the ALGORITHM EFFICIENCY is then illustrated on QARMA PROCESSES , including the procedures of separation and RECONSTRUCTION . \n",
            "this paper presents a method for SOURCE SEPARATION in FREQUENCY DOMAIN . the proposed method is based on the L-POINT DISCRETE FOURIER TRANSFORM of the L-POINT DISCRETE FOURIER TRANSFORM . the proposed method is based on the L-POINT DISCRETE FOURIER TRANSFORM . the proposed method is based on the L-POINT DISCRETE FOURIER TRANSFORM of the L-POINT DISCRETE FOURIER TRANSFORM . the proposed method is based on the L-POINT DISCRETE FOURIER TRANSFORM . the proposed method is based on the L-POINT DISCRETE FOURIER TRANSFORM . the proposed method is based on the L-POINT DISCRETE FOURIER TRANSFORM . the proposed method is based on the L-POINT DISCRETE FOURIER TRANSFORM . the proposed method is based on the L-POINT DISCRETE FOURIER TRANSFORM . the ALGORITHM EFFICIENCY of the proposed method is demonstrated by simulation results .\n",
            "\n",
            "855 1000\n",
            "this paper presents an approach to 3D ROTATION ESTIMATION using discrete spherical harmonic oscillator transforms -lrb- DISCRETE SHOTS -rrb- . DISCRETE SHOTS not only have simple and fast implementation methods but also are compatible with the existing ANGLE ESTIMATION ALGORITHMS related to SPHERICAL HARMONICS . DISCRETE SHOTS of the ROTATED SIGNAL follow the same formulation to the WIGNER-D MATRIX as SPHERICAL HARMONICS transforms . thus , the SPHERICAL HARMONICS RELATED ALGORITHMS could be utilized to DISCRETE SHOTS without modification . furthermore , compared to some existing methods , our approach with DISCRETE SHOTS exhibits higher ACCURACY , higher PRECISION and improved ROBUSTNESS to NOISE if the input signal is sampled uniformly on CARTESIAN GRIDS . the phenomenon results from no <unk> in DISCRETE SHOTS . \n",
            "this paper presents a new method for 3D ROTATION ESTIMATION based on DISCRETE SHOTS . the proposed method is based on the WIGNER-D MATRIX of the WIGNER-D MATRIX . the proposed method is based on the WIGNER-D MATRIX of the WIGNER-D MATRIX and the WIGNER-D MATRIX . the proposed method is based on the use of SPHERICAL HARMONICS RELATED ALGORITHMS , which is based on the WIGNER-D MATRIX . experimental results show the effectiveness of the proposed ANGLE ESTIMATION ALGORITHMS compared to SPHERICAL HARMONICS RELATED ALGORITHMS .\n",
            "\n",
            "856 1000\n",
            "we consider POISSON REGRESSION with the CANONICAL LINK FUNCTION . this POISSON REGRESSION is widely used in REGRESSION ANALYSIS involving COUNT DATA ; one important application in ELECTRICAL ENGINEERING is TRANSMISSION TOMOGRAPHY . in this paper , we establish the VARIABLE SELECTION CONSISTENCY and ESTIMATION CONSISTENCY of the 1-REGULARIZED MAXIMUM-LIKELIHOOD ESTI-MATOR in this POISSON REGRESSION , and characterize the ASYMP-TOTIC SAMPLE COMPLEXITY that ensures consistency even under the COMPRESSIVE SENSING SETTING -lrb- or the N P SETTING in HIGH-DIMENSIONAL STATISTICS -rrb- . \n",
            "this paper presents a new method for TRANSMISSION TOMOGRAPHY based on POISSON REGRESSION . the proposed method is based on the 1-REGULARIZED MAXIMUM-LIKELIHOOD ESTI-MATOR , which is based on the 1-REGULARIZED MAXIMUM-LIKELIHOOD ESTI-MATOR . the proposed method is based on the 1-REGULARIZED MAXIMUM-LIKELIHOOD ESTI-MATOR . the proposed method is based on the 1-REGULARIZED MAXIMUM-LIKELIHOOD ESTI-MATOR . the proposed method is based on the 1-REGULARIZED MAXIMUM-LIKELIHOOD ESTI-MATOR . the ESTIMATION CONSISTENCY of the proposed method is compared with the conventional 1-REGULARIZED MAXIMUM-LIKELIHOOD ESTI-MATOR .\n",
            "\n",
            "857 1000\n",
            "this paper presents a novel , promising approach that allows GREEDY DECISION TREE INDUCTION ALGORITHMS to handle PROBLEMATIC FUNCTIONS such as PARITY FUNCTIONS . lookahead is the standard approach to addressing difficult functions for GREEDY DECISION TREE LEARNERS . nevertheless , this approach is limited to very small PROBLEMATIC FUNCTIONS or <unk> -lrb- 2 or 3 variables -rrb- , because the TIME COMPLEXITY grows more than exponentially with the depth of LOOKAHEAD . in contrast , the approach presented in this paper carries only a CONSTANT RUN-TIME PENALTY . experiments indicate that the approach is effective with only modest amounts of data for PROBLEMATIC FUNCTIONS or <unk> of up to six or seven variables , where the examples themselves may contain numerous other -lrb- irrelevant -rrb- variables as well . \n",
            "this paper addresses the problem of LOOKAHEAD in PROBLEMATIC FUNCTIONS such as LOOKAHEAD . we propose a method for estimating the parameters of the PROBLEMATIC FUNCTIONS and show that it is possible to estimate the parameters of the PARITY FUNCTIONS . the proposed algorithm is based on a CONSTANT RUN-TIME PENALTY , and is shown to be more robust to TIME COMPLEXITY than the conventional GREEDY DECISION TREE INDUCTION ALGORITHMS .\n",
            "\n",
            "858 1000\n",
            "recent advances in SEMANTIC IMAGE SEGMENTATION have mostly been achieved by training DEEP CONVOLUTIONAL NEU-RAL NETWORKS for the task . we show how to improve SEMANTIC SEGMENTATION through the use of CONTEXTUAL INFORMATION . specifically , we explore ` <unk> ' context and ` <unk> ' context with DEEP CNNS . for learning the PATCH-PATCH CONTEXT between image regions , we formulate CONDITIONAL RANDOM FIELDS with CNN-BASED PAIRWISE POTENTIAL FUNCTIONS to capture SEMANTIC CORRELATIONS between neighboring patches . efficient piecewise training of the proposed DEEP CONVOLUTIONAL NEU-RAL NETWORKS is then applied to avoid repeated expensive CRF INFERENCE for BACK PROPAGATION . in order to capture the PATCH-BACKGROUND CONTEXT , we show that a NETWORK DESIGN with traditional MULTI-SCALE IMAGE INPUT and SLIDING PYRAMID POOLING is effective for improving performance . our experiment results set new state-of-the-art performance on a number of popular SEMANTIC SEGMENTATION DATASETS , including NYUDV2 , pascal voc 2012 , PASCAL-CONTEXT , and SIFT-FLOW . particularly , we achieve an INTERSECTION-OVER-UNION SCORE of <unk> on the challenging PASCAL VOC 2012 DATASET . \n",
            "this paper addresses the problem of SEMANTIC IMAGE SEGMENTATION in DEEP CONVOLUTIONAL NEU-RAL NETWORKS . we propose a method for SEMANTIC IMAGE SEGMENTATION based on DEEP CONVOLUTIONAL NEU-RAL NETWORKS . the proposed method is based on the use of CNN-BASED PAIRWISE POTENTIAL FUNCTIONS and CONTEXTUAL INFORMATION . the proposed method is based on the use of CNN-BASED PAIRWISE POTENTIAL FUNCTIONS and CONTEXTUAL INFORMATION . the proposed method is based on the use of CNN-BASED PAIRWISE POTENTIAL FUNCTIONS and CONTEXTUAL INFORMATION . the proposed method is based on the use of DEEP CONVOLUTIONAL NEU-RAL NETWORKS and DEEP CONVOLUTIONAL NEU-RAL NETWORKS . the proposed method is evaluated on PASCAL VOC 2012 DATASET and on a PASCAL VOC 2012 DATASET . the results show that the proposed method is effective for SEMANTIC IMAGE SEGMENTATION , such as SIFT-FLOW and DEEP CONVOLUTIONAL NEU-RAL NETWORKS .\n",
            "\n",
            "859 1000\n",
            "to decode AUDITORY ATTENTION from ELECTROENCEPHALOGRAPHY RECORDINGS in a COCKTAIL-PARTY SCENARIO with two competing speakers a LEAST-SQUARES METHOD has recently been proposed , showing a promising DECODING ACCURACY . this method however requires the CLEAN SPEECH SIGNALS of both the attended and the <unk> speaker to be available as reference signals , which is difficult to achieve from the noisy recorded microphone signals in practice . in addition , optimizing the parameters involved in the SPATIO-TEMPORAL FILTER DESIGN is of crucial importance in order to reach the largest possible DECODING performance . in this paper , the influence of NOISY ACOUSTIC REFERENCE SIGNALS and the SPATIO-TEMPORAL FILTER and REGULARIZATION PARAMETERS on the DECODING performance is investigated . the results show that to some extent the DECODING performance is robust to NOISY ACOUSTIC REFERENCE SIGNALS , depending on the NOISE TYPE . furthermore , we demonstrate the crucial influence of several parameters on the DECODING performance , especially when the ACOUSTIC REFERENCE SIGNALS used for DECODING have been corrupted by noise . \n",
            "this paper addresses the problem of DECODING in ELECTROENCEPHALOGRAPHY RECORDINGS . we propose a method for DECODING based on AUDITORY ATTENTION . the proposed method is based on the SPATIO-TEMPORAL FILTER and the SPATIO-TEMPORAL FILTER . the proposed method is based on the LEAST-SQUARES METHOD and the LEAST-SQUARES METHOD . the proposed method is based on the LEAST-SQUARES METHOD and the LEAST-SQUARES METHOD . the proposed method is compared with the conventional LEAST-SQUARES METHOD and the LEAST-SQUARES METHOD .\n",
            "\n",
            "860 1000\n",
            "<unk> verbs are an important feature of the ENGLISH LANGUAGE . properly identifying PHRASAL VERBS provides the basis for an ENGLISH PARSER to decode the related structures . PHRASAL VERBS have been a challenge to NATURAL LANGUAGE PROCESSING because PHRASAL VERBS sit at the <unk> between lexicon and syntax . traditional NLP FRAMEWORKS that separate the LEXICON MODULE from the ENGLISH PARSER make it difficult to handle this problem properly . this paper presents a FINITE STATE APPROACH that integrates a PHRASAL VERB EXPERT LEXICON between SHALLOW PARSING and DEEP PARSING to handle MORPHO-SYNTACTIC INTERACTION . with precision/recall combined performance benchmarked consistently at <unk> % <unk> % , the PHRASAL VERB IDENTIFICATION PROBLEM has basically been solved with the presented FINITE STATE APPROACH . \n",
            "this paper addresses the problem of SHALLOW PARSING for NATURAL LANGUAGE PROCESSING . we propose a FINITE STATE APPROACH for DEEP PARSING , which is based on the LEXICON MODULE . the LEXICON MODULE is modeled by a PHRASAL VERB EXPERT LEXICON , which is a LEXICON MODULE . the LEXICON MODULE is estimated using the FINITE STATE APPROACH and the LEXICON MODULE . the proposed FINITE STATE APPROACH is applied to the PHRASAL VERB IDENTIFICATION PROBLEM . experimental results show that the proposed method is robust and robust to MORPHO-SYNTACTIC INTERACTION and is robust to MORPHO-SYNTACTIC INTERACTION .\n",
            "\n",
            "861 1000\n",
            "a LIGHTWEIGHT EXTRACTION METHOD derives TEXT SNIPPETS associated to dates from the WEB . the snippets are organized dynamically into answers to definition questions . experiments on standard TEST QUESTION SETS show that TEMPORALLY-ANCHORED TEXT SNIPPETS allow for efficiently answering definition questions at accuracy levels comparable to the best systems , without any need for complex LEXICAL RESOURCES , or specialized PROCESSING MODULES dedicated to finding definitions . \n",
            "this paper presents a new LIGHTWEIGHT EXTRACTION METHOD for TEMPORALLY-ANCHORED TEXT SNIPPETS . the LIGHTWEIGHT EXTRACTION METHOD is based on a LIGHTWEIGHT EXTRACTION METHOD . the proposed LIGHTWEIGHT EXTRACTION METHOD is based on a LIGHTWEIGHT EXTRACTION METHOD , which is based on the LIGHTWEIGHT EXTRACTION METHOD . the proposed LIGHTWEIGHT EXTRACTION METHOD is evaluated on the WEB and on the WEB .\n",
            "\n",
            "862 1000\n",
            "we have developed a method to determine whether a USER UTTERANCE is directed at the system or not . a SPOKEN DIALOGUE SYSTEM should not respond to AUDIO INPUTS that are not directed at it -lrb- i.e. , a user 's <unk> -rrb- , and it therefore needs to detect such inputs to avoid unsuitable responses . we classify the two cases by LOGISTIC REGRESSION based on a FEATURE SET including UTTERANCE TIMING , UTTERANCE LENGTH , and DIALOGUE STATUS . we conducted experiments using <unk> USER UTTERANCES for both TRANSCRIPTION and automatic speech recognition results . results showed that the CLASSIFICATION ACCURACY improved by <unk> and 4.1 points , respectively . we also discuss which FEATURES are effective in the CLASSIFICATION . \n",
            "this paper presents a new method for CLASSIFICATION from AUDIO INPUTS . the proposed method is based on a USER UTTERANCE , which is a USER UTTERANCE and a USER UTTERANCE . the proposed method is based on LOGISTIC REGRESSION , which is based on LOGISTIC REGRESSION and LOGISTIC REGRESSION . the proposed method is based on LOGISTIC REGRESSION , which is based on LOGISTIC REGRESSION and LOGISTIC REGRESSION . the experimental results show that the proposed method is robust and robust to USER UTTERANCES .\n",
            "\n",
            "863 1000\n",
            "a MULTI-IMAGE FOCUS OF ATTENTION MECHANISM has been developed that can quickly distinguish RAISED OBJECTS like BUILDINGS from STRUCTURED BACKGROUND CLUTTER typical to many AERIAL IMAGE SCENARIOS . the underlying approach is the SPACE-SWEEP STEREO METHOD , in which FEATURES from multiple images are <unk> onto a VIRTUAL , HORIZONTAL PLANE that is <unk> <unk> through the scene . BACK-PROJECTED GRADIENT ORIENTATIONS from multiple images are highly correlated when BACK-PROJECTED GRADIENT ORIENTATIONS come from SCENE LOCATIONS containing STRUCTURAL EDGES that are roughly horizontal , like building <unk> and terrain ; otherwise , BACK-PROJECTED GRADIENT ORIENTATIONS tend to be uniformly distributed . these observations are used to define a STRUCTURAL SALIENCE MEASURE that can determine whether a given VOLUME OF SPACE contains a statistically significant number of STRUCTURAL EDGES , without first performing precise reconstruction of those EDGES . the utility of STRUCTURAL SALIENCE for computing focus of attention regions is illustrated on sample data from <unk> , texas . \n",
            "this paper presents a novel method for STRUCTURED BACKGROUND CLUTTER in AERIAL IMAGE SCENARIOS . the proposed method is based on the MULTI-IMAGE FOCUS OF ATTENTION MECHANISM . the proposed method is based on the MULTI-IMAGE FOCUS OF ATTENTION MECHANISM . the proposed method is based on the MULTI-IMAGE FOCUS OF ATTENTION MECHANISM . the proposed method is based on the MULTI-IMAGE FOCUS OF ATTENTION MECHANISM . the proposed method is based on the MULTI-IMAGE FOCUS OF ATTENTION MECHANISM . the proposed method is based on the MULTI-IMAGE FOCUS OF ATTENTION MECHANISM . the proposed method is based on the MULTI-IMAGE FOCUS OF ATTENTION MECHANISM . the proposed method is based on the MULTI-IMAGE FOCUS OF ATTENTION MECHANISM . the proposed method is based on the MULTI-IMAGE FOCUS OF ATTENTION MECHANISM . the proposed method is based on the MULTI-IMAGE FOCUS OF ATTENTION MECHANISM . the proposed method is based on the MULTI-IMAGE FOCUS OF ATTENTION MECHANISM . the proposed method is based on the MULTI-IMAGE FOCUS OF ATTENTION MECHANISM .\n",
            "\n",
            "864 1000\n",
            "information seeking is an important but often difficult task especially when involving large and complex data sets . we hypothesize that a CONTEXT-SENSITIVE INTERACTION PARADIGM can greatly assist users in their information seeking . such a CONTEXT-SENSITIVE INTERACTION PARADIGM allows a system to both understand USER DATA REQUESTS and present the requested information in context . driven by this hypothesis , we have developed a suite of INTELLIGENT USER INTERACTION TECHNOLOGIES and integrated INTELLIGENT USER INTERACTION TECHNOLOGIES in a FULL-FLEDGED , CONTEXT-SENSITIVE INFORMATION SYSTEM . in this paper , we review two sets of key technologies : CONTEXT-SENSITIVE MULTIMODAL INPUT INTERPRETATION and AUTOMATED MULTIMEDIA OUTPUT GENERATION . we also share our evaluation results , which indicate that our CONTEXT-SENSITIVE INTERACTION PARADIGM are capable of supporting CONTEXT-SENSITIVE INFORMATION seeking for practical applications . \n",
            "this paper presents a CONTEXT-SENSITIVE INTERACTION PARADIGM for AUTOMATED MULTIMEDIA OUTPUT GENERATION . the CONTEXT-SENSITIVE INTERACTION PARADIGM is based on a CONTEXT-SENSITIVE INTERACTION PARADIGM . the CONTEXT-SENSITIVE INTERACTION PARADIGM is based on the CONTEXT-SENSITIVE INTERACTION PARADIGM . the CONTEXT-SENSITIVE INTERACTION PARADIGM is based on the CONTEXT-SENSITIVE INTERACTION PARADIGM and the CONTEXT-SENSITIVE INTERACTION PARADIGM . the experimental results show the effectiveness of the proposed CONTEXT-SENSITIVE INTERACTION PARADIGM .\n",
            "\n",
            "865 1000\n",
            "in this paper we present results for two tasks : SOCIAL EVENT DETECTION and SOCIAL NETWORK EXTRACTION from a LITERARY TEXT , AL-ICE IN WONDERLAND . for the first task , our system trained on a NEWS CORPUS using TREE KERNELS and SUPPORT VECTOR MACHINES beats the baseline systems by a statistically significant margin . using this system we extract a SOCIAL NETWORK from AL-ICE IN WONDERLAND . we show that while we achieve an F-MEASURE of about 61 % on SOCIAL EVENT DETECTION , our extracted UN-WEIGHTED NETWORK is not statistically <unk> from the UN-WEIGHTED GOLD NETWORK according to popularly used NETWORK MEASURES . \n",
            "this paper addresses the problem of SOCIAL EVENT DETECTION in LITERARY TEXT . we propose a method for SOCIAL NETWORK EXTRACTION based on TREE KERNELS and TREE KERNELS . the proposed method is based on a UN-WEIGHTED GOLD NETWORK and a UN-WEIGHTED GOLD NETWORK . the proposed method is based on the TREE KERNELS and the AL-ICE IN WONDERLAND . the proposed method is based on a UN-WEIGHTED GOLD NETWORK and a UN-WEIGHTED GOLD NETWORK . the experimental results show the effectiveness of the proposed method in terms of F-MEASURE and F-MEASURE .\n",
            "\n",
            "866 1000\n",
            "this paper addresses the problem of finding glass objects in images . VISUAL CUES obtained by combining the systematic distortions in BACKGROUND TEXTURE occurring at the boundaries of TRANSPARENT OBJECTS with the strong highlights typical of GLASS SURFACES are used to train a hierarchy of CLASSIFIERS , identify GLASS EDGES , and find CONSISTENT SUPPORT REGIONS for these EDGES . qualitative and quantitative experiments involving a number of different CLASSIFIERS and REAL IMAGES are presented . \n",
            "this paper addresses the problem of TRANSPARENT OBJECTS in TRANSPARENT OBJECTS . we propose a method for TRANSPARENT OBJECTS , which is based on the BACKGROUND TEXTURE and the BACKGROUND TEXTURE . the proposed method is based on the VISUAL CUES and the CLASSIFIERS . the proposed method is based on the VISUAL CUES and the VISUAL CUES . the proposed method is evaluated on REAL IMAGES and REAL IMAGES .\n",
            "\n",
            "867 1000\n",
            "we propose a '' SOFT GREEDY '' LEARNING ALGORITHM for building small conjunctions of simple threshold functions , called RAYS , defined on SINGLE REAL-VALUED ATTRIBUTES . we also propose a PAC-BAYES RISK BOUND which is minimized for CLASSIFIERS achieving a non-trivial tradeoff between sparsity -lrb- the number of RAYS used -rrb- and the magnitude of the separating margin of each ray . finally , we test the SOFT GREEDY '' LEARNING ALGORITHM on four DNA MICRO-ARRAY DATA SETS . \n",
            "this paper proposes a new SOFT GREEDY '' LEARNING ALGORITHM based on the SOFT GREEDY '' LEARNING ALGORITHM . the proposed SOFT GREEDY '' LEARNING ALGORITHM is based on the SOFT GREEDY '' LEARNING ALGORITHM . the proposed SOFT GREEDY '' LEARNING ALGORITHM is evaluated on the DNA MICRO-ARRAY DATA SETS . the experimental results show that the proposed SOFT GREEDY '' LEARNING ALGORITHM outperforms the conventional SOFT GREEDY '' LEARNING ALGORITHM in terms of RAYS .\n",
            "\n",
            "868 1000\n",
            "frequency domain blind source separation -lrb- FREQUENCY DOMAIN BLIND SOURCE SEPARATION -rrb- is shown to be equivalent to two sets of FREQUENCY DOMAIN ADAPTIVE MICROPHONE ARRAYS , i.e. , ADAPTIVE NULL BEAM-FORMERS . the minimization of the OFF-DIAGONAL COMPONENTS in the BSS UPDATE EQUATION can be viewed as the minimization of the MEAN SQUARE ERROR in the FREQUENCY DOMAIN BLIND SOURCE SEPARATION . the UNMIXING MATRIX of the FREQUENCY DOMAIN BLIND SOURCE SEPARATION and the FILTER COEFFICIENTS of the FREQUENCY DOMAIN BLIND SOURCE SEPARATION converge to the same solution in the MEAN SQUARE ERROR SENSE if the two source signals are ideally independent . therefore , we can conclude that the performance of the FREQUENCY DOMAIN BLIND SOURCE SEPARATION is upper bounded by that of the FREQUENCY DOMAIN BLIND SOURCE SEPARATION . this understanding clearly explains the poor performance of the FREQUENCY DOMAIN BLIND SOURCE SEPARATION in a real room with long reverberation . the fundamental difference exists in the adaptation period when they should adapt . that is , the FREQUENCY DOMAIN BLIND SOURCE SEPARATION can adapt in the presence of a JAMMER but the absence of a target , whereas the FREQUENCY DOMAIN BLIND SOURCE SEPARATION can adapt in the presence of a target and JAMMER , and also in the presence of only a target . \n",
            "this paper presents a new method for FREQUENCY DOMAIN BLIND SOURCE SEPARATION in FREQUENCY DOMAIN ADAPTIVE MICROPHONE ARRAYS . the proposed method is based on the ADAPTIVE NULL BEAM-FORMERS . the proposed method is based on the ADAPTIVE NULL BEAM-FORMERS . the proposed method is based on the ADAPTIVE NULL BEAM-FORMERS . the proposed method is based on the ADAPTIVE NULL BEAM-FORMERS . the proposed method is based on the ADAPTIVE NULL BEAM-FORMERS . the proposed method is based on the ADAPTIVE NULL BEAM-FORMERS . the proposed method is based on the ADAPTIVE NULL BEAM-FORMERS . the proposed method is compared with the conventional ADAPTIVE NULL BEAM-FORMERS .\n",
            "\n",
            "869 1000\n",
            "we propose a general framework for learning from LABELED AND UNLABELED DATA on a DIRECTED GRAPH in which the structure of the GRAPH including the DIRECTIONALITY OF THE EDGES is considered . the TIME COMPLEXITY of the algorithm derived from this framework is nearly linear due to recently developed NUMERICAL TECHNIQUES . in the absence of labeled instances , this framework can be utilized as a SPECTRAL CLUSTERING METHOD for DIRECTED GRAPHS , which generalizes the SPECTRAL CLUSTERING APPROACH for UNDIRECTED GRAPHS . we have applied our framework to REAL-WORLD WEB CLASSIFICATION PROBLEMS and obtained encouraging results . \n",
            "this paper presents a SPECTRAL CLUSTERING METHOD for REAL-WORLD WEB CLASSIFICATION PROBLEMS . the SPECTRAL CLUSTERING METHOD is based on the SPECTRAL CLUSTERING APPROACH . the SPECTRAL CLUSTERING APPROACH is based on the SPECTRAL CLUSTERING APPROACH . the SPECTRAL CLUSTERING APPROACH is based on the SPECTRAL CLUSTERING APPROACH . the proposed SPECTRAL CLUSTERING METHOD is based on the SPECTRAL CLUSTERING APPROACH . the proposed SPECTRAL CLUSTERING METHOD is based on the SPECTRAL CLUSTERING APPROACH . the proposed SPECTRAL CLUSTERING METHOD is based on the SPECTRAL CLUSTERING APPROACH . the proposed SPECTRAL CLUSTERING METHOD is applied to the REAL-WORLD WEB CLASSIFICATION PROBLEMS .\n",
            "\n",
            "870 1000\n",
            "we present the design and analysis of an approximately incentive-compatible combinatorial auction . in just a single run , the auction is able to extract enough value information from bidders to compute APPROXIMATE TRUTH-INDUCING PAYMENTS . this stands in contrast to current AUCTION DESIGNS that need to repeat the ALLOCATION COMPUTATION as many times as there are bidders to achieve INCENTIVE COMPATIBILITY . the auction is formulated as a KERNEL METHOD , which allows for flexibility in choosing the PRICE STRUCTURE via a KERNEL FUNCTION . our main result characterizes the extent to which our auction is incentive-compatible in terms of the COMPLEXITY of the chosen KERNEL FUNCTION . our analysis of the AUCTION 'S PROPERTIES is based on novel insights connecting the notion of stability in STATISTICAL LEARNING THEORY to that of UNIVERSAL COMPETITIVE EQUILIBRIUM in the AUCTION LITERATURE . \n",
            "this paper addresses the problem of ALLOCATION COMPUTATION in the AUCTION LITERATURE . we propose a method for ALLOCATION COMPUTATION in the AUCTION LITERATURE . the proposed method is based on the KERNEL METHOD . the proposed method is based on the STATISTICAL LEARNING THEORY . the proposed method is based on the KERNEL METHOD . the proposed method is based on the KERNEL METHOD . the proposed method is based on the KERNEL METHOD . the proposed method is based on the KERNEL METHOD . the proposed method is based on the KERNEL METHOD and the KERNEL METHOD .\n",
            "\n",
            "871 1000\n",
            "we propose SCALPEL , a FLEXIBLE METHOD for OBJECT SEG-MENTATION that integrates RICH REGION-MERGING CUES with MID-AND HIGH-LEVEL INFORMATION about OBJECT LAYOUT , CLASS , and scale into the SEGMENTATION PROCESS . unlike competing approaches , SCALPEL uses a cascade of BOTTOM-UP SEGMENTATION MODELS that is capable of learning to ignore boundaries early on , yet use them as a STOPPING CRITERION once the object has been mostly segmented . furthermore , we show how such SCALPEL can be learned efficiently . when paired with a novel method that generates better LOCALIZED SHAPE PRIORS than our competitors , our method leads to a concise , accurate set of SEGMENTATION PROPOSALS ; these proposals are more accurate on the PASCAL VOC2010 DATASET than state-of-the-art methods that use RE-RANKING to filter much larger bags of proposals . the code for our algorithm is available online . \n",
            "this paper presents a novel FLEXIBLE METHOD for OBJECT SEG-MENTATION . the SEGMENTATION PROCESS is formulated as a SEGMENTATION PROCESS . the SEGMENTATION PROCESS is formulated as a STOPPING CRITERION and a STOPPING CRITERION . the SEGMENTATION PROCESS is formulated as a SEGMENTATION PROCESS . the SEGMENTATION PROCESS is formulated as a SEGMENTATION PROCESS . the proposed FLEXIBLE METHOD is based on a STOPPING CRITERION and a STOPPING CRITERION . the SEGMENTATION PROCESS is formulated as a SEGMENTATION PROCESS . the proposed FLEXIBLE METHOD is based on a FLEXIBLE METHOD and is shown to be robust to SEGMENTATION PROPOSALS and SEGMENTATION PROPOSALS .\n",
            "\n",
            "872 1000\n",
            "lexicalized REORDERING MODELS play a crucial role in PHRASE-BASED TRANSLATION SYSTEMS . they are usually learned from the WORD-ALIGNED BILINGUAL CORPUS by examining the REORDERING RELATIONS OF ADJACENT PHRASES . instead of just checking whether there is one phrase adjacent to a given phrase , we argue that it is important to take the number of adjacent phrases into account for better estimations of REORDERING MODELS . we propose to use a STRUCTURE NAMED REORDERING GRAPH , which represents all PHRASE SEGMENTATIONS of a sentence pair , to learn LEX-ICALIZED REORDERING MODELS efficiently . experimental results on the NIST CHINESE-ENGLISH TEST SETS show that our approach significantly outperforms the baseline method . \n",
            "this paper addresses the problem of REORDERING RELATIONS OF ADJACENT PHRASES in PHRASE-BASED TRANSLATION SYSTEMS . in this paper , we propose a method for REORDERING RELATIONS OF ADJACENT PHRASES based on the STRUCTURE NAMED REORDERING GRAPH . the proposed method is based on the WORD-ALIGNED BILINGUAL CORPUS . the proposed method is based on the STRUCTURE NAMED REORDERING GRAPH . the experimental results show that the proposed method is effective for PHRASE-BASED TRANSLATION SYSTEMS .\n",
            "\n",
            "873 1000\n",
            "grapheme-to-phoneme conversion -lrb- g2p -rrb- is usually used within every state-of-the-art ASR SYSTEM to generalize beyond a fixed set of words . although the performance is typically already quite good -lrb- < 10 % PHONEME ERROR RATE -rrb- and pronunciations of important words are checked by a <unk> , further improvements are still desirable , especially for END USER CUSTOMIZATION . in this work , we present and compare five <unk> to tackle the G2P TASK . although most of the methods have already been published and/or are available as OPEN SOURCE SOFTWARE , the reported experiments are done on large state-of-the-art tasks and the used software is from the actual publications . besides an experimental comparison on TEXT DATA for a range of languages -lrb- i.e. measuring the G2P ACCURACY only -rrb- , our focus in this paper is measuring the effect of improved GRAPHEME-TO-PHONEME CONVERSION on lvcsr performance for a challenging G2P TASK . additionally , the effect of using N-BEST PRONUNCIATION VARIANTS instead of single best is investigated briefly . \n",
            "this paper presents a novel ASR SYSTEM for GRAPHEME-TO-PHONEME CONVERSION . the proposed OPEN SOURCE SOFTWARE is based on the END USER CUSTOMIZATION of the OPEN SOURCE SOFTWARE . the proposed OPEN SOURCE SOFTWARE is based on the END USER CUSTOMIZATION and the END USER CUSTOMIZATION . the proposed OPEN SOURCE SOFTWARE is evaluated on the G2P TASK . experimental results show the effectiveness of the proposed ASR SYSTEM in terms of G2P ACCURACY and G2P ACCURACY .\n",
            "\n",
            "874 1000\n",
            "in this paper , we present a novel FEATURE NORMALIZATION METHOD in the LOG-SCALED SPECTRAL DOMAIN for improving the NOISE ROBUSTNESS of speech recognition front-ends . in the proposed FEATURE NORMALIZATION METHOD , a NON-LINEAR CONTRAST STRETCHING is added to the outputs of LOG MEL-FILTERBANKS to imitate the ADAPTATION OF THE AUDITORY SYSTEM under ADVERSE CONDITIONS . this is followed by a TWO-DIMENSIONAL FILTER to smooth out the PROCESSING ARTIFACTS . the proposed FEATURE NORMALIZATION METHOD perform remarkably well on CENSREC-2 IN-CAR DATABASE with an average relative improvement of <unk> % compared to BASELINE MFCC SYSTEM . it is also confirmed that the proposed processing in LOG MFB DOMAIN can be integrated with conventional CEPSTRAL POST-PROCESSING TECHNIQUES to yield further improvements . the proposed FEATURE NORMALIZATION METHOD is simple and requires only a small extra COMPUTATION LOAD . \n",
            "this paper proposes a new FEATURE NORMALIZATION METHOD for NON-LINEAR CONTRAST STRETCHING . the FEATURE NORMALIZATION METHOD is based on the ADAPTATION OF THE AUDITORY SYSTEM in the LOG MFB DOMAIN . the proposed FEATURE NORMALIZATION METHOD is based on the ADAPTATION OF THE AUDITORY SYSTEM in the LOG MFB DOMAIN . the proposed FEATURE NORMALIZATION METHOD is based on the ADAPTATION OF THE AUDITORY SYSTEM . the proposed FEATURE NORMALIZATION METHOD is evaluated on the CENSREC-2 IN-CAR DATABASE and compared with the BASELINE MFCC SYSTEM . the proposed FEATURE NORMALIZATION METHOD is compared with the conventional BASELINE MFCC SYSTEM and the BASELINE MFCC SYSTEM .\n",
            "\n",
            "875 1000\n",
            "this paper describes a new DIALOGUE CONTROL METHOD that utilizes new RECOGNITION PROCESSES called '' <unk> recognition '' and '' PRETENSE-TYPE RECOGNITION '' that we propose based on HUMAN DIALOGUE ANALYSIS . this DIALOGUE CONTROL METHOD provides users with STRESS-FREE VOICE INPUT through REAL-TIME RESPONSES , comprising a NATURALLY CONTROLLED DIALOGUE to obtain information in order to <unk> candidates comprehensively . \n",
            "this paper addresses the problem of HUMAN DIALOGUE ANALYSIS for NATURALLY CONTROLLED DIALOGUE . the DIALOGUE CONTROL METHOD is based on the use of STRESS-FREE VOICE INPUT in the STRESS-FREE VOICE INPUT . the proposed method is based on the use of STRESS-FREE VOICE INPUT in the STRESS-FREE VOICE INPUT . experimental results are presented to demonstrate the effectiveness of the proposed DIALOGUE CONTROL METHOD .\n",
            "\n",
            "876 1000\n",
            "image fusion can be viewed as a process that incorporates essential information from different MODALITY SENSORS into a COMPOSITE IMAGE . the use of bases trained using INDEPENDENT COMPONENT ANALYSIS for IMAGE FUSION has been highlighted recently . common fusion rules can be used in the ICA FUSION FRAMEWORK with promising results . in this paper , the authors propose an ADAPTIVE FUSION SCHEME , based on the ICA FUSION FRAMEWORK , that <unk> the sparsity of the fusion image in the transform domain . \n",
            "this paper presents a novel ADAPTIVE FUSION SCHEME for IMAGE FUSION . the ICA FUSION FRAMEWORK is based on a ICA FUSION FRAMEWORK . the ADAPTIVE FUSION SCHEME is based on a ICA FUSION FRAMEWORK . the proposed ADAPTIVE FUSION SCHEME is based on a ICA FUSION FRAMEWORK . the ADAPTIVE FUSION SCHEME is applied to a COMPOSITE IMAGE .\n",
            "\n",
            "877 1000\n",
            "on-line , spatially localized information about INTERNAL NETWORK PERFORMANCE can greatly assist DYNAMIC ROUTING ALGORITHMS and TRAFFIC TRANSMISSION PROTOCOLS . however , it is impractical to measure NETWORK TRAFFIC at all points in the network . a promising alternative is to measure only at the edge of the network and infer INTERNAL BEHAVIOR from these measurements . in this paper we concentrate on the ESTIMATION AND LOCALIZATION OF INTERNAL DELAYS based on END-TO-END DELAY MEASUREMENTS from sources to receivers . we develop an EM ALGORITHM for computing <unk> of the INTERNAL DELAY DISTRIBUTIONS in cases where the NETWORK DYNAMICS are stationary over the OBSERVATION PERIOD . for time-varying cases , we propose a SEQUENTIAL MONTE CARLO PROCEDURE capable of TRACKING NON-STATIONARY DELAY CHARACTERISTICS . simulations are included to demonstrate the promise of these techniques . \n",
            "this paper presents a new method for ESTIMATION AND LOCALIZATION OF INTERNAL DELAYS based on a SEQUENTIAL MONTE CARLO PROCEDURE . the proposed method is based on the SEQUENTIAL MONTE CARLO PROCEDURE and the EM ALGORITHM . the proposed method is based on the SEQUENTIAL MONTE CARLO PROCEDURE and the SEQUENTIAL MONTE CARLO PROCEDURE . the proposed method is based on the SEQUENTIAL MONTE CARLO PROCEDURE and the EM ALGORITHM . the proposed method is based on the SEQUENTIAL MONTE CARLO PROCEDURE and the EM ALGORITHM . the proposed method is compared with conventional DYNAMIC ROUTING ALGORITHMS and DYNAMIC ROUTING ALGORITHMS .\n",
            "\n",
            "878 1000\n",
            "in this paper , we describe a novel type of FEATURE-CENTRIC CASCADE for fast and accurate FACE DETECTION . the FEATURE-CENTRIC CASCADE is called LOCALLY ASSEMBLED BINARY HAAR FEATURE . LOCALLY ASSEMBLED BINARY HAAR FEATURE is basically inspired by the success of HAAR FEATURE and LOCAL BINARY PATTERN for FACE DETECTION , but it is far beyond a simple combination . in our LOCALLY ASSEMBLED BINARY HAAR FEATURE , HAAR FEATURES are modified to keep only the ORDINAL RELATIONSHIP -lrb- named by BINARY HAAR FEATURE -rrb- rather than the difference between the ACCUMULATED INTENSITIES . several neighboring BINARY HAAR FEATURES are then assembled to capture their co-occurrence with similar idea to LOCAL BINARY PATTERN . we show that the FEATURE-CENTRIC CASCADE is more efficient than HAAR FEATURE and LOCAL BINARY PATTERN both in DISCRIMINATING POWER and COMPUTATIONAL COST . furthermore , a novel efficient DETECTION METHOD called FEATURE-CENTRIC CASCADE is proposed to build an efficient detector , which is developed from the FEATURE-CENTRIC METHOD . experimental results on the <unk> frontal face test set and cmu profile test set show that the proposed LOCALLY ASSEMBLED BINARY HAAR FEATURE can achieve very good results and <unk> DETECTION SPEED . \n",
            "this paper proposes a new DETECTION METHOD for FACE DETECTION . the proposed DETECTION METHOD is based on a LOCALLY ASSEMBLED BINARY HAAR FEATURE and a LOCALLY ASSEMBLED BINARY HAAR FEATURE . the proposed DETECTION METHOD is based on a LOCALLY ASSEMBLED BINARY HAAR FEATURE and a LOCALLY ASSEMBLED BINARY HAAR FEATURE . the proposed DETECTION METHOD is based on a LOCALLY ASSEMBLED BINARY HAAR FEATURE . the proposed DETECTION METHOD is compared with the conventional FEATURE-CENTRIC METHOD and the FEATURE-CENTRIC METHOD . the proposed DETECTION METHOD is compared with the conventional FEATURE-CENTRIC METHOD and the FEATURE-CENTRIC METHOD . the proposed DETECTION METHOD is compared with the conventional FEATURE-CENTRIC METHOD and the FEATURE-CENTRIC METHOD .\n",
            "\n",
            "879 1000\n",
            "computer vision and IMAGE RECOGNITION RESEARCH have a great interest in DIMENSIONALITY REDUCTION TECHNIQUES . generally these techniques are independent of the CLASSIFIER being used and the learning of the CLASSIFIER is carried out after the DIMENSIONALITY REDUCTION is performed , possibly discarding valuable information . in this paper we propose an ITERATIVE ALGORITHM that simultaneously learns a LINEAR PROJECTION BASE and a reduced set of prototypes optimized for the NEAREST-NEIGHBOR CLASSIFIER . the ITERATIVE ALGORITHM is derived by minimizing a suitable estimation of the CLASSIFICATION ERROR PROBABILITY . the proposed ITERATIVE ALGORITHM is assessed through a series of experiments showing a good behavior and a real potential for practical applications . \n",
            "this paper presents a new ITERATIVE ALGORITHM for IMAGE RECOGNITION RESEARCH . the proposed ITERATIVE ALGORITHM is based on the ITERATIVE ALGORITHM . the proposed ITERATIVE ALGORITHM is based on the ITERATIVE ALGORITHM . the proposed ITERATIVE ALGORITHM is based on the ITERATIVE ALGORITHM . the proposed ITERATIVE ALGORITHM is based on the ITERATIVE ALGORITHM . the proposed ITERATIVE ALGORITHM is evaluated on the IMAGE RECOGNITION RESEARCH . the results show that the proposed ITERATIVE ALGORITHM can achieve a significant improvement of the CLASSIFICATION ERROR PROBABILITY compared to the conventional ITERATIVE ALGORITHM .\n",
            "\n",
            "880 1000\n",
            "dimensional sentiment analysis aims to recognize CONTINUOUS NUMERICAL VALUES in multiple dimensions such as the VALENCE-AROUSAL SPACE . compared to the CATEGORICAL APPROACH that focuses on SENTIMENT CLASSIFICATION such as BINARY CLASSIFICATION -lrb- i.e. , positive and negative -rrb- , the DIMENSIONAL APPROACH can provide more FINE-GRAINED SENTIMENT ANALYSIS . this study proposes a REGIONAL CNN-LSTM MODEL consisting of two parts : DIMENSIONAL APPROACH and LSTM to predict the VA RATINGS OF TEXTS . unlike a conventional CNN which considers a whole text as input , the proposed DIMENSIONAL APPROACH uses an individual sentence as a region , dividing an input text into several regions such that the useful AFFECTIVE INFORMATION in each region can be extracted and weighted according to their contribution to the VA PREDICTION . such AFFECTIVE INFORMATION is sequentially integrated across regions using LSTM for VA PREDICTION . by combining the DIMENSIONAL APPROACH and LSTM , both LOCAL INFORMATION within sentences and LONG-DISTANCE DEPENDENCY across sentences can be considered in the PREDICTION PROCESS . experimental results show that the proposed REGIONAL CNN-LSTM MODEL outperforms <unk> , regression based , and NN-BASED METHODS proposed in previous studies . \n",
            "this paper presents a CATEGORICAL APPROACH for SENTIMENT CLASSIFICATION . the REGIONAL CNN-LSTM MODEL is based on a REGIONAL CNN-LSTM MODEL and a REGIONAL CNN-LSTM MODEL . the PREDICTION PROCESS is based on a REGIONAL CNN-LSTM MODEL and a REGIONAL CNN-LSTM MODEL . the PREDICTION PROCESS is based on a REGIONAL CNN-LSTM MODEL and a REGIONAL CNN-LSTM MODEL . the PREDICTION PROCESS is based on a REGIONAL CNN-LSTM MODEL and a REGIONAL CNN-LSTM MODEL . the PREDICTION PROCESS is based on a REGIONAL CNN-LSTM MODEL and a REGIONAL CNN-LSTM MODEL . the proposed CATEGORICAL APPROACH is compared with other NN-BASED METHODS and is shown to outperform the conventional NN-BASED METHODS .\n",
            "\n",
            "881 1000\n",
            "naively collecting translations by <unk> the task to NON-PROFESSIONAL TRANSLATORS yields <unk> , <unk> results if no quality control is exercised . we demonstrate a variety of mechanisms that increase the TRANSLATION QUALITY to near professional levels . specifically , we solicit REDUNDANT TRANSLATIONS and edits to them , and automatically select the best output among them . we propose a set of FEATURES that model both the translations and the translators , such as COUNTRY OF RESIDENCE , LM PERPLEXITY of the TRANSLATION , EDIT RATE from the other translations , and -LRB- OPTIONALLY -RRB- CALIBRATION against PROFESSIONAL TRANSLATORS . using these FEATURES to score the collected translations , we are able to discriminate between acceptable and unacceptable translations . we <unk> the nist 2009 <unk> evaluation set with MECHANICAL TURK , and quantitatively show that our models are able to select translations within the range of quality that we expect from PROFESSIONAL TRANSLATORS . the total cost is more than an order of magnitude lower than PROFESSIONAL TRANSLATION . \n",
            "this paper addresses the problem of PROFESSIONAL TRANSLATION in MECHANICAL TURK . we propose a method for -LRB- OPTIONALLY -RRB- CALIBRATION , which is based on the COUNTRY OF RESIDENCE and the COUNTRY OF RESIDENCE . the proposed method is based on the COUNTRY OF RESIDENCE and the COUNTRY OF RESIDENCE . the proposed method is based on the COUNTRY OF RESIDENCE and the COUNTRY OF RESIDENCE . experimental results show the effectiveness of the proposed method in terms of TRANSLATION QUALITY and TRANSLATION QUALITY .\n",
            "\n",
            "882 1000\n",
            "over-complete representations of images such as UNDECIMATED WA-VELETS have enjoyed immense popularity in recent years . though UNDECIMATED WA-VELETS are efficient for modeling singularities and EDGES , NATURAL IMAGES also consist of textures that are difficult to capture with any CANONICAL TRANSFORMATION . in this work , we develop a new MODELING STRATEGY with a rigorous treatment of TEXTURED REGIONS . using PRINCIPAL COMPONENTS ANALYSIS as an APPROXIMATE CLASSIFIER for EDGES and textures , we partition an IMAGE into COMPRESSIBLE AND INCOMPRESS-IBLE REGIONS -- with corresponding POSTERIOR MEDIAN-BASED DENOISING METHOD matching their behaviors . a POSTERIOR MEDIAN-BASED DENOISING METHOD using these POSTERIOR MEDIAN-BASED DENOISING METHOD is described with preliminary results that demonstrate the effectiveness of this POSTERIOR MEDIAN-BASED DENOISING METHOD . \n",
            "this paper presents a novel POSTERIOR MEDIAN-BASED DENOISING METHOD for NATURAL IMAGES . the POSTERIOR MEDIAN-BASED DENOISING METHOD is based on a CANONICAL TRANSFORMATION and a CANONICAL TRANSFORMATION . the MODELING STRATEGY is based on the CANONICAL TRANSFORMATION and the CANONICAL TRANSFORMATION of the IMAGE . the proposed POSTERIOR MEDIAN-BASED DENOISING METHOD is based on the POSTERIOR MEDIAN-BASED DENOISING METHOD . the proposed POSTERIOR MEDIAN-BASED DENOISING METHOD is based on the POSTERIOR MEDIAN-BASED DENOISING METHOD . the proposed POSTERIOR MEDIAN-BASED DENOISING METHOD is based on the POSTERIOR MEDIAN-BASED DENOISING METHOD . the proposed POSTERIOR MEDIAN-BASED DENOISING METHOD is based on the POSTERIOR MEDIAN-BASED DENOISING METHOD .\n",
            "\n",
            "883 1000\n",
            "we introduce a new BAYESIAN NONPARAMETRIC APPROACH to IDENTIFICATION OF SPARSE DYNAMIC LINEAR SYSTEMS . the IMPULSE RESPONSES are modeled as GAUSSIAN PROCESSES whose <unk> encode the BIBO STABILITY CONSTRAINT , as defined by the recently introduced '' STABLE SPLINE KERNEL '' . SPARSE SOLUTIONS are obtained by placing EXPONENTIAL HYPERPRIORS on the SCALE FACTORS of such kernels . numerical experiments regarding estimation of ARMAX MODELS show that this BAYESIAN NONPARAMETRIC APPROACH provides a definite advantage over a GROUP LAR ALGORITHM and state-of-the-art PARAMETRIC IDENTIFICATION TECHNIQUES based on PREDICTION ERROR MINIMIZATION . \n",
            "this paper proposes a new BAYESIAN NONPARAMETRIC APPROACH for IDENTIFICATION OF SPARSE DYNAMIC LINEAR SYSTEMS . the proposed BAYESIAN NONPARAMETRIC APPROACH is based on the STABLE SPLINE KERNEL and the GROUP LAR ALGORITHM . the proposed BAYESIAN NONPARAMETRIC APPROACH is based on the use of a STABLE SPLINE KERNEL and a STABLE SPLINE KERNEL . the proposed BAYESIAN NONPARAMETRIC APPROACH is compared with the conventional BAYESIAN NONPARAMETRIC APPROACH and the BAYESIAN NONPARAMETRIC APPROACH . the proposed BAYESIAN NONPARAMETRIC APPROACH is compared with the conventional BAYESIAN NONPARAMETRIC APPROACH and the GROUP LAR ALGORITHM .\n",
            "\n",
            "884 1000\n",
            "although SEMI-SUPERVISED LEARNING has been an active area of research , its use in DEPLOYED APPLICATIONS is still relatively rare because the methods are often difficult to implement , fragile in TUNING , or lacking in scalability . this paper presents <I> EXPECTATION REGULARIZATION </I> , a SEMI-SUPERVISED LEARNING METHOD for EXPONENTIAL FAMILY PARAMETRIC MODELS that augments the traditional CONDITIONAL LABEL-LIKELIHOOD OBJECTIVE FUNCTION with an additional term that encourages MODEL PREDICTIONS on UNLABELED DATA to match certain expectations -- such as LABEL PRIORS . the SEMI-SUPERVISED LEARNING METHOD is extremely easy to implement , SCALES as well as LOGISTIC REGRESSION , and can handle NON-INDEPENDENT FEATURES . we present experiments on five different DATA SETS , showing accuracy improvements over other SEMI-SUPERVISED METHODS . \n",
            "this paper proposes a SEMI-SUPERVISED LEARNING METHOD for SEMI-SUPERVISED LEARNING . the proposed SEMI-SUPERVISED LEARNING METHOD is based on the <I> EXPECTATION REGULARIZATION </I> . the proposed SEMI-SUPERVISED LEARNING METHOD is based on the <I> EXPECTATION REGULARIZATION </I> . the proposed SEMI-SUPERVISED LEARNING METHOD is based on the <I> EXPECTATION REGULARIZATION </I> . the proposed SEMI-SUPERVISED LEARNING METHOD is based on the <I> EXPECTATION REGULARIZATION </I> . the proposed SEMI-SUPERVISED LEARNING METHOD is based on the <I> EXPECTATION REGULARIZATION </I> . the proposed SEMI-SUPERVISED LEARNING METHOD is based on the <I> EXPECTATION REGULARIZATION </I> . the proposed SEMI-SUPERVISED LEARNING METHOD is based on the <I> EXPECTATION REGULARIZATION </I> . the proposed SEMI-SUPERVISED LEARNING METHOD is based on the <I> EXPECTATION REGULARIZATION </I> and the SEMI-SUPERVISED LEARNING METHOD .\n",
            "\n",
            "885 1000\n",
            "a novel PARTICLE FILTER , the MEMORY-BASED PARTICLE FILTER , is proposed that can visually track moving objects that have COMPLEX DYNAMICS . we aim to realize robust-ness against ABRUPT OBJECT MOVEMENTS and quick recovery from TRACKING FAILURE caused by factors such as OCCLUSIONS . to that end , we eliminate the MARKOV ASSUMPTION from the previous PARTICLE FILTERING FRAMEWORK and predict the PRIOR DISTRIBUTION of the target state from the LONG-TERM DYNAMICS . more concretely , MEMORY-BASED PARTICLE FILTER stores the past history of the estimated target states , and employs a RANDOM SAMPLING from the history to generate PRIOR DISTRIBUTION ; MEMORY-BASED PARTICLE FILTER represents a novel PF FORMULATION.OUR METHOD can handle NONLINEAR , TIME-VARIANT , and <unk> dynamics , which is not possible within existing PF FRAMEWORKS . accurate PRIOR PREDICTION based on PROPER DYNAMICS MODEL is especially effective for recovering lost tracks , because MEMORY-BASED PARTICLE FILTER can provide possible target states , which can drastically change since the track was lost . we target the face pose of <unk> humans in this paper . quantitative evaluations with MAGNETIC SENSORS confirm improved ACCURACY in FACE POSE ESTIMATION and successful recovery from TRACKING LOSS . the proposed MEMORY-BASED PARTICLE FILTER suggests a new paradigm for MODELING SYSTEMS with COMPLEX DYNAMICS and so offers a various VISUAL TRACKING APPLICATIONS . \n",
            "this paper addresses the problem of FACE POSE ESTIMATION in MAGNETIC SENSORS . in this paper , we propose a PARTICLE FILTERING FRAMEWORK for FACE POSE ESTIMATION . the PROPER DYNAMICS MODEL is based on a MEMORY-BASED PARTICLE FILTER , which is based on the PROPER DYNAMICS MODEL . the PROPER DYNAMICS MODEL is based on the PROPER DYNAMICS MODEL , which is based on the PROPER DYNAMICS MODEL . the proposed PF FORMULATION.OUR METHOD is based on the PROPER DYNAMICS MODEL , which is based on the PROPER DYNAMICS MODEL . the proposed PF FORMULATION.OUR METHOD is based on the PROPER DYNAMICS MODEL . the proposed PF FORMULATION.OUR METHOD is based on a PARTICLE FILTERING FRAMEWORK and is shown to be more robust to OCCLUSIONS than the conventional PARTICLE FILTER .\n",
            "\n",
            "886 1000\n",
            "we present efficient algorithms for dealing with the problem of MISSING INPUTS -LRB- INCOMPLETE FEATURE VECTORS -rrb- during TRAINING and RECALL . our approach is based on the approximation of the INPUT DATA DISTRIBUTION using PARZEN WINDOWS . for RECALL , we obtain CLOSED FORM SOLUTIONS for ARBITRARY FEEDFORWARD NETWORKS . for TRAINING , we show how the BACKPROPAGATION STEP for an INCOMPLETE FEATURE VECTORS can be approximated by a WEIGHTED AVERAGED BACKPROPAGATION STEP . the COMPLEXITY of the CLOSED FORM SOLUTIONS for TRAINING and RECALL is independent of the number of MISSING FEATURES . we verify our theoretical results using one CLASSIFICATION and one REGRESSION PROBLEM . \n",
            "this paper presents a new method for CLASSIFICATION based on a WEIGHTED AVERAGED BACKPROPAGATION STEP . the proposed method is based on the use of a WEIGHTED AVERAGED BACKPROPAGATION STEP and a WEIGHTED AVERAGED BACKPROPAGATION STEP for CLASSIFICATION . the proposed method is based on a WEIGHTED AVERAGED BACKPROPAGATION STEP , which is based on the INPUT DATA DISTRIBUTION . the proposed method is based on the use of a WEIGHTED AVERAGED BACKPROPAGATION STEP and a WEIGHTED AVERAGED BACKPROPAGATION STEP . the COMPLEXITY of the proposed method is compared with the conventional TRAINING .\n",
            "\n",
            "887 1000\n",
            "the EXTRACTION OF STATISTICALLY INDEPENDENT COMPONENTS from HIGH-DIMENSIONAL MULTI-SENSORY INPUT STREAMS is assumed to be an essential component of SENSORY PROCESSING in the brain . such INDEPENDENT COMPONENT ANALYSIS -lrb- or BLIND SOURCE SEPARATION -RRB- could provide a less redundant representation of information about the external world . another powerful PROCESSING STRATEGY is to extract preferentially those components from HIGH-DIMENSIONAL INPUT STREAMS that are related to other INFORMATION SOURCES , such as INTERNAL PREDICTIONS or PROPRIOCEPTIVE FEEDBACK . this PROCESSING STRATEGY allows the optimization of INTERNAL REPRESENTATION according to the INFORMATION BOTTLENECK METHOD . however , CONCRETE LEARNING RULES that implement these general UNSUPERVISED LEARNING PRINCIPLES for spiking neurons are still missing . we show how both INFORMATION BOTTLENECK OPTIMIZATION and the EXTRACTION OF INDEPENDENT COMPONENTS can in principle be implemented with stochastically spiking neurons with <unk> . the new CONCRETE LEARNING RULES that achieves this is derived from ABSTRACT INFORMATION OPTIMIZATION PRINCIPLES . \n",
            "this paper addresses the problem of INFORMATION BOTTLENECK OPTIMIZATION for HIGH-DIMENSIONAL MULTI-SENSORY INPUT STREAMS . we propose a PROCESSING STRATEGY based on INFORMATION BOTTLENECK OPTIMIZATION , which is based on the INFORMATION BOTTLENECK METHOD . the proposed INFORMATION BOTTLENECK METHOD is based on the INFORMATION BOTTLENECK METHOD and the INFORMATION BOTTLENECK METHOD . the proposed method is based on the INFORMATION BOTTLENECK METHOD , which is based on the INFORMATION BOTTLENECK METHOD and the INFORMATION BOTTLENECK METHOD . experimental results show that the proposed method is robust and robust to INTERNAL PREDICTIONS such as PROPRIOCEPTIVE FEEDBACK , PROPRIOCEPTIVE FEEDBACK , and PROPRIOCEPTIVE FEEDBACK .\n",
            "\n",
            "888 1000\n",
            "this opinion paper discusses SUBJECTIVE NATURAL LANGUAGE PROBLEMS in terms of their motivations , applications , CHARACTERIZATIONS , and implications . it argues that such SUBJECTIVE NATURAL LANGUAGE PROBLEMS <unk> increased attention because of their potential to challenge the status of theoretical understanding , PROBLEM-SOLVING METHODS , and evaluation techniques in COMPUTATIONAL LINGUISTICS . the author supports a more HOLIS-TIC APPROACH to such SUBJECTIVE NATURAL LANGUAGE PROBLEMS ; a view that extends beyond OPINION MINING or SENTIMENT ANALYSIS . \n",
            "this paper presents a novel HOLIS-TIC APPROACH for OPINION MINING . the HOLIS-TIC APPROACH is based on the HOLIS-TIC APPROACH and the HOLIS-TIC APPROACH . the proposed HOLIS-TIC APPROACH is based on the HOLIS-TIC APPROACH and the HOLIS-TIC APPROACH . the proposed HOLIS-TIC APPROACH is based on the HOLIS-TIC APPROACH and the HOLIS-TIC APPROACH . experimental results are presented to demonstrate the effectiveness of the proposed HOLIS-TIC APPROACH .\n",
            "\n",
            "889 1000\n",
            "we introduce a new type of PHASE-BASED LOCAL FEATURE based on the phase and amplitude responses of COMPLEX-VALUED STEERABLE FILTERS . the design of this PHASE-BASED LOCAL FEATURE is motivated by a desire to obtain FEATURE VECTORS which are <unk> under COMMON IMAGE DEFORMATIONS , yet distinctive enough to provide useful IDENTITY INFORMATION . a recent proposal for such PHASE-BASED LOCAL FEATURE involves combining DIFFERENTIAL INVARIANTS to particular IMAGE DEFORMATIONS , such as ROTATION . our approach differs in that we consider a wider class of IMAGE DEFORMATIONS , including the addition of NOISE , along with both GLOBAL AND LOCAL BRIGHTNESS VARIATIONS . we use STEERABLE FILTERS to make the FEATURE robust to ROTATION . and we exploit the fact that PHASE DATA is often locally stable with respect to SCALE CHANGES , NOISE , and COMMON BRIGHTNESS CHANGES . we provide empirical results comparing our PHASE-BASED LOCAL FEATURE with one based on DIFFERENTIAL INVARIANTS . the results show that our PHASE-BASED LOCAL FEATURE leads to better performance when dealing with COMMON ILLUMINATION CHANGES and 2-D ROTATION , while giving comparable effects in terms of SCALE CHANGES . \n",
            "this paper addresses the problem of COMMON IMAGE DEFORMATIONS in COMMON IMAGE DEFORMATIONS . we propose a method for estimating the IDENTITY INFORMATION from PHASE DATA using COMPLEX-VALUED STEERABLE FILTERS . the proposed method is based on a PHASE-BASED LOCAL FEATURE , which is based on the IDENTITY INFORMATION and the IDENTITY INFORMATION . the proposed method is based on a PHASE-BASED LOCAL FEATURE , which is based on the IDENTITY INFORMATION and the IDENTITY INFORMATION . the proposed method is based on the IDENTITY INFORMATION and the IDENTITY INFORMATION . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "890 1000\n",
            "word embedding WORD EMBEDDING MODELS learn VECTORIAL WORD REPRESENTATIONS that can be used in a variety of NLP APPLICATIONS . when TRAINING DATA is scarce , these WORD EMBEDDING MODELS risk losing their GENERALIZATION ABILITIES due to the COMPLEXITY of the WORD EMBEDDING MODELS and the OVERFITTING to FINITE DATA . we propose a REGULARIZED EMBEDDING FORMULATION , called ROBUST GRAM , which penalizes OVERFITTING by suppressing the disparity between target and CONTEXT EMBEDDINGS . our experimental analysis shows that the ROBUST GRAM trained on SMALL DATASETS generalizes better compared to alternatives , is more robust to variations in the training set , and correlates well to HUMAN SIMILARITIES in a set of WORD SIMILARITY TASKS . \n",
            "this paper addresses the problem of ROBUST GRAM for NLP APPLICATIONS . we propose a REGULARIZED EMBEDDING FORMULATION , called CONTEXT EMBEDDINGS , for VECTORIAL WORD REPRESENTATIONS . the REGULARIZED EMBEDDING FORMULATION is based on a REGULARIZED EMBEDDING FORMULATION , which is based on the REGULARIZED EMBEDDING FORMULATION . the proposed REGULARIZED EMBEDDING FORMULATION is based on a REGULARIZED EMBEDDING FORMULATION , which is based on the REGULARIZED EMBEDDING FORMULATION . the proposed REGULARIZED EMBEDDING FORMULATION is based on a REGULARIZED EMBEDDING FORMULATION , which is a REGULARIZED EMBEDDING FORMULATION . the proposed REGULARIZED EMBEDDING FORMULATION is based on a REGULARIZED EMBEDDING FORMULATION , which is based on the REGULARIZED EMBEDDING FORMULATION . the proposed REGULARIZED EMBEDDING FORMULATION is shown to outperform the state-of-the-art WORD EMBEDDING MODELS in terms of COMPLEXITY and COMPLEXITY .\n",
            "\n",
            "891 1000\n",
            "in order to overcome several limitations of STRUCTURED LIGHT 3D ACQUISITION METHODS , the COLORS , INTENSITIES , and shapes of the projected patterns are adapted to the scene . based on a crude estimate of the SCENE GEOMETRY and reflectance characteristics , the LOCAL INTENSITY RANGES in the projected patterns are adapted , in order to avoid OVER-AND UNDER-EXPOSURE in the image . this avoids the INFAMOUS SPECULARITY PROBLEMS and generally increases ACCURACY . the estimated geometry also helps to limit the effect of ALIASING caused by the SAMPLING OF FORESHORTENED PATTERNS . furthermore , the approach also <unk> for the adverse effects that small motions during SCANNING would normally have . moreover , the approach yields a CONFIDENCE MEASURE at every pixel of the range image . last but not least , the scanner consists of consumer products only , and therefore is cheap . \n",
            "this paper presents a method for INFAMOUS SPECULARITY PROBLEMS in INTENSITIES . the method is based on the SAMPLING OF FORESHORTENED PATTERNS and the SAMPLING OF FORESHORTENED PATTERNS . the proposed method is based on the SAMPLING OF FORESHORTENED PATTERNS and the SAMPLING OF FORESHORTENED PATTERNS . the proposed method is based on the SAMPLING OF FORESHORTENED PATTERNS and the SAMPLING OF FORESHORTENED PATTERNS . the proposed method is based on the CONFIDENCE MEASURE and the ACCURACY of the STRUCTURED LIGHT 3D ACQUISITION METHODS .\n",
            "\n",
            "892 1000\n",
            "in this paper we demonstrate that SPECTRAL CONVERSION can be successfully applied to the SPEECH ENHANCEMENT PROBLEM as a FEATURE DENOISING METHOD . the enhanced SPECTRAL CONVERSION can be used in the context of the KALMAN FILTER for estimating the CLEAN SPEECH SIGNAL . in essence , instead of estimating the CLEAN SPEECH FEATURES and the CLEAN SPEECH SIGNAL using the ITERATIVE KALMAN FILTER , we show that is more efficient to initially estimate the CLEAN SPEECH FEATURES from the NOISY SPEECH FEATURES using SPECTRAL CONVERSION -lrb- using a training speech corpus -rrb- and then apply the standard KALMAN FILTER . our results show an average improvement compared to the ITERATIVE KALMAN FILTER that can reach 6 db in the AVERAGE SEGMENTAL OUTPUT SIGNAL-TO-NOISE RATIO , in low input snr 's . \n",
            "this paper presents a new FEATURE DENOISING METHOD for SPEECH ENHANCEMENT PROBLEM . the FEATURE DENOISING METHOD is based on the KALMAN FILTER and the KALMAN FILTER . the FEATURE DENOISING METHOD is based on the KALMAN FILTER and the KALMAN FILTER . the FEATURE DENOISING METHOD is applied to the SPEECH ENHANCEMENT PROBLEM . the experimental results show that the proposed FEATURE DENOISING METHOD outperforms the conventional FEATURE DENOISING METHOD in terms of AVERAGE SEGMENTAL OUTPUT SIGNAL-TO-NOISE RATIO and AVERAGE SEGMENTAL OUTPUT SIGNAL-TO-NOISE RATIO .\n",
            "\n",
            "893 1000\n",
            "a PHASE SYNCHRONIZATION METHOD , which provides NON-UNIFORM FREQUENCY OFFSET COMPENSATION needed for wideband ofdm -lsb- 1 -rsb- , is coupled with LOW-COMPLEXITY CHANNEL ESTIMATION in the TIME DOMAIN . <unk> of the CHANNEL IMPULSE RESPONSE leads to an improved performance , while ADAPTIVE SYNCHRONIZATION supports DECISION-DIRECTED OPERATION and yields low overhead . system performance is demonstrated using experimental data transmitted over a 1 km shallow water channel in the 19 <unk> khz band . \n",
            "this paper presents a new method for LOW-COMPLEXITY CHANNEL ESTIMATION . the proposed method is based on a PHASE SYNCHRONIZATION METHOD . the proposed method is based on the PHASE SYNCHRONIZATION METHOD . the proposed method is based on the PHASE SYNCHRONIZATION METHOD . the proposed method is based on the PHASE SYNCHRONIZATION METHOD . the proposed method is based on the PHASE SYNCHRONIZATION METHOD .\n",
            "\n",
            "894 1000\n",
            "this paper discusses our THREE-STAGE APPROACH to a FLEXIBLE VOCABULARY SPEECH UNDERSTANDING SYSTEM , which can detect OUT-OF-VOCABULARY WORDS , and hypothesize their PHONETIC AND OR-THOGRAPHIC TRANSCRIPTIONS . in the first stage , we introduce the COLUMN-BIGRAM FINITE-STATE TRANSDUCER which , while embedding ANGIE SUBLEXICAL MODELS , also supports previously UNSEEN DATA from UNKNOWN WORDS . secondly , the ANGIE SUBLEXICAL MODELS utilize GRAPHEME INFORMATION , providing TIGHTER LINGUISTIC CONSTRAINT as well as INSTANTANEOUS SOUND-TO-LETTER CAPABILITY during RECOGNITION . thirdly , the SYLLABLE-LEVEL LEXICAL UNITS of the first stage are automatically derived via an ITERATIVE PROCEDURE to optimize performance . the THREE-STAGE APPROACH employs ANGIE SUBLEXICAL MODELS to output a WORD NETWORK which is parsed by TINA , our NATURAL LANGUAGE PROCESSOR , in stage three . experiments with a JUPITER IMPLEMENTATION of this THREE-STAGE APPROACH are described in -lsb- 1 -rsb- . \n",
            "this paper presents a THREE-STAGE APPROACH for RECOGNITION . the THREE-STAGE APPROACH is based on the COLUMN-BIGRAM FINITE-STATE TRANSDUCER of the COLUMN-BIGRAM FINITE-STATE TRANSDUCER . the proposed THREE-STAGE APPROACH is based on the THREE-STAGE APPROACH . the proposed THREE-STAGE APPROACH is based on the COLUMN-BIGRAM FINITE-STATE TRANSDUCER . the proposed THREE-STAGE APPROACH is based on the THREE-STAGE APPROACH . the proposed THREE-STAGE APPROACH is based on the COLUMN-BIGRAM FINITE-STATE TRANSDUCER . the proposed THREE-STAGE APPROACH is based on the THREE-STAGE APPROACH . the proposed THREE-STAGE APPROACH is based on a FLEXIBLE VOCABULARY SPEECH UNDERSTANDING SYSTEM . the proposed THREE-STAGE APPROACH is based on a FLEXIBLE VOCABULARY SPEECH UNDERSTANDING SYSTEM . the proposed THREE-STAGE APPROACH is applied to a FLEXIBLE VOCABULARY SPEECH UNDERSTANDING SYSTEM . the proposed THREE-STAGE APPROACH is evaluated on a FLEXIBLE VOCABULARY SPEECH UNDERSTANDING SYSTEM .\n",
            "\n",
            "895 1000\n",
            "we consider the DATA STREAM of reconstructing a DATA STREAM from a small subset of its entries , where the DATA STREAM is assumed to lie in a LOW-DIMENSIONAL LINEAR SUBSPACE , possibly corrupted by noise . it is also important to track the change of underlying SUBSPACE for many applications . this DATA STREAM can be viewed as a SEQUENTIAL LOW-RANK MATRIX COMPLETION PROBLEM in which the SUBSPACE is learned in an ONLINE FASHION . the proposed algorithm , called PARALLEL ESTIMATION AND TRACKING by RECURSIVE LEAST SQUARES , identifies the underlying LOW-DIMENSIONAL SUBSPACE via a RECURSIVE PROCEDURE for each row of the SUBSPACE MATRIX in parallel , and then reconstructs the missing entries via LEAST-SQUARES ESTIMATION if required . RECURSIVE LEAST SQUARES outperforms previous approaches by discounting observations in order to capture LONG-TERM BEHAVIOR OF THE DATA STREAM and be able to adapt to RECURSIVE LEAST SQUARES . NUMERICAL EXAMPLES are provided for DIRECTION-OF-ARRIVAL ESTIMATION and MATRIX COMPLETION , comparing RECURSIVE LEAST SQUARES with STATE OF THE ART BATCH ALGORITHMS . \n",
            "this paper presents a new method for PARALLEL ESTIMATION AND TRACKING based on RECURSIVE LEAST SQUARES . the proposed method is based on a SEQUENTIAL LOW-RANK MATRIX COMPLETION PROBLEM , which is based on the RECURSIVE PROCEDURE . the proposed method is based on the RECURSIVE PROCEDURE . the proposed method is based on the RECURSIVE PROCEDURE . the proposed method is based on the RECURSIVE PROCEDURE . the proposed method is based on the RECURSIVE PROCEDURE . the proposed method is based on the RECURSIVE PROCEDURE . the proposed method is based on the RECURSIVE PROCEDURE . the proposed method is based on the RECURSIVE PROCEDURE . the proposed method is based on a SEQUENTIAL LOW-RANK MATRIX COMPLETION PROBLEM . the proposed method is based on a RECURSIVE PROCEDURE . the proposed method is based on a SEQUENTIAL LOW-RANK MATRIX COMPLETION PROBLEM . the proposed method is based on a SEQUENTIAL LOW-RANK MATRIX COMPLETION PROBLEM . the proposed method is based on a SEQUENTIAL LOW-RANK MATRIX COMPLETION PROBLEM .\n",
            "\n",
            "896 1000\n",
            "we propose a transcription on GRAPHS of recent CONTINUOUS GLOBAL ACTIVE CONTOURS proposed for IMAGE SEGMENTATION to address the problem of BINARY PARTITIONING OF DATA represented by GRAPHS . to do so , using the framework of PARTIAL DIFFERENCE EQUATIONS , we propose a family of NONLOCAL REGULAR-IZATION FUNCTIONALS that verify the CO-AREA FORMULA on GRAPHS . the GRADIENTS of a SUB-GRAPH are introduced and their properties studied . relations , for the case of a SUB-GRAPH , between the introduced NONLOCAL REGULARIZATION FUNCTIONALS and NONLO-CAL DISCRETE PERIMETERS are exhibited and the CO-AREA FORMULA on GRAPHS is introduced . finally , NONLOCAL GLOBAL MINIMIZ-ERS can be considered on GRAPHS with the associated energies . experiments show the benefits of the approach for NONLOCAL IMAGE SEGMENTATION and HIGH DIMENSIONAL DATA CLUSTERING . \n",
            "this paper presents a new method for NONLOCAL IMAGE SEGMENTATION based on NONLOCAL REGULARIZATION FUNCTIONALS . the proposed method is based on a CO-AREA FORMULA , which is based on PARTIAL DIFFERENCE EQUATIONS and NONLOCAL REGULARIZATION FUNCTIONALS . the proposed method is based on the PARTIAL DIFFERENCE EQUATIONS and the PARTIAL DIFFERENCE EQUATIONS . the proposed method is based on NONLOCAL REGULARIZATION FUNCTIONALS and NONLOCAL REGULARIZATION FUNCTIONALS . the proposed method is based on NONLOCAL REGULARIZATION FUNCTIONALS and NONLOCAL REGULARIZATION FUNCTIONALS . the proposed method is based on the use of NONLOCAL REGULARIZATION FUNCTIONALS and NONLOCAL REGULARIZATION FUNCTIONALS . the proposed method is compared with other state-of-the-art methods .\n",
            "\n",
            "897 1000\n",
            "content-based recommender systems use PREFERENCE RATINGS and FEATURES that characterize MEDIA to model users ' interests or information needs for making future recommendations . while previously developed in the MUSIC AND TEXT DOMAINS , we present an initial exploration of CONTENT-BASED RECOMMENDATION for SPOKEN DOCUMENTS using a CORPUS OF PUBLIC DOMAIN INTERNET AUDIO . unlike familiar speech technologies of topic identification and SPOKEN DOCUMENT RETRIEVAL , our recommendation task requires a more comprehensive notion of DOCUMENT RELEVANCE than BAGS-OF-WORDS would supply . inspired by MUSIC RECOMMENDER SYSTEMS , we automatically extract a wide variety of CONTENT-BASED FEATURES to characterize NON-LINGUISTIC ASPECTS of the audio such as SPEAKER , language , GENDER , and environment . to combine these HETEROGENEOUS INFORMATION SOURCES into a single RELEVANCE JUDGEMENT , we evaluate FEATURE , score , and HYBRID FUSION TECHNIQUES . our study provides an essential first exploration of the task and clearly demonstrates the value of a MULTISOURCE APPROACH over a BAG-OF-WORDS BASELINE . \n",
            "this paper addresses the problem of SPOKEN DOCUMENT RETRIEVAL in MUSIC AND TEXT DOMAINS . we propose a MULTISOURCE APPROACH based on a CORPUS OF PUBLIC DOMAIN INTERNET AUDIO and a FEATURE . the proposed MULTISOURCE APPROACH is based on a CORPUS OF PUBLIC DOMAIN INTERNET AUDIO , which is based on the MULTISOURCE APPROACH . the proposed MULTISOURCE APPROACH is based on HYBRID FUSION TECHNIQUES and HYBRID FUSION TECHNIQUES . the proposed MULTISOURCE APPROACH is based on a CORPUS OF PUBLIC DOMAIN INTERNET AUDIO , which is based on HYBRID FUSION TECHNIQUES and HYBRID FUSION TECHNIQUES . experimental results show that the proposed MULTISOURCE APPROACH outperforms the conventional MULTISOURCE APPROACH and the BAG-OF-WORDS BASELINE .\n",
            "\n",
            "898 1000\n",
            "we describe a new system for labeling speech corpora with HIGH-LEVEL GROUP INTERACTION TAGS , called '' meeting acts . '' the system was motivated by a need to assess work seeking to automatically detect MEETING STYLE using DIALOG ACT INFORMATION . we present information about the relationships seen between DIALOG ACT SEQUENCES and MEETING STYLE to motivate the LABELING PROCESS . we provide a summary of the ANNOTATION SYSTEM and labeling procedure , as well as preliminary <unk> reliability statistics on the ICSI MEETING RECORDER CORPUS . \n",
            "this paper presents a new method for DIALOG ACT SEQUENCES . the method is based on a ICSI MEETING RECORDER CORPUS and a LABELING PROCESS . the method is based on the DIALOG ACT INFORMATION and the LABELING PROCESS . the proposed method is based on the ICSI MEETING RECORDER CORPUS and the LABELING PROCESS . the proposed method is tested on a ICSI MEETING RECORDER CORPUS and a ICSI MEETING RECORDER CORPUS .\n",
            "\n",
            "899 1000\n",
            "we define a new DIALOGUE MANAGEMENT STRATEGY LIMITED ENQUIRY NEGOTIATION DIALOGUES designed for enabling simple MAN-MACHINE DIALOGUES in which the parameters -lrb- for which the user will supply values -rrb- of a query to a database are <unk> . the choice of which query to make next is also not <unk> . the DIALOGUE MANAGEMENT STRATEGY LIMITED ENQUIRY NEGOTIATION DIALOGUES is simple and intuitive but permits interestingly complex DIALOGUE BEHAVIOUR . we propose DIALOGUE MANAGEMENT STRATEGY LIMITED ENQUIRY NEGOTIATION DIALOGUES as an addition to a DIALOGUE DESIGNER 'S STANDARD COMPONENTS TOOLBOX along with other well-known ideas such as MENU-TRAVERSAL AND SLOT-FILLING . we illustrate the DIALOGUE MANAGEMENT STRATEGY LIMITED ENQUIRY NEGOTIATION DIALOGUES by examining how DIALOGUE MANAGEMENT STRATEGY LIMITED ENQUIRY NEGOTIATION DIALOGUES accounts for interesting but by no means rare data in a <unk> of oz corpus of business trip planning dialogues . finally , we discuss some more theoretical issues arising from the DIALOGUE MANAGEMENT STRATEGY LIMITED ENQUIRY NEGOTIATION DIALOGUES . \n",
            "this paper presents a new method for DIALOGUE MANAGEMENT STRATEGY LIMITED ENQUIRY NEGOTIATION DIALOGUES based on the DIALOGUE DESIGNER 'S STANDARD COMPONENTS TOOLBOX . the proposed method is based on the DIALOGUE DESIGNER 'S STANDARD COMPONENTS TOOLBOX . the proposed method is based on the DIALOGUE DESIGNER 'S STANDARD COMPONENTS TOOLBOX . the proposed method is based on the DIALOGUE DESIGNER 'S STANDARD COMPONENTS TOOLBOX . the experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "900 1000\n",
            "the recently introduced M-VECTOR APPROACH uses MAXIMUM LIKELIHOOD LINEAR REGRESSION SUPER-VECTORS for SPEAKER VERIFICATION , where MAXIMUM LIKELIHOOD LINEAR REGRESSION SUPER-VECTORS are estimated with respect to a UNIVERSAL BACKGROUND MODEL without any TRANSCRIPTION OF SPEECH SEGMENTS and speaker <unk> are obtained by UNIFORM SEGMENTATION of their MAXIMUM LIKELIHOOD LINEAR REGRESSION SUPER-VECTORS . hence , this M-VECTOR APPROACH does not exploit the PHONETIC CONTENT of the speech segments . in this paper , we propose the integration of an automatic speech recognition -lrb- asr -rrb- based multi-class mllr transformation into the M-VECTOR APPROACH . we consider two variants , with MAXIMUM LIKELIHOOD LINEAR REGRESSION SUPER-VECTORS computed either on the 1-BEST -LRB- HYPOTHESIS -rrb- or on the LATTICE WORD TRANSCRIPTIONS . the former case is able to account for the risk of ASR TRANSCRIPTION ERRORS . we show that the proposed M-VECTOR APPROACH outperform the conventional M-VECTOR APPROACH over various tasks of the NIST SRE 2008 CORE CONDITION . \n",
            "this paper proposes a new M-VECTOR APPROACH for SPEAKER VERIFICATION . the M-VECTOR APPROACH is based on the UNIVERSAL BACKGROUND MODEL . the proposed M-VECTOR APPROACH is based on the UNIVERSAL BACKGROUND MODEL . the proposed M-VECTOR APPROACH is based on the UNIVERSAL BACKGROUND MODEL . the proposed M-VECTOR APPROACH is compared with the conventional M-VECTOR APPROACH and the M-VECTOR APPROACH . the proposed M-VECTOR APPROACH is compared with the conventional M-VECTOR APPROACH and the M-VECTOR APPROACH .\n",
            "\n",
            "901 1000\n",
            "in some SPEAKER RECOGNITION SCENARIOS we find conversations recorded simultaneously over multiple channels . that is the case of the interviews in the NIST SRE DATASET . to take advantage of that , we propose a modification of the PLDA MODEL that considers two different INTER-SESSION VARIABILITY TERMS . the first term is tied between all the recordings belonging to the same conversation whereas the second is not . thus , the former mainly intends to capture the variability due to the PHONETIC CONTENT of the conversation while the latter tries to capture the CHANNEL VARIABILITY . we test this PLDA MODEL on the NIST SRE12 CORE CONDITION using multiple channels per interview to <unk> the speakers . the proposed PLDA MODEL improves the MINIMUM DCF by 26 -- 29 % on TELEPHONE SPEECH and by 1 -- 8 % on interviews compared to the standard PLDA -lrb- scored by the book -rrb- . \n",
            "this paper presents a PLDA MODEL for TELEPHONE SPEECH . the proposed PLDA MODEL is based on the MINIMUM DCF of the PLDA MODEL . the proposed PLDA MODEL is evaluated on the NIST SRE DATASET and on the NIST SRE DATASET . the proposed PLDA MODEL is evaluated on the NIST SRE DATASET and on the NIST SRE DATASET . the proposed PLDA MODEL achieves a higher NIST SRE12 CORE CONDITION than the conventional PLDA MODEL .\n",
            "\n",
            "902 1000\n",
            "in this paper , we present a method that combines the merits of BAYESIAN NONPARAMETRICS , specifically STICK-BREAKING PRIORS , and LARGE-MARGIN KERNEL MACHINES in the context of SEQUENTIAL DATA CLASSIFICATION . the proposed model employs a set of -lrb- theoretically -rrb- infinite interdependent LARGE-MARGIN CLASSIFIERS as MODEL COMPONENTS , that robustly capture LOCAL NONLINEARITY OF COMPLEX DATA . the employed LARGE-MARGIN CLASSIFIERS are connected in the context of a MARKOV-SWITCHING CONSTRUCTION that allows for capturing COMPLEX TEMPORAL DYNAMICS in the MODELED DATASETS . appropriate STICK-BREAKING PRIORS are imposed over the COMPONENT SWITCHING MECHANISM of our model to allow for data-driven determination of the optimal number of COMPONENT LARGE-MARGIN CLASSIFIERS , under a standard NONPARAMETRIC BAYESIAN INFERENCE SCHEME . EFFICIENT MODEL TRAINING is performed under the MAXIMUM ENTROPY DISCRIMINATION FRAMEWORK , which integrates the LARGE-MARGIN PRINCIPLE with BAYESIAN POSTERIOR INFERENCE . we evaluate our method using several REAL-WORLD DATASETS , and compare it to state-of-the-art alternatives . \n",
            "this paper addresses the problem of BAYESIAN POSTERIOR INFERENCE for SEQUENTIAL DATA CLASSIFICATION . in this paper , we propose a NONPARAMETRIC BAYESIAN INFERENCE SCHEME based on the COMPONENT SWITCHING MECHANISM . the proposed NONPARAMETRIC BAYESIAN INFERENCE SCHEME is based on the COMPONENT SWITCHING MECHANISM and the COMPONENT SWITCHING MECHANISM . the proposed NONPARAMETRIC BAYESIAN INFERENCE SCHEME is based on the COMPONENT SWITCHING MECHANISM and the COMPONENT SWITCHING MECHANISM . the proposed NONPARAMETRIC BAYESIAN INFERENCE SCHEME is based on the LARGE-MARGIN PRINCIPLE and the COMPONENT SWITCHING MECHANISM . the proposed NONPARAMETRIC BAYESIAN INFERENCE SCHEME is based on the COMPONENT SWITCHING MECHANISM and the COMPONENT SWITCHING MECHANISM . the proposed NONPARAMETRIC BAYESIAN INFERENCE SCHEME is based on a NONPARAMETRIC BAYESIAN INFERENCE SCHEME and a NONPARAMETRIC BAYESIAN INFERENCE SCHEME .\n",
            "\n",
            "903 1000\n",
            "suppose rate of change of <unk> of a LINEAR TIME-VARIANT SYSTEM modeled via a DIERENCE EQUATION is restricted . the work presented herein is an attempt at developing an algorithm that determines regions in <unk> where such a LINEAR TIME-VARIANT SYSTEM is guaranteed to be globally asymptotically stable . such information can be extremely useful in many applications . some previously published related results are <unk> as well . \n",
            "this paper presents a new method for the DIERENCE EQUATION . the proposed method is based on the use of a LINEAR TIME-VARIANT SYSTEM and a LINEAR TIME-VARIANT SYSTEM . the proposed method is based on a LINEAR TIME-VARIANT SYSTEM .\n",
            "\n",
            "904 1000\n",
            "beat tracking estimation from MUSIC SIGNALS becomes difficult in the presence of HIGHLY PREDOMINANT VOCALS . we compare the performance of five state-of-the-art algorithms on two datasets , a GENERIC ANNOTATED COLLECTION and a dataset comprised of SONG EXCERPTS with HIGHLY PREDOMINANT VOCALS . then , we use seven state-of-the-art AUDIO VOICE SUPPRESSION TECHNIQUES and a simple LOW PASS FILTER to improve BEAT TRACKING ESTIMATIONS in the later case . finally , we evaluate all the PAIRWISE COMBINATIONS between BEAT TRACKING and VOICE SUPPRESSION METHODS . we confirm our hypothesis that VOICE SUPPRESSION improves the mean performance of BEAT TRACKERS for the predominant vocal collection . \n",
            "this paper addresses the problem of BEAT TRACKING ESTIMATION in MUSIC SIGNALS . we propose a method for BEAT TRACKING ESTIMATION based on PAIRWISE COMBINATIONS . the proposed method is based on the use of PAIRWISE COMBINATIONS and PAIRWISE COMBINATIONS . the proposed method is based on the use of PAIRWISE COMBINATIONS and PAIRWISE COMBINATIONS . the proposed method is based on the use of PAIRWISE COMBINATIONS and PAIRWISE COMBINATIONS . experimental results show that the proposed method outperforms the state-of-the-art methods in terms of BEAT TRACKING and VOICE SUPPRESSION .\n",
            "\n",
            "905 1000\n",
            "spoken term detection is a well-known INFORMATION RETRIEVAL TASK that seeks to extract CONTENTFUL INFORMATION from AUDIO by locating occurrences of KNOWN QUERY WORDS OF INTEREST . this paper describes a ZERO-RESOURCE APPROACH to such SPOKEN TERM DETECTION based on pattern matching of SPOKEN TERM QUERIES at the ACOUSTIC LEVEL . the TEMPLATE MATCHING MODULE comprises the cascade of a segmental variant of DYNAMIC TIME WARPING and a SELF-SIMILARITY MATRIX COMPARISON to further improve ROBUSTNESS to SPEECH VARIABILITY . this ZERO-RESOURCE APPROACH notably differs from more traditional TRAIN AND TEST METHODS that , while shown to be very accurate , rely upon the availability of large amounts of LINGUISTIC RESOURCES . we evaluate our ZERO-RESOURCE APPROACH on different <unk> of the SPEECH TEMPLATES : RAW MFCC FEATURES and GAUSSIAN POSTERIORGRAMS , FRENCH AND ENGLISH PHONETIC POSTERI-ORGRAMS output by two different state of the art phoneme <unk> . \n",
            "this paper presents a new ZERO-RESOURCE APPROACH for SPOKEN TERM DETECTION . the proposed ZERO-RESOURCE APPROACH is based on the SELF-SIMILARITY MATRIX COMPARISON and the ZERO-RESOURCE APPROACH . the proposed ZERO-RESOURCE APPROACH is based on the TEMPLATE MATCHING MODULE and the ZERO-RESOURCE APPROACH . the proposed ZERO-RESOURCE APPROACH is based on the TEMPLATE MATCHING MODULE and the ZERO-RESOURCE APPROACH . the proposed ZERO-RESOURCE APPROACH is compared with the TRAIN AND TEST METHODS and the TRAIN AND TEST METHODS . the proposed ZERO-RESOURCE APPROACH is compared with other TRAIN AND TEST METHODS such as SELF-SIMILARITY MATRIX COMPARISON and SELF-SIMILARITY MATRIX COMPARISON . the proposed ZERO-RESOURCE APPROACH is compared with other TRAIN AND TEST METHODS such as SELF-SIMILARITY MATRIX COMPARISON and SELF-SIMILARITY MATRIX COMPARISON .\n",
            "\n",
            "906 1000\n",
            "communication among participants -lrb- agents , robots -rrb- is central to an appearance of COLLECTIVE AI . in this work we deal with the development of LOCAL COMMUNICATION MECHANISMS for REAL MICRORO-BOTIC SWARMS . we demonstrate that despite of very limited capabilities of the MICROROBOT , the specific construction of COMMUNICATION HARDWARE and software allows very extended collective capabilities of the whole SWARM . we propose LOCAL COMMUNICATION MECHANISMS providing INFORMATION CONTENT and context for COLLECTIVE NAVIGATION , coordination and spatial perception in a group of <unk> . \n",
            "this paper presents a method for COLLECTIVE NAVIGATION in COLLECTIVE AI . the proposed method is based on the use of LOCAL COMMUNICATION MECHANISMS for COLLECTIVE NAVIGATION . the proposed method is based on the use of LOCAL COMMUNICATION MECHANISMS to estimate the INFORMATION CONTENT . the proposed method is based on the use of LOCAL COMMUNICATION MECHANISMS to estimate the INFORMATION CONTENT from the SWARM . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "907 1000\n",
            "fisher linear discriminant analysis -lrb- lda -rrb- can be sensitive to the problem data . ROBUST FISHER LDA can systematically alleviate the SENSITIVITY PROBLEM by explicitly incorporating a model of DATA UNCERTAINTY in a CLASSIFICATION PROBLEM and optimizing for the worst-case scenario under this model . the main contribution of this paper is show that with general CONVEX UNCERTAINTY MODELS on the problem data , robust FISHER LINEAR DISCRIMINANT ANALYSIS can be carried out using CONVEX OPTIMIZATION . for a certain type of PRODUCT FORM UNCERTAINTY MODEL , robust FISHER LINEAR DISCRIMINANT ANALYSIS can be carried out at a cost comparable to standard FISHER LINEAR DISCRIMINANT ANALYSIS . the method is demonstrated with some numerical examples . finally , we show how to extend these results to ROBUST KERNEL FISHER DISCRIMINANT ANALYSIS , i.e. , robust FISHER LINEAR DISCRIMINANT ANALYSIS in a HIGH DIMENSIONAL FEATURE SPACE . \n",
            "this paper proposes a new method for ROBUST KERNEL FISHER DISCRIMINANT ANALYSIS . the proposed method is based on the ROBUST KERNEL FISHER DISCRIMINANT ANALYSIS . the proposed method is based on the ROBUST KERNEL FISHER DISCRIMINANT ANALYSIS . the proposed method is based on the ROBUST KERNEL FISHER DISCRIMINANT ANALYSIS . the proposed method is based on the ROBUST KERNEL FISHER DISCRIMINANT ANALYSIS . the proposed method is based on the ROBUST KERNEL FISHER DISCRIMINANT ANALYSIS . the proposed method is based on the ROBUST KERNEL FISHER DISCRIMINANT ANALYSIS . the proposed method is applied to the CLASSIFICATION PROBLEM .\n",
            "\n",
            "908 1000\n",
            "we present an algorithm to find a LOW-DIMENSIONAL DECOMPOSITION of a spectrogram by formulating LOW-DIMENSIONAL DECOMPOSITION as a REGULARIZED NON-NEGATIVE MATRIX FACTORIZATION PROBLEM with a REGULARIZATION TERM chosen to encourage independence . this algorithm provides a better decomposition than standard NMF when the underlying sources are independent . it is directly applicable to NON-SQUARE MATRICES , and it makes better use of additional OBSERVATION STREAMS than previous NONNEGATIVE ICA ALGORITHMS . \n",
            "this paper addresses the problem of REGULARIZED NON-NEGATIVE MATRIX FACTORIZATION PROBLEM in OBSERVATION STREAMS . we propose a new REGULARIZATION TERM based on the REGULARIZED NON-NEGATIVE MATRIX FACTORIZATION PROBLEM . the proposed NONNEGATIVE ICA ALGORITHMS is based on the REGULARIZED NON-NEGATIVE MATRIX FACTORIZATION PROBLEM . the experimental results show that the proposed NONNEGATIVE ICA ALGORITHMS outperforms the existing NONNEGATIVE ICA ALGORITHMS .\n",
            "\n",
            "909 1000\n",
            "in this paper , we investigate the technical challenges that are faced when making a transition from the SPEAKER-DEPENDENT to SPEAKER-INDEPENDENT SPEECH RECOGNITION TECHNOLOGY in MOBILE COMMUNICATION DEVICES . due to <unk> as well as the international nature of the markets and the future applications , SPEAKER INDEPENDENCE implies the development and use of LANGUAGE-INDEPENDENT ASR to avoid LOGISTIC DIFFICULTIES . we propose here an architecture for EMBEDDED MULTILINGUAL SPEECH RECOGNITION SYSTEMS . MULTILINGUAL ACOUSTIC MODELING , AUTOMATIC LANGUAGE IDENTIFICATION , and ON-LINE PRONUNCIATION MODELING are the key features which enable the creation of truly LANGUAGE-AND SPEAKER-INDEPENDENT ASR APPLICATIONS with DYNAMIC VOCABULARIES and SPARSE IMPLEMENTATION RESOURCES . our experimental results confirm the viability of the proposed architecture . while the use of MULTILINGUAL ACOUSTIC MODELS degrades the RECOGNITION RATES only marginally , a RECOGNITION ACCURACY decrease of approximately 4 % is observed due to SUB-OPTIMAL ON-LINE TEXT-TO-PHONEME MAPPING and AUTOMATIC LANGUAGE IDENTIFICATION . this performance loss can nevertheless be compensated by applying ACOUSTIC MODEL ADAPTATION TECHNIQUES . \n",
            "this paper addresses the problem of AUTOMATIC LANGUAGE IDENTIFICATION in MOBILE COMMUNICATION DEVICES . we propose a SUB-OPTIMAL ON-LINE TEXT-TO-PHONEME MAPPING based on MULTILINGUAL ACOUSTIC MODELS and ACOUSTIC MODEL ADAPTATION TECHNIQUES . the proposed ACOUSTIC MODEL ADAPTATION TECHNIQUES is based on the use of MULTILINGUAL ACOUSTIC MODELS and ACOUSTIC MODEL ADAPTATION TECHNIQUES . the proposed ACOUSTIC MODEL ADAPTATION TECHNIQUES is based on the use of MULTILINGUAL ACOUSTIC MODELS and ACOUSTIC MODEL ADAPTATION TECHNIQUES . experimental results show the effectiveness of the proposed EMBEDDED MULTILINGUAL SPEECH RECOGNITION SYSTEMS .\n",
            "\n",
            "910 1000\n",
            "this paper studies the ERROR SENSITIVITY in the estimation of the 3D-MOTION and the normal of a PLANAR SURFACE from an INSTANTANEOUS MOTION ELD . we use the STATISTICAL THEORY of the cramer-rao lower bound for the ERROR CO-VARIANCE in the ESTIMATED MOTION AND STRUCTURE P ARAMETERS which enables the derivation of results valid for any UNBI-ASED ESTIMATOR under the ASSUMPTION OF GAUSSIAN NOISE in the MOTION ELD . the obtained LOWER-BOUND-MATRIX is studied analytically with respect to the MEASUREMENT NOISE , size of the ELD OF VIEW and the MOTION-GEOMETRY CONNGURATION . the main result of this analysis is the coupling between TRANSLATION and ROTATION which is exacerbated if the ELD OF VIEW and the SLANT of the plane become smaller and the deviation of the TRANSLATION from the VIEWING DIRECTION becomes larger . <unk> of this study are the relationships o f the UNCERTAINTY BOUNDS for every UNKNOWN MOTION PARAMETER to the angle between TRANSLATION and the <unk> , the size of the ELD OF VIEW , the distance f r om the PERCEIVED PLANE and the TRANSLATION MAGNITUDE . \n",
            "this paper presents a method for TRANSLATION in the ELD OF VIEW . the proposed method is based on a STATISTICAL THEORY and a UNBI-ASED ESTIMATOR . the proposed method is based on a UNBI-ASED ESTIMATOR and a UNBI-ASED ESTIMATOR . the proposed method is based on the ASSUMPTION OF GAUSSIAN NOISE and the INSTANTANEOUS MOTION ELD . the proposed method is based on the ELD OF VIEW and the INSTANTANEOUS MOTION ELD . the proposed method is based on the ELD OF VIEW and the INSTANTANEOUS MOTION ELD . the proposed method is based on the ELD OF VIEW and the UNCERTAINTY BOUNDS . the proposed method is compared with the conventional UNBI-ASED ESTIMATOR and the UNBI-ASED ESTIMATOR .\n",
            "\n",
            "911 1000\n",
            "<unk> , a HARMONET employing CONNECTIONIST NETWORKS for MUSIC PROCESSING , is presented . after being trained on some dozen BACH CHORALES using ERROR BACKPROPAGATION , the HARMONET is capable of producing FOUR-PART CHORALES in the style of j . <unk> , given a ONE-PART MELODY . our HARMONET solves a MUSICAL REAL-WORLD PROBLEM on a performance level appropriate for MUSICAL PRACTICE . HARMONET 'S POWER is based on -lrb- a -rrb- a new CODING SCHEME capturing MUSICALLY RELEVANT INFORMATION and -lrb- b -rrb- the integration of BACKPROPAGATION AND SYMBOLIC ALGORITHMS in a HIERARCHICAL SYSTEM , combining the advantages of both . \n",
            "this paper presents a novel CODING SCHEME for MUSIC PROCESSING . the CODING SCHEME is based on a HIERARCHICAL SYSTEM for MUSIC PROCESSING . the HIERARCHICAL SYSTEM is based on a HIERARCHICAL SYSTEM , which is based on the MUSICALLY RELEVANT INFORMATION . the CODING SCHEME is based on a HIERARCHICAL SYSTEM . the proposed CODING SCHEME is based on a HIERARCHICAL SYSTEM . the proposed CODING SCHEME is based on a HIERARCHICAL SYSTEM . the proposed CODING SCHEME is based on a HIERARCHICAL SYSTEM . the proposed CODING SCHEME is based on a HIERARCHICAL SYSTEM and is shown to be useful for MUSIC PROCESSING .\n",
            "\n",
            "912 1000\n",
            "large-scale action recognition and VIDEO CATEGORIZATION are important problems in COMPUTER VISION . to address these problems , we propose a novel OBJECT-AND SCENE-BASED SEMANTIC FUSION NETWORK and representation . our OBJECT-AND SCENE-BASED SEMANTIC FUSION NETWORK combines three streams of information using a THREE-LAYER NEURAL NETWORK : -lrb- i -rrb- frame-based low-level cnn features , -lrb- ii -rrb- object features from a state-of-the-art LARGE-SCALE CNN OBJECT-DETECTOR trained to recognize 20k classes , and -lrb- iii -rrb- scene features from a state-of-the-art CNN SCENE-DETECTOR trained to recognize <unk> scenes . the trained OBJECT-AND SCENE-BASED SEMANTIC FUSION NETWORK achieves improvements in SUPERVISED ACTIVITY and VIDEO CATEGORIZATION in two complex LARGE-SCALE DATASETS-ACTIVITYNET and <unk> , respectively . further , by examining and back propagating information through the FUSION NETWORK , SEMANTIC RELATIONSHIPS -LRB- CORRELATIONS -rrb- between VIDEO CLASSES and <unk> can be discovered . these VIDEO CLASS-OBJECT/VIDEO CLASS-SCENE RELATIONSHIPS can in turn be used as SEMANTIC REPRESENTATION for the VIDEO CLASSES themselves . we illustrate effectiveness of this SEMANTIC REPRESENTATION through experiments on ZERO-SHOT ACTION/VIDEO CLASSIFICATION and CLUSTERING . \n",
            "this paper presents a OBJECT-AND SCENE-BASED SEMANTIC FUSION NETWORK for LARGE-SCALE ACTION RECOGNITION . the OBJECT-AND SCENE-BASED SEMANTIC FUSION NETWORK is based on a THREE-LAYER NEURAL NETWORK and a THREE-LAYER NEURAL NETWORK . the THREE-LAYER NEURAL NETWORK is based on the THREE-LAYER NEURAL NETWORK and the THREE-LAYER NEURAL NETWORK . the OBJECT-AND SCENE-BASED SEMANTIC FUSION NETWORK is based on the THREE-LAYER NEURAL NETWORK and the THREE-LAYER NEURAL NETWORK . the proposed OBJECT-AND SCENE-BASED SEMANTIC FUSION NETWORK is based on the THREE-LAYER NEURAL NETWORK and the THREE-LAYER NEURAL NETWORK . the proposed OBJECT-AND SCENE-BASED SEMANTIC FUSION NETWORK is evaluated on LARGE-SCALE ACTION RECOGNITION and LARGE-SCALE ACTION RECOGNITION . the proposed OBJECT-AND SCENE-BASED SEMANTIC FUSION NETWORK is compared with the conventional THREE-LAYER NEURAL NETWORK and the OBJECT-AND SCENE-BASED SEMANTIC FUSION NETWORK .\n",
            "\n",
            "913 1000\n",
            "we present a procedure to automatically derive INTER-PRETABLE DYNAMIC ARTICULATORY PRIMITIVES in a DATA-DRIVEN MANNER from IMAGE SEQUENCES acquired through REAL-TIME MAGNETIC RESONANCE IMAGING . more specifically , we propose a convolutive nonnegative matrix factorization algorithm with SPARSENESS CONSTRAINTS to decompose a given set of IMAGE SEQUENCES into a set of basis IMAGE SEQUENCES and an ACTIVATION MATRIX . we use a RECENTLY-ACQUIRED RT-MRI CORPUS of read speech -lrb- <unk> sentences from 4 speakers -rrb- as a test dataset for this procedure . we choose the free parameters of the algorithm empirically by analyzing algorithm performance for different PARAMETER VALUES . we then validate the extracted basis sequences using an ARTICULATORY RECOGNITION TASK and finally present an interpretation of the extracted basis set of IMAGE SEQUENCES in a GESTURE-BASED ARTICULATORY PHONOLOGY FRAMEWORK . \n",
            "this paper presents a new method for REAL-TIME MAGNETIC RESONANCE IMAGING based on INTER-PRETABLE DYNAMIC ARTICULATORY PRIMITIVES . the proposed method is based on a RECENTLY-ACQUIRED RT-MRI CORPUS , which is based on the ACTIVATION MATRIX . the proposed method is based on the ACTIVATION MATRIX . the proposed method is based on the ACTIVATION MATRIX . the proposed method is based on the RECENTLY-ACQUIRED RT-MRI CORPUS . the proposed method is based on the RECENTLY-ACQUIRED RT-MRI CORPUS . the proposed method is based on the RECENTLY-ACQUIRED RT-MRI CORPUS .\n",
            "\n",
            "914 1000\n",
            "in many MACHINE LEARNING APPLICATIONS , labeling every instance of data is <unk> . MULTIPLE INSTANCE LEARNING , in which training data is provided in the form of labeled bags rather than labeled instances , is one approach for a more relaxed form of SUPERVISED LEARNING . though much progress has been made in analyzing MIL PROBLEMS , existing work considers bags that have a finite number of instances . in this paper we argue that in many applications of MULTIPLE INSTANCE LEARNING -lrb- e.g. IMAGE , AUDIO , etc. -rrb- the bags are better modeled as LOW DIMENSIONAL MANIFOLDS in HIGH DIMENSIONAL FEATURE SPACE . we show that the GEOMETRIC STRUCTURE of such MANIFOLD BAGS affects PAC-LEARNABILITY . we discuss how a learning algorithm that is designed for FINITE SIZED BAGS can be adapted to learn from MANI-FOLD BAGS . furthermore , we propose a simple HEURISTIC that reduces the MEMORY REQUIREMENTS of such algorithms . our experiments on REAL-WORLD DATA validate our analysis and show that our approach works well . \n",
            "this paper addresses the problem of MULTIPLE INSTANCE LEARNING in MACHINE LEARNING APPLICATIONS . we propose a method for MULTIPLE INSTANCE LEARNING based on MULTIPLE INSTANCE LEARNING and MULTIPLE INSTANCE LEARNING . the method is based on the GEOMETRIC STRUCTURE and the GEOMETRIC STRUCTURE . the proposed method is based on MULTIPLE INSTANCE LEARNING and MULTIPLE INSTANCE LEARNING . the proposed method is based on MULTIPLE INSTANCE LEARNING and MULTIPLE INSTANCE LEARNING . the method is based on MULTIPLE INSTANCE LEARNING and MULTIPLE INSTANCE LEARNING . the proposed method is based on MULTIPLE INSTANCE LEARNING and MULTIPLE INSTANCE LEARNING . the proposed method is compared with other state-of-the-art methods in terms of MEMORY REQUIREMENTS and MEMORY REQUIREMENTS .\n",
            "\n",
            "915 1000\n",
            "visual recognition systems for videos using STATISTICAL LEARNING MODELS often show degraded performance when being deployed to a REAL-WORLD ENVIRONMENT , primarily due to the fact that training data can hardly cover sufficient variations in reality . to alleviate this issue , we propose to utilize the OBJECT CORRESPONDENCES in SUCCESSIVE FRAMES as WEAK SUPERVISION to adapt VISUAL RECOGNITION MODELS , which is particularly suitable for HUMAN PROFILE RECOGNITION . specifically , we <unk> this new strategy on an advanced CONVOLUTIONAL NEURAL NETWORK BASED SYSTEM to estimate HUMAN GENDER , age , and race . we enforce the system to output consistent and stable results on FACE IMAGES from the same trajectories in videos by using INCREMENTAL STOCHASTIC TRAINING . our baseline system already achieves competitive performance on GENDER AND AGE ESTIMATION as compared to the state-of-the-art algorithms on the FG-NET DATABASE . further , on two new VIDEO DATASETS containing about <unk> persons , the proposed SUPERVISION OF CORRESPONDENCES improves the ESTIMATION ACCURACY by a large margin over the baseline . \n",
            "this paper addresses the problem of GENDER AND AGE ESTIMATION in FACE IMAGES . we propose a CONVOLUTIONAL NEURAL NETWORK BASED SYSTEM based on INCREMENTAL STOCHASTIC TRAINING for GENDER AND AGE ESTIMATION . the CONVOLUTIONAL NEURAL NETWORK BASED SYSTEM is based on a CONVOLUTIONAL NEURAL NETWORK BASED SYSTEM of the FACE IMAGES . the CONVOLUTIONAL NEURAL NETWORK BASED SYSTEM is based on the use of SUCCESSIVE FRAMES and the SUPERVISION OF CORRESPONDENCES . the ESTIMATION ACCURACY of the proposed VISUAL RECOGNITION MODELS is evaluated using the FG-NET DATABASE . the results show that the proposed method is effective for GENDER AND AGE ESTIMATION in FACE IMAGES .\n",
            "\n",
            "916 1000\n",
            "this paper describes a MODIFIED COMPOSITION ALGORITHM that is used for combining two FINITE-STATE TRANSDUCERS , representing the CONTEXT-DEPENDENT LEXICON and the LANGUAGE MODEL respectively , in LARGE VOCABULARY SPEECH RECOGNTION . this MODIFIED COMPOSITION ALGORITHM is a hybrid between the static and dynamic expansion of the resultant transducer , which maps from CONTEXT-DEPENDENT PHONES to words and is searched during DECODING . the MODIFIED COMPOSITION ALGORITHM is to <unk> part of the RECOGNITION TRANSDUCER and leave the balance to be expanded during DECODING . this MODIFIED COMPOSITION ALGORITHM allows for a FINE-GRAINED TRADE-OFF between space and time in recognition . for example , the time overhead of purely dynamic expansion can be reduced by over <unk> with only a 20 % increase in memory in a collection of LARGE-VOCABULARY RECOGNITION TASKS available on the GOOGLE ANDROID PLATFORM . \n",
            "this paper presents a new method for LARGE VOCABULARY SPEECH RECOGNTION . the proposed method is based on the MODIFIED COMPOSITION ALGORITHM and the MODIFIED COMPOSITION ALGORITHM . the proposed method is based on the MODIFIED COMPOSITION ALGORITHM and the MODIFIED COMPOSITION ALGORITHM . the proposed method is based on the MODIFIED COMPOSITION ALGORITHM and the MODIFIED COMPOSITION ALGORITHM . the proposed method is based on the MODIFIED COMPOSITION ALGORITHM and the MODIFIED COMPOSITION ALGORITHM . the experimental results show that the proposed method is robust and robust to DECODING , such as DECODING , and DECODING .\n",
            "\n",
            "917 1000\n",
            "<unk> object matching is challenging due to IMAGE DISTORTIONS caused by several factors such as ROTATION , TRANSLATION , ILLUMINATION , CROPPING and OCCLUSION . we propose a COMPACT , GLOBAL IMAGE DESCRIPTOR for MANHATTAN SCENES that captures RELATIVE LOCATIONS and strengths of edges along vanishing directions . to construct the DESCRIPTOR , an EDGE MAP is determined per vanishing point , capturing the EDGE STRENGTHS over a range of angles measured at the vanishing point . for matching , descriptors from two scenes are compared across multiple candidate scales and displacements . the matching performance is refined by comparing EDGE SHAPES at the LOCAL MAXIMA of the SCALE-DISPLACEMENT PLOTS . the proposed GLOBAL IMAGE DESCRIPTOR achieves an equal ERROR RATE of 7 % for the ZURICH BUILDINGS DATABASE , indicating significant gains in DISCRIMINATIVE ABILITY over other GLOBAL DESCRIPTORS that rely on AGGREGATE IMAGE STATISTICS but do not exploit the underlying SCENE GEOMETRY . \n",
            "this paper presents a new method for VIEWPOINT-INVARIANT OBJECT MATCHING in MANHATTAN SCENES . the proposed method is based on a COMPACT , GLOBAL IMAGE DESCRIPTOR and a COMPACT , GLOBAL IMAGE DESCRIPTOR . the proposed method is based on a COMPACT , GLOBAL IMAGE DESCRIPTOR and a COMPACT , GLOBAL IMAGE DESCRIPTOR . the proposed method is based on a COMPACT , GLOBAL IMAGE DESCRIPTOR and a COMPACT , GLOBAL IMAGE DESCRIPTOR . the proposed method is based on a COMPACT , GLOBAL IMAGE DESCRIPTOR and a COMPACT , GLOBAL IMAGE DESCRIPTOR . the proposed method is tested on ZURICH BUILDINGS DATABASE and MANHATTAN SCENES . it is shown that the proposed method is robust and robust to IMAGE DISTORTIONS and OCCLUSION .\n",
            "\n",
            "918 1000\n",
            "in this paper we propose a ROBUST ITERATIVE HARD THRESOLDING ALGORITHM for RECONSTRUCTING SPARSE SIGNALS in the presence of IMPULSIVE NOISE . to address this problem , we use a LORENTZIAN COST FUNCTION instead of the í µí ° ¿ 2 cost function employed by the traditional IHT ALGORITHM . the derived ROBUST ITERATIVE HARD THRESOLDING ALGORITHM is comparable in COMPUTATIONAL LOAD to the least squares based IHT . analysis of the proposed ROBUST ITERATIVE HARD THRESOLDING ALGORITHM demonstrates its ROBUSTNESS under HEAVY-TAILED MODELS . simulations show that the proposed ROBUST ITERATIVE HARD THRESOLDING ALGORITHM significantly outperform commonly employed SPARSE RECONSTRUCTION TECHNIQUES in IMPULSIVE ENVIRONMENTS , while providing comparable RECONSTRUCTION QUALITY in less demanding , <unk> environments . \n",
            "this paper proposes a new ROBUST ITERATIVE HARD THRESOLDING ALGORITHM for RECONSTRUCTING SPARSE SIGNALS . the proposed ROBUST ITERATIVE HARD THRESOLDING ALGORITHM is based on the ROBUST ITERATIVE HARD THRESOLDING ALGORITHM . the proposed ROBUST ITERATIVE HARD THRESOLDING ALGORITHM is based on the ROBUST ITERATIVE HARD THRESOLDING ALGORITHM . the proposed ROBUST ITERATIVE HARD THRESOLDING ALGORITHM is compared with the conventional IHT ALGORITHM and the ROBUST ITERATIVE HARD THRESOLDING ALGORITHM . the proposed ROBUST ITERATIVE HARD THRESOLDING ALGORITHM is compared with the conventional IHT ALGORITHM and the ROBUST ITERATIVE HARD THRESOLDING ALGORITHM . the proposed ROBUST ITERATIVE HARD THRESOLDING ALGORITHM outperforms the conventional SPARSE RECONSTRUCTION TECHNIQUES in terms of RECONSTRUCTION QUALITY and RECONSTRUCTION QUALITY .\n",
            "\n",
            "919 1000\n",
            "we analyze the bit error probability of MULTIUSER DEMODULATORS for DIRECT-SEQUENCE BINARY PHASE-SHIFT-KEYING CDMA CHANNEL with ADDITIVE GAUSSIAN NOISE . the problem of MULTIUSER DEMODULATORS is cast into the FINITE-TEMPERATURE DECODING PROBLEM , and REPLICA ANALYSIS is applied to evaluate the performance of the resulting MPM -LRB- MARGINAL POSTERIOR MODE -RRB- DEMODULATORS , which include the optimal DEMODULATOR and the MAP DEMODULATOR as special cases . an approximate implementation of MULTIUSER DEMODULATORS is proposed using ANALOG-VALUED HOPFIELD MODEL as a naive MEAN-FIELD APPROXIMATION to the MPM -LRB- MARGINAL POSTERIOR MODE -RRB- DEMODULATORS , and its performance is also evaluated by the REPLICA ANALYSIS . results of the performance evaluation shows effectiveness of the optimal DEMODULATOR and the MEAN-FIELD DEMODULATOR compared with the conventional one , especially in the cases of small INFORMATION BIT RATE and LOW NOISE LEVEL . \n",
            "this paper presents a ANALOG-VALUED HOPFIELD MODEL based on the MPM -LRB- MARGINAL POSTERIOR MODE -RRB- DEMODULATORS . the ANALOG-VALUED HOPFIELD MODEL is based on the MPM -LRB- MARGINAL POSTERIOR MODE -RRB- DEMODULATORS . the ANALOG-VALUED HOPFIELD MODEL is based on the MPM -LRB- MARGINAL POSTERIOR MODE -RRB- DEMODULATORS . the ANALOG-VALUED HOPFIELD MODEL is based on the MPM -LRB- MARGINAL POSTERIOR MODE -RRB- DEMODULATORS of the DIRECT-SEQUENCE BINARY PHASE-SHIFT-KEYING CDMA CHANNEL . the ANALOG-VALUED HOPFIELD MODEL is applied to the FINITE-TEMPERATURE DECODING PROBLEM . the MPM -LRB- MARGINAL POSTERIOR MODE -RRB- DEMODULATORS is applied to the FINITE-TEMPERATURE DECODING PROBLEM . the results show that the proposed ANALOG-VALUED HOPFIELD MODEL is robust and robust to ADDITIVE GAUSSIAN NOISE .\n",
            "\n",
            "920 1000\n",
            "the ability to rely on SIMILARITY METRICS invariant to IMAGE TRANSFORMATIONS is an important issue for IMAGE CLASSIFICATION TASKS such as FACE OR CHARACTER RECOGNITION . we analyze an INVARIANT METRIC that has performed well for the <unk> tangent <unk> study its limitations when applied to REGULAR IMAGES , showing that the most significant among these -lrb- convergence to LOCAL MINIMA -RRB- can be drastically reduced by computing the distance in a MULTIRESOLUTION SETTING . this leads to the MULTIRESOLUTION TANGENT DISTANCE , which exhibits significantly higher invariance to IMAGE TRANSFORMATIONS , and can be easily combined with ROBUST ESTIMATION PROCEDURES . \n",
            "this paper addresses the problem of FACE OR CHARACTER RECOGNITION in REGULAR IMAGES . we propose a method for ROBUST ESTIMATION PROCEDURES based on the MULTIRESOLUTION TANGENT DISTANCE . the proposed method is based on the use of SIMILARITY METRICS as a INVARIANT METRIC . the proposed method is based on the use of SIMILARITY METRICS as a INVARIANT METRIC . experimental results show that the proposed method is effective for IMAGE CLASSIFICATION TASKS .\n",
            "\n",
            "921 1000\n",
            "this paper combines LINEAR SPARSE CODING and NON-NEGATIVE MATRIX FACTORIZATION into SPARSE NON-NEGATIVE MATRIX FACTORIZATION . in contrast to NON-NEGATIVE MATRIX FACTORIZATION , the new model can <unk> much SPARSER REPRESENTATION via imposing SPARSENESS CONSTRAINTS explicitly ; in contrast to a CLOSE MODEL-NON-NEGATIVE SPARSE CODING , the new model can learn PARTS-BASED REPRESENTATION via fully MULTIPLICATIVE UPDATES because of adapting a GENERALIZED KULLBACK-LEIBLER DIVERGENCE instead of the conventional MEAN SQUARE ERROR for APPROXIMATION ERROR . experiments on MIT-CBCL TRAINING FACES DATA demonstrate the effectiveness of the proposed method . \n",
            "this paper addresses the problem of CLOSE MODEL-NON-NEGATIVE SPARSE CODING for MIT-CBCL TRAINING FACES DATA . we propose a method for SPARSE NON-NEGATIVE MATRIX FACTORIZATION based on SPARSE NON-NEGATIVE MATRIX FACTORIZATION . the proposed method is based on the GENERALIZED KULLBACK-LEIBLER DIVERGENCE and the GENERALIZED KULLBACK-LEIBLER DIVERGENCE . the proposed method is based on the GENERALIZED KULLBACK-LEIBLER DIVERGENCE and the GENERALIZED KULLBACK-LEIBLER DIVERGENCE . the proposed method is based on the GENERALIZED KULLBACK-LEIBLER DIVERGENCE . the proposed method is based on the GENERALIZED KULLBACK-LEIBLER DIVERGENCE and the GENERALIZED KULLBACK-LEIBLER DIVERGENCE .\n",
            "\n",
            "922 1000\n",
            "the MAP SEEKING CIRCUIT has been suggested to address the inverse problem of TRANSFORMATION DISCOVERY as found in SIGNAL PROCESSING , VISION , INVERSE KINEMATICS and many other NATURAL TASKS . according to this idea , a PARALLEL SEARCH in the TRANSFORMATION SPACE of a HIGH DIMENSIONAL PROBLEM can be decomposed into parts efficiently using the ORDERING PROPERTY OF SUPERPOSITIONS . deterministic formulations of the circuit have been suggested . here , we provide a <unk> interpretation of the architecture whereby the SUPERPOSITIONS OF THE CIRCUIT are seen as a series of <unk> over parameters of the transform . based on this , we interpret the weights of the MAP SEEKING CIRCUIT as importance weights . the latter suggests the incorporation of MONTE-CARLO APPROACHES in the MAP SEEKING CIRCUIT , providing improved RESOLUTION OF PARAMETER ESTIMATES within RESOURCE CONSTRAINED IMPLEMENTATIONS . as a final contribution , we model mixed <unk> search strategies of BIOLOGICAL VISION to reduce the problem of COLLUSIONS , a common problem in the standard MAP SEEKING CIRCUIT . \n",
            "this paper addresses the problem of TRANSFORMATION DISCOVERY in BIOLOGICAL VISION . we propose a method for TRANSFORMATION DISCOVERY based on the ORDERING PROPERTY OF SUPERPOSITIONS . the proposed method is based on the ORDERING PROPERTY OF SUPERPOSITIONS and the ORDERING PROPERTY OF SUPERPOSITIONS . the proposed method is based on the ORDERING PROPERTY OF SUPERPOSITIONS and the ORDERING PROPERTY OF SUPERPOSITIONS . the proposed method is based on the ORDERING PROPERTY OF SUPERPOSITIONS and the ORDERING PROPERTY OF SUPERPOSITIONS . the proposed method is based on the ORDERING PROPERTY OF SUPERPOSITIONS and the RESOLUTION OF PARAMETER ESTIMATES . the proposed method is based on the ORDERING PROPERTY OF SUPERPOSITIONS and the ORDERING PROPERTY OF SUPERPOSITIONS . the proposed method is evaluated on NATURAL TASKS and NATURAL TASKS .\n",
            "\n",
            "923 1000\n",
            "wireless technology has allowed for a much wider variety in the design of MICROPHONE ARRAYS for BINAURAL HEARING AIDS . to facilitate the design of these MICROPHONE ARRAYS , this paper investigates the use of a SPHERICAL HEAD MODEL in the design of bilateral and binaural MICROPHONE ARRAYS for HEARING AIDS . the MICROPHONE ARRAYS have been designed using a FREE-FIELD MODEL , a SPHERICAL HEAD MODEL , measurements on an ARTIFICIAL HEAD , and measurements on an ARTIFICIAL HEAD + TORSO . the results show that the FREE-FIELD/SPHERICAL MODELS <unk> the SPEECH-INTELLIGIBILITY WEIGHTED DIRECTIVITY INDEX of the BILATERAL AND BINAURAL ARRAYS by respectively 0.9 / 0.4 and 0.8 / 0.5 db . furthermore the weights designed with the FREE-FIELD/SPHERICAL MODEL yield an SII-DI that is 0.7 / 0.6 db lower for BILATERAL ARRAYS and 0.9 / 0.9 db lower for BINAURAL ARRAYS than the optimal SII-DI . although the results show that the SPHERICAL HEAD MODEL is better in predicting the DI than the FREE-FIELD MODEL , the SPHERICAL HEAD MODEL does not design better weights . \n",
            "this paper presents a new FREE-FIELD/SPHERICAL MODEL for BINAURAL HEARING AIDS . the FREE-FIELD MODEL is based on the SPEECH-INTELLIGIBILITY WEIGHTED DIRECTIVITY INDEX of the FREE-FIELD MODEL and the FREE-FIELD/SPHERICAL MODEL . the proposed FREE-FIELD/SPHERICAL MODEL is based on the SPEECH-INTELLIGIBILITY WEIGHTED DIRECTIVITY INDEX and the SPEECH-INTELLIGIBILITY WEIGHTED DIRECTIVITY INDEX of the FREE-FIELD/SPHERICAL MODEL . the FREE-FIELD/SPHERICAL MODEL is based on the SPEECH-INTELLIGIBILITY WEIGHTED DIRECTIVITY INDEX of the FREE-FIELD MODEL and the FREE-FIELD/SPHERICAL MODEL . the proposed FREE-FIELD/SPHERICAL MODEL is compared with the conventional FREE-FIELD/SPHERICAL MODEL and the FREE-FIELD/SPHERICAL MODEL .\n",
            "\n",
            "924 1000\n",
            "this paper presents MULTI-CONDITIONAL LEARNING , a TRAINING CRITERION based on a product of multiple conditional likelihoods . when combining the traditional conditional probability of '' label given input '' with a generative probability of '' input given label '' the later acts as a surprisingly effective REGULARIZER . when applied to models with LATENT VARIABLES , MULTI-CONDITIONAL LEARNING combines the <unk> capabilities of GENERATIVE TOPIC MODELS , such as LATENT DIRICHLET ALLOCATION and the EXPONENTIAL FAMILY HARMONIUM , with the ACCURACY and ROBUSTNESS of DISCRIMINATIVE CLASSIFIERS , such as LOGISTIC REGRESSION and CONDITIONAL RANDOM FIELDS . we present results on several standard TEXT DATA SETS showing significant reductions in CLASSIFICATION ERROR due to MCL REGULARIZATION , and substantial gains in PRECISION and RECALL due to the LATENT STRUCTURE discovered under MULTI-CONDITIONAL LEARNING . \n",
            "this paper addresses the problem of MULTI-CONDITIONAL LEARNING for TEXT DATA SETS , such as LATENT DIRICHLET ALLOCATION , MCL REGULARIZATION , and MULTI-CONDITIONAL LEARNING . we propose a new TRAINING CRITERION , called LATENT DIRICHLET ALLOCATION , which is based on the TRAINING CRITERION and the TRAINING CRITERION . the proposed method is based on the use of the TRAINING CRITERION and the TRAINING CRITERION . the ROBUSTNESS of the proposed method is compared with the conventional GENERATIVE TOPIC MODELS and the TRAINING CRITERION .\n",
            "\n",
            "925 1000\n",
            "the COMMON SPATIAL PATTERNS ALGORITHM has been widely used in EEG CLASSIFICATION and BRAIN COMPUTER INTERFACE . in this paper , we propose a MULTILINEAR FORMULATION of the COMMON SPATIAL PATTERNS ALGORITHM , termed as <unk> or common tensor discriminant analysis -lrb- <unk> -rrb- for HIGH-ORDER TENSOR DATA . as a natural extension of COMMON SPATIAL PATTERNS ALGORITHM , the proposed MULTILINEAR FORMULATION uses the analogous OPTIMIZATION CRITERIA in COMMON SPATIAL PATTERNS ALGORITHM and a new framework for SIMULTANEOUS OPTIMIZATION OF PROJECTION MATRICES on each mode based on TENSOR ANALYSIS THEORY is developed . experimental results demonstrate that our proposed MULTILINEAR FORMULATION is able to improve CLASSIFICATION ACCURACY of MULTI-CLASS MOTOR IMAGERY EEG . \n",
            "this paper presents a MULTILINEAR FORMULATION for SIMULTANEOUS OPTIMIZATION OF PROJECTION MATRICES . the MULTILINEAR FORMULATION is based on a MULTILINEAR FORMULATION and a MULTILINEAR FORMULATION for SIMULTANEOUS OPTIMIZATION OF PROJECTION MATRICES . the proposed MULTILINEAR FORMULATION is based on a MULTILINEAR FORMULATION and a MULTILINEAR FORMULATION for EEG CLASSIFICATION . the proposed MULTILINEAR FORMULATION is evaluated on a MULTI-CLASS MOTOR IMAGERY EEG . the experimental results show that the proposed MULTILINEAR FORMULATION achieves a better performance in terms of CLASSIFICATION ACCURACY and CLASSIFICATION ACCURACY .\n",
            "\n",
            "926 1000\n",
            "in a LIFELONG LEARNING FRAMEWORK , an LIFELONG LEARNING FRAMEWORK acquires knowledge incrementally over consecutive LEARNING TASKS , continually building upon its experience . recent LIFELONG LEARNING ALGORITHMS have achieved nearly identical performance to BATCH MULTI-TASK LEARNING METHODS while reducing LEARNING TIME by three orders of magnitude . in this paper , we further improve the scalability of LIFELONG LEARNING by developing CURRICULUM SELECTION METHODS that enable an LIFELONG LEARNING FRAMEWORK to actively select the next task to learn in order to maximize performance on future LEARNING TASKS . we demonstrate that ACTIVE TASK SELECTION is highly reliable and effective , allowing an LIFELONG LEARNING FRAMEWORK to learn high performance models using up to 50 % fewer tasks than when the LIFELONG LEARNING FRAMEWORK has no control over the task order . we also explore a variant of TRANSFER LEARNING in the LIFELONG LEARNING SETTING in which the LIFELONG LEARNING FRAMEWORK can focus KNOWLEDGE ACQUISITION toward a particular target task . \n",
            "this paper addresses the problem of ACTIVE TASK SELECTION for LEARNING TASKS . we propose a new LIFELONG LEARNING FRAMEWORK based on LIFELONG LEARNING . the proposed LIFELONG LEARNING ALGORITHMS is based on the use of BATCH MULTI-TASK LEARNING METHODS for ACTIVE TASK SELECTION . the proposed LIFELONG LEARNING ALGORITHMS is based on the use of BATCH MULTI-TASK LEARNING METHODS for ACTIVE TASK SELECTION . experimental results are presented to demonstrate the effectiveness of the proposed LIFELONG LEARNING ALGORITHMS .\n",
            "\n",
            "927 1000\n",
            "the RETRIEVAL OF SOCCER HIGHLIGHTS is a suitable technique for VIDEO INDEXING , required by the MULTIMEDIA DATABASE MANAGEMENT or for the development of television on demand . for these purposes , it should be interesting to have an AUTOMATIC ANNOTATION OF EVENTS happened in SOCCER GAMES . one solution consists in analyzing the AUDIO SOUNDTRACK associated to the SOCCER VIDEO and to detect the interesting frames . in this paper we use the ADAPTIVE TIME-FREQUENCY DECOMPOSITION of the <unk> as a FEATURE EXTRACTION PROCEDURE . this decomposition is based on the MATCHING PURSUIT CONCEPT and a dictionary composed of gabor functions . the parameters provided by these transformations constitute the input of the CLASSIFICATION STAGE . the results provided for REAL SOCCER VIDEO will prove the efficiency of the ADAPTIVE TIME-FREQUENCY REPRESENTATION as a FEATURE EXTRACTION STAGE . \n",
            "this paper presents a method for AUTOMATIC ANNOTATION OF EVENTS in SOCCER VIDEO . the proposed method is based on a MATCHING PURSUIT CONCEPT , which is based on the MATCHING PURSUIT CONCEPT . the proposed method is based on the MATCHING PURSUIT CONCEPT . the proposed method is based on the MATCHING PURSUIT CONCEPT . the proposed method is based on the MATCHING PURSUIT CONCEPT . the proposed method is based on the MATCHING PURSUIT CONCEPT . the proposed method is based on the MATCHING PURSUIT CONCEPT . the proposed method is based on the RETRIEVAL OF SOCCER HIGHLIGHTS . the proposed method is based on the MATCHING PURSUIT CONCEPT . the proposed method is based on the RETRIEVAL OF SOCCER HIGHLIGHTS .\n",
            "\n",
            "928 1000\n",
            "in this paper , we evaluate the performance of existing and new objective measures in terms of predicting the quality of REVERBERANT SPEECH and speech enhanced by DEREVERBERATION ALGORITHMS . we use SUBJECTIVE QUALITY RATINGS designed to evaluate the quality of speech along three dimensions : SPEECH COLORATION , REVERBERATION TAIL EFFECT and OVERALL SPEECH QUALITY . experimental results assess the correlations between the proposed OBJECTIVE QUALITY MEASURES and the three SUBJECTIVE RATING SCALES and suggest that PESQ-BASED MEASURES can very reliably predict the quality of REVERBERANT AND DEREVERBERATED SPEECH . \n",
            "this paper presents a new method for REVERBERANT AND DEREVERBERATED SPEECH . the proposed method is based on the use of PESQ-BASED MEASURES and PESQ-BASED MEASURES . the proposed method is based on the use of PESQ-BASED MEASURES and PESQ-BASED MEASURES . the OBJECTIVE QUALITY MEASURES of the proposed method is compared with the conventional DEREVERBERATION ALGORITHMS . the proposed method is evaluated in terms of OVERALL SPEECH QUALITY and SUBJECTIVE QUALITY RATINGS .\n",
            "\n",
            "929 1000\n",
            "prediction by <unk> does not rely on any specific MODEL STRUCTURE , and is thus much more flexible than approaches based on PARAMETRIC BEHAVIOURAL MODELS . since accurate predictions are obtained for extremely short training sequences , it generally performs better than PREDICTION METHODS using PARAMETRIC MODELS . application to NONLINEAR SYSTEM INVERSION is considered \n",
            "this paper addresses the problem of PREDICTION in NONLINEAR SYSTEM INVERSION . we propose a method for PREDICTION based on the MODEL STRUCTURE . the proposed method is based on the use of PARAMETRIC BEHAVIOURAL MODELS , and is based on the use of PARAMETRIC BEHAVIOURAL MODELS . experimental results show that the proposed method outperforms the state-of-the-art methods .\n",
            "\n",
            "930 1000\n",
            "the recent explosion of interest in GRAPH CUT METHODS in COMPUTER VISION naturally <unk> the question : what ENERGY FUNCTIONS can be minimized via GRAPH CUTS ? this question was first attacked by two papers of kolmogorov and <unk> -lsb- 23 , 24 -rsb- , in which they dealt with FUNCTIONS with PAIR-WISE AND TRIPLEWISE PIXEL INTERACTIONS . in this work , we extend their results in two directions . first , we examine the case of K-WISE PIXEL INTERACTIONS ; the results are derived from a purely ALGEBRAIC APPROACH . second , we discuss the applicability of provably APPROXIMATE ALGORITHMS . both of these developments should help researchers best understand what can and can not be achieved when designing GRAPH CUT BASED ALGORITHMS . \n",
            "this paper addresses the problem of PAIR-WISE AND TRIPLEWISE PIXEL INTERACTIONS in COMPUTER VISION . in this paper , we propose a new ALGEBRAIC APPROACH based on the ALGEBRAIC APPROACH . the proposed method is based on the use of GRAPH CUTS to estimate the ENERGY FUNCTIONS . the proposed method is based on the use of GRAPH CUTS . the proposed method is based on the use of GRAPH CUTS . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "931 1000\n",
            "the objective of this paper is to <unk> students understanding of SAMPLING through experimentation . the scope of this paper is very narrow , focusing on a single LABORATORY EXERCISE for learning about SAMPLING , which is a critically important topic for students to comprehend . the LABORATORY EXERCISE makes an ideal platform for studying SAMPLING , ALIASING , and QUANTIZATION , because not only does the LABORATORY EXERCISE have a built in A/D CONVERTER and all the supporting electronics , but LABORATORY EXERCISE has the DISPLAY CAPABILITIES in both the time and frequency domains . additionally , students obtain a better understanding of test and measurement equipment such as the LABORATORY EXERCISE . \n",
            "this paper presents a new method for SAMPLING based on SAMPLING and SAMPLING . the proposed method is based on the use of SAMPLING and SAMPLING . the proposed method is based on the use of QUANTIZATION and SAMPLING . the experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "932 1000\n",
            "recently there has developed considerable interest in using <unk> with PCA OR-THOGONALITY . almost all previous methods concentrate on <unk> out some <unk> . here we develop a new approach which zeros out whole variables automatically . we formulate a VECTOR L1 PENALIZED PCA CRITERION and optimize VECTOR L1 PENALIZED PCA CRITERION by <unk> descent along GEODESIC on a GRASSMAN MANIFOLD . this ensures that each step obeys PCA OR-THOGONALITY as well as an invariance property of the criterion . we show in simulations that VECTOR L1 PENALIZED PCA CRITERION outperforms a previous SVPCA ALGORITHM and apply VECTOR L1 PENALIZED PCA CRITERION to a real high dimensional functional magnetic resonance imaging -lrb- fmri -rrb- data . \n",
            "this paper proposes a new SVPCA ALGORITHM for PCA OR-THOGONALITY . the proposed VECTOR L1 PENALIZED PCA CRITERION is based on a VECTOR L1 PENALIZED PCA CRITERION . the proposed SVPCA ALGORITHM is compared with the conventional SVPCA ALGORITHM and the conventional SVPCA ALGORITHM .\n",
            "\n",
            "933 1000\n",
            "this paper demonstrates how an INSTRUMENTAL SPEECH-QUALITY MEASURE based on the comparison of AUDITORY-NERVE RING-PATTERNS can be constructed . four available subjective tests prove that the MEAN OPINION SCORES MOS estimated by the objective measure are in good agreement with the subjectively obtained results . \n",
            "this paper proposes a new method for AUDITORY-NERVE RING-PATTERNS based on the INSTRUMENTAL SPEECH-QUALITY MEASURE . the proposed method is based on the use of the INSTRUMENTAL SPEECH-QUALITY MEASURE . the proposed method is based on the INSTRUMENTAL SPEECH-QUALITY MEASURE .\n",
            "\n",
            "934 1000\n",
            "the concepts of MSS -LRB- MAXIMAL SATISFIABLE SUBSET -RRB- and COMSS -lrb- also called MINIMAL CORRECTION SUBSET -RRB- play a key role in many A.I. APPROACHES and techniques . in this paper , a novel algorithm for partitioning a BOOLEAN CNF FORMULA into one MINIMAL CORRECTION SUBSET -RRB- and the corresponding COMSS is introduced . extensive empirical evaluation shows that it is more robust and more efficient on most instances than currently available techniques . \n",
            "this paper presents a new method for the MSS -LRB- MAXIMAL SATISFIABLE SUBSET -RRB- . the proposed method is based on the BOOLEAN CNF FORMULA and the BOOLEAN CNF FORMULA . the proposed method is based on the BOOLEAN CNF FORMULA and the A.I. APPROACHES . the experimental results show the effectiveness of the proposed method in terms of MINIMAL CORRECTION SUBSET -RRB- and A.I. APPROACHES .\n",
            "\n",
            "935 1000\n",
            "we improve the AUTOMATIC SPEECH RECOGNITION of BROADCAST NEWS using paradigms from WEB 2.0 to obtain TIME-AND TOPIC-RELEVANT TEXT DATA for LANGUAGE MODELING . we elaborate an UN-SUPERVISED TEXT COLLECTION AND DECODING STRATEGY that includes <unk> appropriate texts from RSS FEEDS , complementing UN-SUPERVISED TEXT COLLECTION AND DECODING STRATEGY with texts from TWITTER , LANGUAGE MODEL and vocabulary adaptation , as well as a 2-PASS DECODING . the WORD ERROR RATES of the tested FRENCH BROADCAST NEWS SHOWS from europe 1 are reduced by almost 32 % relative with an underlying LANGUAGE MODEL from the <unk> project -lsb- 1 -rsb- and by almost 4 % with an underlying LANGUAGE MODEL from the QUAERO PROJECT . the UN-SUPERVISED TEXT COLLECTION AND DECODING STRATEGY that we use for the TEXT NORMALIZATION , the collection of RSS FEEDS together with the text on the related websites , a TF-IDF-BASED TOPIC WORDS EXTRACTION , as well as the opportunity for LANGUAGE MODEL INTERPOLATION are available in our RAPID LANGUAGE ADAPTATION TOOLKIT -lsb- 2 -rsb- -lsb- 3 -rsb- . \n",
            "this paper presents a UN-SUPERVISED TEXT COLLECTION AND DECODING STRATEGY for AUTOMATIC SPEECH RECOGNITION . the RAPID LANGUAGE ADAPTATION TOOLKIT is based on the RAPID LANGUAGE ADAPTATION TOOLKIT and the LANGUAGE MODEL . the RAPID LANGUAGE ADAPTATION TOOLKIT is based on the RAPID LANGUAGE ADAPTATION TOOLKIT and the RAPID LANGUAGE ADAPTATION TOOLKIT . the RAPID LANGUAGE ADAPTATION TOOLKIT is based on the RAPID LANGUAGE ADAPTATION TOOLKIT and the RAPID LANGUAGE ADAPTATION TOOLKIT . the RAPID LANGUAGE ADAPTATION TOOLKIT is based on the RAPID LANGUAGE ADAPTATION TOOLKIT and the RAPID LANGUAGE ADAPTATION TOOLKIT . the RAPID LANGUAGE ADAPTATION TOOLKIT is evaluated on the FRENCH BROADCAST NEWS SHOWS . the results show that the proposed UN-SUPERVISED TEXT COLLECTION AND DECODING STRATEGY is effective for AUTOMATIC SPEECH RECOGNITION in FRENCH BROADCAST NEWS SHOWS .\n",
            "\n",
            "936 1000\n",
            "this paper describes a WIDE-COVERAGE STATISTICAL PARSER that uses COMBINATORY CATEGORIAL GRAMMAR to derive DEPENDENCY STRUCTURES . the WIDE-COVERAGE STATISTICAL PARSER differs from most existing WIDE-COVERAGE TREE-BANK PARSERS in capturing the LONG-RANGE DEPENDENCIES inherent in constructions such as COORDINATION , EXTRACTION , raising and CONTROL , as well as the standard LOCAL PREDICATE-ARGUMENT DEPENDENCIES . a set of DEPENDENCY STRUCTURES used for training and testing the WIDE-COVERAGE STATISTICAL PARSER is obtained from a TREEBANK OF CCG NORMAL-FORM DERIVATIONS , which have been derived -lrb- semi - -rrb- automatically from the PENN TREEBANK . the WIDE-COVERAGE STATISTICAL PARSER correctly recovers over 80 % of LABELLED DEPENDENCIES , and around 90 % of UNLABELLED DEPENDENCIES . \n",
            "this paper presents a WIDE-COVERAGE STATISTICAL PARSER for EXTRACTION . the proposed WIDE-COVERAGE STATISTICAL PARSER is based on the TREEBANK OF CCG NORMAL-FORM DERIVATIONS and the TREEBANK OF CCG NORMAL-FORM DERIVATIONS . the proposed WIDE-COVERAGE STATISTICAL PARSER is based on the TREEBANK OF CCG NORMAL-FORM DERIVATIONS . the proposed WIDE-COVERAGE STATISTICAL PARSER is based on the TREEBANK OF CCG NORMAL-FORM DERIVATIONS . the proposed WIDE-COVERAGE STATISTICAL PARSER is based on the TREEBANK OF CCG NORMAL-FORM DERIVATIONS . the proposed WIDE-COVERAGE STATISTICAL PARSER is compared with other WIDE-COVERAGE TREE-BANK PARSERS and WIDE-COVERAGE TREE-BANK PARSERS . the proposed WIDE-COVERAGE STATISTICAL PARSER is compared with other WIDE-COVERAGE TREE-BANK PARSERS and WIDE-COVERAGE TREE-BANK PARSERS .\n",
            "\n",
            "937 1000\n",
            "a fundamental challenge in the design of OBJECTIVE MODELS for ESTIMATION OF SPEECH SIGNAL QUALITY lies in the shortage of SUBJECTIVELY LABELLED DATABASES . this problem is particularly relevant when developing QUALITY ASSESSMENT MODELS for wide-band -lrb- 16 khz sampling rate -rrb- signals where DATABASES are scarce . we explore the possibility for seamlessly integrating a QUALITY PRIOR in the form of a NARROW-BAND QUALITY ESTIMATE into the framework of a NON-INTRUSIVE WIDE-BAND QUALITY ASSESSMENT ALGORITHM . experimental results confirm that the proposed NON-INTRUSIVE WIDE-BAND QUALITY ASSESSMENT ALGORITHM can be used to improve performance over a BASELINE WIDE-BAND SYSTEM without a NARROW-BAND PRIOR . \n",
            "this paper presents a novel NON-INTRUSIVE WIDE-BAND QUALITY ASSESSMENT ALGORITHM for ESTIMATION OF SPEECH SIGNAL QUALITY . the proposed NON-INTRUSIVE WIDE-BAND QUALITY ASSESSMENT ALGORITHM is based on a NARROW-BAND PRIOR and a NARROW-BAND PRIOR . the proposed NON-INTRUSIVE WIDE-BAND QUALITY ASSESSMENT ALGORITHM is based on a NARROW-BAND PRIOR and a NARROW-BAND PRIOR . the proposed NON-INTRUSIVE WIDE-BAND QUALITY ASSESSMENT ALGORITHM is compared with the conventional BASELINE WIDE-BAND SYSTEM and the NON-INTRUSIVE WIDE-BAND QUALITY ASSESSMENT ALGORITHM . the proposed NON-INTRUSIVE WIDE-BAND QUALITY ASSESSMENT ALGORITHM is compared with the conventional BASELINE WIDE-BAND SYSTEM .\n",
            "\n",
            "938 1000\n",
            "this paper reports an investigation of FEATURES relevant for classifying two SPEAKING STYLES , namely , CONVERSATIONAL SPEAKING STYLE and clear -lrb- e.g. HYPER-ARTICULATED -RRB- SPEAKING STYLE . SPECTRAL AND PROSODIC FEATURES were automatically extracted from speech and classified using DECISION TREE CLASSIFIERS and MULTI-LAYER PERCEPTRONS to achieve ACCURACIES of about 71 % and 77 % respectively . more interestingly , we found that out of the 56 FEATURES only about 9 FEATURES are needed to capture the most PREDICTIVE POWER . while perceptual studies have shown that SPECTRAL CUES are more useful than PROSODIC FEATURES for <unk> -lsb- 1 -rsb- , here we find PROSODIC FEATURES are more important for CLASSIFICATION . \n",
            "this paper addresses the problem of CLASSIFICATION in SPEAKING STYLES . we propose a method for CLASSIFICATION based on MULTI-LAYER PERCEPTRONS . the proposed method is based on the use of PROSODIC FEATURES and SPECTRAL CUES . the proposed method is based on the use of PROSODIC FEATURES and SPECTRAL CUES . the proposed method is compared with conventional FEATURES and MULTI-LAYER PERCEPTRONS .\n",
            "\n",
            "939 1000\n",
            "<unk> approaches allow a CROWDSOURCING SYSTEM to identify reliable workers to whom tasks can be <unk> . in CROWDSOURCING SYSTEM that can be modeled as MULTI-AGENT TRUST NETWORKS consist of RESOURCE CONSTRAINED TRUSTEE AGENTS -lrb- i.e. , workers -rrb- , workers may need to further <unk> tasks to others if they determine that they can not complete all pending tasks before the <unk> <unk> . existing REPUTATION-BASED DECISION-MAKING MODELS can not help workers decide when and to whom to <unk> tasks . in this paper , we proposed a REPUTATION AWARE TASK SUB-DELEGATION APPROACH to bridge this gap . by jointly considering a WORKER 'S REPUTATION , WORKLOAD , the price of its effort and its trust relationships with others , REPUTATION AWARE TASK SUB-DELEGATION APPROACH can be implemented as an INTELLIGENT AGENT to help workers make SUB-DELEGATION DECISIONS in a distributed manner . the resulting task allocation maximizes social welfare through efficient utilization of the collective capacity of a crowd , and provides provable performance guarantees . experimental comparisons with state-of-the-art approaches based on the EPINIONS TRUST NETWORK demonstrate significant advantages of REPUTATION AWARE TASK SUB-DELEGATION APPROACH under HIGH WORKLOAD CONDITIONS . \n",
            "this paper presents a new REPUTATION AWARE TASK SUB-DELEGATION APPROACH for MULTI-AGENT TRUST NETWORKS . the REPUTATION AWARE TASK SUB-DELEGATION APPROACH is based on the REPUTATION AWARE TASK SUB-DELEGATION APPROACH . the REPUTATION AWARE TASK SUB-DELEGATION APPROACH is based on the REPUTATION AWARE TASK SUB-DELEGATION APPROACH . the proposed REPUTATION AWARE TASK SUB-DELEGATION APPROACH is based on the REPUTATION AWARE TASK SUB-DELEGATION APPROACH . the proposed REPUTATION AWARE TASK SUB-DELEGATION APPROACH is based on the REPUTATION AWARE TASK SUB-DELEGATION APPROACH . the proposed REPUTATION AWARE TASK SUB-DELEGATION APPROACH is based on the REPUTATION AWARE TASK SUB-DELEGATION APPROACH . the proposed REPUTATION AWARE TASK SUB-DELEGATION APPROACH is applied to the REPUTATION AWARE TASK SUB-DELEGATION APPROACH and is shown to outperform the conventional REPUTATION-BASED APPROACHES .\n",
            "\n",
            "940 1000\n",
            "traditional MICROPHONE ARRAY SPEECH RECOGNITION SYSTEMS simply recognise the enhanced output of the array . as the level of SIGNAL ENHANCEMENT depends on the number of microphones , such MICROPHONE ARRAY SPEECH RECOGNITION SYSTEMS do not achieve acceptable SPEECH RECOGNITION performance for arrays having only a few microphones . for SMALL MICROPHONE ARRAYS , we instead propose using the enhanced output to estimate a RELIABILITY MASK , which is then used in MISSING DATA SPEECH RECOGNITION . in MISSING DATA SPEECH RECOGNITION , the DECODED SEQUENCE depends on the RELIABILITY MASK of each input feature . this RELIABILITY MASK is usually based on the signal to noise ratio in each frequency band . in this paper , we use the energy difference between the NOISY INPUT and the enhanced output of a small microphone array to determine the FREQUENCY BAND RELIABILITY . RECOGNITION experiments with a small array demonstrate the effectiveness of the technique , compared to both traditional MICROPHONE ARRAY ENHANCEMENT and a BASELINE MISSING DATA SYSTEM . \n",
            "this paper presents a new method for MISSING DATA SPEECH RECOGNITION in MICROPHONE ARRAY SPEECH RECOGNITION SYSTEMS . the proposed method is based on the RELIABILITY MASK and the RELIABILITY MASK . the proposed method is based on the RELIABILITY MASK and the RELIABILITY MASK . the proposed method is based on the RELIABILITY MASK and the RELIABILITY MASK . the proposed method is based on a BASELINE MISSING DATA SYSTEM and a BASELINE MISSING DATA SYSTEM . the proposed method is tested on a BASELINE MISSING DATA SYSTEM and a BASELINE MISSING DATA SYSTEM .\n",
            "\n",
            "941 1000\n",
            "this paper presents a novel approach for recognizing and interpreting dimensions in ENGINEERING DRAWINGS . it starts by detecting potential DIMENSION FRAMES , each comprising only the line and text components of a dimension , then verifies them by detecting the DIMENSION SYMBOLS . by removing the prerequisite of SYMBOL RECOGNITION from DETECTION OF DIMENSION SETS , our method is capable of handling LOW QUALITY DRAWINGS . we also propose a RECONSTRUCTION ALGORITHM for rebuilding the DRAWING ENTITIES based on the RECOGNIZED DIMENSION ANNOTATIONS . a COORDINATE GRID STRUCTURE is introduced to represent and analyze TWO-DIMENSIONAL SPATIAL CONSTRAINTS between entities ; this simplifies and unifies the process of RECTIFYING DEVIATIONS OF ENTITY DIMENSIONS induced during SCANNING and VECTORIZATION . \n",
            "this paper presents a novel RECONSTRUCTION ALGORITHM for SYMBOL RECOGNITION . the RECONSTRUCTION ALGORITHM is based on the RECONSTRUCTION ALGORITHM and the RECONSTRUCTION ALGORITHM . the proposed RECONSTRUCTION ALGORITHM is based on the RECONSTRUCTION ALGORITHM and the RECONSTRUCTION ALGORITHM . the proposed RECONSTRUCTION ALGORITHM is based on the RECONSTRUCTION ALGORITHM and the RECONSTRUCTION ALGORITHM . the proposed RECONSTRUCTION ALGORITHM is based on the RECONSTRUCTION ALGORITHM and the RECONSTRUCTION ALGORITHM . the proposed RECONSTRUCTION ALGORITHM is based on the RECONSTRUCTION ALGORITHM and the RECONSTRUCTION ALGORITHM . experimental results show the effectiveness of the proposed RECONSTRUCTION ALGORITHM .\n",
            "\n",
            "942 1000\n",
            "in the present paper , a <unk> markov model -lrb- HSMM TRAINING -rrb- based speech synthesis system is proposed . in a hidden markov model -lrb- HMM -rrb- based speech synthesis system which we have proposed , RHYTHM AND TEMPO are controlled by STATE DURATION PROBABILITY DISTRIBUTIONS modeled by SINGLE GAUSSIAN DISTRIBUTIONS . to synthesis speech , it constructs a SENTENCE HMM corresponding to an <unk> given text and determine STATE DURATIONS maximizing their probabilities , then a SPEECH PARAMETER VECTOR SEQUENCE is generated for the given state sequence . however , there is an inconsistency : although the speech is synthesized from HMMS with EXPLICIT STATE DURATION PROBABILITY DISTRIBUTIONS , HMMS are trained without them . in the present paper , we introduce an HSMM TRAINING , which is an HMM with EXPLICIT STATE DURATION PROBABILITY DISTRIBUTIONS , into the HMM-BASED SPEECH SYNTHESIS SYSTEM . experimental results show that the use of HSMM TRAINING improves the naturalness of the SYNTHESIZED SPEECH . \n",
            "this paper presents a method for HSMM TRAINING based on SINGLE GAUSSIAN DISTRIBUTIONS . the proposed method is based on the STATE DURATION PROBABILITY DISTRIBUTIONS of the HMM . the proposed method is based on the use of SINGLE GAUSSIAN DISTRIBUTIONS and the STATE DURATION PROBABILITY DISTRIBUTIONS . the proposed method is based on the STATE DURATION PROBABILITY DISTRIBUTIONS and the STATE DURATION PROBABILITY DISTRIBUTIONS . the proposed method is based on the STATE DURATION PROBABILITY DISTRIBUTIONS . the proposed method is applied to the HMM-BASED SPEECH SYNTHESIS SYSTEM . the experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "943 1000\n",
            "we propose an improved maximum a POSTERIORI LEARNING ALGORITHM of CONTINUOUS-DENSITY HIDDEN MARKOV MODEL PARAMETERS for SPEAKER ADAPTATION . the algorithm is developed by sequentially combining three ADAPTATION APPROACHES . first , the clusters of SPEAKER-INDEPENDENT HMM PARAMETERS are locally transformed through a group of TRANSFORMATION FUNCTIONS . then , the transformed HMM PARAMETERS are globally smoothed via the MAP ADAPTATION . within the MAP ADAPTATION , the parameters of UNSEEN UNITS in ADAPTATION DATA are further adapted by employing the TRANSFER VECTOR INTERPOLATION SCHEME . experiments show that the combined algorithm converges rapidly and outperforms those other ADAPTATION METHODS . \n",
            "this paper addresses the problem of SPEAKER ADAPTATION in UNSEEN UNITS . in this paper , we propose a POSTERIORI LEARNING ALGORITHM based on TRANSFORMATION FUNCTIONS . the proposed TRANSFER VECTOR INTERPOLATION SCHEME is based on a POSTERIORI LEARNING ALGORITHM , which is based on a POSTERIORI LEARNING ALGORITHM . the proposed TRANSFER VECTOR INTERPOLATION SCHEME is based on the use of a POSTERIORI LEARNING ALGORITHM and a POSTERIORI LEARNING ALGORITHM for SPEAKER ADAPTATION . experimental results demonstrate the effectiveness of the proposed method .\n",
            "\n",
            "944 1000\n",
            "the COMPOSITIONAL NATURE OF VISUAL OBJECTS significantly limits their REPRESENTATION COMPLEXITY and renders learning of STRUCTURED OBJECT MODELS tractable . adopting this MOD-ELING STRATEGY we both -lrb- i -rrb- automatically decompose objects into a HIERARCHY OF RELEVANT COMPOSITIONS and we -lrb- ii -rrb- learn such a COMPOSITIONAL REPRESENTATION for each category without SUPERVISION . the COMPOSITIONAL REPRESENTATION supports FEATURE SHARING already on the lowest level of small image patches . COMPOSITIONS are represented as PROBABILITY DISTRIBUTIONS over their constituent parts and the relations between them . the GLOBAL SHAPE OF OBJECTS is captured by a GRAPHICAL MODEL which combines all compositions . INFERENCE based on the underlying STATISTICAL MODEL is then employed to obtain a CATEGORY LEVEL OBJECT RECOGNITION SYSTEM . experiments on LARGE STANDARD BENCHMARK DATASETS underline the competitive recognition performance of this GRAPHICAL MODEL and GRAPHICAL MODEL provide insights into the learned COMPO-SITIONAL STRUCTURE OF OBJECTS . \n",
            "this paper presents a novel MOD-ELING STRATEGY for COMPO-SITIONAL STRUCTURE OF OBJECTS in STRUCTURED OBJECT MODELS . the STATISTICAL MODEL is based on a STATISTICAL MODEL of the COMPOSITIONAL REPRESENTATION . the STATISTICAL MODEL is based on the COMPOSITIONAL NATURE OF VISUAL OBJECTS of the GRAPHICAL MODEL . the STATISTICAL MODEL is based on the STATISTICAL MODEL and the STATISTICAL MODEL . the proposed MOD-ELING STRATEGY is based on a GRAPHICAL MODEL and is applied to the COMPOSITIONAL NATURE OF VISUAL OBJECTS . the proposed MOD-ELING STRATEGY is based on a STATISTICAL MODEL and is shown to be useful for INFERENCE .\n",
            "\n",
            "945 1000\n",
            "action recognition in VIDEOS is a challenging task due to the complexity of the SPATIO-TEMPORAL PATTERNS to model and the difficulty to acquire and learn on large quantities of VIDEO DATA . DEEP LEARNING , although a breakthrough for IMAGE CLASSIFICATION and showing promise for VIDEOS , has still not clearly <unk> ACTION RECOGNITION METHODS using HAND-CRAFTED FEATURES , even when training on massive datasets . in this paper , we introduce HYBRID VIDEO CLASSIFICATION ARCHITEC-TURES based on carefully designed UNSUPERVISED REPRESENTATIONS OF HAND-CRAFTED SPATIO-TEMPORAL FEATURES classified by SUPERVISED DEEP NETWORKS . as we show in our experiments on five popular benchmarks for ACTION RECOGNITION , our HYBRID MODEL combines the best of both worlds : it is data efficient -lrb- trained on 150 to <unk> short clips -rrb- and yet improves significantly on the state of the art , including recent DEEP MODELS trained on millions of MANUALLY LABELLED IMAGES and VIDEOS . \n",
            "this paper addresses the problem of ACTION RECOGNITION from VIDEO DATA . we propose a HYBRID MODEL based on SUPERVISED DEEP NETWORKS for ACTION RECOGNITION . the HYBRID MODEL is based on the use of HAND-CRAFTED FEATURES and HAND-CRAFTED FEATURES . the proposed HYBRID MODEL is based on a HYBRID MODEL , which is based on the UNSUPERVISED REPRESENTATIONS OF HAND-CRAFTED SPATIO-TEMPORAL FEATURES . experimental results demonstrate the effectiveness of the proposed HYBRID MODEL .\n",
            "\n",
            "946 1000\n",
            "<unk> -lrb- <unk> information -rrb- is an <unk> <unk> spoken document RETRIEVAL -lrb- CL-SDR -rrb- system developed during the johns hopkins university summer workshop 2000 . we integrate SPEECH RECOGNITION , MACHINE TRANSLATION , and INFORMATION RETRIEVAL TECHNOLOGIES to perform CL-SDR . MEI -LRB- MANDARIN-ENGLISH INFORMATION -RRB- advocates a MULTI-SCALE PARADIGM , where both CHINESE WORDS and <unk> -lrb- characters and syllables -rrb- are used in RETRIEVAL . the use of SUBWORD UNITS can complement the WORD UNIT in handling the problems of CHINESE WORD TOKENIZATION AMBIGUITY , CHINESE HOMOPHONE AMBIGUITY , and OUT-OF-VOCABULARY WORDS in AUDIO INDEXING . this paper focuses on MULTI-SCALE AUDIO INDEXING in MEI -LRB- MANDARIN-ENGLISH INFORMATION -RRB- . experiments are based on the topic detection and tracking corpora -lrb- <unk> and <unk> -rrb- , where we indexed VOICE OF AMERICA MANDARIN NEWS BROADCASTS by SPEECH RECOGNITION on both the WORD AND SUBWORD SCALES . in this paper , we discuss the development of the MEI SYLLABLE RECOGNIZER , the representations of SPOKEN DOCUMENTS using OVERLAPPING SUBWORD N-GRAMS and LATTICE STRUCTURES . results show that augmenting words with <unk> is beneficial to CL-SDR performance . \n",
            "this paper presents a new method for VOICE OF AMERICA MANDARIN NEWS BROADCASTS , which is based on MEI -LRB- MANDARIN-ENGLISH INFORMATION -RRB- and MEI -LRB- MANDARIN-ENGLISH INFORMATION -RRB- . the proposed method is based on MEI -LRB- MANDARIN-ENGLISH INFORMATION -RRB- and MEI -LRB- MANDARIN-ENGLISH INFORMATION -RRB- . the proposed method is based on the MULTI-SCALE PARADIGM and the MULTI-SCALE PARADIGM . the proposed method is based on MEI -LRB- MANDARIN-ENGLISH INFORMATION -RRB- and MEI -LRB- MANDARIN-ENGLISH INFORMATION -RRB- . the proposed method is based on MEI -LRB- MANDARIN-ENGLISH INFORMATION -RRB- and MEI -LRB- MANDARIN-ENGLISH INFORMATION -RRB- . the proposed method is evaluated on the VOICE OF AMERICA MANDARIN NEWS BROADCASTS . the results show that the proposed method is robust and robust to OUT-OF-VOCABULARY WORDS , RETRIEVAL , and RETRIEVAL .\n",
            "\n",
            "947 1000\n",
            "we describe a ROBUST SPEECH UNDERSTANDING SYSTEM based on our newly developed approach to SPOKEN LANGUAGE PROCESSING . we show that a robust NLU SYSTEM can be rapidly developed using a relatively simple SPEECH RECOGNIZER to provide sufficient information for DATABASE RETRIEVAL by SPOKEN LANGUAGE PROCESSING . our experimental ROBUST SPEECH UNDERSTANDING SYSTEM consists of three components : a SPEECH RECOGNIZER based on HMM , a NATURAL LANGUAGE PARSER based on CONCEPTUAL RELATIONAL GRAMMAR and a DATA RETRIEVAL SYSTEM based on the ATIS DATABASE . with the use of the ROBUST SPEECH UNDERSTANDING SYSTEM , DATABASE QUERY TASKS can be successfully performed . \n",
            "this paper presents a ROBUST SPEECH UNDERSTANDING SYSTEM for SPOKEN LANGUAGE PROCESSING . the ROBUST SPEECH UNDERSTANDING SYSTEM consists of a CONCEPTUAL RELATIONAL GRAMMAR , a CONCEPTUAL RELATIONAL GRAMMAR , and a NATURAL LANGUAGE PARSER . the DATA RETRIEVAL SYSTEM is based on the CONCEPTUAL RELATIONAL GRAMMAR and a NATURAL LANGUAGE PARSER for SPOKEN LANGUAGE PROCESSING . the DATA RETRIEVAL SYSTEM is based on a CONCEPTUAL RELATIONAL GRAMMAR and a DATA RETRIEVAL SYSTEM . the DATA RETRIEVAL SYSTEM is evaluated on a ATIS DATABASE and a ATIS DATABASE .\n",
            "\n",
            "948 1000\n",
            "the UNSHIELDED TWISTED PAIR can be used as a TRANSMISSION MEDIA for LOCAL DISTRIBUTION NETWORKS . to maintain a high transmission throughput , an ANALOG or a DIGITAL ADAPTIVE CHANNEL EQUALIZER is usually required in the RECEIVER to minimize the effect of INTER-SYMBOL INTERFERENCE . under the observation that the high sampling rate high precision <unk> and subsequent DIGITAL ADAPTIVE SIGNAL PROCESSING is an expensive approach , a DIRECT EQUALIZATION METHOD , where the EQUALIZER is implemented in the TRANSMITTER , is proposed for SYMMETRICAL TWISTED PAIR TRANSMISSION CHANNELS . this DIRECT EQUALIZATION METHOD can also be applied to the ANALOG EQUALIZATION APPROACH for REDUCED SYSTEM COMPLEXITY . \n",
            "this paper presents a novel ANALOG EQUALIZATION APPROACH for DIGITAL ADAPTIVE SIGNAL PROCESSING . the ANALOG EQUALIZATION APPROACH is based on a EQUALIZER , which is a RECEIVER of the TRANSMITTER . the proposed ANALOG EQUALIZATION APPROACH is based on a DIRECT EQUALIZATION METHOD , which is based on the EQUALIZER . the proposed ANALOG EQUALIZATION APPROACH is based on a DIRECT EQUALIZATION METHOD . the proposed ANALOG EQUALIZATION APPROACH is based on a DIRECT EQUALIZATION METHOD . the proposed ANALOG EQUALIZATION APPROACH is based on a DIRECT EQUALIZATION METHOD . the proposed ANALOG EQUALIZATION APPROACH is based on a DIRECT EQUALIZATION METHOD and is shown to be useful for DIGITAL ADAPTIVE SIGNAL PROCESSING .\n",
            "\n",
            "949 1000\n",
            "semi-supervised learning algorithms commonly incorporate the available BACKGROUND KNOWLEDGE such that an expression of the derived model 's quality is improved . depending on the specific CONTEXT QUALITY can take several forms and can be related to the generalization performance or to a simple CLUSTERING COHERENCE MEASURE . recently , a novel perspective of SEMI-SUPERVISED LEARNING has been put forward , that associates SEMI-SUPERVISED CLUSTERING with the efficiency of SPECTRAL METHODS . more precisely , it has been demonstrated that the appropriate use of PARTIAL SUPERVISION can bias the DATA LAPLACIAN MATRIX such that the necessary EIGENVEC-TOR COMPUTATIONS are provably accelerated . this result allows DATA MINING PRACTITIONERS to use BACKGROUND KNOWLEDGE not only for improving the quality of clustering results , but also for accelerating the required EIGENVEC-TOR COMPUTATIONS . in this paper we initially provide a high level overview of the relevant efficiency maximizing SEMI-SUPERVISED METHODS such that their THEORETICAL INTUITIONS are comprehensively outlined . consecutively , we demonstrate how these methods can be extended to handle multiple clusters and also discuss possible issues that may arise in the CONTINUOUS SEMI-SUPERVISED SOLUTION . finally , we illustrate the proposed extensions empirically in the context of TEXT CLUSTERING . \n",
            "this paper addresses the problem of DATA MINING PRACTITIONERS for TEXT CLUSTERING . we propose a new CLUSTERING COHERENCE MEASURE based on the DATA LAPLACIAN MATRIX of the DATA LAPLACIAN MATRIX . the proposed method is based on the DATA LAPLACIAN MATRIX of the DATA LAPLACIAN MATRIX . the proposed method is based on the CLUSTERING COHERENCE MEASURE of the DATA LAPLACIAN MATRIX . the proposed method is based on the CLUSTERING COHERENCE MEASURE of the DATA LAPLACIAN MATRIX . the proposed method is shown to outperform the conventional SEMI-SUPERVISED METHODS in terms of CONTEXT QUALITY and CONTEXT QUALITY .\n",
            "\n",
            "950 1000\n",
            "we describe a UNIFIED FORMULATION and algorithm to find an extremely sparse representation for CALCIUM IMAGE SEQUENCES in terms of CELL LOCATIONS , CELL SHAPES , SPIKE TIMINGS and IMPULSE RESPONSES . solution of a single OPTIMIZATION PROBLEM yields CELL SEGMENTATIONS and ACTIVITY ESTIMATES that are on par with the state of the art , without the need for HEURISTIC PRE-OR POSTPROCESSING . experiments on REAL AND SYNTHETIC DATA demonstrate the viability of the proposed method . \n",
            "this paper presents a UNIFIED FORMULATION for CALCIUM IMAGE SEQUENCES . the proposed UNIFIED FORMULATION is based on the UNIFIED FORMULATION and the UNIFIED FORMULATION . the proposed UNIFIED FORMULATION is based on a UNIFIED FORMULATION and a UNIFIED FORMULATION . the proposed UNIFIED FORMULATION is based on a UNIFIED FORMULATION and HEURISTIC PRE-OR POSTPROCESSING . the proposed UNIFIED FORMULATION is based on a UNIFIED FORMULATION and HEURISTIC PRE-OR POSTPROCESSING .\n",
            "\n",
            "951 1000\n",
            "we have previously proposed a cross-validation -lrb- CV -rrb- based gaussian mixture optimization method that efficiently optimizes the MODEL STRUCTURE based on CV LIKELIHOOD . in this study , we propose AGGREGATED CROSS-VALIDATION that introduces a BAGGING-LIKE APPROACH in the CV FRAMEWORK to reinforce the MODEL SELECTION ABILITY . while a single model is used in CV to evaluate a HELD-OUT SUBSET , AGGREGATED CROSS-VALIDATION uses multiple models to reduce the variance in the SCORE ESTIMATION . by integrating AGGREGATED CROSS-VALIDATION instead of CV in the GAUSSIAN MIXTURE OPTIMIZATION ALGORITHM , an AGCV LIKELIHOOD BASED GAUSSIAN MIXTURE OPTIMIZATION ALGORITHM is obtained . the AGCV LIKELIHOOD BASED GAUSSIAN MIXTURE OPTIMIZATION ALGORITHM works efficiently by using sufficient statistics and can be applied to LARGE MODELS such as GAUSSIAN MIXTURE HMM . the proposed AGCV LIKELIHOOD BASED GAUSSIAN MIXTURE OPTIMIZATION ALGORITHM is evaluated by SPEECH RECOGNITION experiments on ORAL PRESENTATIONS and it is shown that lower WORD ERROR RATES are obtained by the AGCV LIKELIHOOD BASED GAUSSIAN MIXTURE OPTIMIZATION ALGORITHM when compared to CV AND MDL BASED METHODS . \n",
            "this paper proposes a new AGCV LIKELIHOOD BASED GAUSSIAN MIXTURE OPTIMIZATION ALGORITHM for SPEECH RECOGNITION . the proposed AGCV LIKELIHOOD BASED GAUSSIAN MIXTURE OPTIMIZATION ALGORITHM is based on a GAUSSIAN MIXTURE OPTIMIZATION ALGORITHM , which is based on the AGCV LIKELIHOOD BASED GAUSSIAN MIXTURE OPTIMIZATION ALGORITHM . the proposed AGCV LIKELIHOOD BASED GAUSSIAN MIXTURE OPTIMIZATION ALGORITHM is based on the AGCV LIKELIHOOD BASED GAUSSIAN MIXTURE OPTIMIZATION ALGORITHM . the proposed AGCV LIKELIHOOD BASED GAUSSIAN MIXTURE OPTIMIZATION ALGORITHM is based on the AGCV LIKELIHOOD BASED GAUSSIAN MIXTURE OPTIMIZATION ALGORITHM . the proposed AGCV LIKELIHOOD BASED GAUSSIAN MIXTURE OPTIMIZATION ALGORITHM is based on the AGCV LIKELIHOOD BASED GAUSSIAN MIXTURE OPTIMIZATION ALGORITHM . the proposed AGCV LIKELIHOOD BASED GAUSSIAN MIXTURE OPTIMIZATION ALGORITHM is compared with the conventional CV AND MDL BASED METHODS . the proposed AGCV LIKELIHOOD BASED GAUSSIAN MIXTURE OPTIMIZATION ALGORITHM is compared with the conventional CV AND MDL BASED METHODS . the proposed AGCV LIKELIHOOD BASED GAUSSIAN MIXTURE OPTIMIZATION ALGORITHM is compared with conventional CV AND MDL BASED METHODS and the CV AND MDL BASED METHODS .\n",
            "\n",
            "952 1000\n",
            "we introduce the TERM COSEGMENTATION which denotes the task of segmenting simultaneously the common parts of an IMAGE PAIR . a GENERATIVE MODEL for COSEGMENTATION is presented . INFERENCE in the GENERATIVE MODEL leads to minimizing an energy with an MRF TERM ENCODING SPATIAL COHERENCY and a GLOBAL CONSTRAINT which attempts to match the APPEARANCE HISTOGRAMS of the common parts . this energy has not been proposed previously and its OPTIMIZATION is challenging and NP-HARD . for this TERM COSEGMENTATION a novel OPTIMIZATION SCHEME which we call TRUST REGION GRAPH CUTS is presented . we demonstrate that this OPTIMIZATION SCHEME has the potential to improve a wide range of research : OBJECT DRIVEN IMAGE RETRIEVAL , VIDEO TRACKING and segmentation , and INTERACTIVE IMAGE EDITING . the power of the OPTIMIZATION SCHEME lies in its generality , the common part can be a RIGID/NON-RIGID OBJECT -lrb- or scene -rrb- , observed from different viewpoints or even similar objects of the same class . \n",
            "this paper presents a GENERATIVE MODEL for OBJECT DRIVEN IMAGE RETRIEVAL . the GENERATIVE MODEL is based on the GENERATIVE MODEL and the GENERATIVE MODEL . the OPTIMIZATION SCHEME is based on the GENERATIVE MODEL and the GENERATIVE MODEL . the proposed OPTIMIZATION SCHEME is based on the GENERATIVE MODEL and the GENERATIVE MODEL . the proposed OPTIMIZATION SCHEME is based on the GENERATIVE MODEL and the GENERATIVE MODEL . the proposed OPTIMIZATION SCHEME is based on the GENERATIVE MODEL and the OPTIMIZATION SCHEME . the proposed OPTIMIZATION SCHEME is based on a GENERATIVE MODEL and is shown to be robust to COSEGMENTATION and VIDEO TRACKING .\n",
            "\n",
            "953 1000\n",
            "in this paper , we present a novel approach for RELATION EXTRACTION using only TERM PAIRS as the input without TEXTUAL FEATURES . we aim to build a single JOINT SPACE for each relation which is then used to produce RELATION SPECIFIC TERM EMBEDDINGS . the proposed method fits particularly well for domains in which similar arguments are often associated with similar relations . it can also handle the situation when the LABELED DATA is limited . the proposed method is evaluated both theoretically with a proof for the CLOSED-FORM SOLUTION and experimentally with promising results on both DBPEDIA AND MEDICAL RELATIONS . \n",
            "this paper addresses the problem of RELATION EXTRACTION in JOINT SPACE . we propose a method for RELATION EXTRACTION based on RELATION SPECIFIC TERM EMBEDDINGS . the proposed method is based on the RELATION SPECIFIC TERM EMBEDDINGS and the RELATION SPECIFIC TERM EMBEDDINGS . the proposed method is based on a CLOSED-FORM SOLUTION . the proposed method is evaluated on the DBPEDIA AND MEDICAL RELATIONS .\n",
            "\n",
            "954 1000\n",
            "for PITCH TRACKING of a single speaker , a common requirement is to find the optimal path through a set of VOICED OR VOICELESS PITCH ESTIMATES over a sequence of time frames . DYNAMIC PROGRAMMING ALGORITHMS have been applied before to this problem . here , the PITCH CANDIDATES are provided by a MULTI-CHANNEL AUTOCORRELATION-BASED ESTIMATOR , and DYNAMIC PROGRAMMING ALGORITHMS is extended to PITCH TRACKING of multiple concurrent speakers . we use the resulting PITCH INFORMATION to enhance HARMONIC CONTENT in NOISY SPEECH and to obtain SEPARATIONS OF TARGET from INTERFERING SPEECH . \n",
            "this paper presents a method for PITCH TRACKING from NOISY SPEECH . the MULTI-CHANNEL AUTOCORRELATION-BASED ESTIMATOR is based on a MULTI-CHANNEL AUTOCORRELATION-BASED ESTIMATOR . the MULTI-CHANNEL AUTOCORRELATION-BASED ESTIMATOR is based on the MULTI-CHANNEL AUTOCORRELATION-BASED ESTIMATOR . the proposed MULTI-CHANNEL AUTOCORRELATION-BASED ESTIMATOR is based on the MULTI-CHANNEL AUTOCORRELATION-BASED ESTIMATOR . the proposed method is based on the MULTI-CHANNEL AUTOCORRELATION-BASED ESTIMATOR . the proposed method is based on the MULTI-CHANNEL AUTOCORRELATION-BASED ESTIMATOR . the proposed method is based on the MULTI-CHANNEL AUTOCORRELATION-BASED ESTIMATOR . the proposed method is based on the MULTI-CHANNEL AUTOCORRELATION-BASED ESTIMATOR and is shown to be robust to VOICED OR VOICELESS PITCH ESTIMATES .\n",
            "\n",
            "955 1000\n",
            "finding meaningful , structured representations of 3D POINT CLOUD DATA has become a core task for SPATIAL PERCEPTION APPLICATIONS . in this paper we introduce a method for constructing COMPACT GENERATIVE REPRESENTATIONS OF PCD at multiple levels of detail . as opposed to DETERMINISTIC STRUCTURES such as VOXEL GRIDS or OCTREES , we propose PROBABILISTIC SUBDIVISIONS of the data through LOCAL MIXTURE MODELING , and show how these SUBDIVISIONS can provide a MAXIMUM LIKELIHOOD SEGMENTATION of the data . the final representation is hierarchical , compact , PARA-METRIC , and statistically derived , facilitating RUN-TIME OCCUPANCY CALCULATIONS through STOCHASTIC SAMPLING . unlike traditional DETERMINISTIC SPATIAL SUBDIVISION METHODS , our technique enables DYNAMIC CREATION OF VOXEL GRIDS according the application 's best needs . in contrast to other GENER-ATIVE MODELS for 3D POINT CLOUD DATA , we explicitly enforce SPARSITY among points and mixtures , a technique which we call EXPECTATION SPARSIFICATION . this leads to a highly PARALLEL HIERARCHICAL EXPECTATION MAXIMIZATION ALGORITHM well-suited for the 3D POINT CLOUD DATA and real-time execution . we explore the trade-offs between MODEL FIDELITY and MODEL SIZE at various levels of detail , our tests showing favorable performance when compared to OCTREE AND NDT-BASED METHODS . \n",
            "this paper addresses the problem of DYNAMIC CREATION OF VOXEL GRIDS in VOXEL GRIDS such as PARA-METRIC , SPARSITY , and VOXEL GRIDS . we propose a PARALLEL HIERARCHICAL EXPECTATION MAXIMIZATION ALGORITHM for DYNAMIC CREATION OF VOXEL GRIDS , which is based on the PARALLEL HIERARCHICAL EXPECTATION MAXIMIZATION ALGORITHM . the proposed PARALLEL HIERARCHICAL EXPECTATION MAXIMIZATION ALGORITHM is based on the PARALLEL HIERARCHICAL EXPECTATION MAXIMIZATION ALGORITHM and the PARALLEL HIERARCHICAL EXPECTATION MAXIMIZATION ALGORITHM . the proposed PARALLEL HIERARCHICAL EXPECTATION MAXIMIZATION ALGORITHM is based on the PARALLEL HIERARCHICAL EXPECTATION MAXIMIZATION ALGORITHM and the PARALLEL HIERARCHICAL EXPECTATION MAXIMIZATION ALGORITHM . the proposed PARALLEL HIERARCHICAL EXPECTATION MAXIMIZATION ALGORITHM is based on a COMPACT GENERATIVE REPRESENTATIONS OF PCD and is shown to be more robust to RUN-TIME OCCUPANCY CALCULATIONS than the conventional GENER-ATIVE MODELS .\n",
            "\n",
            "956 1000\n",
            "this paper wants to discuss several aspects of MULTIMODAL/MULTIMEDIA LANGUAGE RESOURCES such as the use of METADATA DESCRIPTIONS for easy location purposes , their COLLABORATIVE ANNOTATION and exploitation via internet , the generation of SYNCHRONIZED MEDIA AND TEXT STREAMS in DISTRIBUTED ENVIRONMENTS , and general annotation formats . these aspects that although they may be discussed independently have to fit together seamlessly to offer users an adequate exploitation environment that is up to the huge amount of data that is available in modern MULTI-MEDIA CORPORA and is able to exploit fully the current technology advancements . \n",
            "this paper addresses the problem of COLLABORATIVE ANNOTATION in DISTRIBUTED ENVIRONMENTS . we propose a method for COLLABORATIVE ANNOTATION based on METADATA DESCRIPTIONS . the proposed method is based on the use of METADATA DESCRIPTIONS , which can be used for COLLABORATIVE ANNOTATION . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "957 1000\n",
            "for high performance CDMA COMMUNICATIONS , MULTIUSER DETECTION is often required to suppress the multiple access interference <unk> . most MULTIUSER DETECTORS rely on accurate channel information to recover the MULTIUSER DIGITAL SIGNALS . this paper studies the BLIND CHANNEL ESTIMATION PROBLEM for DS-CDMA SYSTEMS using APERIODIC SPREADING CODES . the MAXIMUM LIKELIHOOD ML ESTIMATOR is formulated for CHANNEL ESTIMATION . we rst convert the MULTIUSER PARAMETER ESTIMATION PROBLEM into a set of single user optimization problems via ALTERNATING OPTIMIZATION , and then determine the CHANNEL PARAMETERS for each user using an ITERATIVE ALGORITHM derived . it is shown by COMPUTER SIMULATION that this ITERATIVE ALGORITHM can reach GLOBAL MAXIMA almost always under MEDIUM SNR VALUES . \n",
            "this paper presents a new method for MULTIUSER DETECTION based on APERIODIC SPREADING CODES . the proposed ITERATIVE ALGORITHM is based on the use of APERIODIC SPREADING CODES and a MAXIMUM LIKELIHOOD ML ESTIMATOR for MULTIUSER DETECTION . the proposed ITERATIVE ALGORITHM is based on the MAXIMUM LIKELIHOOD ML ESTIMATOR , which is based on the MAXIMUM LIKELIHOOD ML ESTIMATOR . the proposed ITERATIVE ALGORITHM is based on the MAXIMUM LIKELIHOOD ML ESTIMATOR . the proposed ITERATIVE ALGORITHM is based on the MAXIMUM LIKELIHOOD ML ESTIMATOR . the proposed ITERATIVE ALGORITHM is based on the MAXIMUM LIKELIHOOD ML ESTIMATOR . the proposed ITERATIVE ALGORITHM is applied to the BLIND CHANNEL ESTIMATION PROBLEM .\n",
            "\n",
            "958 1000\n",
            "understanding the DEPENDENCY STRUCTURE of a set of variables is a key component in various SIGNAL PROCESSING APPLICATIONS which involve DATA ASSOCIATION . the simple task of detecting whether any dependency exists is particularly difficult when models of the data are unknown or difficult to characterize because of HIGH-DIMENSIONAL MEASUREMENTS . we review the use of NONPARAMETRIC TESTS for characterizing dependency and how to carry out these tests with HIGH-DIMENSIONAL OBSERVATIONS . in addition we present a method to assess the significance of the tests . \n",
            "this paper addresses the problem of DATA ASSOCIATION in HIGH-DIMENSIONAL MEASUREMENTS . we propose a method for DATA ASSOCIATION based on DATA ASSOCIATION . the method is based on the DEPENDENCY STRUCTURE and the DEPENDENCY STRUCTURE . the proposed method is based on the use of DATA ASSOCIATION .\n",
            "\n",
            "959 1000\n",
            "it has been shown that large gains in SPEECH INTELLIGIBILITY can be obtained by using the BINARY MASK APPROACH which retains the TIME-FREQUENCY UNITS of the mixture signal that are stronger than the INTERFERING NOISE -lrb- <unk> -rrb- -lrb- i.e. , snr > 0 db -rrb- , and removes the T-F UNITS where the INTERFERING NOISE dominates . in this paper , we introduce a new BINARY MASK for improving SPEECH INTELLIGIBILITY based on NOISE DISTORTION CONSTRAINTS . a BINARY MASK is designed to retain NOISE OVERESTIMATED T-F UNITS while discarding noise underestimated T-F UNITS . listening tests were conducted to evaluate the new BINARY MASK in terms of intelligibility . results from the listening tests indicated that large gains in intelligibility can be achieved by the application of the proposed BINARY MASK to NOISE-CORRUPTED SPEECH even at extremely low snr levels -lrb- <unk> db -rrb- . \n",
            "this paper presents a new BINARY MASK APPROACH for NOISE-CORRUPTED SPEECH . the BINARY MASK APPROACH is based on the BINARY MASK APPROACH . the BINARY MASK APPROACH is based on the BINARY MASK APPROACH of the BINARY MASK . the BINARY MASK is based on the BINARY MASK APPROACH of the BINARY MASK . the BINARY MASK APPROACH is applied to the NOISE-CORRUPTED SPEECH . the experimental results show that the proposed BINARY MASK APPROACH is effective for SPEECH INTELLIGIBILITY .\n",
            "\n",
            "960 1000\n",
            "inspired by MULTI-SCALE TENSOR VOTING , a COMPUTATIONAL FRAMEWORK for PERCEPTUAL GROUPING AND SEGMENTATION , we propose an EDGE-DIRECTED TECHNIQUE for COLOR IMAGE SUPER-RESOLUTION given a single LOW-RESOLUTION COLOR IMAGE . our EDGE-DIRECTED TECHNIQUE combines the advantages of EDGE-DIRECTED , RECONSTRUCTION-BASED AND LEARNING-BASED METHODS , and is unique in two ways . first , we consider simultaneously all the three COLOR CHANNELS in our MULTI-SCALE TENSOR VOTING FRAMEWORK to produce a MULTI-SCALE EDGE REPRESENTATION to guide the process of HIGH-RESOLUTION COLOR IMAGE RECONSTRUCTION , which is subject to the BACK PROJECTION CONSTRAINT . fine details are inferred without noticeable blurry or ringing artifacts . second , the inference of HIGH-RESOLUTION CURVES is achieved by MULTI-SCALE TENSOR VOTING , using the DENSE VOTING FIELD as an EDGE-PRESERVING SMOOTHNESS PRIOR which is derived geometrically without any TIME-CONSUMING LEARNING PROCEDURE . qualitative and quantitative results indicate that our EDGE-DIRECTED TECHNIQUE produces convincing results in complex test cases typically used by state-of-the-art IMAGE SUPER-RESOLUTION TECHNIQUES . \n",
            "this paper proposes a MULTI-SCALE TENSOR VOTING FRAMEWORK for COLOR IMAGE SUPER-RESOLUTION . the MULTI-SCALE TENSOR VOTING FRAMEWORK is based on a MULTI-SCALE TENSOR VOTING FRAMEWORK for COLOR IMAGE SUPER-RESOLUTION . the TIME-CONSUMING LEARNING PROCEDURE is based on a MULTI-SCALE TENSOR VOTING FRAMEWORK , which is based on a MULTI-SCALE TENSOR VOTING FRAMEWORK . the proposed EDGE-DIRECTED TECHNIQUE is based on a MULTI-SCALE TENSOR VOTING FRAMEWORK , which is based on the MULTI-SCALE TENSOR VOTING FRAMEWORK . the proposed EDGE-DIRECTED TECHNIQUE is based on a MULTI-SCALE TENSOR VOTING FRAMEWORK . the proposed EDGE-DIRECTED TECHNIQUE is based on a MULTI-SCALE TENSOR VOTING FRAMEWORK . the proposed EDGE-DIRECTED TECHNIQUE is based on a MULTI-SCALE TENSOR VOTING FRAMEWORK . the proposed EDGE-DIRECTED TECHNIQUE is based on a MULTI-SCALE TENSOR VOTING FRAMEWORK . the proposed EDGE-DIRECTED TECHNIQUE is based on a MULTI-SCALE TENSOR VOTING FRAMEWORK . the proposed EDGE-DIRECTED TECHNIQUE is based on a MULTI-SCALE TENSOR VOTING FRAMEWORK . the proposed EDGE-DIRECTED TECHNIQUE is based on a MULTI-SCALE TENSOR VOTING FRAMEWORK and is shown to be robust to PERCEPTUAL GROUPING AND SEGMENTATION .\n",
            "\n",
            "961 1000\n",
            "in many REAL-TIME APPLICATIONS , SAMPLE VALUES and time <unk> are delivered in pairs , where sampling times are non-uniform . FREQUENCY ANALYSIS using NON-UNIFORM DATA occurs in various REAL LIFE PROBLEMS and EMBEDDED SYSTEMS , such as VIBRA-TIONAL ANALYSIS in cars and control of packet network queue lengths . our contribution is to first overview different ways to approximate the FOURIER TRANSFORM , and secondly to give ANALYTICAL EXPRESSIONS for how NON-UNIFORM SAMPLING affects these approximations . the results are expressed in terms of FREQUENCY WINDOWS describing how a single frequency in the CONTINUOUS TIME SIGNAL is smeared out in the FREQUENCY DOMAIN , or , more precisely , in the expected value of the FOURIER TRANSFORM APPROXIMATION . \n",
            "this paper addresses the problem of NON-UNIFORM SAMPLING in EMBEDDED SYSTEMS . we propose a method for NON-UNIFORM SAMPLING based on NON-UNIFORM SAMPLING . the proposed method is based on the use of NON-UNIFORM SAMPLING , which is based on the FOURIER TRANSFORM APPROXIMATION of the FOURIER TRANSFORM . the proposed method is based on the use of NON-UNIFORM DATA , which is based on the FOURIER TRANSFORM APPROXIMATION . the proposed method is based on the FOURIER TRANSFORM APPROXIMATION , which is based on the FOURIER TRANSFORM APPROXIMATION . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "962 1000\n",
            "to precisely model the time dependency of FEATURES is one of the important issues for SPEECH RECOGNITION . SEGMENTAL UNIT INPUT HMM with a DIMENSIONALITY REDUCTION METHOD is widely used to address this issue . LINEAR DISCRIMINANT ANALYSIS and HETEROSCEDASTIC DISCRIMINANT ANALYSIS are classical and popular approaches to reduce dimensionality . however , it is difficult to find one particular criterion suitable for any kind of data set in carrying out DIMENSION-ALITY REDUCTION while preserving DISCRIMINATIVE INFORMATION . in this paper , we propose a new framework which we call POWER LINEAR DISCRIMINANT ANALYSIS . POWER LINEAR DISCRIMINANT ANALYSIS can describe various criteria including LINEAR DISCRIMINANT ANALYSIS and HETEROSCEDASTIC DISCRIMINANT ANALYSIS with one parameter . experimental results show that the POWER LINEAR DISCRIMINANT ANALYSIS is more effective than PCA , LINEAR DISCRIMINANT ANALYSIS , and HETEROSCEDASTIC DISCRIMINANT ANALYSIS for various DATA SETS . \n",
            "this paper proposes a new DIMENSIONALITY REDUCTION METHOD for SPEECH RECOGNITION . the proposed DIMENSIONALITY REDUCTION METHOD is based on the use of DISCRIMINATIVE INFORMATION and DISCRIMINATIVE INFORMATION . the proposed DIMENSIONALITY REDUCTION METHOD is compared with the conventional PCA and PCA . the proposed DIMENSIONALITY REDUCTION METHOD is compared with conventional PCA and PCA . the proposed DIMENSIONALITY REDUCTION METHOD is compared with conventional PCA and PCA .\n",
            "\n",
            "963 1000\n",
            "in this work , a novel approach for NONLINEAR ACOUSTIC ECHO CANCELLATION is proposed . the main innovative idea of the proposed method is to model only the small region of the ECHO PATH around the direct path by a group of PARALLEL HAMMERSTEIN MODELS , to estimate a NON-LINEAR PREPROCESSOR by correlations between the LINEAR KERNELS OF THE HAMMERSTEIN SUBMODELS , and to describe the remaining ECHO PATH by a simple HAMMERSTEIN MODEL with the PREPROCESSOR determined in the aforementioned way . while the COMPUTATIONAL COMPLEXITY of such a system increases only slightly in comparison to a LINEAR ECHO CANCELLER , experiments with SPEECH RECORDINGS from a SMARTPHONE in different environments confirm a significantly increased ECHO CANCELLATION performance . \n",
            "this paper presents a new HAMMERSTEIN MODEL for NONLINEAR ACOUSTIC ECHO CANCELLATION . the HAMMERSTEIN MODEL is based on a NON-LINEAR PREPROCESSOR of the ECHO PATH and the ECHO PATH . the proposed HAMMERSTEIN MODEL is based on a NON-LINEAR PREPROCESSOR . the proposed HAMMERSTEIN MODEL is based on a NON-LINEAR PREPROCESSOR . the proposed HAMMERSTEIN MODEL is based on a NON-LINEAR PREPROCESSOR . the proposed HAMMERSTEIN MODEL is evaluated on the ECHO PATH and the COMPUTATIONAL COMPLEXITY of the proposed HAMMERSTEIN MODEL .\n",
            "\n",
            "964 1000\n",
            "a method of LOCALISING OBJECTS in images is proposed . possible <unk> are evaluated using the CONTOUR DISCRIMINANT , a LIKELIHOOD RATIO which is derived from a PROBABILISTIC MODEL of the FEATURE DETECTION PROCESS . we treat each step in this process probabilistically , including the occurrence of CLUTTER FEATURES , and derive the OBSERVATION DENSITIES for both correct \\ target '' <unk> and incorrect \\ clutter '' <unk> . the CONTOUR DISCRIMINANT distinguishes target objects from the background even in heavy clutter , making only the most general assumptions about the form that clutter might take . the method generates samples <unk> to avoid the cost of processing an entire IMAGE , and promises to be particularly suited to the task of INITIALISING CONTOUR TRACKERS based on SAMPLING METHODS . \n",
            "this paper addresses the problem of LOCALISING OBJECTS in LOCALISING OBJECTS . we propose a PROBABILISTIC MODEL based on a PROBABILISTIC MODEL . the proposed PROBABILISTIC MODEL is based on the LIKELIHOOD RATIO of the IMAGE . the proposed PROBABILISTIC MODEL is based on a PROBABILISTIC MODEL . the proposed PROBABILISTIC MODEL is based on the LIKELIHOOD RATIO of the IMAGE .\n",
            "\n",
            "965 1000\n",
            "in this paper , the problem of TWO-DIMENSIONAL FREQUENCY ESTIMATION of a complex sinusoid embedded in a WHITE GAUSSIAN ADDITIVE NOISE is addressed . a new FREQUENCY ESTIMATOR based on a LEAST SQUARE PLANE FITTING of the estimated autocorrelation phase of the signal is derived . this algorithm requires a 2-D PHASE UNWRAPPING STEP which can be easily done . this algorithm is shown to be unbiased and attains the CRAMER RAO BOUNDS for high signal to NOISE RATIO -LRB- ACCURACY and ROBUSTNESS of this new 2-D FREQUENCY ESTI-MATOR are statistically assessed by MONTE CARLO SIMULATIONS . the results obtained show that a good LOCAL FREQUENCY ESTIMATION can be achieved with a very simple algorithm , and a very small amount of points used for the AUTOCORRELATION ESTIMATION . \n",
            "this paper proposes a new method for TWO-DIMENSIONAL FREQUENCY ESTIMATION based on LEAST SQUARE PLANE FITTING . the proposed FREQUENCY ESTIMATOR is based on the 2-D FREQUENCY ESTI-MATOR . the proposed FREQUENCY ESTIMATOR is based on the 2-D FREQUENCY ESTI-MATOR . the proposed FREQUENCY ESTIMATOR is based on the 2-D FREQUENCY ESTI-MATOR . the ROBUSTNESS of the proposed FREQUENCY ESTIMATOR is compared with the conventional FREQUENCY ESTIMATOR . the ROBUSTNESS of the proposed FREQUENCY ESTIMATOR is compared with the conventional MONTE CARLO SIMULATIONS .\n",
            "\n",
            "966 1000\n",
            "goal driven learning -lrb- <unk> -rrb- focuses on systems that determine by themselves what has to be learnt and how to learn GOAL DRIVEN LEARNING . typically GOAL DRIVEN LEARNING use META-REASONING CAPABILITIES over a BASE REASONER , identifying LEARNING GOALS and devising strategies . in this paper we present a novel GOAL DRIVEN LEARNING to deal with complex AI SYSTEMS where the META-REASONING MODULE has to analyze the reasoning trace of multiple components with potentially different LEARNING PARADIGMS . our GOAL DRIVEN LEARNING works by distributing the generation of LEARNING STRATEGIES among the different modules instead of <unk> GOAL DRIVEN LEARNING in the META-REASONER . we implemented our GOAL DRIVEN LEARNING in the GOAL DRIVEN LEARNING , that works in the AIRSPACE TASK ORDERS DOMAIN , showing an increase in performance . \n",
            "this paper addresses the problem of GOAL DRIVEN LEARNING for AI SYSTEMS in AI SYSTEMS . in particular , we propose a BASE REASONER for GOAL DRIVEN LEARNING , which is based on GOAL DRIVEN LEARNING . the proposed LEARNING STRATEGIES is based on a BASE REASONER , which is based on the META-REASONING MODULE . the proposed LEARNING STRATEGIES is based on a BASE REASONER and is shown to be robust to META-REASONING CAPABILITIES .\n",
            "\n",
            "967 1000\n",
            "with improved RECOGNITION ACCURACIES for LVCSR TASKS , it has become possible to search large collections of SPONTANEOUS SPEECH for a variety of information . the MALACH CORPUS OF HOLOCAUST TESTIMONIALS is one such collection , in which we are interested in automatically transcribing and retrieving portions that are relevant to NAMED ENTITIES such as people , places , and organizations . since the <unk> were gathered from thousands of people in countries throughout europe , an extremely large number of potential NAMED ENTITIES are possible , and this causes a well-known dilemma : increasing the size of the vocabulary allows for more of these words to be recognized , but also increases confusability , and can harm RECOGNITION performance . however , the MALACH CORPUS OF HOLOCAUST TESTIMONIALS , like many other collections , includes SIDE INFORMATION or MALACH CORPUS OF HOLOCAUST TESTIMONIALS that can be exploited to provide prior information on exactly which NAMED ENTITIES are likely to appear . this paper proposes a method that capitalizes on this prior information to reduce NAMED-ENTITY RECOGNITION ERRORS by over 50 % relative , and simultaneously decrease the overall WORD ERROR RATE by 7 % relative . the MALACH CORPUS OF HOLOCAUST TESTIMONIALS we use derives from a PRE-INTERVIEW QUESTIONAIRE that includes the names of friends , <unk> , places visited , membership of organizations , synonyms of place names , and similar information . by augmenting the LEXICON AND LANGUAGE MODEL with this information on a SPEAKER-BY-SPEAKER BASIS , we are able to exploit the TEXTUAL INFORMATION that is already available in the corpus to facilitate much improved SPEECH RECOGNITION . \n",
            "this paper addresses the problem of SPEECH RECOGNITION in SPONTANEOUS SPEECH . we propose a method for SPEECH RECOGNITION based on the MALACH CORPUS OF HOLOCAUST TESTIMONIALS . the proposed method is based on a MALACH CORPUS OF HOLOCAUST TESTIMONIALS , which is based on the MALACH CORPUS OF HOLOCAUST TESTIMONIALS . the proposed method is based on the MALACH CORPUS OF HOLOCAUST TESTIMONIALS . the proposed method is based on the MALACH CORPUS OF HOLOCAUST TESTIMONIALS , which is based on the MALACH CORPUS OF HOLOCAUST TESTIMONIALS . the proposed method is based on the MALACH CORPUS OF HOLOCAUST TESTIMONIALS and the MALACH CORPUS OF HOLOCAUST TESTIMONIALS .\n",
            "\n",
            "968 1000\n",
            "when the goal is to achieve the best correct CLASSIFICATION RATE , CROSS ENTROPY and MEAN SQUARED ERROR are typical COST FUNCTIONS used to optimize CLASSIFIER performance . however , for many REAL-WORLD CLASSIFICATION PROBLEMS , the ROC CURVE is a more meaningful performance measure . we demonstrate that minimizing CROSS ENTROPY or MEAN SQUARED ERROR does not necessarily maximize the area under the ROC CURVE . we then consider alternative objective functions for training a CLASSIFIER to maximize the ROC CURVE directly . we propose an OBJECTIVE FUNCTION that is an approximation to the WILCOXON-MANN-WHITNEY STATISTIC , which is equivalent to the ROC CURVE . the proposed OBJECTIVE FUNCTION is differentiable , so GRADIENT-BASED METHODS can be used to train the CLASSIFIER . we apply the new OBJECTIVE FUNCTION to REAL-WORLD CUSTOMER BEHAVIOR PREDICTION PROBLEMS for a WIRELESS SERVICE PROVIDER and a CABLE SERVICE PROVIDER , and achieve reliable improvements in the ROC CURVE . \n",
            "this paper presents a CLASSIFIER for REAL-WORLD CLASSIFICATION PROBLEMS . the CLASSIFIER is based on a WILCOXON-MANN-WHITNEY STATISTIC and a WILCOXON-MANN-WHITNEY STATISTIC . the CLASSIFIER is based on a WILCOXON-MANN-WHITNEY STATISTIC and a WILCOXON-MANN-WHITNEY STATISTIC . the CLASSIFIER is based on a WILCOXON-MANN-WHITNEY STATISTIC and a WILCOXON-MANN-WHITNEY STATISTIC . the CLASSIFICATION RATE of the proposed CLASSIFIER is evaluated using REAL-WORLD CLASSIFICATION PROBLEMS . the results show that the CLASSIFICATION RATE of the proposed CLASSIFIER is comparable to that of the conventional GRADIENT-BASED METHODS .\n",
            "\n",
            "969 1000\n",
            "the RECURSIVE LEAST SQUARES ALGORITHM is well known and has been widely used for many years . most analyses of RECURSIVE LEAST SQUARES ALGORITHM have assumed statistical properties of the data or the NOISE PROCESS , but recent robust h ∞ analyses have been used to bound the ratio of the performance of the algorithm to the total NOISE . in this paper , we provide an ADDITIVE ANALYSIS bounding the difference between performance and NOISE . our ADDITIVE ANALYSIS provides additional CONVERGENCE GUARANTEES in general , and particular benefits for STRUCTURED INPUT DATA . we illustrate the ADDITIVE ANALYSIS using HUMAN SPEECH and white NOISE . \n",
            "this paper presents a new method for ADDITIVE ANALYSIS based on STRUCTURED INPUT DATA . the proposed method is based on the RECURSIVE LEAST SQUARES ALGORITHM of the STRUCTURED INPUT DATA . the proposed method is based on the RECURSIVE LEAST SQUARES ALGORITHM . the proposed method is based on the RECURSIVE LEAST SQUARES ALGORITHM . the proposed method is tested on the STRUCTURED INPUT DATA .\n",
            "\n",
            "970 1000\n",
            "both LEARNING AND INFERENCE TASKS on BAYESIAN NETWORKS are np-hard in general . BOUNDED TREE-WIDTH BAYESIAN NETWORKS have recently received a lot of attention as a way to circumvent this complexity issue ; however , while inference on BOUNDED TREE-WIDTH NETWORKS is tractable , the LEARNING PROBLEM remains np-hard even for <unk> 2 . in this paper , we propose BOUNDED VERTEX COVER NUMBER BAYESIAN NETWORKS as an alternative to BOUNDED TREE-WIDTH NETWORKS . in particular , we show that both inference and learning can be done in POLYNOMIAL TIME for any fixed vertex cover number bound k , in contrast to the general and bounded <unk> cases ; on the other hand , we also show that LEARNING PROBLEM is w -lsb- 1 -rsb- - hard in parameter k. furthermore , we give an alternative way to learn BOUNDED VERTEX COVER NUMBER BAYESIAN NETWORKS using INTEGER LINEAR PROGRAMMING , and show this is feasible in practice . \n",
            "this paper addresses the problem of LEARNING PROBLEM in BOUNDED TREE-WIDTH NETWORKS . we propose a new LEARNING PROBLEM based on the BOUNDED VERTEX COVER NUMBER BAYESIAN NETWORKS . the LEARNING PROBLEM is formulated as a LEARNING PROBLEM . the LEARNING PROBLEM is formulated as a LEARNING PROBLEM . we demonstrate the effectiveness of the proposed method on a LEARNING PROBLEM .\n",
            "\n",
            "971 1000\n",
            "<unk> and competitive <unk> algorithms are almost ubiquitous in MODELING PATTERN FORMATION in CORTICAL DEVELOPMENT . we analyse in theoretical detail a particular model -lrb- adapted from <unk> & <unk> , 1999 -rrb- for the development of id <unk> patterns , which places COMPETITIVE AND INTERACTIVE CORTICAL INFLUENCES , and free and restricted initial <unk> onto a COMMON FOOTING . \n",
            "this paper addresses the problem of MODELING PATTERN FORMATION in CORTICAL DEVELOPMENT . in particular , we propose a method for MODELING PATTERN FORMATION in the presence of CORTICAL DEVELOPMENT . the proposed method is based on the MODELING PATTERN FORMATION . experimental results show that the proposed method outperforms the existing methods .\n",
            "\n",
            "972 1000\n",
            "natural and artificial neural circuits must be capable of traversing specific STATE SPACE TRAJECTORIES . a natural approach to this NATURAL AND ARTIFICIAL NEURAL CIRCUITS is to learn the relevant TRAJECTORIES from examples . unfortunately , gradient descent learning of COMPLEX TRAJECTORIES in AMORPHOUS NETWORKS is unsuccessful . we suggest a possible approach where TRAJECTORIES are realized by combining simple OSCIL-LATORS , in various modular ways . we contrast two regimes of FAST AND SLOW OSCILLATIONS . in all cases , we show that banks of oscillators with BOUNDED FREQUENCIES have UNIVERSAL APPROXIMATION PROPERTIES . open questions are also discussed briefly . \n",
            "this paper presents a method for FAST AND SLOW OSCILLATIONS in AMORPHOUS NETWORKS . the proposed method is based on the use of FAST AND SLOW OSCILLATIONS and the UNIVERSAL APPROXIMATION PROPERTIES . the proposed method is based on the use of FAST AND SLOW OSCILLATIONS and the UNIVERSAL APPROXIMATION PROPERTIES . the proposed method is based on the use of AMORPHOUS NETWORKS , and is shown to be robust to FAST AND SLOW OSCILLATIONS .\n",
            "\n",
            "973 1000\n",
            "<unk> algorithms have been surprisingly successful in computing approximately optimal solutions for partially observable markov decision processes -lrb- pomdps -rrb- in HIGH DIMENSIONAL BELIEF SPACES . in this work , we seek to understand the BELIEF-SPACE PROPERTIES that allow some POMDP PROBLEMS to be approximated efficiently and thus help to explain the POINT-BASED ALGORITHMS ' success often observed in the experiments . we show that an approximately optimal POMDP SOLUTION can be computed in TIME POLYNOMIAL in the covering number of a REACHABLE BELIEF SPACE , which is the subset of the BELIEF SPACE reachable from a given belief point . we also show that under the weaker condition of having a small covering number for an OPTIMAL REACHABLE SPACE , which is the subset of the BELIEF SPACE reachable under an OPTIMAL POLICY , computing an approximately optimal solution is np-hard . however , given a suitable set of points that '' cover '' an optimal <unk> space well , an APPROXIMATE SOLUTION can be computed in POLYNOMIAL TIME . the covering number highlights several interesting properties that reduce the complexity of POMDP PLANNING in practice , e.g. , FULLY OBSERVED STATE VARIABLES , BELIEFS with SPARSE SUPPORT , SMOOTH BELIEFS , and CIRCULANT STATE-TRANSITION MATRICES . \n",
            "this paper addresses the problem of POMDP PLANNING in HIGH DIMENSIONAL BELIEF SPACES . we propose a POMDP SOLUTION based on the OPTIMAL POLICY and the OPTIMAL POLICY . the proposed method is based on a POMDP SOLUTION and a POMDP SOLUTION . the proposed method is based on a POMDP SOLUTION and a POMDP SOLUTION . the proposed method is based on a POMDP SOLUTION and a POMDP SOLUTION . the proposed method is shown to be robust to SMOOTH BELIEFS and SMOOTH BELIEFS .\n",
            "\n",
            "974 1000\n",
            "many domains are naturally organized in an ABSTRACTION HIERARCHY or TAXONOMY , where the instances in '' nearby '' classes in the TAXONOMY are similar . in this paper , we provide a general PROBABILISTIC FRAMEWORK for CLUSTERING DATA into a set of classes organized as a TAXONOMY , where each class is associated with a PROB-ABILISTIC MODEL from which the data was generated . the PROBABILISTIC FRAMEWORK simultaneously optimizes three things : the assignment of data instances to CLUSTERS , the models associated with the CLUSTERS , and the structure of the ABSTRACTION HIERARCHY . a unique feature of our PROBABILISTIC FRAMEWORK is that PROBABILISTIC FRAMEWORK utilizes GLOBAL OPTIMIZATION ALGORITHMS for both of the last two steps , reducing the SENSITIVITY to NOISE and the <unk> to LOCAL MAXIMA that are characteristic of algorithms that only take LOCAL STEPS . we provide a THEORETICAL ANALYSIS for our PROBABILISTIC FRAMEWORK , showing that PROBABILISTIC FRAMEWORK converges to a local maximum of the probability of model and data . we present experimental results on SYNTHETIC DATA , and on REAL DATA in the domains of gene expression and text . \n",
            "this paper presents a novel PROBABILISTIC FRAMEWORK for CLUSTERING DATA . the PROBABILISTIC FRAMEWORK is based on a PROBABILISTIC FRAMEWORK of the PROB-ABILISTIC MODEL . the proposed PROBABILISTIC FRAMEWORK is based on a PROBABILISTIC FRAMEWORK of the PROB-ABILISTIC MODEL . the proposed PROBABILISTIC FRAMEWORK is based on a PROBABILISTIC FRAMEWORK , which is based on the PROB-ABILISTIC MODEL . the proposed PROBABILISTIC FRAMEWORK is based on the THEORETICAL ANALYSIS . the proposed PROBABILISTIC FRAMEWORK is based on a PROBABILISTIC FRAMEWORK . the proposed PROBABILISTIC FRAMEWORK is based on a PROBABILISTIC FRAMEWORK and is shown to be more robust to NOISE than the conventional PROB-ABILISTIC MODEL .\n",
            "\n",
            "975 1000\n",
            "this paper considers the problem of computing placement of points in 3 dimensional space given two UNCALIBRATED PERSPECTIVE VIEWS . the main theorem shows that the placement of the points is determined only up to an arbitrary projective transformation of <unk> . given additional ground control points , however , the location of the points and the CAMERA PARAMETERS may be determined . the method is linear and non-iterative whereas previously known methods for solving the CAMERA CALIBRATION and placement to take proper account of both <unk> points and IMAGE CORRESPONDENCES are unsatisfactory in requiring either ITERATIVE METHODS or MODEL RESTRICTIONS . as a result of the main theorem , it is possible to determine PROJECTIVE INVARIANTS OF 3-D GEOMETRIC CONFIGURATIONS from two PERSPECTIVE VIEWS . \n",
            "this paper addresses the problem of CAMERA CALIBRATION in PERSPECTIVE VIEWS . we propose a method for CAMERA CALIBRATION based on CAMERA CALIBRATION and CAMERA CALIBRATION . the proposed method is based on the use of MODEL RESTRICTIONS and MODEL RESTRICTIONS . experimental results show that the proposed method is robust and robust to CAMERA CALIBRATION and CAMERA CALIBRATION .\n",
            "\n",
            "976 1000\n",
            "we propose SPEAKER CLUSTERING METHODS based on the VOCAL-TRACT-SIZE RELATED ARTICULATORY PARAMETERS associated with individual speakers . two parameters characterizing GROSS VOCAL-TRACT DIMENSIONS are rst derived from formants of <unk> japanese vowels , and are then used to CLUSTER a total of <unk> male japanese speakers . the resultant SPEAKER CLUSTERS are found to be signicantly dierent from the SPEAKER CLUSTERS obtained by conventional ACOUSTIC CRITERIA . JAPANESE PHONEME RECOGNITION experiments are carried out using SPEAKER-CLUSTERED TIED-STATE HMMS trained for each CLUSTER . compared with the BASELINE GENDER DEPENDENT MODEL , 5.7 % of RECOGNITION ERROR REDUCTION has been achieved based on the CLUSTERING METHOD using VOCAL-TRACT PARAMETERS . \n",
            "this paper presents a new CLUSTERING METHOD for JAPANESE PHONEME RECOGNITION . the proposed CLUSTERING METHOD is based on the use of SPEAKER-CLUSTERED TIED-STATE HMMS for JAPANESE PHONEME RECOGNITION . the proposed CLUSTERING METHOD is based on a BASELINE GENDER DEPENDENT MODEL . the proposed CLUSTERING METHOD is based on a BASELINE GENDER DEPENDENT MODEL , which is based on a BASELINE GENDER DEPENDENT MODEL . the proposed CLUSTERING METHOD is based on a BASELINE GENDER DEPENDENT MODEL . the proposed CLUSTERING METHOD is applied to JAPANESE PHONEME RECOGNITION .\n",
            "\n",
            "977 1000\n",
            "we address the problem of automatically learning the main steps to complete a certain task , such as changing a CAR TIRE , from a set of NARRATED INSTRUCTION VIDEOS . the contributions of this paper are threefold . first , we develop a JOINT MODEL for VIDEO AND NATURAL LANGUAGE NARRATION that takes advantage of the complementary nature of the two signals . second , we collect an ANNOTATED DATASET of 57 INTERNET INSTRUCTION VIDEOS containing more than <unk> frames for two tasks -lrb- changing CAR TIRE and <unk> <unk> -rrb- . third , we experimentally demonstrate that the proposed JOINT MODEL automatically discovers , in an UNSUPERVISED MANNER , the main steps to achieve each task and locate them within the input videos . the results further show that the proposed JOINT MODEL outperforms SINGLE-MODALITY BASELINES , demonstrating the benefits of JOINT MODELING VIDEO and text . \n",
            "this paper presents a JOINT MODEL for JOINT MODELING VIDEO . the JOINT MODEL is based on the JOINT MODEL . the JOINT MODEL is based on the JOINT MODEL . the JOINT MODEL is based on the JOINT MODEL . the proposed JOINT MODEL is based on a JOINT MODEL . the proposed JOINT MODEL is compared with conventional SINGLE-MODALITY BASELINES .\n",
            "\n",
            "978 1000\n",
            "the FULL-BAND ADAPTIVE HARMONIC MODEL can be used by the ADAPTIVE ITERATIVE REFINEMENT ALGORITHM to accurately model the perceived characteristics of a SPEECH RECORDING . however , the LEAST SQUARES SOLUTION used in the current AHM-AIR makes the f 0 refinement in air time consuming , limiting the use of this algorithm for large databases . in this paper , a PEAK PICKING APPROACH is suggested as a substitution to the LEAST SQUARES SOLUTION . in order to integrate the adaptivity scheme of FULL-BAND ADAPTIVE HARMONIC MODEL in the PEAK PICKING APPROACH , an ADAPTIVE DISCRETE FOURIER TRANSFORM is also suggested in this paper , whose FREQUENCY BASIS can fully follow the frequency variations of the F 0 CURVE . evaluations have shown an AVERAGE TIME REDUCTION of 5.5 times compared to the LS SOLUTION APPROACH , while the QUALITY of the <unk> is preserved compared to the original AHM-AIR . \n",
            "this paper proposes a new ADAPTIVE ITERATIVE REFINEMENT ALGORITHM for SPEECH RECORDING . the proposed ADAPTIVE ITERATIVE REFINEMENT ALGORITHM is based on the ADAPTIVE DISCRETE FOURIER TRANSFORM . the proposed ADAPTIVE ITERATIVE REFINEMENT ALGORITHM is based on the ADAPTIVE DISCRETE FOURIER TRANSFORM . the proposed ADAPTIVE ITERATIVE REFINEMENT ALGORITHM is based on the ADAPTIVE DISCRETE FOURIER TRANSFORM . the proposed ADAPTIVE ITERATIVE REFINEMENT ALGORITHM is based on a PEAK PICKING APPROACH . the proposed ADAPTIVE ITERATIVE REFINEMENT ALGORITHM is evaluated on a SPEECH RECORDING . the results show that the proposed ADAPTIVE ITERATIVE REFINEMENT ALGORITHM achieves a better performance than the conventional LS SOLUTION APPROACH .\n",
            "\n",
            "979 1000\n",
            "digital images nowadays show large appearance <unk> on PICTURE STYLES , in terms of COLOR TONE , CONTRAST , VIGNETTING , and etc. . these ` PICTURE STYLES ' are directly related to the SCENE RADIANCE , image pipeline of the camera , and POST PROCESSING FUNCTIONS -lrb- e.g. , PHOTOGRAPHY EFFECT FILTERS -rrb- . due to the COMPLEXITY and nonlinearity of these factors , popular GRADIENT-BASED IMAGE DESCRIPTORS generally are not invariant to different PICTURE STYLES , which could degrade the performance for OBJECT RECOGNITION . given that images shared online or created by individual users are taken with a wide range of devices and may be processed by various POST PROCESSING FUNCTIONS , to find a ROBUST OBJECT RECOGNITION SYSTEM is useful and challenging . in this paper , we investigate the influence of PICTURE STYLES on OBJECT RECOGNITION by making a connection between IMAGE DE-SCRIPTORS and a PIXEL MAPPING FUNCTION G , and accordingly propose an ADAPTIVE APPROACH based on a G-INCORPORATED KERNEL DESCRIPTOR and multiple KERNEL LEARNING , without estimating or specifying the IMAGE STYLES used in training and testing . we conduct experiments on the DOMAIN ADAPTATION DATA SET , the OXFORD FLOWER DATA SET , and several variants of the FLOWER DATA SET by introducing popular PHOTOGRAPHY EFFECTS through POST-PROCESSING . the results demonstrate that the proposed ADAPTIVE APPROACH consistently yields RECOGNITION improvements over standard DESCRIPTORS in all studied cases . \n",
            "this paper addresses the problem of OBJECT RECOGNITION in DIGITAL IMAGES . in this paper , we propose a G-INCORPORATED KERNEL DESCRIPTOR based on the G-INCORPORATED KERNEL DESCRIPTOR . the proposed G-INCORPORATED KERNEL DESCRIPTOR is based on the G-INCORPORATED KERNEL DESCRIPTOR and the G-INCORPORATED KERNEL DESCRIPTOR . the proposed ADAPTIVE APPROACH is based on the G-INCORPORATED KERNEL DESCRIPTOR and the G-INCORPORATED KERNEL DESCRIPTOR . the proposed ADAPTIVE APPROACH is based on the G-INCORPORATED KERNEL DESCRIPTOR and the G-INCORPORATED KERNEL DESCRIPTOR . the proposed ADAPTIVE APPROACH is based on the G-INCORPORATED KERNEL DESCRIPTOR and the ADAPTIVE APPROACH . the proposed ADAPTIVE APPROACH is evaluated on a DOMAIN ADAPTATION DATA SET and on a DOMAIN ADAPTATION DATA SET .\n",
            "\n",
            "980 1000\n",
            "nuisance attribute projection -lrb- <unk> -rrb- and WITHIN-CLASS COVARIANCE NORMALIZATION are two effective techniques for INTERSESSION VARIABILITY COMPENSATION in SVM BASED SPEAKER VERIFICATION SYSTEMS . however , by normalizing or removing the NUISANCE SUBSPACE containing the SESSION VARIABILITY can not guarantee to enlarge the distance between speakers . in this paper , we investigated the probability of using LINEAR DISCRIMINANT ANALYSIS for DISCRIMINATIVE TRAINING . to cope with the small sample size problem which prevents us from using LINEAR DISCRIMINANT ANALYSIS directly , we adapted the WITHIN-CLASS COVARIANCE NORMALIZATION , which first projects the whole FEATURE SPACE into a relatively low dimensional SUBSPACE by LINEAR DISCRIMINANT ANALYSIS , and then performs LINEAR DISCRIMINANT ANALYSIS in the SUBSPACE . by some modification , the WITHIN-CLASS COVARIANCE NORMALIZATION can be <unk> into a kind of WITHIN-CLASS COVARIANCE NORMALIZATION , which we called WITHIN-CLASS COVARIANCE NORMALIZATION . experiments on NIST SRE TASKS showed that , the WITHIN-CLASS COVARIANCE NORMALIZATION outperformed the conventional WITHIN-CLASS COVARIANCE NORMALIZATION , especially in LOW DIMENSIONAL FEATURE SPACE . \n",
            "this paper addresses the problem of INTERSESSION VARIABILITY COMPENSATION in NIST SRE TASKS . we propose a LINEAR DISCRIMINANT ANALYSIS based on the NUISANCE ATTRIBUTE PROJECTION and the NUISANCE ATTRIBUTE PROJECTION . the NUISANCE ATTRIBUTE PROJECTION is based on the NUISANCE ATTRIBUTE PROJECTION and the WITHIN-CLASS COVARIANCE NORMALIZATION . the proposed SVM BASED SPEAKER VERIFICATION SYSTEMS is based on the NUISANCE ATTRIBUTE PROJECTION and the WITHIN-CLASS COVARIANCE NORMALIZATION . the proposed SVM BASED SPEAKER VERIFICATION SYSTEMS is compared with the conventional LINEAR DISCRIMINANT ANALYSIS and the SVM BASED SPEAKER VERIFICATION SYSTEMS .\n",
            "\n",
            "981 1000\n",
            "neurophysiological changes in the brain associated with MAJOR DEPRESSION DISORDER can <unk> ARTICULATORY PRECISION in SPEECH PRODUCTION . motivated by this observation , we address the hypothesis that ARTICULATORY FEATURES , as manifested through FORMANT FREQUENCY TRACKS , can help in AUTOMATICALLY CLASSIFYING DEPRESSION STATE . specifically , we investigate the relative importance of VOCAL TRACT FORMANT FREQUENCIES and their DYNAMIC FEATURES from SUSTAINED VOWELS and CONVERSATIONAL SPEECH . using a database consisting of AUDIO from 35 subjects with CLINICAL MEASURES OF DEPRESSION SEVERITY , we explore the performance of GAUSSIAN MIXTURE MODEL and SUPPORT VECTOR MACHINE CLASSIFIERS . with only FORMANT FREQUENCIES and their dynamics given by VELOCITY and acceleration , we show that DEPRESSION STATE can be classified with an optimal SENSITIVITY/SPECIFICITY/AREA under the GAUSSIAN MIXTURE MODEL for GMMS and SUPPORT VECTOR MACHINE CLASSIFIERS , respectively . future work will involve merging our FORMANT-BASED CHARACTERIZATION with VOCAL SOURCE and prosodic features . \n",
            "this paper presents a new method for AUTOMATICALLY CLASSIFYING DEPRESSION STATE in CONVERSATIONAL SPEECH . the proposed method is based on a GAUSSIAN MIXTURE MODEL and a GAUSSIAN MIXTURE MODEL . the proposed method is based on a GAUSSIAN MIXTURE MODEL and a GAUSSIAN MIXTURE MODEL . the proposed method is based on a GAUSSIAN MIXTURE MODEL and a GAUSSIAN MIXTURE MODEL . the proposed method is based on a GAUSSIAN MIXTURE MODEL and a GAUSSIAN MIXTURE MODEL . the proposed method is based on a GAUSSIAN MIXTURE MODEL and a GAUSSIAN MIXTURE MODEL . the proposed method is based on a GAUSSIAN MIXTURE MODEL and a GAUSSIAN MIXTURE MODEL . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "982 1000\n",
            "this paper describes a novel method for a WORD SENSE DISAM-BIGUATION that utilizes <unk> -lrb- i.e. SYNONYMS , HYPERNYMS , MERONYMS , etc in wordnet -rrb- of a target word and RAW CORPORA . the method <unk> senses of a target word by selecting a relative that most probably occurs in a new sentence including the target word . only one CO-OCCURRENCE FREQUENCY MATRIX is utilized to efficiently disambiguate senses of many target words . experiments on several ENGLISH DATUM present that our proposed method achieves a good performance . \n",
            "this paper presents a new method for WORD SENSE DISAM-BIGUATION . the method is based on the CO-OCCURRENCE FREQUENCY MATRIX and the CO-OCCURRENCE FREQUENCY MATRIX . the method is based on the CO-OCCURRENCE FREQUENCY MATRIX and the CO-OCCURRENCE FREQUENCY MATRIX . the proposed method is based on the CO-OCCURRENCE FREQUENCY MATRIX and the CO-OCCURRENCE FREQUENCY MATRIX .\n",
            "\n",
            "983 1000\n",
            "kalman FILTERING is a CLASSICAL PROBLEM of significant interest in the context of a DISTRIBUTED APPLICATION for WIRELESS SENSOR NETWORKS . in this paper we consider a specific algorithm for DISTRIBUTED KALMAN FILTERING proposed recently by <unk> -lsb- 1 -rsb- and present a SCAL-ABLE WIRELESS COMMUNICATION ARCHITECTURE suited for implementation in WIRELESS SENSOR NETWORKS . the proposed SCAL-ABLE WIRELESS COMMUNICATION ARCHITECTURE uses a DATA DRIVEN AVERAGE CONSENSUS FRAMEWORK . this allows us to explicitly characterize the delay vs. estimate accuracy tradeoff in FILTERING . by exploiting the structure of the DISTRIBUTED FILTERING COMPUTATIONS , we derive an optimal COMMUNICATION RESOURCE ALLOCATION POLICY for minimizing the COMPONENT-WISE STATE ESTIMATION ERROR . furthermore , our SCAL-ABLE WIRELESS COMMUNICATION ARCHITECTURE is scalable in terms of the network size n . we provide simulation results demonstrating the performance of our SCAL-ABLE WIRELESS COMMUNICATION ARCHITECTURE . \n",
            "this paper presents a SCAL-ABLE WIRELESS COMMUNICATION ARCHITECTURE for WIRELESS SENSOR NETWORKS . the DATA DRIVEN AVERAGE CONSENSUS FRAMEWORK is based on the DATA DRIVEN AVERAGE CONSENSUS FRAMEWORK . the DATA DRIVEN AVERAGE CONSENSUS FRAMEWORK is based on the DATA DRIVEN AVERAGE CONSENSUS FRAMEWORK . the DATA DRIVEN AVERAGE CONSENSUS FRAMEWORK is based on the DATA DRIVEN AVERAGE CONSENSUS FRAMEWORK . the proposed DATA DRIVEN AVERAGE CONSENSUS FRAMEWORK is based on the DATA DRIVEN AVERAGE CONSENSUS FRAMEWORK . the proposed DATA DRIVEN AVERAGE CONSENSUS FRAMEWORK is based on the DATA DRIVEN AVERAGE CONSENSUS FRAMEWORK . the proposed DATA DRIVEN AVERAGE CONSENSUS FRAMEWORK is based on the DATA DRIVEN AVERAGE CONSENSUS FRAMEWORK . the proposed DATA DRIVEN AVERAGE CONSENSUS FRAMEWORK is based on the DATA DRIVEN AVERAGE CONSENSUS FRAMEWORK .\n",
            "\n",
            "984 1000\n",
            "in this work , we introduce a novel IMPLICIT REPRESENTATION OF SHAPE which is based on assigning to each PIXEL a probability that this PIXEL is inside the shape . this PROBABILIS-TIC REPRESENTATION OF SHAPE resolves two important drawbacks of alternative IMPLICIT SHAPE REPRESENTATIONS such as the LEVEL SET METHOD : firstly , the SPACE OF SHAPES is convex in the sense that arbitrary convex combinations of a set of shapes again correspond to a valid shape . secondly , we prove that the introduction of SHAPE PRIORS into VARIATIONAL IMAGE SEGMENTATION leads to functionals which are convex with respect to SHAPE DEFORMATIONS . for a large class of commonly considered -lrb- spatially continuous -rrb- functionals , we prove that -- under MILD REGULARITY ASSUMPTIONS -- SEGMENTATION and TRACKING with STATISTICAL SHAPE PRIORS can be performed in a globally optimal manner . in experiments on TRACKING a walking person through a cluttered scene we demonstrate the advantage of GLOBAL VERSUS LOCAL OPTIMALITY . \n",
            "this paper addresses the problem of PROBABILIS-TIC REPRESENTATION OF SHAPE in VARIATIONAL IMAGE SEGMENTATION . we propose a method for TRACKING based on the IMPLICIT REPRESENTATION OF SHAPE . the proposed method is based on the IMPLICIT REPRESENTATION OF SHAPE , which is based on the IMPLICIT REPRESENTATION OF SHAPE . the proposed method is based on the IMPLICIT REPRESENTATION OF SHAPE . the proposed method is based on the IMPLICIT REPRESENTATION OF SHAPE . the proposed method is based on the IMPLICIT REPRESENTATION OF SHAPE . the proposed method is based on the IMPLICIT REPRESENTATION OF SHAPE . the proposed method is based on the IMPLICIT REPRESENTATION OF SHAPE . the proposed method is based on the IMPLICIT REPRESENTATION OF SHAPE . the proposed method is compared with the conventional LEVEL SET METHOD and the LEVEL SET METHOD .\n",
            "\n",
            "985 1000\n",
            "a framework for APPROXIMATE SIGNAL PROCESSING is introduced which can be used to design novel classes of algorithms for performing dft and stft calculations . in particular , we focus on the derivation of MULTI-STAGE INCREMEN-TAL REENEMENT ALGORITHMS that meet a variety of DESIGN CRITERIA on the TRADEOO achieved at each stage between SOLUTION QUALITY and COMPUTATIONAL COST . \n",
            "this paper addresses the problem of APPROXIMATE SIGNAL PROCESSING in the presence of TRADEOO . we propose a method for APPROXIMATE SIGNAL PROCESSING based on MULTI-STAGE INCREMEN-TAL REENEMENT ALGORITHMS . the proposed method is based on the use of the DESIGN CRITERIA and the SOLUTION QUALITY . experimental results show that the proposed method outperforms the conventional method in terms of SOLUTION QUALITY and SOLUTION QUALITY .\n",
            "\n",
            "986 1000\n",
            "model MIGRATION in SPEAKER RECOGNITION is a task of converting PARAMETRICALLY-OBSOLETE MODELS to new structures and configurations without the requirement to store the original SPEECH WAVEFORMS or feature vector sequences along with the MODEL MIGRATION . the need for MODEL MIGRATION arises in large-scale deployments of SPEAKER RECOGNITION TECHNOLOGY in which the potential for LEGACY PROBLEMS increases as the evolving technology may require CONFIGURATION CHANGES thus <unk> already existing USER VOICE ACCOUNTS . a MIGRATION may represent the only alternative to otherwise costly user <unk> or WAVEFORM STORAGE and , as a new research problem , presents the challenge of developing algorithms to minimize the loss in ACCURACY in the MIGRATED ACCOUNTS . this paper reports on further enhancements of a STATISTICAL MIGRATION TECHNIQUE based on GAUSSIAN MIXTURE MODELS , introduced previously . the present STATISTICAL MIGRATION TECHNIQUE is based on a STOCHASTIC SYNTHESIS OF FEATURE SEQUENCES from obsolete MODEL MIGRATION that are subsequently used to create the new MODEL MIGRATION . here , in addition to GAUSSIAN MEANS and PRIORS , as utilized in the previous contribution , also the covariances are included resulting in significant performance gains in the MIGRATED MODELS , compared to the MEAN-ONLY METHOD . overall , measured on the NIST 2003 CELLULAR TASK , the described STATISTICAL MIGRATION TECHNIQUE achieves a MODEL MIGRATION incurring a loss in performance of <unk> % relative to a full <unk> from waveforms , dependent on the type of MISMATCH between the obsolete and the new configuration . the inclusion of the COVARIANCE INFORMATION is shown to reduce the loss of performance by a factor of <unk> as compared to the BASELINE MEAN-ONLY MIGRATION TECHNIQUE . \n",
            "this paper proposes a new STATISTICAL MIGRATION TECHNIQUE for SPEAKER RECOGNITION . the proposed STATISTICAL MIGRATION TECHNIQUE is based on a STATISTICAL MIGRATION TECHNIQUE and a STATISTICAL MIGRATION TECHNIQUE . the proposed STATISTICAL MIGRATION TECHNIQUE is based on a STATISTICAL MIGRATION TECHNIQUE and a STATISTICAL MIGRATION TECHNIQUE . the proposed STATISTICAL MIGRATION TECHNIQUE is based on a STATISTICAL MIGRATION TECHNIQUE , which is based on the COVARIANCE INFORMATION . the proposed STATISTICAL MIGRATION TECHNIQUE is compared with the BASELINE MEAN-ONLY MIGRATION TECHNIQUE and the BASELINE MEAN-ONLY MIGRATION TECHNIQUE . the proposed STATISTICAL MIGRATION TECHNIQUE is compared with the BASELINE MEAN-ONLY MIGRATION TECHNIQUE and the BASELINE MEAN-ONLY MIGRATION TECHNIQUE . the proposed STATISTICAL MIGRATION TECHNIQUE is compared with the conventional BASELINE MEAN-ONLY MIGRATION TECHNIQUE and the BASELINE MEAN-ONLY MIGRATION TECHNIQUE .\n",
            "\n",
            "987 1000\n",
            "automated spoken dialog AUTOMATED SPOKEN DIALOG SYSTEMS require SYSTEMATIC PROCEDURES for evaluating performance and DIAGNOSING PROBLEMS . we present an INTERACTIVE TOOL that provides graphical views of how callers navigate through such AUTOMATED SPOKEN DIALOG SYSTEMS , enabling FINE-GRAINED ANALYSIS for SYSTEM EVALUATION and BUSINESS INTELLIGENCE . the input is a FEED OF CALL-LOGS . the output is an EMPIRICAL DIALOG TRAJECTORY ANALYSIS represented as STOCHASTIC FINITE STATE MACHINES , accessible via the WEB . COMPLEXITY is managed by an AUTOMATIC TOKENIZATION PROCEDURE that hides fine details until needed . users can generate selective views of parts of the DIALOG at high resolution -lrb- with access to CALL DATA -rrb- , or zoom out to a summary . the INTERACTIVE TOOL provides DIALOG SYSTEM DEVELOPERS with all the information they need from a single source , and is in use with DIRECTED-DIALOG AND NATURAL-LANGUAGE APPLICATIONS . \n",
            "this paper presents a new method for AUTOMATED SPOKEN DIALOG SYSTEMS based on STOCHASTIC FINITE STATE MACHINES . the proposed method is based on a FEED OF CALL-LOGS and a FEED OF CALL-LOGS based on STOCHASTIC FINITE STATE MACHINES . the proposed method is based on the FEED OF CALL-LOGS , which is based on the FEED OF CALL-LOGS . the proposed method is based on the FEED OF CALL-LOGS , which is based on the FEED OF CALL-LOGS . the proposed method is based on a FEED OF CALL-LOGS , which is based on the FEED OF CALL-LOGS . the proposed method is based on the FEED OF CALL-LOGS and the AUTOMATIC TOKENIZATION PROCEDURE . the proposed method is evaluated on the WEB .\n",
            "\n",
            "988 1000\n",
            "motion can be estimated by <unk> the EDGES of a <unk> -rrb- <unk> object using ACTIT -RRB- E CONTOURS , and registering them to , <unk> to obtain the MOTION MODEL PARAMETERS . this idea can be applied to PATIENT MOTION during the acquisition of an MRI to eliminate MOTION ARTIFACTS in the image . the data obtained <unk> , y the MRI <unk> ~ ~ , the K-SPACE , can be <unk> -rsb- <unk> into several . SUBBANDS such that each subband is acquired in a small fraction of the full <unk> , YING TIME . these sub bands create in -lsb- -rsb- -lrb- ~ <unk> tissue feature maps called SUBBAND IMAGES . <unk> , q <unk> ~ le contours , the RELATIVE MOTION is analyzed <unk> ~ th -lrb- ' <unk> sub <unk> -rrb- , d image ~ to determine the MOTIO ~ L PARAMETERS . <unk> , y these MOTION PARAMETERS at <unk> to correct the SUBBANDS , thus <unk> , y the k ; - sp -lrb- ~ <unk> . this has the potential to yield clear , NOISE-FREE MR IMA , <unk> . \n",
            "this paper presents a method for MRI based on ACTIT -RRB- E CONTOURS . the proposed method is based on a NOISE-FREE MR IMA . the method is based on the use of ACTIT -RRB- E CONTOURS to estimate the MOTION PARAMETERS of the SUBBAND IMAGES . the proposed method is based on the MOTIO ~ L PARAMETERS of the SUBBAND IMAGES . the proposed method is based on the use of ACTIT -RRB- E CONTOURS for MRI . the proposed method is based on a NOISE-FREE MR IMA and is shown to be robust to MOTION ARTIFACTS .\n",
            "\n",
            "989 1000\n",
            "we present a CONNECTIONIST ARCHITECTURE that can learn a model of the relations between PERCEPTIONS and actions and use this model for BEHAVIOR PLANNING . STATE REPRESENTATIONS are learned with a GROWING SELF-ORGANIZING LAYER which is directly coupled to a PERCEPTION and a MOTOR LAYER . knowledge about possible STATE TRANSITIONS is encoded in the LATERAL CONNECTIVITY . MOTOR SIGNALS modulate this LATERAL CONNECTIVITY and a DYNAMIC FIELD on the LAYER organizes a PLANNING PROCESS . all mechanisms are LOCAL AND ADAPTATION is based on HEBBIAN IDEAS . the model is continuous in the action , PERCEPTION , and time domain . \n",
            "this paper addresses the problem of PERCEPTION in MOTOR SIGNALS . we propose a method for PERCEPTION , which is based on a CONNECTIONIST ARCHITECTURE and a CONNECTIONIST ARCHITECTURE . the proposed method is based on a CONNECTIONIST ARCHITECTURE , which is based on the CONNECTIONIST ARCHITECTURE . the proposed method is based on the CONNECTIONIST ARCHITECTURE and the CONNECTIONIST ARCHITECTURE . the proposed method is based on the CONNECTIONIST ARCHITECTURE and the CONNECTIONIST ARCHITECTURE . the proposed method is based on the CONNECTIONIST ARCHITECTURE and the CONNECTIONIST ARCHITECTURE . the proposed method is based on the CONNECTIONIST ARCHITECTURE and the CONNECTIONIST ARCHITECTURE .\n",
            "\n",
            "990 1000\n",
            "we present an empirical study on the use of SEMANTIC INFORMATION for CONCEPT SEG-MENTATION AND LABELING , which is an important step for SEMANTIC PARSING . we represent the alternative analyses output by a state-of-the-art CSL PARSER with TREE STRUCTURES , which we rerank with a CLASSIFIER trained on two types of SEMANTIC TREE KERNELS : one PROCESSING STRUCTURES built with words , concepts and BROWN CLUSTERS , and another one using SEMANTIC SIMILARITY among the words composing the structure . the results on a corpus from the RESTAURANT DOMAIN show that our SEMANTIC KERNELS exploiting SIMILARITY MEASURES out-perform state-of-the-art <unk> . \n",
            "this paper addresses the problem of CONCEPT SEG-MENTATION AND LABELING in RESTAURANT DOMAIN . we propose a method for CONCEPT SEG-MENTATION AND LABELING based on SEMANTIC TREE KERNELS . the proposed method is based on a CSL PARSER , which is based on SEMANTIC TREE KERNELS . the proposed method is based on a CSL PARSER , which is based on SEMANTIC TREE KERNELS . the proposed method is based on a CSL PARSER . the proposed method is based on a CSL PARSER , which is based on a CSL PARSER . the proposed method is shown to outperform the conventional CLASSIFIER in terms of SEMANTIC SIMILARITY and SEMANTIC SIMILARITY .\n",
            "\n",
            "991 1000\n",
            "clustering is a basic task in a variety of MACHINE LEARNING APPLICATIONS . partitioning a set of INPUT VECTORS into compact , WELL-SEPARATED SUBSETS can be severely affected by the presence of MODEL-INCOMPATIBLE INPUTS called OUTLIERS . the present paper develops robust clustering algorithms for jointly partitioning the data and identifying the OUTLIERS . the novel approach relies on translating SCARCITY OF OUTLIERS to SPARSITY in a JUDICIOUSLY DEFINED DOMAIN , to <unk> three widely used CLUSTERING SCHEMES : HARD K-MEANS , FUZZY K-MEANS , and PROBABILISTIC CLUSTERING . CLUSTER CENTERS and assignments are iteratively updated in CLOSED FORM . the developed OUTLIER-AWARE ALGORITHMS are guaranteed to converge , while their COMPUTATIONAL COMPLEXITY is of the same order as their OUTLIER-AGNOSTIC COUNTERPARTS . preliminary simulations validate the analytical claims . \n",
            "this paper addresses the problem of PROBABILISTIC CLUSTERING for MACHINE LEARNING APPLICATIONS , such as CLUSTERING , CLUSTERING , and CLUSTERING . we propose a method for estimating the parameters of the INPUT VECTORS and show that it is possible to estimate the OUTLIER-AGNOSTIC COUNTERPARTS of the INPUT VECTORS . we show that the proposed method can be applied to MACHINE LEARNING APPLICATIONS , such as CLUSTERING and CLUSTERING . we show that the proposed method is robust to OUTLIERS , such as CLUSTERING and CLUSTERING . the proposed method is compared with other OUTLIER-AWARE ALGORITHMS such as CLUSTERING and CLUSTERING .\n",
            "\n",
            "992 1000\n",
            "in CHINESE , most of the LANGUAGE PROCESSING starts from WORD SEGMENTATION and PART-OF-SPEECH TAGGING . these two steps <unk> the word from a sequence of characters and predict the SYNTACTIC LABELS for each SEGMENTED WORD . in this paper , we present two distinct SEQUENTIAL TAGGING MODELS for the above two tasks . the first SEQUENTIAL TAGGING MODELS was basically similar to previous work which made use of CONDITIONAL RANDOM FIELDS and set of PREDEFINED DICTIONARIES to recognize WORD BOUNDARIES . second , we revise and modify SUPPORT VECTOR MACHINE BASED CHUNKING MODEL to label the POS TAG in the POS TAGGING TASK . our SUPPORT VECTOR MACHINE BASED CHUNKING MODEL in the WS TASK achieves moderately RANK among all participants , while in the POS TAGGING TASK , it reaches very competitive results . \n",
            "this paper presents a SUPPORT VECTOR MACHINE BASED CHUNKING MODEL for PART-OF-SPEECH TAGGING . the SUPPORT VECTOR MACHINE BASED CHUNKING MODEL is based on a SUPPORT VECTOR MACHINE BASED CHUNKING MODEL and a SUPPORT VECTOR MACHINE BASED CHUNKING MODEL . the proposed SUPPORT VECTOR MACHINE BASED CHUNKING MODEL is based on a SUPPORT VECTOR MACHINE BASED CHUNKING MODEL and a SUPPORT VECTOR MACHINE BASED CHUNKING MODEL . the proposed SUPPORT VECTOR MACHINE BASED CHUNKING MODEL is based on a SUPPORT VECTOR MACHINE BASED CHUNKING MODEL and a SUPPORT VECTOR MACHINE BASED CHUNKING MODEL . the proposed SUPPORT VECTOR MACHINE BASED CHUNKING MODEL is based on a SUPPORT VECTOR MACHINE BASED CHUNKING MODEL and is shown to be robust to WORD BOUNDARIES and WORD BOUNDARIES .\n",
            "\n",
            "993 1000\n",
            "robust <unk> is defined for the ANALYSIS OF SIGNALS with HEAVY-TAILED DISTRIBUTION NOISE . in the form of a robust spectrogram -lrb- <unk> -rrb- RSPEC BASED INSTANTANEOUS FREQUENCY ESTIMATOR can be used for the ANALYSIS OF NONSTATIONARY SIGNALS . in this paper a RSPEC BASED INSTANTANEOUS FREQUENCY ESTIMATOR , with a TIME-VARYING WINDOW LENGTH , is presented . the optimal choice of the WINDOW LENGTH can resolve the BIAS-VARIANCE TRADE-OFF in the RSPEC BASED IF ESTIMATION . however , RSPEC BASED INSTANTANEOUS FREQUENCY ESTIMATOR depends on the unknown nonlinearity of the RSPEC BASED INSTANTANEOUS FREQUENCY ESTIMATOR . the RSPEC BASED INSTANTANEOUS FREQUENCY ESTIMATOR used in this paper is able to provide the ACCURACY close to the one that could be achieved if the RSPEC BASED INSTANTANEOUS FREQUENCY ESTIMATOR , to be estimated , were known in advance . simulations show good ACCURACY ability of the ADAPTIVE ALGORITHM and good robustness property with respect to RARE HIGH MAGNITUDE NOISE VALUES . \n",
            "this paper presents a RSPEC BASED INSTANTANEOUS FREQUENCY ESTIMATOR for ANALYSIS OF NONSTATIONARY SIGNALS . the proposed ADAPTIVE ALGORITHM is based on the RSPEC BASED INSTANTANEOUS FREQUENCY ESTIMATOR . the proposed ADAPTIVE ALGORITHM is based on the RSPEC BASED INSTANTANEOUS FREQUENCY ESTIMATOR . the proposed ADAPTIVE ALGORITHM is based on the RSPEC BASED INSTANTANEOUS FREQUENCY ESTIMATOR . the proposed ADAPTIVE ALGORITHM is based on the RSPEC BASED INSTANTANEOUS FREQUENCY ESTIMATOR . the proposed ADAPTIVE ALGORITHM is based on the RSPEC BASED INSTANTANEOUS FREQUENCY ESTIMATOR . the proposed ADAPTIVE ALGORITHM is based on the RSPEC BASED INSTANTANEOUS FREQUENCY ESTIMATOR . the proposed ADAPTIVE ALGORITHM is applied to the ANALYSIS OF NONSTATIONARY SIGNALS .\n",
            "\n",
            "994 1000\n",
            "random PROJECTION has been suggested as a means of DIMENSION-ALITY REDUCTION , where the original data are projected onto a SUB-SPACE using a RANDOM MATRIX . it represents a COMPUTATIONALLY SIMPLE METHOD that approximately preserves the EUCLIDEAN DISTANCE of any two points through the PROJECTION . moreover , as we are able to produce various RANDOM MATRICES , there may be some possibility of finding a RANDOM MATRIX that gives a better SPEECH RECOGNITION ACCURACY among these RANDOM MATRICES . in this paper , we investigate the feasibility of RANDOM PROJECTION for SPEECH FEATURE EXTRACTION . to obtain an optimal result from among many -lrb- infinite -rrb- RANDOM MATRICES , a VOTE-BASED RANDOM-PROJECTION COMBINATION is introduced in this paper , where VOTE-BASED RANDOM-PROJECTION COMBINATION is applied to RANDOM-PROJECTION-BASED FEATURES . its effectiveness is confirmed by WORD RECOGNITION experiments . \n",
            "this paper presents a new COMPUTATIONALLY SIMPLE METHOD for SPEECH FEATURE EXTRACTION . the COMPUTATIONALLY SIMPLE METHOD is based on a COMPUTATIONALLY SIMPLE METHOD of the RANDOM MATRIX and the EUCLIDEAN DISTANCE . the proposed COMPUTATIONALLY SIMPLE METHOD is based on a COMPUTATIONALLY SIMPLE METHOD of the RANDOM MATRIX . the proposed COMPUTATIONALLY SIMPLE METHOD is based on the COMPUTATIONALLY SIMPLE METHOD . the proposed COMPUTATIONALLY SIMPLE METHOD is based on a COMPUTATIONALLY SIMPLE METHOD . the proposed COMPUTATIONALLY SIMPLE METHOD is based on a COMPUTATIONALLY SIMPLE METHOD and is shown to be more accurate than the conventional COMPUTATIONALLY SIMPLE METHOD .\n",
            "\n",
            "995 1000\n",
            "this paper introduces an approach for enabling existing MULTI-VIEW STEREO METHODS to operate on extremely large UNSTRUCTURED PHOTO COLLECTIONS . the main idea is to decompose the collection into a set of overlapping sets of photos that can be processed in parallel , and to merge the resulting reconstructions . this OVERLAPPING CLUSTERING PROBLEM is formulated as a CONSTRAINED OPTIMIZATION and solved iteratively . the MERGING ALGORITHM , designed to be parallel and <unk> , incorporates robust FILTERING STEPS to eliminate LOW-QUALITY RECONSTRUCTIONS and enforce GLOBAL VISIBILITY CONSTRAINTS . the approach has been tested on several large datasets downloaded from FLICKR.COM , including one with over ten thousand images , yielding a 3D RECONSTRUCTION with nearly thirty million points . \n",
            "this paper addresses the problem of 3D RECONSTRUCTION in UNSTRUCTURED PHOTO COLLECTIONS . in this paper , we propose a new method for 3D RECONSTRUCTION based on the MERGING ALGORITHM . the proposed method is based on the MERGING ALGORITHM . the proposed method is based on the use of GLOBAL VISIBILITY CONSTRAINTS and the GLOBAL VISIBILITY CONSTRAINTS . experimental results show the effectiveness of the proposed method .\n",
            "\n",
            "996 1000\n",
            "in VISUAL RECOGNITION PROBLEMS , the common DATA DISTRIBUTION MISMATCHES between training and testing make DOMAIN ADAPTATION essential . however , IMAGE DATA is difficult to manually divide into the DISCRETE DOMAINS required by ADAPTATION ALGORITHMS , and the standard practice of <unk> datasets with domains is a weak proxy for all the real conditions that alter the statistics in complex ways -lrb- lighting , POSE , BACKGROUND , RESOLUTION , etc. -rrb- we propose an approach to automatically discover LATENT DOMAINS in IMAGE OR VIDEO DATASETS . our formulation imposes two key properties on domains : MAXIMUM DISTINCTIVENESS and MAXIMUM LEARNABILITY . by MAXIMUM DISTINCTIVENESS , we require the underlying distributions of the identified domains to be different from each other to the MAXIMUM EXTENT ; by MAXIMUM LEARNABILITY , we ensure that a strong DISCRIMINATIVE MODEL can be learned from the domain . we devise a NONPARAMETRIC FORMULATION and efficient OPTIMIZATION PROCEDURE that can successfully discover domains among both TRAINING AND TEST DATA . we extensively evaluate our approach on OBJECT RECOGNITION and HUMAN ACTIVITY RECOGNITION TASKS . \n",
            "this paper presents a new method for OBJECT RECOGNITION from IMAGE DATA . the proposed method is based on the DISCRIMINATIVE MODEL and the DISCRIMINATIVE MODEL . the proposed method is based on the DISCRIMINATIVE MODEL and the DISCRIMINATIVE MODEL . the proposed method is based on the DISCRIMINATIVE MODEL and the DISCRIMINATIVE MODEL . the proposed method is based on the DISCRIMINATIVE MODEL and the DISCRIMINATIVE MODEL . the proposed method is based on the DISCRIMINATIVE MODEL and the DISCRIMINATIVE MODEL . the proposed method is evaluated on the IMAGE OR VIDEO DATASETS and the HUMAN ACTIVITY RECOGNITION TASKS .\n",
            "\n",
            "997 1000\n",
            "the CODING EFFICIENCY of the new VIDEO CODING STANDARD , HIGH EFFICIENCY VIDEO CODING , is strongly associated with better use of SPATIO-TEMPORAL REDUNDANCIES thanks to an increased number of competing CODING MODES . however , this competition involves a massive increase in SIGNALING BITRATE which becomes a possible limit for the next generation of EN-CODER . this paper proposes a new CODING SCHEME that breaks with conventional approaches . CODING SCHEME exploits a more complex DECODER able to reproduce the choice of the ENCODER based on CAUSAL REFERENCES , eliminating thus the need to SIGNAL CODING MODES and associated parameters . the general outline of this new CODING SCHEME and a proposed implementation are described in this paper . experimental results under common test conditions report an AVERAGE BITRATE SAVING of 1.7 % at the same quality compared to HEVC for a wide range of VIDEO SEQUENCES . \n",
            "this paper presents a new method for HIGH EFFICIENCY VIDEO CODING based on HEVC . the proposed method is based on a CODING SCHEME of the DECODER . the proposed method is based on a CODING SCHEME , which is based on the DECODER . the proposed method is based on the DECODER of the DECODER . the proposed method is based on the DECODER . the proposed method is based on the DECODER . the proposed method is based on the DECODER . the proposed method is based on a CODING SCHEME . the proposed method is evaluated on VIDEO SEQUENCES and compared to the conventional CODING SCHEME .\n",
            "\n",
            "998 1000\n",
            "-lrb- <unk> , <unk> , and <unk> 2001 -rrb- introduced a formula to predict the number of nodes ida * will expand given the STATIC DISTRIBUTION OF HEURISTIC VALUES . their formula proved to be very accurate but it is only accurate under the following limitations : -lrb- 1 -rrb- the HEURISTIC must be consistent ; -lrb- 2 -rrb- the prediction is for a large RANDOM SAMPLE of START STATES -lrb- or for large thresholds -rrb- . in this paper we generalize the STATIC DISTRIBUTION to a CONDITIONAL DISTRIBUTION OF HEURISTIC VALUES . we then propose a new formula for predicting the performance of ida * that works well for INCONSISTENT HEURISTICS -lrb- <unk> et al. 2007 -rrb- and for any set of START STATES , not just a RANDOM SAMPLE . we also show how the formula can be enhanced to work well for SINGLE START STATES . experimental results demonstrate the ACCURACY of our method in all these situations . \n",
            "this paper presents a new method for SINGLE START STATES based on the CONDITIONAL DISTRIBUTION OF HEURISTIC VALUES . the proposed method is based on the CONDITIONAL DISTRIBUTION OF HEURISTIC VALUES of the RANDOM SAMPLE . the proposed method is based on the CONDITIONAL DISTRIBUTION OF HEURISTIC VALUES of the RANDOM SAMPLE . the proposed method is based on the CONDITIONAL DISTRIBUTION OF HEURISTIC VALUES of the RANDOM SAMPLE . the ACCURACY of the proposed method is compared with the conventional HEURISTIC .\n",
            "\n",
            "999 1000\n",
            "in this paper , a new ARRAY SIGNAL PROCESSING TECHNIQUE by using PARTICLE SWARM OPTIMIZATION is proposed to identify MULTIPATH CHANNEL PARAMETERS . the proposed ARRAY SIGNAL PROCESSING TECHNIQUE provides estimates to the CHANNEL PARAMETERS by finding a global minimum of an OPTIMIZATION PROBLEM . since the OPTIMIZATION PROBLEM is formulated in the <unk> function -lrb- <unk> -rrb- domain of the TRANSMITTED SIGNAL and the received array outputs , the proposed ARRAY SIGNAL PROCESSING TECHNIQUE is called as PARTICLE SWARM OPTIMIZATION . the performance of the PARTICLE SWARM OPTIMIZATION is compared with the space alternating GENERALIZED EXPECTATION MAXIMIZATION TECHNIQUE and with another recently proposed PSO BASED TECHNIQUE for various SNR VALUES . simulation results indicate the superior performance of the PSO BASED TECHNIQUE over mentioned techniques for all SNR VALUES . \n",
            "this paper proposes a new ARRAY SIGNAL PROCESSING TECHNIQUE for PARTICLE SWARM OPTIMIZATION . the proposed ARRAY SIGNAL PROCESSING TECHNIQUE is based on the GENERALIZED EXPECTATION MAXIMIZATION TECHNIQUE . the proposed ARRAY SIGNAL PROCESSING TECHNIQUE is based on the GENERALIZED EXPECTATION MAXIMIZATION TECHNIQUE . the proposed GENERALIZED EXPECTATION MAXIMIZATION TECHNIQUE is compared with the conventional PSO BASED TECHNIQUE and the GENERALIZED EXPECTATION MAXIMIZATION TECHNIQUE .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#@title GENERATION FOR 5 EPOCHS\n",
        "!python generator.py -save /content/drive/MyDrive/rithanya/darri_det/GraphWriter/output/4.vloss-3.654094.lr-0.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhxqDBHy0WNb",
        "outputId": "8e108ecb-f27c-4aca-c7e8-56f7dd96367a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bleu_1:\t 11.73305246241061\n",
            "Bleu_2:\t 6.510385029851508\n",
            "Bleu_3:\t 3.866799290534126\n",
            "Bleu_4:\t 2.334433424872817\n",
            "METEOR:\t 6.334862325170072\n",
            "ROUGE_L: 14.326774548513225\n"
          ]
        }
      ],
      "source": [
        "#@title METRICS ORIGINAL 5 EPOCH\n",
        "!python eval.py /content/drive/MyDrive/rithanya/darri_det/GraphWriter/outputs/4.vloss-3.654094.lr-0.1.inputs.beam_predictions.cmdline /content/drive/MyDrive/rithanya/darri_det/GraphWriter/data/preprocessed.test.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nw0C7AH93xEe",
        "outputId": "4f318dd9-052c-478d-f3fa-e46a4c318bf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-24 10:17:31.274557: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-24 10:17:31.274614: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-24 10:17:31.275950: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-24 10:17:31.283389: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-24 10:17:32.399232: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Save File Exists, OverWrite? <CTL-C> for noyes\n",
            "Loading Data from  data/preprocessed.train.tsv\n",
            "building vocab\n",
            "done\n",
            "Sorting training data by len\n",
            "ds sizes:\t11880\t26156\t684\t1000\tVocab sizes:\n",
            "src 6343\n",
            "ent 53343\n",
            "nerd 8\n",
            "rel 17\n",
            "out 11738\n",
            "graph\n",
            "cuda:0\n",
            "ckpt restored\n",
            "epoch  5 lr 0.1\n",
            "Training\t2\n",
            "1584 of like 40k -- current avg loss  3.5088580208595355\n",
            "3184 of like 40k -- current avg loss  3.5113911784473975\n",
            "4784 of like 40k -- current avg loss  3.5112140577373694\n",
            "6384 of like 40k -- current avg loss  3.517566853597349\n",
            "7984 of like 40k -- current avg loss  3.5203109231883873\n",
            "9584 of like 40k -- current avg loss  3.520056048299315\n",
            "11184 of like 40k -- current avg loss  3.5207956311358233\n",
            "12784 of like 40k -- current avg loss  3.5189372550262528\n",
            "14384 of like 40k -- current avg loss  3.519980732670615\n",
            "15984 of like 40k -- current avg loss  3.5189617458167852\n",
            "17584 of like 40k -- current avg loss  3.5198126989022724\n",
            "19184 of like 40k -- current avg loss  3.521152754840103\n",
            "20784 of like 40k -- current avg loss  3.5219171151828546\n",
            "22384 of like 40k -- current avg loss  3.5221029013033167\n",
            "23984 of like 40k -- current avg loss  3.522169281117832\n",
            "25584 of like 40k -- current avg loss  3.5224156314093595\n",
            "1\n",
            "29324 of like 40k -- current avg loss  3.5016070860109654\n",
            "32524 of like 40k -- current avg loss  3.4824663100537596\n",
            "35724 of like 40k -- current avg loss  3.4670323743597016\n",
            "3\n",
            "AVG TRAIN LOSS:  3.461376444687528\t PPL:  31.860800977019554\n",
            "Evaluating\twe consider a D for on the to over the the to <unk> with N about the of the <unk> . . the I for such are the I are the and however , the I , a available . we propose this D to the of I of between the and the . and of to . have N in the <unk> -lrb- -lrb- . we to the D of we are in the to are to to the the I D . is a . the are in the D general and is available . . . the N . we , of of from from from from from from from from from by from from from from from from from from from from from from from from from from from from from from\n",
            "VAL LOSS:  3.670569787979126\t PPL:  39.274277497361155\n",
            "Saving model\n",
            "epoch  6 lr 0.1\n",
            "Training\t1\n",
            "3168 of like 40k -- current avg loss  3.2480832687532057\n",
            "6368 of like 40k -- current avg loss  3.239656338140593\n",
            "9568 of like 40k -- current avg loss  3.237340926326637\n",
            "2\n",
            "13464 of like 40k -- current avg loss  3.265145321226658\n",
            "15064 of like 40k -- current avg loss  3.2870913664837564\n",
            "16664 of like 40k -- current avg loss  3.3031405874930835\n",
            "18264 of like 40k -- current avg loss  3.3165368596242804\n",
            "19864 of like 40k -- current avg loss  3.3288591211474907\n",
            "21464 of like 40k -- current avg loss  3.3397464598464826\n",
            "23064 of like 40k -- current avg loss  3.3492793394301112\n",
            "24664 of like 40k -- current avg loss  3.356157984240195\n",
            "26264 of like 40k -- current avg loss  3.363926690488264\n",
            "27864 of like 40k -- current avg loss  3.3700125452912886\n",
            "29464 of like 40k -- current avg loss  3.3758334051225938\n",
            "31064 of like 40k -- current avg loss  3.3803588970708933\n",
            "32664 of like 40k -- current avg loss  3.384875756223325\n",
            "34264 of like 40k -- current avg loss  3.3883579161410404\n",
            "35864 of like 40k -- current avg loss  3.3922396445949774\n",
            "37464 of like 40k -- current avg loss  3.3952797787988285\n",
            "3\n",
            "AVG TRAIN LOSS:  3.400549790213916\t PPL:  29.980578545815376\n",
            "Evaluating\twe consider a novel for on the that on the the to <unk> with the about the and the <unk> . . the I for N and the I , the and however , the I , a known . we show this D of the the <unk> of between the and O . and of to . have . in I <unk> -lrb- -lrb- . we to this C of we and in the to are to to the the I D . is a . and is in the D general I is a . . . the <unk> . we , of of by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by\n",
            "VAL LOSS:  3.648281690597534\t PPL:  38.40861143598734\n",
            "Saving model\n",
            "epoch  7 lr 0.1\n",
            "Training\t3\n",
            "1\n",
            "3852 of like 40k -- current avg loss  3.2710568842981957\n",
            "7052 of like 40k -- current avg loss  3.2566901584004246\n",
            "10252 of like 40k -- current avg loss  3.2492982248193902\n",
            "2\n",
            "14148 of like 40k -- current avg loss  3.2652956970943916\n",
            "15748 of like 40k -- current avg loss  3.279834046774417\n",
            "17348 of like 40k -- current avg loss  3.2911572554212083\n",
            "18948 of like 40k -- current avg loss  3.3012321253529526\n",
            "20548 of like 40k -- current avg loss  3.309795055125784\n",
            "22148 of like 40k -- current avg loss  3.3181656600982463\n",
            "23748 of like 40k -- current avg loss  3.3253299086026034\n",
            "25348 of like 40k -- current avg loss  3.3315136984580884\n",
            "26948 of like 40k -- current avg loss  3.337034105564484\n",
            "28548 of like 40k -- current avg loss  3.3417232155849974\n",
            "30148 of like 40k -- current avg loss  3.3460627041240105\n",
            "31748 of like 40k -- current avg loss  3.3496911654593107\n",
            "33348 of like 40k -- current avg loss  3.352654261141211\n",
            "34948 of like 40k -- current avg loss  3.354897537069898\n",
            "36548 of like 40k -- current avg loss  3.3568848886204923\n",
            "38148 of like 40k -- current avg loss  3.360077493880997\n",
            "AVG TRAIN LOSS:  3.3612355080271556\t PPL:  28.824782137784666\n",
            "Evaluating\twe present a new for on a and to the the the and with a in the and the <unk> . . the approaches for I and a I are the and however , the I and a available . we propose this D to the the I of between the and the . and of to . are , in the <unk> -lrb- -lrb- . we to our I of we and in the to are to to a the O I . can a . we are in the D general I is available . . . a N . \n",
            "VAL LOSS:  3.579939586639404\t PPL:  35.87137367136756\n",
            "Saving model\n",
            "epoch  8 lr 0.1\n",
            "Training\t1\n",
            "3168 of like 40k -- current avg loss  3.1955579170072923\n",
            "6368 of like 40k -- current avg loss  3.1896920311990096\n",
            "9568 of like 40k -- current avg loss  3.186083512162684\n",
            "3\n",
            "2\n",
            "14148 of like 40k -- current avg loss  3.2195334438553695\n",
            "15748 of like 40k -- current avg loss  3.2325801970482115\n",
            "17348 of like 40k -- current avg loss  3.2439784154876574\n",
            "18948 of like 40k -- current avg loss  3.252919477124946\n",
            "20548 of like 40k -- current avg loss  3.2617979459964928\n",
            "22148 of like 40k -- current avg loss  3.2697840827196454\n",
            "23748 of like 40k -- current avg loss  3.275944348033759\n",
            "25348 of like 40k -- current avg loss  3.2822761031492886\n",
            "26948 of like 40k -- current avg loss  3.2863400106669287\n",
            "28548 of like 40k -- current avg loss  3.292080988114966\n",
            "30148 of like 40k -- current avg loss  3.296627049232899\n",
            "31748 of like 40k -- current avg loss  3.301511128663386\n",
            "33348 of like 40k -- current avg loss  3.30498925836249\n",
            "34948 of like 40k -- current avg loss  3.308228073587294\n",
            "36548 of like 40k -- current avg loss  3.3107145391278237\n",
            "38148 of like 40k -- current avg loss  3.3127124363788334\n",
            "AVG TRAIN LOSS:  3.3136517650578634\t PPL:  27.485312334312553\n",
            "Evaluating\twe present a O for on the for over the the the and with the in the and the -lrb- the . the approaches for the and a D are the and however , the I is known possible . we show this D of the the I of between the and the , and of properties . are . in the <unk> <unk> . . we to the properties of we and in the to are to to the the O I . is a . we is in the D general I is used . . . a N . \n",
            "VAL LOSS:  3.5664155712127688\t PPL:  35.38951434124488\n",
            "Saving model\n",
            "epoch  9 lr 0.1\n",
            "Training\t1\n",
            "3168 of like 40k -- current avg loss  3.172355066646229\n",
            "6368 of like 40k -- current avg loss  3.162479871481507\n",
            "9568 of like 40k -- current avg loss  3.152902471580633\n",
            "3\n",
            "2\n",
            "14148 of like 40k -- current avg loss  3.1821847907021317\n",
            "15748 of like 40k -- current avg loss  3.1948390730086818\n",
            "17348 of like 40k -- current avg loss  3.2070870690832827\n",
            "18948 of like 40k -- current avg loss  3.2165525678796105\n",
            "20548 of like 40k -- current avg loss  3.2237703437723493\n",
            "22148 of like 40k -- current avg loss  3.2301928246518603\n",
            "23748 of like 40k -- current avg loss  3.2366238959172526\n",
            "25348 of like 40k -- current avg loss  3.241799847417888\n",
            "26948 of like 40k -- current avg loss  3.2470156133254524\n",
            "28548 of like 40k -- current avg loss  3.252028923066738\n",
            "30148 of like 40k -- current avg loss  3.2569616192988127\n",
            "31748 of like 40k -- current avg loss  3.2611837846079026\n",
            "33348 of like 40k -- current avg loss  3.2646375698223666\n",
            "34948 of like 40k -- current avg loss  3.268415365255137\n",
            "36548 of like 40k -- current avg loss  3.2706526230967277\n",
            "38148 of like 40k -- current avg loss  3.2732655081319475\n",
            "AVG TRAIN LOSS:  3.274529169586079\t PPL:  26.430778136288453\n",
            "Evaluating\tin present a new for on a and . the the a , with the in the and a <unk> the . the methods for the and a I is the and and in the I , a available . we show this D to the the I of between the and the . and of properties . are , in the T -lrb- properties . we to the I of we are in the to are to to the the O I . is a . we are in the D general I is available . . . a N . \n",
            "VAL LOSS:  3.5609427490234373\t PPL:  35.1963628458857\n",
            "Saving model\n",
            "epoch  10 lr 0.1\n",
            "Training\t1\n",
            "3168 of like 40k -- current avg loss  3.1367942179092254\n",
            "6368 of like 40k -- current avg loss  3.1351823231682707\n",
            "9568 of like 40k -- current avg loss  3.126493423678803\n",
            "3\n",
            "2\n",
            "14148 of like 40k -- current avg loss  3.1521758575509558\n",
            "15748 of like 40k -- current avg loss  3.1650515974309803\n",
            "17348 of like 40k -- current avg loss  3.174366743025319\n",
            "18948 of like 40k -- current avg loss  3.1836114567443996\n",
            "20548 of like 40k -- current avg loss  3.19019595528243\n",
            "22148 of like 40k -- current avg loss  3.1966757172816536\n",
            "23748 of like 40k -- current avg loss  3.2021406846193425\n",
            "25348 of like 40k -- current avg loss  3.207203965317249\n",
            "26948 of like 40k -- current avg loss  3.213353541266481\n",
            "28548 of like 40k -- current avg loss  3.2167101425167157\n",
            "30148 of like 40k -- current avg loss  3.222397957522562\n",
            "31748 of like 40k -- current avg loss  3.2261597005369023\n",
            "33348 of like 40k -- current avg loss  3.2297157600398716\n",
            "34948 of like 40k -- current avg loss  3.2325551780012143\n",
            "36548 of like 40k -- current avg loss  3.2351590003697996\n",
            "38148 of like 40k -- current avg loss  3.2379501074430665\n",
            "AVG TRAIN LOSS:  3.238861826554803\t PPL:  25.50467647571561\n",
            "Evaluating\twe present a new for on O for over the the much with with N in the and T <unk> environment . the methods for I are a D are the , however , the I and a available in we propose this D to the the I behavior between the and N . and different problems . have have in the T -lrb- properties . we to the I of we are in the to are to to the the D I . is a . we are in the D general model is available . . . a N . we analysis are are and and by by by by by by by by by by by by from from from from from from from from from from from from from , from from\n",
            "VAL LOSS:  3.5564499588012697\t PPL:  35.03858766183102\n",
            "Saving model\n",
            "epoch  11 lr 0.1\n",
            "Training\t1\n",
            "3168 of like 40k -- current avg loss  3.106606615914239\n",
            "6368 of like 40k -- current avg loss  3.1013761513197244\n",
            "9568 of like 40k -- current avg loss  3.095118342434682\n",
            "3\n",
            "2\n",
            "14148 of like 40k -- current avg loss  3.118745353958652\n",
            "15748 of like 40k -- current avg loss  3.131535428957737\n",
            "17348 of like 40k -- current avg loss  3.139957379931977\n",
            "18948 of like 40k -- current avg loss  3.148473917494249\n",
            "20548 of like 40k -- current avg loss  3.1571534519555713\n",
            "22148 of like 40k -- current avg loss  3.163638222146512\n",
            "Error in sys.excepthook:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/exceptiongroup/_formatting.py\", line 68, in exceptiongroup_excepthook\n",
            "    def exceptiongroup_excepthook(\n",
            "KeyboardInterrupt\n",
            "\n",
            "Original exception was:\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/rithanya/darri_det/GraphWriter/train.py\", line 128, in <module>\n",
            "    main(args)\n",
            "  File \"/content/drive/MyDrive/rithanya/darri_det/GraphWriter/train.py\", line 113, in main\n",
            "    train(m,o,ds,args)\n",
            "  File \"/content/drive/MyDrive/rithanya/darri_det/GraphWriter/train.py\", line 32, in train\n",
            "    p,z,planlogits = m(b)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/drive/MyDrive/rithanya/darri_det/GraphWriter/models/newmodel.py\", line 82, in forward\n",
            "    prev = torch.cat((a,k),1)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "#@title resume training ORIGINAL 10 EPOCHS\n",
        "!python train.py -save /content/drive/MyDrive/rithanya/darri_det/GraphWriter/output2 -ckpt /content/drive/MyDrive/rithanya/darri_det/GraphWriter/output/4.vloss-3.654094.lr-0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKyWhlMlfbHC",
        "outputId": "0d6acfc5-38f8-404e-812e-f88b386210cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-24 12:44:42.630497: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-24 12:44:42.630543: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-24 12:44:42.631928: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-24 12:44:42.639580: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-24 12:44:43.730710: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Loading Data from  data/preprocessed.train.tsv\n",
            "building vocab\n",
            "done\n",
            "Vocab sizes:\n",
            "src 6343\n",
            "ent 53343\n",
            "nerd 8\n",
            "rel 17\n",
            "out 11738\n",
            "graph\n",
            "0 1000\n",
            "/content/drive/MyDrive/rithanya/darri_det/GraphWriter/models/newmodel.py:152: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  cx = torch.tensor(hx)\n",
            "we present a LEARNING ARCHITECTURE for LEXICAL SEMANTIC CLASSIFICATION PROBLEMS that <unk> TASK-SPECIFIC TRAINING DATA with BACKGROUND DATA encoding general '' world knowledge '' . the LEARNING ARCHITECTURE compiles knowledge contained in a <unk> into additional training data , and integrates TASK-SPECIFIC AND BACKGROUND DATA through a novel HIERARCHICAL LEARNING ARCHITECTURE . experiments on a WORD SENSE DISAMBIGUATION TASK provide empirical evidence that this '' HIERARCHICAL LEARNING ARCHITECTURE '' outperforms a state-of-the-art standard '' flat '' one . \n",
            "this paper addresses the problem of LEXICAL SEMANTIC CLASSIFICATION PROBLEMS from a single image . in particular , we propose a novel approach to LEXICAL SEMANTIC CLASSIFICATION PROBLEMS in the context of LEXICAL SEMANTIC CLASSIFICATION PROBLEMS . the proposed approach is based on a novel LEARNING ARCHITECTURE , which is based on a novel LEARNING ARCHITECTURE , which is able to deal with BACKGROUND DATA . the proposed approach is evaluated on a WORD SENSE DISAMBIGUATION TASK and is shown to significantly outperform the state of the art in the literature .\n",
            "\n",
            "1 1000\n",
            "we consider the STOCHASTIC MULTI-ARMED BANDIT PROBLEM with a PRIOR DISTRIBUTION on the REWARD DISTRIBUTIONS . we are interested in studying PRIOR-FREE AND PRIOR-DEPENDENT REGRET BOUNDS , very much in the same spirit than the usual DISTRIBUTION-FREE AND DISTRIBUTION-DEPENDENT BOUNDS for the NON-BAYESIAN STOCHASTIC BANDIT . we first show that THOMPSON SAMPLING attains an optimal <unk> bound in the sense that for any PRIOR DISTRIBUTION its BAYESIAN REGRET is bounded from above by 14 √ <unk> . this result is <unk> in the sense that there exists a PRIOR DISTRIBUTION such that any algorithm has a BAYESIAN REGRET bounded from below by 1 20 √ <unk> . we also study the case of THOMPSON SAMPLING for the setting of <unk> et al. -lsb- 2013 -rsb- -lrb- where the optimal mean is known as well as a lower bound on the smallest gap -rrb- and we show that in this case the regret of THOMPSON SAMPLING is in fact uniformly bounded over time , thus showing that THOMPSON SAMPLING can greatly take advantage of the nice properties of these THOMPSON SAMPLING . \n",
            "this paper addresses the problem of STOCHASTIC MULTI-ARMED BANDIT PROBLEM in a STOCHASTIC MULTI-ARMED BANDIT PROBLEM with REWARD DISTRIBUTIONS . in particular , we consider the problem of STOCHASTIC MULTI-ARMED BANDIT PROBLEM in a STOCHASTIC MULTI-ARMED BANDIT PROBLEM with REWARD DISTRIBUTIONS . in particular , we consider the problem of STOCHASTIC MULTI-ARMED BANDIT PROBLEM in a STOCHASTIC MULTI-ARMED BANDIT PROBLEM with REWARD DISTRIBUTIONS . we show that this problem can be solved efficiently by using the PRIOR-FREE AND PRIOR-DEPENDENT REGRET BOUNDS . we show that this problem can be solved efficiently by using the PRIOR-FREE AND PRIOR-DEPENDENT REGRET BOUNDS . we also show that the proposed algorithm is able to solve the problem of STOCHASTIC MULTI-ARMED BANDIT PROBLEM .\n",
            "\n",
            "2 1000\n",
            "the paper presents a new application of AUTOMATIC SPEECH PROCESSING in the AMBIENT ASSISTED LIVING AREA , developed in the course of a three year research project . recording and automatic processing of SPOKEN CONVERSATIONS plays a major role in this solution enabling effective search in a PERSONAL AUDIO ARCHIVE and fast browsing of conversations . processing of ELDERLY CONVERSATIONAL SPEECH recorded by a distant pda microphone poses a great challenge . the SPEECH PROCESSING FLOW includes TRANSCRIPTION , SPEAKER TRACKING and combined indexing and search of spoken terms and participating speakers identity extracted from the AUDIO . we present the entire application and individual SPEECH PROCESSING COMPONENTS as well as evaluation results of the individual components and of the END-TO-END SPOKEN INFORMATION RETRIEVAL SOLUTION . \n",
            "this paper presents a novel approach to SPEAKER TRACKING in ELDERLY CONVERSATIONAL SPEECH . the approach is based on a SPEECH PROCESSING FLOW , a SPEECH PROCESSING FLOW , and an END-TO-END SPOKEN INFORMATION RETRIEVAL SOLUTION . the proposed method consists of two stages : -lrb- 1 -rrb- a TRANSCRIPTION , and -lrb- 2 -rrb- a SPEECH PROCESSING FLOW based on a SPEECH PROCESSING FLOW . the proposed method consists of two stages : -lrb- 1 -rrb- a TRANSCRIPTION , and -lrb- 2 -rrb- a SPEECH PROCESSING FLOW based on AUTOMATIC SPEECH PROCESSING and SPEAKER TRACKING . the proposed method consists of two stages : 1 -rrb- a SPEECH PROCESSING FLOW , 2 -rrb- a SPEECH PROCESSING FLOW , and 3 -rrb- a SPEAKER TRACKING based on AUTOMATIC SPEECH PROCESSING and SPEAKER TRACKING . experimental results demonstrate the effectiveness of the proposed method .\n",
            "\n",
            "3 1000\n",
            "we address the problem of estimating the WORD ERROR RATE of an AUTOMATIC SPEECH RECOGNITION SYSTEM without using ACOUSTIC TEST DATA . this is an important problem which is faced by the designers of new applications which use AUTOMATIC SPEECH RECOGNITION SYSTEM . quick estimate of WORD ERROR RATE early in the DESIGN CYCLE can be used to guide the decisions involving DIALOG STRATEGY and GRAMMAR DESIGN . our approach involves estimating the PROBABILITY DISTRIBUTION of the WORD HYPOTHESES produced by the underlying AUTOMATIC SPEECH RECOGNITION SYSTEM given the TEXT TEST CORPUS . a critical component of this system is a PHONEMIC CONFUSION MODEL which seeks to capture the errors made by AUTOMATIC SPEECH RECOGNITION SYSTEM on the ACOUSTIC DATA at a PHONEMIC LEVEL . we use a CONFUSION MODEL composed of PROBABILISTIC PHONEME SEQUENCE CONVERSION RULES which are learned from PHONEMIC TRANSCRIPTION PAIRS obtained by LEAVE-ONE-OUT DECODING of the training set . we show reasonably close estimation of WORD ERROR RATE when applying the system to test sets from different domains . \n",
            "this paper presents a novel AUTOMATIC SPEECH RECOGNITION SYSTEM based on GRAMMAR DESIGN and GRAMMAR DESIGN . the proposed AUTOMATIC SPEECH RECOGNITION SYSTEM is based on the CONFUSION MODEL and GRAMMAR DESIGN . the proposed AUTOMATIC SPEECH RECOGNITION SYSTEM is based on the use of LEAVE-ONE-OUT DECODING and GRAMMAR DESIGN . the proposed AUTOMATIC SPEECH RECOGNITION SYSTEM is based on the use of WORD HYPOTHESES and GRAMMAR DESIGN . the proposed AUTOMATIC SPEECH RECOGNITION SYSTEM is evaluated on a TEXT TEST CORPUS with a TEXT TEST CORPUS . the results show that the proposed AUTOMATIC SPEECH RECOGNITION SYSTEM is effective in improving the WORD ERROR RATE of the AUTOMATIC SPEECH RECOGNITION SYSTEM in terms of WORD ERROR RATE and GRAMMAR DESIGN . the proposed AUTOMATIC SPEECH RECOGNITION SYSTEM is evaluated on the TEXT TEST CORPUS and the results show that the proposed AUTOMATIC SPEECH RECOGNITION SYSTEM is effective in improving the WORD ERROR RATE of the AUTOMATIC SPEECH RECOGNITION SYSTEM .\n",
            "\n",
            "4 1000\n",
            "mel-frequency cepstral coefficients -lrb- MEL-FREQUENCY CEPSTRAL COEFFICIENTS -rrb- are the most widely used FEATURES for SPEECH RECOGNITION . these are derived from the POWER SPECTRUM of the SPEECH SIGNAL . recently , the CEPSTRAL FEATURES derived from the MODIFIED GROUP DELAY FUNCTION have been studied by <unk> and <unk> -lsb- 6 -rsb- for SPEECH RECOGNITION . in this paper , we propose to use the product of the POWER SPECTRUM and the GROUP DELAY FUNCTION , and derive the MEL-FREQUENCY CEPSTRAL COEFFICIENTS from the product MEL-FREQUENCY CEPSTRAL COEFFICIENTS . this MEL-FREQUENCY CEPSTRAL COEFFICIENTS combines the information from the MAGNITUDE SPECTRUM as well as the PHASE SPECTRUM . the MEL-FREQUENCY CEPSTRAL COEFFICIENTS of the MODIFIED GROUP DELAY FUNCTION are also investigated in this paper . results show that the CEPSTRAL FEATURES derived from the POWER SPECTRUM perform better than that from the MODIFIED GROUP DELAY FUNCTION , and the product MEL-FREQUENCY CEPSTRAL COEFFICIENTS based FEATURES provide the best performance . \n",
            "in this paper , we propose a novel method for SPEECH RECOGNITION based on a MODIFIED GROUP DELAY FUNCTION . the proposed method is based on the use of a MODIFIED GROUP DELAY FUNCTION to estimate the MAGNITUDE SPECTRUM of the SPEECH SIGNAL . the proposed method is based on the use of a MODIFIED GROUP DELAY FUNCTION to estimate the MAGNITUDE SPECTRUM of the MEL-FREQUENCY CEPSTRAL COEFFICIENTS . the proposed method is based on the use of the MEL-FREQUENCY CEPSTRAL COEFFICIENTS to estimate the POWER SPECTRUM of the MEL-FREQUENCY CEPSTRAL COEFFICIENTS . the proposed method is applied to SPEECH RECOGNITION . the experimental results show that the proposed method is effective in improving the SPEECH RECOGNITION performance .\n",
            "\n",
            "5 1000\n",
            "three-dimensional -lrb- 3d -rrb- graphic scenes require considerable NETWORK BANDWIDTH to be transmitted and computing power to be rendered on users ' terminals . toward high-quality display in real time , we propose a SENDER-DRIVEN MECHANISM for STREAMING 3D SCENES in a RESOURCE-CONSTRAINED ENVIRONMENT . by pre-processing the database , objects in the scene are properly weighted upon their rendering importance , and their resolutions are selected accordingly to reduce the bit rate . partially ordered delivery is then performed using DECODING INDEPENDENCIES between the objects . simulation results show the efficacy of the proposed SENDER-DRIVEN MECHANISM . for a test benchmark , for example , the proposed algorithm outperforms the comparing HEURISTIC by 4 db under a 100-KB BIT RATE . \n",
            "this paper presents a novel approach to STREAMING 3D SCENES . the proposed method is based on the use of a SENDER-DRIVEN MECHANISM for STREAMING 3D SCENES . the proposed method is based on a SENDER-DRIVEN MECHANISM , which is based on a SENDER-DRIVEN MECHANISM . the proposed method is based on a SENDER-DRIVEN MECHANISM for STREAMING 3D SCENES . the experimental results show that the proposed method is effective in improving the performance of STREAMING 3D SCENES .\n",
            "\n",
            "6 1000\n",
            "we present in this paper a system which automatically builds , from REAL IMAGES , a SCENE MODEL containing both 3D GEOMETRIC INFORMATION OF THE SCENE STRUCTURE and its PHOTOMETRIC INFORMATION under various ILLUMINATION CONDITIONS . the GEOMETRIC STRUCTURE is recovered from IMAGES taken from distinct viewpoints . STRUCTURE-FROM-MOTION AND CORRELATION-BASED STEREO TECHNIQUES are used to match pix-els between IMAGES of different viewpoints and to reconstruct the scene in 3D SPACE . the GEOMETRIC STRUCTURE is extracted from IMAGES taken under different ILLUMINATION CONDITIONS -lrb- orientation , POSITION and intensity of the light sources -rrb- . this is achieved by computing a LOW-DIMENSIONAL LINEAR SPACE of the <unk> volume , and is represented by a set of BASIS IMAGES . the model that has been built can be used to create realistic <unk> from different viewpoints and ILLUMINATION CONDITIONS . applications include OBJECT RECOGNITION , VIRTUAL REALITY and PRODUCT ADVERTISEMENT . \n",
            "this paper presents a novel approach to OBJECT RECOGNITION in IMAGES . the proposed approach is based on the use of a set of BASIS IMAGES in the form of a set of BASIS IMAGES , each of which is a set of BASIS IMAGES . the proposed approach is based on the use of a 3D GEOMETRIC INFORMATION OF THE SCENE STRUCTURE in the 3D SPACE of the 3D SPACE . the proposed approach is based on the use of BASIS IMAGES and PRODUCT ADVERTISEMENT in the 3D SPACE . the proposed method is evaluated on REAL IMAGES , VIRTUAL REALITY , VIRTUAL REALITY and VIRTUAL REALITY . the experimental results show that the proposed method outperforms the existing methods in terms of OBJECT RECOGNITION and VIRTUAL REALITY .\n",
            "\n",
            "7 1000\n",
            "we seek to both detect and segment objects in images . to exploit both LOCAL IMAGE DATA as well as CONTEXTUAL INFORMATION , we introduce BOOSTED RANDOM FIELDS , which uses BOOSTING to learn the GRAPH STRUCTURE and local evidence of a CONDITIONAL RANDOM FIELD . the GRAPH STRUCTURE is learned by assembling GRAPH FRAGMENTS in an ADDITIVE MODEL . the connections between individual pixels are not very informative , but by using DENSE GRAPHS , we can pool information from large regions of the image ; DENSE MODELS also support efficient inference . we show how CONTEXTUAL INFORMATION from other objects can improve DETECTION performance , both in terms of ACCURACY and SPEED , by using a COMPUTATIONAL CASCADE . we apply our system to detect <unk> and things in office and street scenes . 1 . \n",
            "this paper addresses the problem of DETECTION from LOCAL IMAGE DATA . in particular , we propose a novel approach to DETECTION based on LOCAL IMAGE DATA . the proposed approach is based on the use of GRAPH FRAGMENTS extracted from the LOCAL IMAGE DATA . the proposed approach is based on the use of GRAPH FRAGMENTS extracted from the LOCAL IMAGE DATA . the proposed approach is based on the use of GRAPH FRAGMENTS extracted from the LOCAL IMAGE DATA . the proposed method is evaluated in terms of ACCURACY and SPEED . the experimental results show that the proposed method achieves better performance than the state-of-the-art methods .\n",
            "\n",
            "8 1000\n",
            "we investigate the COMPUTATIONAL COMPLEXITY of testing DOMINANCE and CONSISTENCY in CP-NETS . up until now , the complexity of DOMINANCE has been determined only for RESTRICTED CLASSES in which the DEPENDENCY GRAPH of the CP-NETS is acyclic . however , there are preferences of interest that define CYCLIC DEPENDENCY GRAPHS ; CYCLIC DEPENDENCY GRAPHS are modeled with GENERAL CP-NETS . we show here that both DOMINANCE and CONSISTENCY testing for GENERAL CP-NETS are PSPACE-COMPLETE . the reductions used in the proofs are from STRIPS PLANNING , and thus establish strong connections between both areas . \n",
            "this paper addresses the problem of STRIPS PLANNING in STRIPS PLANNING . in particular , we focus on the problem of STRIPS PLANNING , and show that the COMPUTATIONAL COMPLEXITY of the algorithm is bounded by a factor of o -lrb- 1 / √ t -rrb- , where d is the length of the DEPENDENCY GRAPH , and the number of RESTRICTED CLASSES in the DEPENDENCY GRAPH is bounded by a constant factor of o -lrb- 1 / √ t -rrb- . the COMPUTATIONAL COMPLEXITY of the algorithm is linear in the size of the DEPENDENCY GRAPH , and the number of RESTRICTED CLASSES is bounded by a factor of o -lrb- 1 / √ t -rrb- . the COMPUTATIONAL COMPLEXITY of the algorithm is o -lrb- 1 / √ t -rrb- , where d is the number of RESTRICTED CLASSES .\n",
            "\n",
            "9 1000\n",
            "<unk> was once regarded as an indication of good sleep . but recently it has been known to be one of the symptoms which indicate SLEEP DISORDERED BREATHING such as SLEEP APNEA SYNDROME . moreover , HEAVY SNORING caused by ORAL BREATHING sometimes leads BENIGN SNORERS to be <unk> . thus , it is important to detect ORAL SNORING for MEDICAL TREATMENT in the earlier stage , but we can not know our own <unk> . this paper describes a method to detect ORAL SNORING by extracting the ACOUSTIC PROPERTIES OF SNORING SOUNDS and using the K-NEAREST NEIGHBOR CLASSIFIER . as a result , over 92 % of SNORING SOUNDS are successfully classified under the various CROSS VALIDATION EVALUATIONS . \n",
            "in this paper , we present a novel approach to SLEEP APNEA SYNDROME based on a K-NEAREST NEIGHBOR CLASSIFIER . the proposed approach is based on the use of a K-NEAREST NEIGHBOR CLASSIFIER , a K-NEAREST NEIGHBOR CLASSIFIER , and a K-NEAREST NEIGHBOR CLASSIFIER . the proposed approach is based on the use of a K-NEAREST NEIGHBOR CLASSIFIER and a K-NEAREST NEIGHBOR CLASSIFIER . the proposed approach is evaluated on a MEDICAL TREATMENT and compared to the CROSS VALIDATION EVALUATIONS . the results show that the proposed approach is able to detect SNORING SOUNDS in real-time , and is robust to ORAL BREATHING .\n",
            "\n",
            "10 1000\n",
            "existing research on SENTIMENT ANALYSIS mainly uses SENTIMENT WORDS and phrases to determine SENTIMENTS expressed in DOCUMENTS and sentences . techniques have also been developed to find such words and phrases using dictionaries and domain corpora . however , there are still other types of words and phrases that do not bear SENTIMENTS on their own , but when they appear in some particular contexts , they imply POSITIVE OR NEGATIVE OPINIONS . one class of such words or phrases is those that express resources such as WATER , ELECTRICITY , GAS , etc. . for example , '' this <unk> uses a lot of ELECTRICITY '' is negative but '' this <unk> uses little WATER '' is positive . extracting such RESOURCE WORDS and phrases are important for SENTIMENT ANALYSIS . this paper formulates the problem based on a BIPARTITE GRAPH and proposes a novel ITERATIVE ALGORITHM to solve the problem . experimental results using diverse REAL-LIFE SENTIMENT CORPORA show good results . \n",
            "this paper presents a novel approach to SENTIMENT ANALYSIS from REAL-LIFE SENTIMENT CORPORA . the proposed approach is based on the use of SENTIMENTS and SENTIMENTS in the BIPARTITE GRAPH . the approach is based on the use of SENTIMENTS and SENTIMENTS . the method is based on the use of SENTIMENTS and SENTIMENTS . the proposed method is evaluated on REAL-LIFE SENTIMENT CORPORA , ELECTRICITY , WATER , and WATER .\n",
            "\n",
            "11 1000\n",
            "recently , <unk> and <unk> demonstrated that popular DIFFUSION-BASED PROCEDURES to compute a quick approximation to the first NONTRIVIAL EIGENVECTOR of a DATA GRAPH LAPLACIAN exactly solve certain REGULARIZED SEMI-DEFINITE PROGRAMS . in this paper , we extend that result by providing a statistical interpretation of their APPROXIMATION PROCEDURE . our interpretation will be analogous to the manner in which 2-REGULARIZED OR 1-REGULARIZED 2-REGRESSION -lrb- often called RIDGE REGRESSION and LASSO REGRESSION , respectively -rrb- can be interpreted in terms of a GAUSSIAN PRIOR or a LAPLACE PRIOR , respectively , on the COEFFICIENT VECTOR of the REGRESSION PROBLEM . our framework will imply that the solutions to the MAHONEY-ORECCHIA REGULARIZED SDP can be interpreted as REGULARIZED ESTIMATES of the <unk> of the DATA GRAPH LAPLACIAN . conversely , it will imply that the solution to this REGULARIZED ESTIMATION PROBLEM can be computed very quickly by running , e.g. , the fast DIFFUSION-BASED PAGERANK PROCEDURE for computing an approximation to the first NONTRIVIAL EIGENVECTOR of the DATA GRAPH LAPLACIAN . empirical results are also provided to illustrate the manner in which APPROXIMATE EIGENVECTOR COMPUTATION implicitly performs STATISTICAL REGULARIZATION , relative to running the corresponding exact algorithm . \n",
            "this paper addresses the problem of STATISTICAL REGULARIZATION in a REGULARIZED ESTIMATION PROBLEM . the main idea is to design a GAUSSIAN PRIOR for the REGRESSION PROBLEM . the proposed algorithm is based on the DIFFUSION-BASED PAGERANK PROCEDURE , which is based on the DIFFUSION-BASED PAGERANK PROCEDURE . the proposed algorithm is based on the use of the LAPLACE PRIOR , which is a GAUSSIAN PRIOR of the COEFFICIENT VECTOR . the proposed algorithm is based on the DIFFUSION-BASED PAGERANK PROCEDURE , which is based on the DIFFUSION-BASED PAGERANK PROCEDURE . the proposed algorithm is based on the DIFFUSION-BASED PAGERANK PROCEDURE , which is a generalization of the DIFFUSION-BASED PAGERANK PROCEDURE . the proposed algorithm is applied to the problem of RIDGE REGRESSION and LASSO REGRESSION . the performance of the proposed algorithm is demonstrated on a variety of DIFFUSION-BASED PROCEDURES and LASSO REGRESSION .\n",
            "\n",
            "12 1000\n",
            "network ALARM TRIAGE refers to GROUPING and <unk> a stream of LOW-LEVEL DEVICE HEALTH INFORMATION to help operators find and fix problems . today , this process tends to be largely manual because existing RULE-BASED TOOLS can not easily evolve with the network . we present CUET , a CUET that uses INTERACTIVE MACHINE LEARNING to constantly learn from the TRIAGING DECISIONS OF OPERATORS . CUET then uses that learning in novel visualizations to help them quickly and accurately <unk> alarms . unlike prior INTERACTIVE MACHINE LEARNING SYSTEMS , CUET handles a highly dynamic environment where the groups of interest are not known a priori and evolve constantly . our evaluations with real operators and data from a large network show that CUET significantly improves the speed and ACCURACY of ALARM TRIAGE . \n",
            "this paper presents a novel approach to INTERACTIVE MACHINE LEARNING for GROUPING . the proposed approach is based on the use of RULE-BASED TOOLS to improve the ACCURACY of the NETWORK ALARM TRIAGE . the proposed approach is based on the use of RULE-BASED TOOLS , which is based on the TRIAGING DECISIONS OF OPERATORS . the proposed approach is based on the use of RULE-BASED TOOLS , which allows the TRIAGING DECISIONS OF OPERATORS to be adapted to the target language . the proposed approach is evaluated on a variety of RULE-BASED TOOLS . the results show that the proposed method is effective in improving the ACCURACY and ACCURACY of the proposed method .\n",
            "\n",
            "13 1000\n",
            "we consider the problem of CLUSTERING DATA lying on multiple SUBSPACES of unknown and possibly different dimensions . we show that one can represent the SUBSPACES with a set of POLYNOMIALS whose derivatives at a data point give normal vectors to the SUBSPACE associated with the data point . since the POLYNOMIALS can be estimated linearly from data , SUBSPACE CLUSTERING is reduced to classifying one point per SUBSPACE . we do so by choosing points in the data set that minimize a DISTANCE FUNCTION . a basis for the complement of each SUBSPACE is then recovered by applying standard PCA to the set of derivatives -lrb- normal vectors -rrb- at those points . the final result is a new GPCA ALGORITHM for SUBSPACE CLUSTERING based on simple LINEAR AND POLYNOMIAL ALGEBRA . our experiments show that our GPCA ALGORITHM outperforms existing ALGEBRAIC ALGORITHMS based on POLYNOMIAL FACTORIZATION and provides a good INITIALIZATION to ITERATIVE TECHNIQUES such as K-SUBSPACE and EM . we also present applications of PCA on COMPUTER VISION PROBLEMS such as VANISHING POINT DETECTION , FACE CLUSTERING , and NEWS VIDEO SEGMENTATION . \n",
            "this paper proposes a novel GPCA ALGORITHM based on a LINEAR AND POLYNOMIAL ALGEBRA . the proposed GPCA ALGORITHM is based on a LINEAR AND POLYNOMIAL ALGEBRA , which is a DISTANCE FUNCTION of the SUBSPACE . the proposed GPCA ALGORITHM is based on a LINEAR AND POLYNOMIAL ALGEBRA , which is a DISTANCE FUNCTION of the SUBSPACE . the proposed GPCA ALGORITHM is based on a LINEAR AND POLYNOMIAL ALGEBRA , which is a DISTANCE FUNCTION . the proposed GPCA ALGORITHM is applied to the problem of NEWS VIDEO SEGMENTATION , such as NEWS VIDEO SEGMENTATION , FACE CLUSTERING , and NEWS VIDEO SEGMENTATION . the proposed GPCA ALGORITHM is evaluated on a variety of COMPUTER VISION PROBLEMS including NEWS VIDEO SEGMENTATION , NEWS VIDEO SEGMENTATION , NEWS VIDEO SEGMENTATION and NEWS VIDEO SEGMENTATION . the experimental results show that the proposed GPCA ALGORITHM is effective in the context of NEWS VIDEO SEGMENTATION , especially in the presence of CLUSTERING DATA and NEWS VIDEO SEGMENTATION .\n",
            "\n",
            "14 1000\n",
            "in an attempt to estimate the SEMANTIC CASES for NOUN-PARTICLE-VERB TRIPLES in the ATR DIALOGUE CORPUS , the authors propose a MEASURE OF DISTANCE based on STATISTICS OF DEPENDENT NOUN-PARTICLE-VERB TRIPLES . a CLUSTERING ANALYSIS of all the triples in the corpus was conducted using the MEASURE OF DISTANCE . competence of the proposed MEASURE OF DISTANCE is verified by examination of the distribution of the SINGLE-CASE CLUSTERS . by use of the score derived from the MEASURE OF DISTANCE of the training corpus , the authors conducted the estimation of the correct semantic case for a given NOUN-PARTICLE-VERB TRIPLES in the test corpus . the result remarkably differentiates the particles with respect to the ESTIMATION ACCURACIES . for instance , particle ` <unk> ' has accuracies over 80 % , while ` de ' has accuracies less than 40 % . the CORRELATION ANALYSIS between the ACCURACY and the CONSISTENCY RATES indicates that the particles of higher consistency have also tendencies to higher accuracies . \n",
            "this paper addresses the problem of CLUSTERING ANALYSIS in the ATR DIALOGUE CORPUS . we propose a new method for CLUSTERING ANALYSIS based on STATISTICS OF DEPENDENT NOUN-PARTICLE-VERB TRIPLES . the proposed method consists of two steps : -lrb- 1 -rrb- a set of SINGLE-CASE CLUSTERS in the STATISTICS OF DEPENDENT NOUN-PARTICLE-VERB TRIPLES , and -lrb- 2 -rrb- a MEASURE OF DISTANCE based on the STATISTICS OF DEPENDENT NOUN-PARTICLE-VERB TRIPLES . the proposed method is evaluated on a ATR DIALOGUE CORPUS containing SEMANTIC CASES , and the results show that the proposed method is effective in improving the ACCURACY of the proposed CLUSTERING ANALYSIS .\n",
            "\n",
            "15 1000\n",
            "this paper provides a new perspective on HUMAN MOTION ANALYSIS , namely regarding human motions in video as GENERAL DISCRETE TIME SIGNALS . while this seems an intuitive idea , research on HUMAN MOTION ANALYSIS has attracted little attention from the SIGNAL PROCESSING COMMUNITY . SOPHISTICATED SIGNAL PROCESSING TECHNIQUES create important opportunities for new solutions to the problem of HUMAN MOTION ANALYSIS . this paper investigates how the deformations of human silhouettes -lrb- or shapes -rrb- during ARTICULATED MOTION can be used as DISCRIMINATING FEATURES to implicitly capture MOTION DYNAMICS . in particular , we demonstrate the applicability of two widely used SIGNAL TRANSFORM METHODS , namely the DISCRETE FOURIER TRANSFORM and DISCRETE WAVELET TRANSFORM , for CHARACTERIZATION AND RECOGNITION OF HUMAN MOTION SEQUENCES . experimental results show the effectiveness of the proposed method on two state-of-the-art DATA SETS . \n",
            "this paper addresses the problem of CHARACTERIZATION AND RECOGNITION OF HUMAN MOTION SEQUENCES from ARTICULATED MOTION . we propose a novel approach to the CHARACTERIZATION AND RECOGNITION OF HUMAN MOTION SEQUENCES from ARTICULATED MOTION . the proposed approach is based on a DISCRETE WAVELET TRANSFORM , a DISCRETE WAVELET TRANSFORM , and a DISCRETE WAVELET TRANSFORM . the proposed approach is based on the use of a DISCRETE WAVELET TRANSFORM and a DISCRETE WAVELET TRANSFORM to estimate the MOTION DYNAMICS . the experimental results show that the proposed method is effective and robust to ARTICULATED MOTION .\n",
            "\n",
            "16 1000\n",
            "spectral maxima sound coding algorithms , for example N-OF-M STRATEGIES , used in COMMERCIAL COCHLEAR IMPLANT DEVICES rely on selecting channels with the highest energy in each frequency band . this SPECTRAL MAXIMA SOUND CODING ALGORITHMS works well in quiet , but is inherently problematic in noisy conditions when NOISE dominates the target , and NOISE-DOMINANT CHANNELS are mistakenly selected for STIMULATION . a new CHANNEL SELECTION CRITERION is proposed to addresses this shortcoming which adaptively assigns weights to each TIME-FREQUENCY UNIT based on the FORMANT LOCATION OF SPEECH and INSTANTANEOUS SIGNAL to NOISE ratio . the performance of the proposed SPECTRAL MAXIMA SOUND CODING ALGORITHMS is evaluated <unk> with three COCHLEAR IMPLANT USERS in different NOISE SCENARIOS . results indicate that the proposed SPECTRAL MAXIMA SOUND CODING ALGORITHMS improves SPEECH INTELLIGIBILITY and PERCEPTION QUALITY , particularly at LOW SIGNAL-TO-NOISE RATIO . significance of the proposed SPECTRAL MAXIMA SOUND CODING ALGORITHMS lies in its ability to be integrated with the existing SOUND CODING FRAMEWORK employed within COMMERCIAL COCHLEAR IMPLANT PROCESSORS , making SPECTRAL MAXIMA SOUND CODING ALGORITHMS easier to adapt for RESOURCE-LIMITED AND TIME CRITICAL DEVICES . \n",
            "this paper presents a novel SOUND CODING FRAMEWORK for COCHLEAR IMPLANT USERS in COMMERCIAL COCHLEAR IMPLANT DEVICES . the proposed SPECTRAL MAXIMA SOUND CODING ALGORITHMS are based on a SOUND CODING FRAMEWORK of the INSTANTANEOUS SIGNAL of the INSTANTANEOUS SIGNAL and the INSTANTANEOUS SIGNAL of the INSTANTANEOUS SIGNAL . the proposed SPECTRAL MAXIMA SOUND CODING ALGORITHMS are based on the CHANNEL SELECTION CRITERION . the proposed SPECTRAL MAXIMA SOUND CODING ALGORITHMS are applied to COCHLEAR IMPLANT USERS in order to reduce the PERCEPTION QUALITY . the proposed SPECTRAL MAXIMA SOUND CODING ALGORITHMS are evaluated in terms of PERCEPTION QUALITY and LOW SIGNAL-TO-NOISE RATIO . the performance of the proposed SPECTRAL MAXIMA SOUND CODING ALGORITHMS is demonstrated on a variety of COMMERCIAL COCHLEAR IMPLANT DEVICES . the results show that the proposed SPECTRAL MAXIMA SOUND CODING ALGORITHMS can reduce the PERCEPTION QUALITY by up to 50 % compared to the conventional SPECTRAL MAXIMA SOUND CODING ALGORITHMS .\n",
            "\n",
            "17 1000\n",
            "the SCENE FLOW in BINOCULAR STEREO SETUP is estimated using a SEED GROWING ALGORITHM . a pair of calibrated and synchronized cameras observe a scene and output a sequence of IMAGES . the algorithm jointly computes a DISPARITY MAP between the STEREO IMAGES and OPTICAL FLOW MAPS between consecutive frames . having the CALIBRATION , this is a representation of the SCENE FLOW , i.e. a 3D VELOCITY VECTOR is associated with each RECONSTRUCTED 3D POINT . the proposed algorithm starts from correspondence seeds and propagates the correspondences to the neighborhood . it is accurate for complex scenes with large motion and produces TEMPORALLY COHERENT STEREO DISPARITY and OPTICAL FLOW results . the algorithm is fast due to INHERENT SEARCH SPACE REDUCTION . \n",
            "in this paper , we propose a novel SEED GROWING ALGORITHM for STEREO IMAGES . the proposed approach is based on the use of a SEED GROWING ALGORITHM to estimate the 3D VELOCITY VECTOR and the OPTICAL FLOW MAPS . the proposed SEED GROWING ALGORITHM is based on a SEED GROWING ALGORITHM , which is based on a SEED GROWING ALGORITHM . the proposed SEED GROWING ALGORITHM is based on a SEED GROWING ALGORITHM that uses a SEED GROWING ALGORITHM to estimate the 3D VELOCITY VECTOR and the OPTICAL FLOW MAPS . the experimental results show that the proposed SEED GROWING ALGORITHM is able to estimate the 3D VELOCITY VECTOR of the RECONSTRUCTED 3D POINT and the OPTICAL FLOW MAPS .\n",
            "\n",
            "18 1000\n",
            "in this paper we report on advances regarding our approach to porting an AUTOMATIC SPEECH RECOGNITION SYSTEM to a new target task . in case there is not enough ACOUSTIC DATA available to allow for thorough estimation of HMM PARAMETERS it is impossible to train an appropriate model . the basic idea to overcome this problem is to create a TASK INDEPENDENT SEED MODEL that can cope with all tasks equally well . however , the performance of such <unk> model is of course lower than the performance of TASK DEPENDENT MODELS -lrb- if these were available -rrb- . so , the TASK INDEPENDENT SEED MODEL is gradually enhanced by using its own RECOGNITION results for INCREMENTAL ONLINE TASK ADAPTATION . here , we use a MULTILINGUAL ROMANIC/GERMANIC SEED MODEL for a SLAVIC TARGET TASK . in tests on SLOVENE DIGITS MULTILINGUAL MODELING yields the best RECOGNITION ACCURACY compared to other LANGUAGE DEPENDENT MODELS . applying UNSUPERVISED ONLINE TASK ADAPTATION we observe a remarkable boost of RECOGNITION performance . \n",
            "this paper addresses the problem of UNSUPERVISED ONLINE TASK ADAPTATION in the context of SLOVENE DIGITS MULTILINGUAL MODELING for UNSUPERVISED ONLINE TASK ADAPTATION . in particular , we propose a novel MULTILINGUAL ROMANIC/GERMANIC SEED MODEL based on UNSUPERVISED ONLINE TASK ADAPTATION . the proposed MULTILINGUAL ROMANIC/GERMANIC SEED MODEL is based on the use of a MULTILINGUAL ROMANIC/GERMANIC SEED MODEL , which is based on a MULTILINGUAL ROMANIC/GERMANIC SEED MODEL . the proposed MULTILINGUAL ROMANIC/GERMANIC SEED MODEL is evaluated on the SLAVIC TARGET TASK , and the results show that the proposed MULTILINGUAL ROMANIC/GERMANIC SEED MODEL is effective in improving the RECOGNITION ACCURACY of the TASK DEPENDENT MODELS . moreover , the RECOGNITION ACCURACY of the proposed MULTILINGUAL ROMANIC/GERMANIC SEED MODEL is improved by the proposed MULTILINGUAL ROMANIC/GERMANIC SEED MODEL .\n",
            "\n",
            "19 1000\n",
            "this paper presents a new UNSUPERVISED ALGORITHM -lrb- WORDENDS -rrb- for INFERRING WORD BOUNDARIES from TRANSCRIBED ADULT CONVERSATIONS . PHONE NGRAMS before and after observed pauses are used to bootstrap a simple DIS-CRIMINATIVE MODEL OF BOUNDARY MARKING . this fast algorithm delivers high performance even on MORPHOLOGICALLY COMPLEX WORDS in ENGLISH and ARABIC , and promising results on accurate PHONETIC TRANSCRIPTIONS with extensive pronunciation variation . expanding training data beyond the traditional MINIATURE DATASETS pushes performance numbers well above those previously reported . this suggests that WORDENDS is a viable model of CHILD LANGUAGE ACQUISITION and might be useful in SPEECH UNDERSTANDING . \n",
            "in this paper , we present a novel approach to DIS-CRIMINATIVE MODEL OF BOUNDARY MARKING for CHILD LANGUAGE ACQUISITION . the proposed approach is based on the use of a WORDENDS for DIS-CRIMINATIVE MODEL OF BOUNDARY MARKING , which is based on a WORDENDS for DIS-CRIMINATIVE MODEL OF BOUNDARY MARKING . the proposed method is based on the use of PHONE NGRAMS and WORDENDS . the proposed method is based on a WORDENDS for DIS-CRIMINATIVE MODEL OF BOUNDARY MARKING . the proposed method is evaluated on a variety of MINIATURE DATASETS and ARABIC . the results show that the proposed method outperforms the state-of-the-art methods in terms of INFERRING WORD BOUNDARIES and ARABIC .\n",
            "\n",
            "20 1000\n",
            "we report an investigation of the perception of AMERICAN ENGLISH PHONEMES by DUTCH LISTENERS proficient in ENGLISH . listeners identified either the CONSONANT or the VOWEL in most possible ENGLISH cv and vc syllables . the syllables were embedded in MULTISPEAKER BABBLE at three SIGNAL-TO-NOISE RATIOS -lrb- 16 db , 8 db , and 0 db -rrb- . effects of SIGNAL-TO-NOISE RATIO on VOWEL AND CONSONANT IDENTIFICATION are discussed as a function of SYLLABLE POSITION and of relationship to the NATIVE PHONEME INVENTORY . comparison of the results with previously reported data from NATIVE LISTENERS reveals that noise affected the responding of native and non-native listeners similarly . \n",
            "in this paper we present a method for VOWEL AND CONSONANT IDENTIFICATION in the presence of ENGLISH . the method is based on the use of a NATIVE PHONEME INVENTORY and a VOWEL to estimate the SYLLABLE POSITION and the VOWEL . the method is based on the use of a NATIVE PHONEME INVENTORY and a NATIVE PHONEME INVENTORY to estimate the SYLLABLE POSITION and the VOWEL . the performance of the proposed method is evaluated on a variety of AMERICAN ENGLISH PHONEMES . the results show that the performance of the proposed method is comparable to the state of the art .\n",
            "\n",
            "21 1000\n",
            "the STATUS QUO APPROACH to training OBJECT DETECTORS requires expensive BOUNDING BOX ANNOTATIONS . our STATUS QUO APPROACH takes a markedly different direction : we transfer TRACKED OBJECT BOXES from WEAKLY-LABELED VIDEOS to WEAKLY-LABELED IMAGES to automatically generate PSEUDO GROUND-TRUTH BOXES , which replace MANUALLY ANNOTATED BOUNDING BOXES . we first mine DISCRIMINATIVE REGIONS in the WEAKLY-LABELED IMAGE COLLECTION that <unk> appear in the POSI-TIVE/NEGATIVE IMAGES . we then match those regions to videos and retrieve the corresponding TRACKED OBJECT BOXES . finally , we design a HOUGH TRANSFORM ALGORITHM to vote for the best box to serve as the PSEUDO GT for each image , and use HOUGH TRANSFORM ALGORITHM to train an OBJECT DETECTOR . together , DISCRIMINATIVE REGIONS lead to state-of-the-art <unk> detection results on the PASCAL 2007 AND 2010 DATASETS . \n",
            "this paper presents a novel approach to WEAKLY-LABELED IMAGE COLLECTION based on the HOUGH TRANSFORM ALGORITHM . the proposed STATUS QUO APPROACH is based on the use of a STATUS QUO APPROACH to estimate the DISCRIMINATIVE REGIONS of the TRACKED OBJECT BOXES . the proposed method is based on the use of PSEUDO GROUND-TRUTH BOXES in the form of a STATUS QUO APPROACH . the proposed method is based on the use of PSEUDO GROUND-TRUTH BOXES in the form of PSEUDO GROUND-TRUTH BOXES . the proposed method is evaluated on both PASCAL 2007 AND 2010 DATASETS . the experimental results show that the proposed method outperforms the state-of-the-art methods in terms of both PSEUDO GT and BOUNDING BOX ANNOTATIONS .\n",
            "\n",
            "22 1000\n",
            "in this paper we study how children of different age groups -lrb- 8 and 12 years old -rrb- and with different cultural backgrounds -lrb- dutch and PAKISTANI -rrb- signal positive and negative emotions in AUDIOVISUAL SPEECH . data was collected in an ethical way using a simple but surprisingly effective game in which pairs of participants have to guess whether an upcoming card will contain a higher or lower number than a reference card . the data thus collected was used in a series of CROSS-CULTURAL PERCEPTION STUDIES , in which dutch and PAKISTANI observers classified emotional expressions of dutch and PAKISTANI speakers . results show that CLASSIFICATION ACCURACY is uniformly high for PAKISTANI CHILDREN , but drops for older and for winning dutch children 1 . \n",
            "this paper presents a novel approach to PAKISTANI CHILDREN . the proposed approach is based on the use of a set of PAKISTANI CHILDREN , each of which is a set of AUDIOVISUAL SPEECH . the proposed approach is based on the use of PAKISTANI CHILDREN . the proposed approach is evaluated on a number of PAKISTANI CHILDREN . the experimental results show that the proposed method is effective in improving the CLASSIFICATION ACCURACY of PAKISTANI CHILDREN .\n",
            "\n",
            "23 1000\n",
            "the EXPECTATION-MAXIMIZATION is the dominant algorithm for estimating the parameters of a GAUSS MIXTURE . recently , GAUSS MIXTURE VECTOR QUANTIZATION based on the LLOYD ALGORITHM has been applied successfully as an alternative for both COMPRESSION and CLASSIFICATION . we investigate the performance of the two algorithms for <unk> 's in IMAGE RETRIEVAL . the ASYMPTOTIC LIKELIHOOD APPROXIMATION is used as a SIMILARITY CRITERION to compare <unk> 's directly . the two algorithms result in very close retrieval performance . we demonstrate that the closeness comes from the close mutual approximation of the estimated GM PARAMETER VALUES and that the two algorithms have similar CONVERGENCE SPEED . our analysis shows that GAUSS MIXTURE VECTOR QUANTIZATION has roughly half the COMPUTATIONAL COMPLEXITY of EM . \n",
            "in this paper , we propose a novel method for IMAGE RETRIEVAL in IMAGE RETRIEVAL . the proposed method is based on the idea of GAUSS MIXTURE VECTOR QUANTIZATION , which is a generalization of the LLOYD ALGORITHM to GAUSS MIXTURE VECTOR QUANTIZATION . the proposed method is based on the idea of GAUSS MIXTURE VECTOR QUANTIZATION , which is based on the ASYMPTOTIC LIKELIHOOD APPROXIMATION of the LLOYD ALGORITHM . in the proposed method , a SIMILARITY CRITERION is used to estimate the GM PARAMETER VALUES . the proposed method is based on the ASYMPTOTIC LIKELIHOOD APPROXIMATION of the LLOYD ALGORITHM . the proposed method is evaluated in terms of CONVERGENCE SPEED and COMPUTATIONAL COMPLEXITY . the experimental results show that the proposed method is effective in reducing the COMPUTATIONAL COMPLEXITY and the COMPUTATIONAL COMPLEXITY of the proposed method .\n",
            "\n",
            "24 1000\n",
            "over the last few years , two of the main research directions in MACHINE LEARNING of NATURAL LANGUAGE PROCESSING have been the study of SEMI-SUPERVISED LEARNING ALGORITHMS as a way to train CLASSIFIERS when the LABELED DATA is scarce , and the study of ways to exploit knowledge and global information in STRUCTURED LEARNING TASKS . in this paper , we suggest a method for incorporating DOMAIN KNOWLEDGE in SEMI-SUPERVISED LEARNING ALGORITHMS . our novel framework unifies and can exploit several kinds of task specific constraints . the experimental results presented in the INFORMATION EXTRACTION DOMAIN demonstrate that applying constraints helps the model to generate better feedback during learning , and hence the framework allows for high performance learning with significantly less training data than was possible before on these tasks . \n",
            "this paper addresses the problem of NATURAL LANGUAGE PROCESSING in the presence of LABELED DATA . the proposed SEMI-SUPERVISED LEARNING ALGORITHMS are based on the use of SEMI-SUPERVISED LEARNING ALGORITHMS for MACHINE LEARNING . the proposed SEMI-SUPERVISED LEARNING ALGORITHMS are based on the use of SEMI-SUPERVISED LEARNING ALGORITHMS for MACHINE LEARNING . the proposed SEMI-SUPERVISED LEARNING ALGORITHMS are applied to the problem of NATURAL LANGUAGE PROCESSING in the context of NATURAL LANGUAGE PROCESSING . the experimental results show that the proposed SEMI-SUPERVISED LEARNING ALGORITHMS can significantly improve the performance of the SEMI-SUPERVISED LEARNING ALGORITHMS in the presence of LABELED DATA .\n",
            "\n",
            "25 1000\n",
            "many approaches to REINFORCEMENT LEARNING combine NEURAL NETWORKS or other PARAMETRIC FUNCTION APPROXIMATORS with a form of TEMPORAL-DIIERENCE LEARNING to estimate the value function of a MARKOV DECISION PROCESS . a signiicant disadvantage of those procedures is that the resulting LEARNING ALGORITHMS are frequently unstable . in this work , we present a new , KERNEL-BASED APPROACH to REINFORCEMENT LEARNING which overcomes this diiculty and provably converges to a unique solution . by contrast to existing LEARNING ALGORITHMS , our KERNEL-BASED APPROACH can also be shown to be consistent in the sense that its costs converge to the optimal costs asymptotically . our focus is on learning in an AVERAGE-COST FRAMEWORK and on a practical application to the optimal PORTFOLIO CHOICE PROBLEM . \n",
            "this paper addresses the problem of PORTFOLIO CHOICE PROBLEM and REINFORCEMENT LEARNING . in particular , we consider the problem of REINFORCEMENT LEARNING and REINFORCEMENT LEARNING . in particular , we consider the problem of REINFORCEMENT LEARNING and REINFORCEMENT LEARNING . we propose a new KERNEL-BASED APPROACH , called NEURAL NETWORKS , which is based on a KERNEL-BASED APPROACH . we show that the proposed KERNEL-BASED APPROACH can be applied to a wide range of LEARNING ALGORITHMS and REINFORCEMENT LEARNING .\n",
            "\n",
            "26 1000\n",
            "we propose a BLOCK-ITERATIVE PARALLEL DECOMPOSITION METHOD to solve QUADRATIC SIGNAL RECOVERY PROBLEMS under CONVEX CONSTRAINTS . the idea of the BLOCK-ITERATIVE PARALLEL DECOMPOSITION METHOD is to <unk> the original MULTI-CONSTRAINT PROBLEM into a sequence of simple QUADRATIC MINIMIZATIONS over the intersection of two <unk> constructed by linearizing blocks of constraints . the implementation of the BLOCK-ITERATIVE PARALLEL DECOMPOSITION METHOD is quite exible thanks to its BLOCK-PARALLEL STRUCTURE . in addition a wide range of complex constraints can be incorporated since the BLOCK-ITERATIVE PARALLEL DECOMPOSITION METHOD does not require exact CONSTRAINT ENFORCEMENT at each step but merely APPROXIMATE ENFORCEMENT via LINEARIZATION . an application to DECONVOLUTION is demonstrated . \n",
            "in this paper , we propose a novel BLOCK-ITERATIVE PARALLEL DECOMPOSITION METHOD for QUADRATIC SIGNAL RECOVERY PROBLEMS . the proposed BLOCK-ITERATIVE PARALLEL DECOMPOSITION METHOD is based on the use of CONVEX CONSTRAINTS for APPROXIMATE ENFORCEMENT . the proposed BLOCK-ITERATIVE PARALLEL DECOMPOSITION METHOD is based on the use of CONVEX CONSTRAINTS for DECONVOLUTION . the proposed BLOCK-ITERATIVE PARALLEL DECOMPOSITION METHOD is based on the use of CONVEX CONSTRAINTS , which is a MULTI-CONSTRAINT PROBLEM . the proposed BLOCK-ITERATIVE PARALLEL DECOMPOSITION METHOD is applied to the problem of DECONVOLUTION , which is a MULTI-CONSTRAINT PROBLEM . the proposed BLOCK-ITERATIVE PARALLEL DECOMPOSITION METHOD is applied to the problem of DECONVOLUTION , and the experimental results demonstrate the effectiveness of the proposed BLOCK-ITERATIVE PARALLEL DECOMPOSITION METHOD .\n",
            "\n",
            "27 1000\n",
            "this paper proposes a practical OPTIMIZATION METHOD for LAYERED NEURAL NETWORKS , by which the optimal model and parameter can be found simultaneously . ` i \\ te modify the conventional INFORMATION CRITERION into a differentiable function of parameters , and then , minimize INFORMATION CRITERION , while controlling INFORMATION CRITERION back to the ordinary form . effectiveness of this OPTIMIZATION METHOD is discussed theoretically and experimentally . \n",
            "in this paper , we propose a novel OPTIMIZATION METHOD for LAYERED NEURAL NETWORKS . the proposed OPTIMIZATION METHOD is based on the use of a LAYERED NEURAL NETWORKS for LAYERED NEURAL NETWORKS . the proposed OPTIMIZATION METHOD is based on the use of the INFORMATION CRITERION of the LAYERED NEURAL NETWORKS . the proposed OPTIMIZATION METHOD is applied to the problem of LAYERED NEURAL NETWORKS . the experimental results show that the proposed OPTIMIZATION METHOD is effective in the sense that the proposed LAYERED NEURAL NETWORKS is very effective .\n",
            "\n",
            "28 1000\n",
            "this paper describes ongoing research on a JAPANESE-TO-ENGLISH SPEECH-TO-SPEECH TRANSLATION SYSTEM for '' controlled <unk> '' , such as TV NEWS and COMMENTARY PROGRAMS in which the SPEAKING STYLES are controlled as a <unk> . we have adopted the DATA-DRIVEN APPROACH since the tv programs in question cover a wide range of topics , and because it seems much too labor intensive to <unk> TRANSLATION RULES . the DATA-DRIVEN APPROACH therefore requires a <unk> PARALLEL CORPUS for the target domain , although the ABSOLUTE SIZE needed is not so easy to obtain . there are also difficulties inherent in <unk> such as the need to handle long sentences , averaging over 25 words , and the REALIZATION OF SIMULTANEITY . these problems are presented in the light of our available corpora and we go on to present the kinds of problems we have to solve . finally , we present our prospective system architecture and introduce the present status of the work . \n",
            "in this paper , we present a novel approach to the problem of REALIZATION OF SIMULTANEITY in the context of TV NEWS . the proposed approach is based on the use of a DATA-DRIVEN APPROACH , a DATA-DRIVEN APPROACH , and COMMENTARY PROGRAMS . the proposed approach is based on the use of COMMENTARY PROGRAMS and COMMENTARY PROGRAMS . the proposed approach is evaluated on a PARALLEL CORPUS , and the results show that the proposed method is effective in improving the performance of the JAPANESE-TO-ENGLISH SPEECH-TO-SPEECH TRANSLATION SYSTEM .\n",
            "\n",
            "29 1000\n",
            "efficient two-step algorithms are described for optimizing the STOPBAND RESPONSE of the PROTOTYPE FILTER for COSINE-MODULATED AND MODIFIED DFT FILTER BANKS either in the MINIMAX or in the least-mean-square sense subject to the maximum allowable ALIASING AND AMPLITUDE ERRORS . the first step involves finding a good START-UP SOLUTION using a simple technique . this START-UP SOLUTION is improved in the second step by using NONLINEAR OPTIMIZATION . several examples are included illustrating the flexibility of the proposed START-UP SOLUTION for making compromises between the required FILTER LENGTHS and the ALIASING AND AMPLITUDE ERRORS . these examples show that by allowing very small amplitude and ALIASING ERRORS , the stopband performance of the resulting PROTOTYPE FILTER is significantly improved compared to the corresponding PERFECT-RECONSTRUCTION FILTER BANK . alternatively , the FILTER ORDERS and , consequently , the overall delay can be significantly reduced to achieve practically the same performance . \n",
            "in this paper , we propose a novel PROTOTYPE FILTER for COSINE-MODULATED AND MODIFIED DFT FILTER BANKS . the proposed PROTOTYPE FILTER is based on a PROTOTYPE FILTER , which is based on a PROTOTYPE FILTER . the proposed PROTOTYPE FILTER is based on the use of NONLINEAR OPTIMIZATION to estimate the STOPBAND RESPONSE . the proposed PROTOTYPE FILTER is based on the use of NONLINEAR OPTIMIZATION to estimate the ALIASING AND AMPLITUDE ERRORS of the PERFECT-RECONSTRUCTION FILTER BANK . the proposed PROTOTYPE FILTER is compared with the PERFECT-RECONSTRUCTION FILTER BANK and the PERFECT-RECONSTRUCTION FILTER BANK . experimental results show that the proposed PROTOTYPE FILTER is effective in reducing the number of FILTER LENGTHS in the presence of ALIASING ERRORS .\n",
            "\n",
            "30 1000\n",
            "in this work we describe two distinct novel improvements to our SPEAKER DIARIZATION SYSTEM , previously proposed for ANALYSIS OF MEETING SPEECH . the first approach focuses on recurrent selection of REPRESENTATIVE SPEECH SEGMENTS for SPEAKER CLUSTERING while the other is based on PARTICIPANT INTERACTION PATTERN MODELING . the former selects SPEECH SEGMENTS with high relevance to SPEAKER CLUSTERING , especially from a ROBUST CLUSTER MODELING PERSPECTIVE , and keeps updating them throughout CLUSTERING PROCEDURES . the latter statistically models CONVERSATION PATTERNS between meeting participants and applies it as a PRIORI INFORMATION when refining diarization results . experimental results reveal that the two proposed approaches provide performance enhancement by <unk> % -lrb- relative -rrb- in terms of DI-ARIZATION ERROR RATE in tests on 13 meeting excerpts from various MEETING SPEECH CORPORA . \n",
            "this paper presents a novel approach to PARTICIPANT INTERACTION PATTERN MODELING based on PARTICIPANT INTERACTION PATTERN MODELING . the proposed approach is based on the use of a ROBUST CLUSTER MODELING PERSPECTIVE and a ROBUST CLUSTER MODELING PERSPECTIVE to estimate the CONVERSATION PATTERNS . the proposed approach is based on the use of a ROBUST CLUSTER MODELING PERSPECTIVE , which is able to deal with SPEECH SEGMENTS . the proposed approach is evaluated on a database of MEETING SPEECH CORPORA and MEETING SPEECH CORPORA . the experimental results show that the proposed approach is effective in improving the ANALYSIS OF MEETING SPEECH performance in terms of DI-ARIZATION ERROR RATE and DI-ARIZATION ERROR RATE .\n",
            "\n",
            "31 1000\n",
            "we model the responses of cells in VISUAL AREA VI during NATURAL VISION . our model consists of a CLASSICAL ENERGY MECHANISM whose output is divided by NONCLASSICAL GAIN CONTROL and texture contrast mechanisms . we apply this model to review movies , a STIMULUS SEQUENCE that <unk> the stimulation a cell receives during free viewing of NATURAL IMAGES . data were collected from three cells using five different review movies , and the model was fit separately to the data from each movie . for the ENERGY MECHANISM alone we find modest but significant correlations -lrb- re = <unk> , <unk> , <unk> , 0.35 -rrb- between model and data . these correlations are improved somewhat when we allow for SUPPRESSIVE SURROUND EFFECTS -lrb- re + g = <unk> , <unk> , <unk> , <unk> -rrb- . in one case the inclusion of a DELAYED SUPPRESSIVE SURROUND dramatically improves the fit to the data by modifying the time course of the model 's response . \n",
            "this paper addresses the problem of DELAYED SUPPRESSIVE SURROUND in the presence of NATURAL IMAGES . the main contribution of this paper is a new method to estimate the STIMULUS SEQUENCE based on the CLASSICAL ENERGY MECHANISM . the proposed approach is based on the CLASSICAL ENERGY MECHANISM , which is based on the CLASSICAL ENERGY MECHANISM . the proposed approach is based on the CLASSICAL ENERGY MECHANISM . the proposed method is based on the CLASSICAL ENERGY MECHANISM . the proposed method is evaluated in the context of VISUAL AREA VI .\n",
            "\n",
            "32 1000\n",
            "the most popular TIME-FREQUENCY ANALYSIS TOOL , the SHORT-TIME FOURIER TRANSFORM , suffers from BLURRY HARMONIC REPRESENTATION when VOICED SPEECH undergoes changes in pitch . these relatively fast variations lead to INCONSISTENT BINS in frequency domain and can not be accurately described by the FOURIER ANALYSIS with high resolution both in time and frequency . in this paper a new ANALYSIS TOOL , called SHORT-TIME FOURIER TRANSFORM is presented , offering more precise time-frequency representation of speech signals . the base of this SHORT-TIME FOURIER TRANSFORM is composed of QUADRATIC CHIRPS that follow the PITCH TENDENCY SEGMENT-BY-SEGMENT . comparative results between the proposed SHORT-TIME FOURIER TRANSFORM and popular TIME-FREQUENCY TECHNIQUES reveal an improvement in TIME-FREQUENCY LOCALIZATION and finer SPECTRAL REPRESENTATION . since the signal can be resynthesized from its SHORT-TIME FOURIER TRANSFORM , the proposed ANALYSIS TOOL is also suitable for FILTERING PURPOSES . \n",
            "this paper presents a novel ANALYSIS TOOL called BLURRY HARMONIC REPRESENTATION , called BLURRY HARMONIC REPRESENTATION , for TIME-FREQUENCY LOCALIZATION in VOICED SPEECH . the proposed TIME-FREQUENCY ANALYSIS TOOL is based on a TIME-FREQUENCY ANALYSIS TOOL called the SHORT-TIME FOURIER TRANSFORM , which is based on a BLURRY HARMONIC REPRESENTATION called the SHORT-TIME FOURIER TRANSFORM . the proposed TIME-FREQUENCY ANALYSIS TOOL is a TIME-FREQUENCY ANALYSIS TOOL , a BLURRY HARMONIC REPRESENTATION , and a BLURRY HARMONIC REPRESENTATION . the proposed ANALYSIS TOOL is based on a BLURRY HARMONIC REPRESENTATION , called the SHORT-TIME FOURIER TRANSFORM , which is a SPECTRAL REPRESENTATION . the proposed ANALYSIS TOOL is a TIME-FREQUENCY ANALYSIS TOOL , which is able to deal with INCONSISTENT BINS in the presence of INCONSISTENT BINS . the experimental results show that the proposed ANALYSIS TOOL is effective in reducing the number of INCONSISTENT BINS in the presence of VOICED SPEECH .\n",
            "\n",
            "33 1000\n",
            "we present an algorithm for REVERBERANT SPEECH ENHANCEMENT using one microphone . we first propose a novel PITCH-BASED REVERBERATION MEASURE for ESTIMATING REVERBERATION TIME based on the distribution of RELATIVE TIME LAGS . this measure of PITCH STRENGTH correlates with REVERBERATION and decreases systematically as detrimental effects of REVERBERATION on HARMONIC STRUCTURE increase . then a REVERBERANT SPEECH ENHANCEMENT METHOD is developed to estimate and <unk> later ECHO COMPONENTS . the results show that our approach appreciably reduces REVERBERATION EFFECTS . \n",
            "in this paper , we present a novel approach to REVERBERANT SPEECH ENHANCEMENT based on a PITCH-BASED REVERBERATION MEASURE . the proposed approach is based on a PITCH-BASED REVERBERATION MEASURE , which is based on a PITCH-BASED REVERBERATION MEASURE . the proposed method uses a PITCH-BASED REVERBERATION MEASURE to estimate the PITCH STRENGTH of the ECHO COMPONENTS . the proposed method is evaluated in terms of PITCH STRENGTH and REVERBERATION EFFECTS . the results show that the proposed method is effective in reducing the number of ECHO COMPONENTS in the presence of REVERBERATION .\n",
            "\n",
            "34 1000\n",
            "this paper presents several fundamental FREQUENCY-DOMAIN BOUNDS for a NON-NEGATIVE IMPULSE RESPONSE FILTER . UPPER-BOUNDS on POWER SPECTRAL ATTENUATION and POWER SPECTRAL GAIN in GEOMETRICALLY SPACED FREQUENCY REGIONS are derived , when POWER SPECTRAL ATTENUATION near frequency zero is limited . by analyzing the <unk> of these bounds , the relationship between the MAXIMALLY ALLOWABLE POWER ATTENUA-TION and gain is also treated . all results hold for both CONTINUOUS AND DISCRETE-TIME DOMAINS . \n",
            "this paper addresses the problem of GEOMETRICALLY SPACED FREQUENCY REGIONS in the presence of GEOMETRICALLY SPACED FREQUENCY REGIONS . we propose a novel method to estimate the POWER SPECTRAL GAIN of the NON-NEGATIVE IMPULSE RESPONSE FILTER . the proposed algorithm is based on the use of the MAXIMALLY ALLOWABLE POWER ATTENUA-TION of the NON-NEGATIVE IMPULSE RESPONSE FILTER . the proposed algorithm is based on the MAXIMALLY ALLOWABLE POWER ATTENUA-TION of the NON-NEGATIVE IMPULSE RESPONSE FILTER . the proposed algorithm is applied to the problem of GEOMETRICALLY SPACED FREQUENCY REGIONS , and the results show that the proposed algorithm is able to achieve the same performance as the optimal NON-NEGATIVE IMPULSE RESPONSE FILTER .\n",
            "\n",
            "35 1000\n",
            "this paper presents a framework for <unk> belief change in PROPOSITIONAL HORN LOGIC . we firstly establish a PARALLEL INTERPOLATION THEOREM for HORN LOGIC and show that <unk> 's finest splitting theorem holds with HORN FORMULAE . by reformulating PARIKH 'S RELEVANCE CRITERION in the setting of horn belief change , we construct a RELEVANCE-BASED PARTIAL MEET HORN CONTRACTION OPERATOR and provide a REPRESENTATION THEOREM for the operator . interestingly , we find that this RELEVANCE-BASED PARTIAL MEET HORN CONTRACTION OPERATOR can be fully characterised by <unk> and <unk> 's postulates for PARTIAL MEET HORN CONTRACTION as well as PARIKH 'S RELEVANCE POSTULATE without requiring any change on the postulates , which is qualitatively different from the case in CLASSICAL PROPOSITIONAL LOGIC . \n",
            "this paper presents a novel REPRESENTATION THEOREM for PROPOSITIONAL HORN LOGIC . the proposed RELEVANCE-BASED PARTIAL MEET HORN CONTRACTION OPERATOR is based on a RELEVANCE-BASED PARTIAL MEET HORN CONTRACTION OPERATOR , which is based on a RELEVANCE-BASED PARTIAL MEET HORN CONTRACTION OPERATOR . the proposed RELEVANCE-BASED PARTIAL MEET HORN CONTRACTION OPERATOR is based on a PARALLEL INTERPOLATION THEOREM , which is based on the PARIKH 'S RELEVANCE CRITERION . the proposed RELEVANCE-BASED PARTIAL MEET HORN CONTRACTION OPERATOR is based on a RELEVANCE-BASED PARTIAL MEET HORN CONTRACTION OPERATOR , which is based on the PARIKH 'S RELEVANCE CRITERION . the proposed RELEVANCE-BASED PARTIAL MEET HORN CONTRACTION OPERATOR is applied to the PARIKH 'S RELEVANCE POSTULATE of the CLASSICAL PROPOSITIONAL LOGIC , and the results show that the proposed RELEVANCE-BASED PARTIAL MEET HORN CONTRACTION OPERATOR is very effective .\n",
            "\n",
            "36 1000\n",
            "in this paper , based on LARGE SPEECH CORPUS with prosodic structure label -lrb- <unk> -rrb- , we present some statistic result on ACOUSTIC PARAMETER at PROSODIC BOUNDARY . we study the SYLLABLE DURATION , INTENSITY and pitch at the boundary and select a SERIAL ACOUSTIC PARAMETER to train a CART . then the CART was employed to classify the PROSODIC BOUNDARY TYPE . the result shows that the PARAMETER characterize ACOUSTIC FEATURE of the PROSODIC BOUNDARY and the trained CART can classify different BOUNDARY EFFICIENCY . so it is possible to train STATISTICAL MODEL for PROSODIC BOUNDARY LOCATION in MANDARIN , this is very important both for SPEECH RECOGNITION and synthesis . \n",
            "in this paper , we propose a novel STATISTICAL MODEL for SPEECH RECOGNITION . the proposed method is based on a STATISTICAL MODEL that uses a STATISTICAL MODEL to estimate the PROSODIC BOUNDARY TYPE and the PROSODIC BOUNDARY . the proposed STATISTICAL MODEL is based on a STATISTICAL MODEL that uses a STATISTICAL MODEL to estimate the PROSODIC BOUNDARY TYPE and the INTENSITY . the proposed STATISTICAL MODEL is applied to the problem of SPEECH RECOGNITION in a LARGE SPEECH CORPUS . the experimental results show that the proposed method is robust to SYLLABLE DURATION , INTENSITY , and INTENSITY .\n",
            "\n",
            "37 1000\n",
            "our goal is to obtain a NOISE-FREE , HIGH RESOLUTION IMAGE , from an observed , noisy , low resolution -lrb- lr -rrb- IMAGE . the conventional approach of preprocessing the IMAGE with a DENOISING ALGORITHM , followed by applying a SUPER-RESOLUTION ALGORITHM , has an important limitation : along with NOISE , some high frequency content of the IMAGE -lrb- particularly textural detail -rrb- is invariably lost during the DENOISING STEP . this ` denoising loss ' restricts the performance of the subsequent SR STEP , wherein the challenge is to synthesize such TEXTURAL DETAILS . in this paper , we show that high frequency content in the NOISY IMAGE -lrb- which is <unk> removed by DENOISING ALGORITHMS -rrb- can be effectively used to obtain the MISSING TEXTURAL DETAILS in the HR DOMAIN . to do so , we first obtain HR VERSIONS of both the NOISY AND THE DENOISED IMAGES , using a PATCH-SIMILARITY BASED SR ALGORITHM . we then show that by taking a CONVEX COMBINATION of orientation and frequency selective bands of the noisy and the denoised HR IMAGE , we can obtain a desired HR IMAGE where -lrb- i -rrb- some of the TEXTURAL SIGNAL lost in the DENOISING STEP is effectively recovered in the HR DOMAIN , and -lrb- ii -rrb- additional textures can be easily synthesized by appropriately constraining the parameters of the CONVEX COMBINATION . we show that this PART-RECOVERY AND PART-SYNTHESIS OF TEXTURES through our algorithm yields HR IMAGE that are visually more pleasing than those obtained using the conventional PROCESSING PIPELINE . furthermore , our results show a consistent improvement in NUMERICAL METRICS , further <unk> the ability of our algorithm to recover LOST SIGNAL . \n",
            "this paper addresses the problem of NOISE in the HR DOMAIN . we propose a novel method to estimate the MISSING TEXTURAL DETAILS of a scene from a NOISE-FREE , HIGH RESOLUTION IMAGE . the proposed PATCH-SIMILARITY BASED SR ALGORITHM is based on a PATCH-SIMILARITY BASED SR ALGORITHM that exploits the PART-RECOVERY AND PART-SYNTHESIS OF TEXTURES in the HR DOMAIN . the proposed method is based on a PATCH-SIMILARITY BASED SR ALGORITHM that exploits the PART-RECOVERY AND PART-SYNTHESIS OF TEXTURES in the HR DOMAIN . the proposed method is based on a PATCH-SIMILARITY BASED SR ALGORITHM that exploits the PART-RECOVERY AND PART-SYNTHESIS OF TEXTURES in the HR DOMAIN . the proposed method is based on a PATCH-SIMILARITY BASED SR ALGORITHM that exploits the PART-RECOVERY AND PART-SYNTHESIS OF TEXTURES in the HR DOMAIN . the experimental results show that the proposed method outperforms the existing methods in terms of both NOISY AND THE DENOISED IMAGES and NOISE .\n",
            "\n",
            "38 1000\n",
            "we examined the sequence of DECISION PROBLEMS that are encountered in the game of TETRIS and found that most of the problems are easy in the following sense : one can choose well among the available actions without knowing an EVALUATION FUNCTION that scores well in the game . this is a consequence of three conditions that are prevalent in the game : simple DOMINANCE , CUMULATIVE DOMINANCE , and NONCOMPENSATION . these conditions can be exploited to develop faster and more effective LEARNING ALGORITHMS . in addition , they allow certain types of DOMAIN KNOWLEDGE to be incorporated with ease into a LEARNING ALGORITHMS . among the SEQUENTIAL DECISION PROBLEMS we encounter , it is unlikely that TETRIS is unique or rare in having these properties . \n",
            "this paper addresses the problem of SEQUENTIAL DECISION PROBLEMS in the presence of SEQUENTIAL DECISION PROBLEMS , CUMULATIVE DOMINANCE , and DOMINANCE . we propose a new algorithm for SEQUENTIAL DECISION PROBLEMS that is based on the use of DOMAIN KNOWLEDGE , CUMULATIVE DOMINANCE , CUMULATIVE DOMINANCE , and DOMINANCE . we show that our algorithm can be applied to SEQUENTIAL DECISION PROBLEMS , and we show how it can be applied to the problem of SEQUENTIAL DECISION PROBLEMS .\n",
            "\n",
            "39 1000\n",
            "camera images saved in RAW FORMAT are being adopted in COMPUTER VISION TASKS since RAW VALUES represent MINIMALLY PROCESSED SENSOR RESPONSES . CAMERA MANUFACTURERS , however , have yet to adopt a standard for RAW IMAGES and current RAW-RGB VALUES are device specific due to different SENSORS SPECTRAL SENSITIVITIES . this results in significantly different RAW IMAGES for the same scene captured with different cameras . this paper focuses on estimating a MAPPING that can convert a RAW IMAGE of an arbitrary scene and illumination from one camera 's raw space to another . to this end , we examine various MAPPING STRATEGIES including LINEAR AND NON-LINEAR TRANSFORMATIONS applied both in a GLOBAL AND ILLUMINATION-SPECIFIC MANNER . we show that ILLUMINATION-SPECIFIC MAPPINGS give the best result , however , at the expense of requiring a large number of transformations . to address this issue , we introduce an ILLUMINATION-INDEPENDENT MAPPING APPROACH that uses WHITE-BALANCING to assist in reducing the number of required transformations . we show that this ILLUMINATION-INDEPENDENT MAPPING APPROACH achieves state-of-the-art results on a range of CONSUMER CAMERAS and IMAGES OF ARBITRARY SCENES and ILLUMINATIONS . \n",
            "in this paper , we propose a novel ILLUMINATION-INDEPENDENT MAPPING APPROACH for IMAGES OF ARBITRARY SCENES . the proposed ILLUMINATION-INDEPENDENT MAPPING APPROACH is based on the use of a ILLUMINATION-INDEPENDENT MAPPING APPROACH and a ILLUMINATION-INDEPENDENT MAPPING APPROACH . the proposed ILLUMINATION-INDEPENDENT MAPPING APPROACH is based on a ILLUMINATION-INDEPENDENT MAPPING APPROACH , which is a MAPPING . the proposed ILLUMINATION-INDEPENDENT MAPPING APPROACH is based on the use of a MAPPING and a ILLUMINATION-INDEPENDENT MAPPING APPROACH . the proposed ILLUMINATION-INDEPENDENT MAPPING APPROACH is based on a ILLUMINATION-INDEPENDENT MAPPING APPROACH , which is a MAPPING . the proposed ILLUMINATION-INDEPENDENT MAPPING APPROACH is applied to IMAGES OF ARBITRARY SCENES , IMAGES OF ARBITRARY SCENES , IMAGES OF ARBITRARY SCENES and CONSUMER CAMERAS . the experimental results show that the proposed ILLUMINATION-INDEPENDENT MAPPING APPROACH outperforms the state-of-the-art methods in terms of both LINEAR AND NON-LINEAR TRANSFORMATIONS and ILLUMINATIONS .\n",
            "\n",
            "40 1000\n",
            "<unk> knowledge representation formalisms can be used to represent objective , <unk> facts about an APPLICATION DOMAIN . NOTIONS like BELIEF , INTENTIONS , and time which are essential for the REPRESENTATION OF MULTI-AGENT ENVIRONMENTS can only be expressed in a very limited way . for such notions , MODAL LOGICS with possible worlds semantics provides a formally well-founded and <unk> basis . this paper presents a framework for integrating MODAL OPERATORS into TERMINOLOGICAL KNOWLEDGE REPRESENTATION LANGUAGES . these MODAL OPERATORS can be used both inside of CONCEPT EXPRESSIONS and in front of <unk> and <unk> axioms . we introduce SYNTAX and semantics of the extended language , and show that satisfiability of finite sets of formulas is decidable , provided that all MODAL OPERATORS are interpreted in the basic logic k , and that the increasing DOMAIN ASSUMPTION is used . \n",
            "this paper addresses the problem of TERMINOLOGICAL KNOWLEDGE REPRESENTATION LANGUAGES in TERMINOLOGICAL KNOWLEDGE REPRESENTATION LANGUAGES . in particular , we focus on the problem of TERMINOLOGICAL KNOWLEDGE REPRESENTATION LANGUAGES in the APPLICATION DOMAIN . we show that the DOMAIN ASSUMPTION of the TERMINOLOGICAL KNOWLEDGE REPRESENTATION FORMALISMS can be expressed as a DOMAIN ASSUMPTION , and that the DOMAIN ASSUMPTION can be expressed as a DOMAIN ASSUMPTION . we show that this problem can be solved efficiently using the DOMAIN ASSUMPTION of the MODAL OPERATORS . we show that our approach can be applied to TERMINOLOGICAL KNOWLEDGE REPRESENTATION LANGUAGES , such as BELIEF , INTENTIONS , and INTENTIONS .\n",
            "\n",
            "41 1000\n",
            "various methods have been proposed for AUTOMATIC SYNONYM ACQUISITION , as SYNONYMS are one of the most fundamental LEXICAL KNOWLEDGE . whereas many methods are based on CONTEXTUAL CLUES OF WORDS , little attention has been paid to what kind of categories of CONTEX-TUAL INFORMATION are useful for the purpose . this study has experimentally investigated the impact of CONTEXTUAL INFORMATION SELECTION , by extracting three kinds of WORD RELATIONSHIPS from corpora : dependency , SENTENCE CO-OCCURRENCE , and PROXIMITY . the evaluation result shows that while dependency and PROXIMITY perform relatively well by themselves , combination of two or more kinds of CONTEXTUAL INFORMATION gives more stable performance . we <unk> further investigated useful selection of DEPENDENCY RELATIONS and MODIFICATION CATEGORIES , and it is found that modification has the greatest contribution , even greater than the widely adopted SUBJECT-OBJECT COMBINATION . \n",
            "this paper addresses the problem of AUTOMATIC SYNONYM ACQUISITION in AUTOMATIC SYNONYM ACQUISITION . we propose a novel approach to the problem of AUTOMATIC SYNONYM ACQUISITION . the proposed approach is based on the use of LEXICAL KNOWLEDGE , which is based on a SUBJECT-OBJECT COMBINATION , a SUBJECT-OBJECT COMBINATION , and a SUBJECT-OBJECT COMBINATION . the proposed approach is based on the use of a SUBJECT-OBJECT COMBINATION , which is able to capture the CONTEXTUAL CLUES OF WORDS , such as SENTENCE CO-OCCURRENCE , PROXIMITY , and PROXIMITY . the proposed approach is evaluated on a variety of MODIFICATION CATEGORIES , including a variety of MODIFICATION CATEGORIES , and AUTOMATIC SYNONYM ACQUISITION .\n",
            "\n",
            "42 1000\n",
            "we present a PLANNER NAMED TRANSITION CONSTRAINTS for PARALLEL PLANNING . TCPP constructs a new CONSTRAINT MODEL from DOMAIN TRANSITION GRAPHS of a given planning problem . TCPP encodes the CONSTRAINT MODEL by using TABLE CONSTRAINTS that allow do n't <unk> or WILD CARDS as CELL VALUES . TCPP uses MINION THE CONSTRAINT SOLVER to solve the CONSTRAINT MODEL and returns the PARALLEL PLAN . empirical results exhibit the efficiency of our CONSTRAINT MODEL over state-of-the-art CONSTRAINT-BASED PLANNERS . \n",
            "this paper proposes a new CONSTRAINT MODEL based on DOMAIN TRANSITION GRAPHS for PARALLEL PLANNING . the proposed CONSTRAINT MODEL is based on the use of DOMAIN TRANSITION GRAPHS for PARALLEL PLANNING . the proposed CONSTRAINT MODEL is based on the use of DOMAIN TRANSITION GRAPHS for PARALLEL PLANNING . the proposed CONSTRAINT MODEL is based on the use of TABLE CONSTRAINTS , which is a PARALLEL PLAN . the proposed CONSTRAINT MODEL is applied to the problem of PARALLEL PLANNING . the experimental results show that the proposed CONSTRAINT MODEL significantly improves the performance of the proposed CONSTRAINT MODEL .\n",
            "\n",
            "43 1000\n",
            "in this paper we consider the problem of describing the action being performed by HUMAN FIGURES in still IMAGES . we will attack this problem using an UNSUPERVISED LEARNING APPROACH , attempting to discover the set of ACTION CLASSES present in a large collection of TRAINING IMAGES . these ACTION CLASSES will then be used to label test IMAGES . our UNSUPERVISED LEARNING APPROACH uses the COARSE SHAPE of the HUMAN FIGURES to match pairs of IMAGES . the distance between a pair of IMAGES is computed using a LINEAR PROGRAMMING RELAXATION TECHNIQUE . this is a computationally expensive process , and we employ a fast PRUNING METHOD to enable its use on a large collection of IMAGES . SPECTRAL CLUSTERING is then performed using the resulting distances . we present CLUSTERING AND IMAGE LABELING results on a variety of datasets . \n",
            "this paper addresses the problem of CLUSTERING AND IMAGE LABELING from IMAGES . we propose a novel approach to the problem of CLUSTERING AND IMAGE LABELING from IMAGES . our approach is based on the use of a LINEAR PROGRAMMING RELAXATION TECHNIQUE that allows us to estimate the COARSE SHAPE of a scene from a single image . we propose a novel method based on SPECTRAL CLUSTERING to estimate the COARSE SHAPE of the ACTION CLASSES . we demonstrate the effectiveness of our method on a variety of IMAGES . our results show that the proposed method outperforms the state-of-the-art methods in terms of both CLUSTERING AND IMAGE LABELING and SPECTRAL CLUSTERING .\n",
            "\n",
            "44 1000\n",
            "creating large amounts of ANNOTATED DATA to train STATISTICAL PCFG PARSERS is expensive , and the performance of such PARSERS declines when training and test data are taken from different domains . in this paper we use SELF-TRAINING in order to improve the quality of a PARSER and to adapt PARSER to a different domain , using only small amounts of MANUALLY ANNOTATED SEED DATA . we report significant improvement both when the SEED AND TEST DATA are in the same domain and in the OUT-OF-DOMAIN ADAPTATION SCENARIO . in particular , we achieve 50 % reduction in ANNOTATION COST for the IN-DOMAIN CASE , yielding an improvement of 66 % over previous work , and a <unk> % reduction for the DOMAIN ADAPTATION CASE . this is the first time that SELF-TRAINING with small labeled datasets is applied successfully to these tasks . we were also able to formulate a characterization of when SELF-TRAINING is valuable . \n",
            "this paper addresses the problem of STATISTICAL PCFG PARSERS in the context of STATISTICAL PCFG PARSERS . in particular , we focus on the problem of SELF-TRAINING , and propose a novel PARSER based on SELF-TRAINING . in the proposed STATISTICAL PCFG PARSERS , a PARSER is trained on a small set of MANUALLY ANNOTATED SEED DATA , which is trained on a small set of ANNOTATED DATA , and the ANNOTATION COST of the PARSER is comparable to that of the previous methods . the performance of the proposed PARSER is evaluated on a MANUALLY ANNOTATED SEED DATA and a OUT-OF-DOMAIN ADAPTATION SCENARIO . the results show that the proposed approach is effective in improving the ANNOTATION COST of the DOMAIN ADAPTATION CASE .\n",
            "\n",
            "45 1000\n",
            "arai has developed several PHYSICAL MODELS of the HUMAN VOCAL TRACT for education and has reported that PHYSICAL MODELS are intuitive and helpful for students of acoustics and speech science . we first reviewed DYNAMIC MODELS , including the SLIDING THREE-TUBE MODEL and the FLEXIBLE-TONGUE MODEL . we then developed a HEAD-SHAPED MODEL with a SLIDING TONGUE , which has the advantages of both the S3T AND FLEXIBLE-TONGUE MODELS . we also developed a COMPUTER-CONTROLLED VERSION of the UMEDA & TERANISHI MODEL , as the original HEAD-SHAPED MODEL was hard to manipulate precisely by hand . these HEAD-SHAPED MODEL are useful when teaching the dynamic aspects of speech . \n",
            "this paper presents a novel approach to the design of a SLIDING THREE-TUBE MODEL based on a SLIDING THREE-TUBE MODEL and a HEAD-SHAPED MODEL . the proposed method is based on a HEAD-SHAPED MODEL and a FLEXIBLE-TONGUE MODEL . the proposed method is based on a UMEDA & TERANISHI MODEL and a FLEXIBLE-TONGUE MODEL . the proposed method is based on the UMEDA & TERANISHI MODEL and the FLEXIBLE-TONGUE MODEL . the proposed method is based on a UMEDA & TERANISHI MODEL and a FLEXIBLE-TONGUE MODEL . the proposed method is evaluated on a variety of S3T AND FLEXIBLE-TONGUE MODELS including a SLIDING THREE-TUBE MODEL and a FLEXIBLE-TONGUE MODEL .\n",
            "\n",
            "46 1000\n",
            "we provide a new analysis of an efficient MARGIN-BASED ALGORITHM for SELECTIVE SAMPLING in CLASSIFICATION PROBLEMS . using the so-called <unk> LOW NOISE CONDITION to parametrize the INSTANCE DISTRIBUTION , we show bounds on the CONVERGENCE RATE to the BAYES RISK of both the fully supervised and the SELECTIVE SAMPLING VERSIONS of the basic algorithm . our analysis reveals that , excluding LOGARITHMIC FACTORS , the average risk of the SELECTIVE SAMPLER converges to the BAYES RISK at rate n − -lrb- 1 + α -rrb- -lrb- 2 + α -rrb- / 2 -lrb- 3 + α -rrb- where n denotes the number of queried labels , and α > 0 is the exponent in the LOW NOISE CONDITION . for all α > √ 3 − 1 ≈ 0.73 this CONVERGENCE RATE is asymptotically faster than the rate n − -lrb- 1 + α -rrb- / -lrb- 2 + α -rrb- achieved by the fully supervised version of the same CLASSIFIER , which queries all labels , and for α → ∞ the two rates exhibit an exponential gap . experiments on TEXTUAL DATA reveal that simple variants of the proposed SELECTIVE SAMPLER perform much better than popular and similarly efficient competitors . \n",
            "in this paper , we propose a novel MARGIN-BASED ALGORITHM for CLASSIFICATION PROBLEMS . the proposed MARGIN-BASED ALGORITHM is based on the use of SELECTIVE SAMPLING VERSIONS , which is a generalization of the MARGIN-BASED ALGORITHM to the INSTANCE DISTRIBUTION . the proposed MARGIN-BASED ALGORITHM is based on a MARGIN-BASED ALGORITHM , which is a generalization of the MARGIN-BASED ALGORITHM to the INSTANCE DISTRIBUTION . the proposed MARGIN-BASED ALGORITHM is compared to the BAYES RISK of the BAYES RISK and the CONVERGENCE RATE of the CLASSIFIER . the CONVERGENCE RATE of the proposed CLASSIFIER is compared to a LOW NOISE CONDITION , and the CONVERGENCE RATE of the MARGIN-BASED ALGORITHM is comparable to that of the BAYES RISK .\n",
            "\n",
            "47 1000\n",
            "bilingual speakers are known for their ability to <unk> or mix their languages during communication . this phenomenon occurs when BILINGUALS substitute a word or phrase from one language with a phrase or word from another language . for CODE-SWITCHING SPEECH RECOGNITION , it is essential to collect a LARGE-SCALE CODE-SWITCHING SPEECH DATABASE for MODEL TRAINING . in order to ease the negative effect caused by the DATA SPARSENESS PROBLEM in training code-switching speech recognizers , this study proposes a DATA-DRIVEN APPROACH to PHONE SET CONSTRUCTION by integrating ACOUSTIC FEATURES and CROSS-LINGUAL CONTEXT-SENSITIVE ARTICULATORY FEATURES into DISTANCE MEASURE between phone units . KL-DIVERGENCE and a HIERARCHICAL PHONE UNIT CLUSTERING ALGORITHM are used in this study to cluster similar phone units to reduce the need of the training data for MODEL CONSTRUCTION . the experimental results show that the proposed DATA-DRIVEN APPROACH outperforms other traditional PHONE SET CONSTRUCTION METHODS . \n",
            "this paper presents a novel DATA-DRIVEN APPROACH for CODE-SWITCHING SPEECH RECOGNITION . the proposed DATA-DRIVEN APPROACH is based on the use of ACOUSTIC FEATURES extracted from the ACOUSTIC FEATURES . the proposed DATA-DRIVEN APPROACH is based on the use of ACOUSTIC FEATURES and CROSS-LINGUAL CONTEXT-SENSITIVE ARTICULATORY FEATURES . the proposed DATA-DRIVEN APPROACH is based on the use of ACOUSTIC FEATURES extracted from the ACOUSTIC FEATURES of the ACOUSTIC FEATURES . the proposed DATA-DRIVEN APPROACH is evaluated on a LARGE-SCALE CODE-SWITCHING SPEECH DATABASE and compared to PHONE SET CONSTRUCTION METHODS . the proposed DATA-DRIVEN APPROACH is compared to other PHONE SET CONSTRUCTION METHODS . the proposed DATA-DRIVEN APPROACH is applied to CODE-SWITCHING SPEECH RECOGNITION , and the results show that the proposed DATA-DRIVEN APPROACH is effective in improving the CODE-SWITCHING SPEECH RECOGNITION performance .\n",
            "\n",
            "48 1000\n",
            "we describe a generic framework for integrating various STOCHASTIC MODELS OF DISCOURSE COHERENCE in a manner that takes advantage of their individual strengths . an integral part of this framework are algorithms for searching and training these STOCHASTIC MODELS OF DISCOURSE COHERENCE . we evaluate the performance of our models and algorithms and show empirically that UTILITY-TRAINED LOG-LINEAR COHERENCE MODELS out-perform each of the individual COHERENCE MODELS considered . \n",
            "this paper proposes a novel approach to the problem of STOCHASTIC MODELS OF DISCOURSE COHERENCE . the proposed UTILITY-TRAINED LOG-LINEAR COHERENCE MODELS is based on the use of a set of COHERENCE MODELS , which are more robust than conventional COHERENCE MODELS . in addition , the proposed UTILITY-TRAINED LOG-LINEAR COHERENCE MODELS is able to deal with the number of COHERENCE MODELS . the experimental results show that the proposed UTILITY-TRAINED LOG-LINEAR COHERENCE MODELS are more effective than conventional COHERENCE MODELS .\n",
            "\n",
            "49 1000\n",
            "fast <unk> methods for MAXIMUM MARGIN MATRIX FACTORIZATION were recently shown to have great promise -lrb- <unk> & <unk> , 2005 -rrb- , including significantly outperforming the previous state-of-the-art methods on some standard COLLABORATIVE PREDICTION BENCHMARKS -lrb- including <unk> -rrb- . in this paper , we investigate ways to further improve the performance of MAXIMUM MARGIN MATRIX FACTORIZATION , by casting MAXIMUM MARGIN MATRIX FACTORIZATION within an ENSEMBLE APPROACH . we explore and evaluate a variety of alternative ways to define such ensembles . we show that our resulting ensembles can perform significantly better than a single MMMF MODEL , along multiple EVALUATION METRICS . in fact , we find that ensembles of partially trained MMMF MODEL can sometimes even give better predictions in TOTAL TRAINING TIME comparable to a single MMMF MODEL . \n",
            "this paper addresses the problem of MAXIMUM MARGIN MATRIX FACTORIZATION in the context of MAXIMUM MARGIN MATRIX FACTORIZATION . in particular , we propose a new ENSEMBLE APPROACH , called MAXIMUM MARGIN MATRIX FACTORIZATION , which is a generalization of the well-known ENSEMBLE APPROACH for MAXIMUM MARGIN MATRIX FACTORIZATION . the proposed ENSEMBLE APPROACH is based on the use of FAST GRADIENT-BASED METHODS , which is a generalization of the standard ENSEMBLE APPROACH . the proposed ENSEMBLE APPROACH is evaluated on several EVALUATION METRICS . the experimental results show that the proposed MMMF MODEL is able to significantly outperform the state-of-the-art methods in terms of both TOTAL TRAINING TIME and TOTAL TRAINING TIME .\n",
            "\n",
            "50 1000\n",
            "this paper investigates JAW MOVEMENTS in the production of <unk> syllables with and without VOWELS . we test the hypothesis that / l , r / in the SYLLABLE NUCLEUS POSITION show a degree of JAW OPENING comparable to VOWELS , therefore providing a RISING-FALLING SONORITY PROFILE even in syllables lacking VOWELS . we also investigate whether the PHONEMIC LENGTH DISTINCTION occurring for both VOWELS and syllabic consonants is implemented in a similar fashion for the different nucleus types . our articulatory data show that the JAW ACTIVITY during SYLLABIC LIQUIDS is indeed comparable to that of VOWELS , and that the JAW is recruited to help maintain the MAIN LINGUAL ARTICULATION . this became evident in particular in an interaction between NUCLEUS TYPE and PHONEMIC LENGTH EFFECTS . \n",
            "this paper presents a novel method to recover the JAW ACTIVITY of a scene from a single image . the method is based on a RISING-FALLING SONORITY PROFILE , which is invariant to PHONEMIC LENGTH EFFECTS and PHONEMIC LENGTH EFFECTS . the proposed approach is based on a RISING-FALLING SONORITY PROFILE , which is based on a RISING-FALLING SONORITY PROFILE . the proposed approach is based on a RISING-FALLING SONORITY PROFILE , which is based on the RISING-FALLING SONORITY PROFILE and PHONEMIC LENGTH EFFECTS . the proposed method is evaluated on a variety of VOWELS and VOWELS . the results show that the proposed method can recover the NUCLEUS TYPE and the PHONEMIC LENGTH EFFECTS of the VOWELS .\n",
            "\n",
            "51 1000\n",
            "the COIFMAN WAVELETS created by <unk> have more zero moments than imposed by specifications . this results in systems with approximately equal numbers of ZERO SCALING FUNCTION and WAVELET MOMENTS and gives a partitioning of the systems into three well defined classes . the NONUNIQUE SOLUTIONS are more complex than for DAUBECHIES WAVELETS . \n",
            "this paper addresses the problem of DAUBECHIES WAVELETS in the context of DAUBECHIES WAVELETS . the main idea is to design a new ZERO SCALING FUNCTION that is based on the concept of WAVELET MOMENTS . the proposed NONUNIQUE SOLUTIONS is based on the use of the WAVELET MOMENTS , WAVELET MOMENTS , and WAVELET MOMENTS . the performance of the proposed NONUNIQUE SOLUTIONS is evaluated in terms of the number of NONUNIQUE SOLUTIONS . the experimental results show that the proposed NONUNIQUE SOLUTIONS are more effective than the conventional NONUNIQUE SOLUTIONS .\n",
            "\n",
            "52 1000\n",
            "techniques such as PROBABILISTIC TOPIC MODELS and LATENT-SEMANTIC INDEXING have been shown to be broadly useful at automatically extracting the TOPICAL OR SEMANTIC CONTENT OF DOCUMENTS , or more generally for DIMENSION-REDUCTION OF SPARSE COUNT DATA . these types of models and algorithms can be viewed as generating an ABSTRACTION from the words in a document to a LOWER-DIMENSIONAL LATENT VARIABLE REPRESENTATION that captures what the document is generally about beyond the specific words it contains . in this paper we propose a new PROBABILISTIC MODEL that <unk> this PROBABILISTIC MODEL by representing each document as a combination of -lrb- a -rrb- a BACKGROUND DISTRIBUTION over COMMON WORDS , -lrb- b -rrb- a MIXTURE DISTRIBUTION over general topics , and -lrb- c -rrb- a DISTRIBUTION over words that are treated as being specific to that document . we illustrate how this PROBABILISTIC MODEL can be used for INFORMATION RETRIEVAL by matching documents both at a general topic level and at a specific WORD LEVEL , providing an advantage over techniques that only match documents at a general level -lrb- such as TOPIC MODELS or LATENT-SEMATIC INDEXING -rrb- or that only match documents at the specific WORD LEVEL -lrb- such as <unk> -rrb- . \n",
            "this paper addresses the problem of INFORMATION RETRIEVAL and LATENT-SEMANTIC INDEXING . the main contribution of this paper is a novel PROBABILISTIC MODEL to the problem of INFORMATION RETRIEVAL and LATENT-SEMANTIC INDEXING . the proposed PROBABILISTIC MODEL is based on the use of a LOWER-DIMENSIONAL LATENT VARIABLE REPRESENTATION and a LOWER-DIMENSIONAL LATENT VARIABLE REPRESENTATION . the proposed PROBABILISTIC MODEL is based on a PROBABILISTIC MODEL of the BACKGROUND DISTRIBUTION and the MIXTURE DISTRIBUTION . the proposed PROBABILISTIC MODEL is based on a PROBABILISTIC MODEL of the BACKGROUND DISTRIBUTION and the MIXTURE DISTRIBUTION . the proposed PROBABILISTIC MODEL is applied to the problem of INFORMATION RETRIEVAL and LATENT-SEMANTIC INDEXING . the performance of the proposed PROBABILISTIC MODEL is demonstrated on a variety of COMMON WORDS and LATENT-SEMANTIC INDEXING .\n",
            "\n",
            "53 1000\n",
            "planning in DYNAMIC CONTINUOUS ENVIRONMENTS requires reasoning about NONLINEAR CONTINUOUS EFFECTS , which previous HIERARCHICAL TASK NETWORK PLANNERS do not support . in this paper , we extend an existing HTN PLANNER with a new STATE PROJECTION ALGORITHM . to our knowledge , this is the first HTN PLANNER that can reason about NONLINEAR CONTINUOUS EFFECTS . we use a WAIT ACTION to <unk> this HTN PLANNER to consider CONTINUOUS EFFECTS in a given state . we also introduce a new PLANNING DOMAIN to demonstrate the benefits of planning with NONLINEAR CONTINUOUS EFFECTS . we compare our approach with a LINEAR CONTINUOUS EFFECTS PLANNER and a DISCRETE EFFECTS HTN PLANNER on a BENCHMARK DOMAIN , which reveals that its additional costs are largely mitigated by DOMAIN KNOWLEDGE . finally , we present an initial application of this algorithm in a practical domain , a NAVY TRAINING SIMULATION , illustrating the utility of this approach for planning in DYNAMIC CONTINUOUS ENVIRONMENTS . \n",
            "this paper addresses the problem of PLANNING in DYNAMIC CONTINUOUS ENVIRONMENTS . we propose a novel approach to the problem of PLANNING in a PLANNING DOMAIN . the proposed approach is based on a STATE PROJECTION ALGORITHM with a LINEAR CONTINUOUS EFFECTS PLANNER . the proposed approach is based on a LINEAR CONTINUOUS EFFECTS PLANNER , which is a LINEAR CONTINUOUS EFFECTS PLANNER with a LINEAR CONTINUOUS EFFECTS PLANNER and a LINEAR CONTINUOUS EFFECTS PLANNER . the proposed approach is based on a LINEAR CONTINUOUS EFFECTS PLANNER and a LINEAR CONTINUOUS EFFECTS PLANNER . the proposed approach is evaluated on a BENCHMARK DOMAIN , and the results show that the proposed method is effective in improving the performance of the HTN PLANNER .\n",
            "\n",
            "54 1000\n",
            "this paper considers an AUTOMATIC VOICE RESPONSE APPLICATION in which a WORD UTTERANCE is inserted into a fixed carrier sentence . an important task here is to adjust the F 0 CONTOUR of the INSERTED WORD according to the F 0 CONTEXT of the carrier sentence . instead of generating the F 0 CONTOUR on syllable basis , we employ an approach to adjust the F 0 CONTOUR of the whole word . in this approach , two questions arise : -lrb- a -rrb- how to evaluate the F 0 CONTEXT and -lrb- b -rrb- how to adjust the F 0 CONTOUR suitably for the context . we have found that the F 0 CONTOUR of a word can be appropriately regulated in a <unk> word-level f 0 range -lrb- <unk> 0 r -rrb- . after estimating the WF 0 RS of the preceding and succeeding words , the <unk> 0 r of the INSERTED WORD is set at the mean of these WF 0 RS . the F 0 CONTOUR of the INSERTED WORD is then mapped to the <unk> 0 r taking into account the TONE COMBINATION of the word . a perceptual evaluation experiment showed that the adjusted f 0 was coordinated well with the context . \n",
            "this paper addresses the problem of estimating the F 0 CONTOUR of a WORD UTTERANCE in a WORD UTTERANCE . the problem of WF 0 RS in a WORD UTTERANCE is a fundamental problem in the AUTOMATIC VOICE RESPONSE APPLICATION . in this paper , we propose a novel method to estimate the F 0 CONTOUR of the INSERTED WORD with WF 0 RS . the proposed algorithm is based on the WF 0 RS of the INSERTED WORD with WF 0 RS . the proposed algorithm is tested on a variety of WF 0 RS .\n",
            "\n",
            "55 1000\n",
            "state-of-the-art COMPUTER-ASSISTED TRANSLATION ENGINES are based on a STATISTICAL PREDICTION ENGINE , which interactively provides completions to what a HUMAN TRANSLATOR TYPES . the integration of HUMAN SPEECH into a COMPUTER-ASSISTED TRANSLATION ENGINES is also a challenging area and is the aim of this paper . so far , only a few methods for integrating STATISTICAL MACHINE TRANSLATION MODELS with AUTOMATIC SPEECH RECOGNITION MODELS have been studied . they were mainly based on N-BEST RESCORING APPROACH . N-BEST RESCOR-ING is not an appropriate SEARCH METHOD for building a COMPUTER-ASSISTED TRANSLATION ENGINES . in this paper , we study the incorporation of AUTOMATIC SPEECH RECOGNITION MODELS and ASR MODELS using FINITE-STATE AUTOMATA . we also propose some transducers based on AUTOMATIC SPEECH RECOGNITION MODELS for rescoring the ASR WORD GRAPHS . \n",
            "this paper presents a novel SEARCH METHOD for COMPUTER-ASSISTED TRANSLATION ENGINES . the proposed approach is based on the use of a STATISTICAL PREDICTION ENGINE for COMPUTER-ASSISTED TRANSLATION ENGINES . the proposed approach is based on the use of a STATISTICAL PREDICTION ENGINE for COMPUTER-ASSISTED TRANSLATION ENGINES . the proposed approach is based on the use of a STATISTICAL PREDICTION ENGINE for COMPUTER-ASSISTED TRANSLATION ENGINES . the proposed approach is based on the use of a STATISTICAL PREDICTION ENGINE , which is able to deal with HUMAN SPEECH in the presence of HUMAN SPEECH . the experimental results show that the proposed approach is effective in improving the performance of COMPUTER-ASSISTED TRANSLATION ENGINES .\n",
            "\n",
            "56 1000\n",
            "one DUMMY SYMBOL has to be included for the sequence in this paper , a new GENERATION and an INFORMATION CONSTRUCTION METHODS for a MODULATED ORTHOGONAL SEQUENCE are suggested the sequence is generated by only INTEGER SUMS and MODULAR TECHNIQUES . the AUTOCORRELATION AND CRAM CORRELATION CHARACTERISTICS of the sequence are investigated via a new procedure . a modified sequence also having the orthogonality and satisfying the mathematical lower bound of the CROSS-CORRELATION is proposed , and the SYMBOL ERROR PROBABILITY of the sequence is investigated . \n",
            "in this paper , we propose a novel approach to GENERATION in the context of GENERATION . the proposed method is based on the use of MODULAR TECHNIQUES and MODULAR TECHNIQUES to estimate the AUTOCORRELATION AND CRAM CORRELATION CHARACTERISTICS of the DUMMY SYMBOL . the proposed method is based on the use of MODULAR TECHNIQUES and INFORMATION CONSTRUCTION METHODS to estimate the AUTOCORRELATION AND CRAM CORRELATION CHARACTERISTICS . the proposed method is based on the use of MODULAR TECHNIQUES and MODULAR TECHNIQUES to estimate the AUTOCORRELATION AND CRAM CORRELATION CHARACTERISTICS of the DUMMY SYMBOL . experimental results show that the proposed method outperforms the state-of-the-art methods .\n",
            "\n",
            "57 1000\n",
            "this work presents our research results on the capability of PASSIVE SOURCE LOCALIZATION using a TOWED MULTI-MODULE ARRAY in UNDERWATER ACOUSTIC ENVIRONMENTS . we developed new CRAMER-RAO LOWER BOUND results for assessing the ARRAY 'S SOURCE LOCALIZATION CAPABILITY under ENVIRONMENTAL UNCERTAINTIES , such as ARRAY MODULES ' positions and ACOUSTIC-FIELD 'S SPATIAL COHERENCE . we also developed a TWO-STAGE APPROACH and provide details for PASSIVE RANGING by utilizing data either <unk> or coherently , depending on the availability of SPATIAL COHERENCE in the received data from multiple modules of TOWED ARRAY . \n",
            "this paper presents a novel TWO-STAGE APPROACH for PASSIVE SOURCE LOCALIZATION in UNDERWATER ACOUSTIC ENVIRONMENTS . the proposed TWO-STAGE APPROACH is based on a TWO-STAGE APPROACH for PASSIVE SOURCE LOCALIZATION . the proposed TWO-STAGE APPROACH is based on a TWO-STAGE APPROACH for PASSIVE SOURCE LOCALIZATION . the proposed TWO-STAGE APPROACH is based on a TWO-STAGE APPROACH , which is based on a TWO-STAGE APPROACH . the proposed TWO-STAGE APPROACH is applied to PASSIVE SOURCE LOCALIZATION , and the CRAMER-RAO LOWER BOUND is shown to be robust to ENVIRONMENTAL UNCERTAINTIES . the proposed TWO-STAGE APPROACH is evaluated on a variety of UNDERWATER ACOUSTIC ENVIRONMENTS , including ARRAY MODULES , ACOUSTIC-FIELD 'S SPATIAL COHERENCE , and ACOUSTIC-FIELD 'S SPATIAL COHERENCE .\n",
            "\n",
            "58 1000\n",
            "this paper proposes an approach for AUTOMATIC ROAD EXTRACTION in AERIAL IMAGERY which exploits the SCALE-SPACE BEHAVIOR OF ROADS in combination with GEOMETRIC CONSTRAINED SNAKE-BASED EDGE EXTRACTION . the approach not only has few parameters to be adjusted , but for the first time allows for a BRIDGING OF SHADOWS and partially occluded areas using the heavily disturbed evidence in the image . the road network is constructed after extracting <unk> of various shape and topology . reasonable results are obtained which are evaluated based on ground truth . \n",
            "this paper addresses the problem of AUTOMATIC ROAD EXTRACTION from AERIAL IMAGERY . we propose a novel approach to the problem of AUTOMATIC ROAD EXTRACTION from AERIAL IMAGERY . the proposed approach is based on the use of GEOMETRIC CONSTRAINED SNAKE-BASED EDGE EXTRACTION in order to deal with AERIAL IMAGERY . the proposed approach is based on the use of GEOMETRIC CONSTRAINED SNAKE-BASED EDGE EXTRACTION and GEOMETRIC CONSTRAINED SNAKE-BASED EDGE EXTRACTION . experimental results demonstrate the effectiveness of the proposed approach .\n",
            "\n",
            "59 1000\n",
            "<unk> residual vector quantizers -lrb- MULTISTAGE RESIDUAL VECTOR QUANTIZERS -rrb- with optimal DIRECT SUM DECODER CODEBOOKS have been successfully designed and implemented for DATA COMPRESSION . due to its MULTISTAGE STRUCTURE , MULTISTAGE RESIDUAL VECTOR QUANTIZERS has the ability to densely populate the INPUT SPACE with VORONOI CELL PARTITIONS . the same design concept has yielded good results in the application of IMAGE-CONTENT CLASSIFICATION -lsb- 1 -rsb- . furthermore , the MULTISTAGE RVQ , with STAGE-WISE CODEBOOKS , provides an opportunity to perform FINE-GRAINED FEATURE ATTRIBUTION for IMAGE UNDERSTANDING , in general , and feature foundation data generation for NATURAL AND MAN-MADE STRUCTURE RECOGNITION , in specific . in -lsb- 1 -rsb- , the information at the stages of MULTISTAGE RESIDUAL VECTOR QUANTIZERS is heuristically integrated to perform CLASS CONDITIONAL PATTERN RECOGNITION ; hence the process is not robust . MARKOV RANDOM FIELD provides a suitable BAYESIAN FRAMEWORK to integrate the information available at the various stages of MULTISTAGE RESIDUAL VECTOR QUANTIZERS to achieve OPTIMIZED CLASSIFICATION in the MAXIMUM A-POSTERIORI SENSE . \n",
            "this paper addresses the problem of FINE-GRAINED FEATURE ATTRIBUTION for IMAGE UNDERSTANDING . in particular , we propose a novel approach to FINE-GRAINED FEATURE ATTRIBUTION for IMAGE UNDERSTANDING . in the proposed BAYESIAN FRAMEWORK , a MARKOV RANDOM FIELD is used to represent the INPUT SPACE of the INPUT SPACE . the proposed MULTISTAGE RESIDUAL VECTOR QUANTIZERS is based on a MARKOV RANDOM FIELD , which is based on a MARKOV RANDOM FIELD . in the proposed BAYESIAN FRAMEWORK , a MARKOV RANDOM FIELD is used to represent the INPUT SPACE of the INPUT SPACE . the proposed MULTISTAGE RESIDUAL VECTOR QUANTIZERS is applied to the NATURAL AND MAN-MADE STRUCTURE RECOGNITION for IMAGE UNDERSTANDING . the experimental results show that the proposed method can achieve better performance than the state-of-the-art methods .\n",
            "\n",
            "60 1000\n",
            "in this work we make use of recent advances in DATA DRIVEN CLASSIFICATION to improve standard approaches for BINOCULAR STEREO MATCHING and SINGLE VIEW DEPTH ESTIMATION . SURFACE NORMAL DIRECTION ESTIMATION has become feasible and shown to work reliably on STATE OF THE ART BENCHMARK DATASETS . information about the SURFACE ORIENTATION contributes crucial information about the SCENE GEOMETRY in cases where standard approaches struggle . we describe , how the responses of such a CLASSIFIER can be included in GLOBAL STEREO MATCHING APPROACHES . one of the strengths of our CLASSIFIER is , that we can use the CLASSIFIER RESPONSES for a whole set of directions and let the final optimization decide about the SURFACE ORIENTATION . this is important in cases where based on the CLASSIFIER , multiple different SURFACE ORIENTATIONS seem likely . we evaluate our CLASSIFIER on two challenging REAL-WORLD DATASETS for the two proposed applications . for the BINOCULAR STEREO MATCHING we use ROAD SCENE IMAGERY taken from a car and for the SINGLE VIEW DEPTH ESTIMATION we use IMAGES taken in INDOOR ENVIRONMENTS . \n",
            "in this paper , we propose a novel CLASSIFIER for SINGLE VIEW DEPTH ESTIMATION and SINGLE VIEW DEPTH ESTIMATION . the proposed CLASSIFIER is based on the use of SURFACE ORIENTATIONS and CLASSIFIER RESPONSES . the proposed CLASSIFIER is based on the use of SURFACE NORMAL DIRECTION ESTIMATION and SURFACE NORMAL DIRECTION ESTIMATION . the proposed CLASSIFIER is based on the use of SURFACE NORMAL DIRECTION ESTIMATION and SURFACE NORMAL DIRECTION ESTIMATION . the proposed CLASSIFIER is evaluated on two REAL-WORLD DATASETS . the experimental results show that the proposed CLASSIFIER significantly improves the performance of BINOCULAR STEREO MATCHING and SINGLE VIEW DEPTH ESTIMATION .\n",
            "\n",
            "61 1000\n",
            "one goal of NATURAL LANGUAGE GENERATION is to produce COHERENT TEXT that presents information in a LOGICAL ORDER . in this paper , we show that TOPOLOGICAL FIELDS , which model HIGH-LEVEL CLAUSAL STRUCTURE , are an important component of LOCAL COHERENCE in GERMAN . first , we show in a sentence ordering experiment that TOPOLOGI-CAL FIELD INFORMATION improves the ENTITY GRID MODEL of <unk> and <unk> -lrb- 2008 -rrb- more than GRAMMATICAL ROLE and simple CLAUSAL ORDER INFORMATION do , particularly when MANUAL ANNOTATIONS of this information are not available . then , we incorporate the model enhanced with TOPOLOG-ICAL FIELDS into a NATURAL LANGUAGE GENERATION SYSTEM that generates CONSTITUENT ORDERS for GERMAN TEXT , and show that the added COHERENCE COMPONENT improves performance slightly , though not statistically significantly . \n",
            "this paper presents a novel NATURAL LANGUAGE GENERATION SYSTEM for NATURAL LANGUAGE GENERATION in GERMAN TEXT . the proposed NATURAL LANGUAGE GENERATION SYSTEM is based on the use of TOPOLOG-ICAL FIELDS and CLAUSAL ORDER INFORMATION . the proposed NATURAL LANGUAGE GENERATION SYSTEM is based on a ENTITY GRID MODEL that uses TOPOLOGICAL FIELDS and CLAUSAL ORDER INFORMATION to estimate the COHERENCE COMPONENT . the proposed NATURAL LANGUAGE GENERATION SYSTEM is based on the use of TOPOLOG-ICAL FIELDS and CLAUSAL ORDER INFORMATION . the proposed NATURAL LANGUAGE GENERATION SYSTEM is applied to GERMAN , and the experimental results show that the proposed NATURAL LANGUAGE GENERATION SYSTEM is robust to LOCAL COHERENCE and MANUAL ANNOTATIONS .\n",
            "\n",
            "62 1000\n",
            "this paper explores PACKET LOSS RECOVERY in CLIENT-SERVER AUTOMATIC SPEECH RECOGNITION SYSTEMS . a FORWARD ERROR CORRECTION SYSTEM is designed and tested over several CHANNEL LOSS MODELS , at variable amounts of DATA ACQUISITION DELAY . in experiments with SIMULATED PACKET LOSS , the CLIENT-SERVER AUTOMATIC SPEECH RECOGNITION SYSTEMS provides robust asr performance which degrades gracefully as PACKET LOSS RATES increase . comparing this CLIENT-SERVER AUTOMATIC SPEECH RECOGNITION SYSTEMS to several alternatives under LOW AND MEDIUM LOSS CHANNEL CONDITIONS , we found one approach -lrb- multiple transmission plus interpolation -rrb- that yielded similar performance , but the CLIENT-SERVER AUTOMATIC SPEECH RECOGNITION SYSTEMS should scale better to lower bit rate conditions . \n",
            "this paper presents a novel approach to PACKET LOSS RECOVERY based on PACKET LOSS RECOVERY for CLIENT-SERVER AUTOMATIC SPEECH RECOGNITION SYSTEMS . the proposed CLIENT-SERVER AUTOMATIC SPEECH RECOGNITION SYSTEMS is based on the use of a set of CHANNEL LOSS MODELS , which are used in conjunction with a FORWARD ERROR CORRECTION SYSTEM . the proposed CLIENT-SERVER AUTOMATIC SPEECH RECOGNITION SYSTEMS is based on the use of PACKET LOSS RECOVERY for PACKET LOSS RECOVERY . the performance of the proposed CLIENT-SERVER AUTOMATIC SPEECH RECOGNITION SYSTEMS is evaluated on a SIMULATED PACKET LOSS . the results show that the proposed approach is effective in improving the performance of CLIENT-SERVER AUTOMATIC SPEECH RECOGNITION SYSTEMS for CLIENT-SERVER AUTOMATIC SPEECH RECOGNITION SYSTEMS .\n",
            "\n",
            "63 1000\n",
            "this paper considers application of DEEP BELIEF NETS to NATURAL LANGUAGE CALL ROUTING . DEEP BELIEF NETS have been successfully applied to a number of tasks , including IMAGE , AUDIO AND SPEECH CLASSIFICATION , thanks to the recent discovery of an efficient LEARNING TECHNIQUE . DEEP BELIEF NETS learn a MULTI-LAYER GENERATIVE MODEL from UNLABELED DATA and the FEATURES discovered by this MULTI-LAYER GENERATIVE MODEL are then used to initialize a FEED-FORWARD NEURAL NETWORK which is fine-tuned with BACKPROPAGATION . we compare a DBN-INITIALIZED NEURAL NETWORK to three widely used TEXT CLASSIFICATION ALGORITHMS ; SUPPORT VECTOR MACHINES , BOOSTING and MAXIMUM ENTROPY . the MULTI-LAYER GENERATIVE MODEL gives a call -- routing classification accuracy that is equal to the best of the other models even though MULTI-LAYER GENERATIVE MODEL currently uses an impoverished representation of the input . \n",
            "in this paper , we propose a novel approach to NATURAL LANGUAGE CALL ROUTING based on DEEP BELIEF NETS . the proposed MULTI-LAYER GENERATIVE MODEL is based on the use of DEEP BELIEF NETS , a MULTI-LAYER GENERATIVE MODEL , and a MULTI-LAYER GENERATIVE MODEL . the proposed MULTI-LAYER GENERATIVE MODEL is based on the use of DEEP BELIEF NETS , which is a MULTI-LAYER GENERATIVE MODEL . the proposed MULTI-LAYER GENERATIVE MODEL is based on the use of DEEP BELIEF NETS , BOOSTING , BOOSTING , and MAXIMUM ENTROPY . the experimental results show that the proposed method is effective in improving the NATURAL LANGUAGE CALL ROUTING performance .\n",
            "\n",
            "64 1000\n",
            "approaches to BINAURAL AND STEREO SPEECH SEGREGATION have often assumed that LOCALIZATION INFORMATION can be used as a primary cue to achieve segregation of a target signal . results produced by these systems degrade significantly in the presence of ROOM REVERBERATION . in this work , we present an alternative framework to achieve LOCALIZATION OF GROUPS OF TIME-FREQUENCY UNITS . we show that grouping across time and frequency allows the use of LOCALIZATION INFORMATION as an important cue for SEQUENTIAL GROUPING OF TIME-FREQUENCY OBJECTS . we analyze the level of TIME-FREQUENCY GROUPING needed to achieve accurate OBJECT LOCALIZATION and show preliminary binaural segregation results using the proposed framework . results indicate that both LOCALIZATION INFORMATION and segregation performance can be improved by grouping across time and frequency . \n",
            "in this paper we present a novel approach to OBJECT LOCALIZATION based on TIME-FREQUENCY GROUPING . the approach is based on the use of TIME-FREQUENCY GROUPING in the form of TIME-FREQUENCY GROUPING . the proposed approach is based on the use of TIME-FREQUENCY GROUPING and LOCALIZATION INFORMATION . the proposed method is based on the use of TIME-FREQUENCY GROUPING and LOCALIZATION INFORMATION . the experimental results show that the proposed approach is effective and robust to ROOM REVERBERATION .\n",
            "\n",
            "65 1000\n",
            "this paper shows that a FIRST-ORDER UNIFICATION-BASED SEMANTIC INTERPRETATION for various coordinate constructs is possible without an explicit use of LAMBDA EXPRESSIONS if we slightly modify the standard MONTAGOVIAN SEMANTICS OF COORDINATION . this modification , along with PARTIAL EXECUTION , completely eliminates the LAMBDA REDUCTION STEPS during SEMANTIC INTERPRETATION . \n",
            "this paper addresses the problem of PARTIAL EXECUTION in the presence of LAMBDA EXPRESSIONS . the main contribution of this paper is twofold : -lrb- 1 -rrb- we show that it is possible to recover the LAMBDA EXPRESSIONS of the LAMBDA EXPRESSIONS , and -lrb- 2 -rrb- we show that it is possible to estimate the LAMBDA EXPRESSIONS of a given set of LAMBDA EXPRESSIONS . we also show that our algorithm can be applied to a wide range of LAMBDA REDUCTION STEPS .\n",
            "\n",
            "66 1000\n",
            "<unk> optimization via COST ACCUMULATION has become very popular for STEREO ESTIMATION in COMPUTER VISION APPLICATIONS and is often combined with a SEMI-GLOBAL COST INTEGRATION STRATEGY , known as SGM . this paper introduces this combination as a general and effective OPTIMIZATION TECHNIQUE . it is the first time that this concept is applied to 3D MEDICAL IMAGE REGISTRATION . the presented algorithm , SGM-3D , employs a COARSE-TO-FINE STRATEGY and reduces the SEARCH SPACE DIMENSION for CONSECUTIVE PYRAMID LEVELS by a fixed linear rate . this allows it to handle large displacements to an extent that is required for CLINICAL APPLICATIONS in HIGH DIMENSIONAL DATA . SGM-3D is evaluated in context of PULMONARY MOTION ANALYSIS on the recently extended <unk> benchmark that provides ten 4d computed tomography -lrb- ct -rrb- image data sets , as well as ten challenging 3D CT SCAN PAIRS from the COPDGENE STUDY ARCHIVE . results show that both REGISTRATION ERRORS as well as run-time performance are very competitive with current state-of-the-art methods . \n",
            "this paper addresses the problem of 3D MEDICAL IMAGE REGISTRATION in a COPDGENE STUDY ARCHIVE . we propose a novel SEMI-GLOBAL COST INTEGRATION STRATEGY , called SGM , for STEREO ESTIMATION in CLINICAL APPLICATIONS . the proposed SEMI-GLOBAL COST INTEGRATION STRATEGY is based on a COARSE-TO-FINE STRATEGY , which is a COARSE-TO-FINE STRATEGY for STEREO ESTIMATION . the proposed SEMI-GLOBAL COST INTEGRATION STRATEGY is based on a COARSE-TO-FINE STRATEGY , which is based on a COARSE-TO-FINE STRATEGY . the proposed SEMI-GLOBAL COST INTEGRATION STRATEGY is based on a COARSE-TO-FINE STRATEGY , which is based on a COARSE-TO-FINE STRATEGY . the proposed SEMI-GLOBAL COST INTEGRATION STRATEGY is applied to 3D CT SCAN PAIRS , and the experimental results demonstrate the effectiveness of the proposed SEMI-GLOBAL COST INTEGRATION STRATEGY .\n",
            "\n",
            "67 1000\n",
            "in this paper , we propose a new MULTI-VIEW DOMAIN GENERALIZATION APPROACH for VISUAL RECOGNITION , in which we aim to use the source domain samples with multiple types of FEATURES -lrb- i.e. , MULTI-VIEW FEATURES -rrb- to learn ROBUST CLASSIFIERS that can generalize well to any UNSEEN TARGET DOMAIN . considering the recent works show the DOMAIN GENERALIZATION CAPABILITY can be enhanced by fusing multiple SVM CLASSIFIERS , we build upon EXEMPLAR SVMS to learn a set of SVM CLASSIFIERS by using one positive sample and all negative samples in the source domain each time . when the source domain samples come from multiple LATENT DOMAINS , we expect the WEIGHT VECTORS of exemplar svm clas-sifiers can be organized into multiple hidden clusters . to exploit such CLUSTER STRUCTURE , we organize the WEIGHT VECTORS learnt on each view as a WEIGHT MATRIX and seek the LOW-RANK REPRESENTATION by reconstructing this WEIGHT MATRIX using itself as the dictionary . to enforce the consistency of INHERENT CLUSTER STRUCTURES discovered from the WEIGHT MATRICES learnt on different views , we introduce a new MULTI-VIEW DOMAIN GENERALIZATION APPROACH to minimize the mismatch between any two representation matrices on different views . we also develop an efficient ALTERNATING OPTIMIZATION ALGORITHM and further extend our MULTI-VIEW DOMAIN GENERALIZATION APPROACH for VISUAL RECOGNITION by exploiting the manifold structure of UNLABELED TARGET DOMAIN SAMPLES . comprehensive experiments for VISUAL RECOGNITION clearly demonstrate the effectiveness of our MULTI-VIEW DOMAIN GENERALIZATION APPROACH for DOMAIN GENERALIZATION and VISUAL RECOGNITION . \n",
            "in this paper , we propose a novel MULTI-VIEW DOMAIN GENERALIZATION APPROACH for VISUAL RECOGNITION and VISUAL RECOGNITION . the proposed MULTI-VIEW DOMAIN GENERALIZATION APPROACH is based on a MULTI-VIEW DOMAIN GENERALIZATION APPROACH , which is a generalization of the standard ALTERNATING OPTIMIZATION ALGORITHM . the proposed MULTI-VIEW DOMAIN GENERALIZATION APPROACH is based on the use of EXEMPLAR SVMS for DOMAIN GENERALIZATION and DOMAIN GENERALIZATION . the proposed MULTI-VIEW DOMAIN GENERALIZATION APPROACH is based on the use of EXEMPLAR SVMS for DOMAIN GENERALIZATION . the proposed MULTI-VIEW DOMAIN GENERALIZATION APPROACH is based on the use of EXEMPLAR SVMS for DOMAIN GENERALIZATION and DOMAIN GENERALIZATION . the proposed MULTI-VIEW DOMAIN GENERALIZATION APPROACH is applied to the problem of DOMAIN GENERALIZATION and VISUAL RECOGNITION . the experimental results show that the proposed MULTI-VIEW DOMAIN GENERALIZATION APPROACH is effective for VISUAL RECOGNITION and VISUAL RECOGNITION .\n",
            "\n",
            "68 1000\n",
            "audio TAGS correspond to KEYWORDS that people use to describe different aspects of a MUSIC CLIP , such as the genre , MOOD , and <unk> . since SOCIAL TAGS are usually assigned by people with different levels of MUSICAL KNOWLEDGE , they inevitably contain NOISY INFORMATION . by treating the TAG counts as costs , we can model the AUDIO TAGGING PROBLEM as a COST-SENSITIVE CLASSIFICATION PROBLEM . in addition , TAG CORRELATION is another useful information for AUTOMATIC AUDIO TAGGING since some TAGS often co-occur . by considering the co-occurrences of TAGS , we can model the AUDIO TAGGING PROBLEM as a COST-SENSITIVE CLASSIFICATION PROBLEM . to exploit the TAG COUNT and correlation information jointly , we formulate the AUDIO TAGGING PROBLEM as a novel COST-SENSITIVE MULTI-LABEL LEARNING PROBLEM . the results of AUDIO TAG ANNOTATION and retrieval experiments demonstrate that the new approach outperforms our MIREX 2009 WINNING METHOD . \n",
            "this paper addresses the problem of AUTOMATIC AUDIO TAGGING from a single MUSIC CLIP . we propose a novel approach to AUTOMATIC AUDIO TAGGING based on TAG CORRELATION . the proposed approach is based on the use of TAGS extracted from the AUDIO TAGS to estimate the TAG COUNT . the proposed approach is based on the use of TAGS extracted from the AUDIO TAGS to estimate the TAG COUNT . the proposed method is based on the use of TAGS in the TAG , which is a COST-SENSITIVE MULTI-LABEL LEARNING PROBLEM . the proposed MIREX 2009 WINNING METHOD is applied to the AUDIO TAGGING PROBLEM in a COST-SENSITIVE CLASSIFICATION PROBLEM . experimental results show that the proposed method outperforms the state-of-the-art methods .\n",
            "\n",
            "69 1000\n",
            "we formulate the problem of BIPARTITE GRAPH INFERENCE as a SUPERVISED LEARNING PROBLEM , and propose a new method to solve it from the viewpoint of DISTANCE METRIC LEARNING . the method involves the learning of two mappings of the heterogeneous objects to a UNIFIED EUCLIDEAN SPACE representing the NETWORK TOPOLOGY OF THE BIPARTITE GRAPH , where the GRAPH is easy to infer . the algorithm can be formulated as an OPTIMIZATION PROBLEM in a REPRODUCING KERNEL HILBERT SPACE . we report encouraging results on the problem of COMPOUND-PROTEIN INTERACTION NETWORK RECONSTRUCTION from CHEMICAL STRUCTURE DATA and GENOMIC SEQUENCE DATA . \n",
            "this paper addresses the problem of COMPOUND-PROTEIN INTERACTION NETWORK RECONSTRUCTION from CHEMICAL STRUCTURE DATA and GENOMIC SEQUENCE DATA . we propose a novel method to solve the problem of COMPOUND-PROTEIN INTERACTION NETWORK RECONSTRUCTION in a UNIFIED EUCLIDEAN SPACE . we formulate the SUPERVISED LEARNING PROBLEM as a SUPERVISED LEARNING PROBLEM in a REPRODUCING KERNEL HILBERT SPACE , and formulate the OPTIMIZATION PROBLEM as a SUPERVISED LEARNING PROBLEM . we show that the COMPOUND-PROTEIN INTERACTION NETWORK RECONSTRUCTION can be solved efficiently using BIPARTITE GRAPH INFERENCE . we demonstrate the effectiveness of our method on a variety of GENOMIC SEQUENCE DATA and GENOMIC SEQUENCE DATA .\n",
            "\n",
            "70 1000\n",
            "the classical approach to SAMPLING TIME-INVARIANT SPATIAL FIELDS uses STATIC SENSORS distributed over space . we study a new approach involving MOBILE SENSORS that move through space measuring the FIELD VALUES along their paths . a single MOVING SENSOR can take measurements over a WIDE SPATIAL AREA thus acting as a substitute for a potentially large number of STATIC SENSORS . a MOVING SENSOR encounters the SPATIAL FIELD in its path in the form of a TIME-DOMAIN SIGNAL . hence a TIME-DOMAIN ANTI-ALIASING FILTER can be employed at the MOBILE SENSORS to limit the amount of OUT-OF-BAND NOISE prior to SAMPLING . we analytically quantify the advantage of MOBILE SENSORS over STATIC SENSING in rejecting OUT-OF-BAND NOISE . we also demonstrate via simulations the improvement in RECONSTRUCTION ACCURACY that can be obtained using MOBILE SENSORS and FILTERING in a TEMPERATURE MEASUREMENT PROBLEM . \n",
            "this paper addresses the problem of SAMPLING TIME-INVARIANT SPATIAL FIELDS from a MOVING SENSOR . in particular , we consider the problem of recovering a TIME-DOMAIN SIGNAL from a MOVING SENSOR , and use a TIME-DOMAIN ANTI-ALIASING FILTER to estimate the FIELD VALUES of the TIME-DOMAIN SIGNAL . we show that this problem can be viewed as a TEMPERATURE MEASUREMENT PROBLEM . we show that the TEMPERATURE MEASUREMENT PROBLEM can be solved efficiently using a TIME-DOMAIN ANTI-ALIASING FILTER . we also show that the SPATIAL FIELD of the TIME-DOMAIN SIGNAL can be approximated by a MOVING SENSOR . we show that the proposed algorithm is able to recover the FIELD VALUES of the TIME-DOMAIN SIGNAL , and the RECONSTRUCTION ACCURACY of the algorithm is much faster .\n",
            "\n",
            "71 1000\n",
            "when modeling structured outputs such as IMAGE SEG-MENTATIONS , PREDICTION can be improved by accurately MOD-ELING STRUCTURE present in the labels . a key challenge is developing TRACTABLE MODELS that are able to capture COMPLEX HIGH LEVEL STRUCTURE like shape . in this work , we study the learning of a general class of PATTERN-LIKE HIGH ORDER POTENTIAL , which we call COMPOSITIONAL HIGH ORDER PATTERN POTENTIALS . we show that COMPOSITIONAL HIGH ORDER PATTERN POTENTIALS include the LINEAR DEVIATION PATTERN POTENTIALS of <unk> et al. -lsb- 26 -rsb- and also RESTRICTED BOLTZMANN MACHINES ; we also establish the near equivalence of these two models . experimentally , we show that performance is affected significantly by the degree of variability present in the datasets , and we define a QUANTITATIVE VARIABILITY MEASURE to aid in studying this . we then improve COMPOSITIONAL HIGH ORDER PATTERN POTENTIALS performance in HIGH VARIABILITY DATASETS with two primary contributions : -lrb- a -rrb- developing a LOSS-SENSITIVE JOINT LEARNING PROCEDURE , so that INTERNAL PATTERN PARAMETERS can be learned in conjunction with other model potentials to minimize expected loss ; and -lrb- b -rrb- learning an IMAGE-DEPENDENT MAPPING that encourages or <unk> patterns depending on IMAGE FEATURES . we also explore varying how multiple patterns are composed , and learning CONVOLUTIONAL PATTERNS . quantitative results on challenging highly variable datasets show that the JOINT LEARNING and IMAGE-DEPENDENT HIGH ORDER POTENTIALS can improve performance . \n",
            "this paper proposes a novel LOSS-SENSITIVE JOINT LEARNING PROCEDURE for COMPOSITIONAL HIGH ORDER PATTERN POTENTIALS . the proposed COMPOSITIONAL HIGH ORDER PATTERN POTENTIALS is based on a novel LOSS-SENSITIVE JOINT LEARNING PROCEDURE , called LINEAR DEVIATION PATTERN POTENTIALS , which is able to capture the COMPLEX HIGH LEVEL STRUCTURE of the IMAGE FEATURES . the proposed LOSS-SENSITIVE JOINT LEARNING PROCEDURE is based on a LOSS-SENSITIVE JOINT LEARNING PROCEDURE , which is able to capture the COMPLEX HIGH LEVEL STRUCTURE of the IMAGE FEATURES . the proposed approach is based on a novel LOSS-SENSITIVE JOINT LEARNING PROCEDURE , called LINEAR DEVIATION PATTERN POTENTIALS , which is able to deal with COMPLEX HIGH LEVEL STRUCTURE in the presence of COMPLEX HIGH LEVEL STRUCTURE . the proposed approach is based on a LOSS-SENSITIVE JOINT LEARNING PROCEDURE , which is able to deal with COMPLEX HIGH LEVEL STRUCTURE in the presence of COMPLEX HIGH LEVEL STRUCTURE . we demonstrate the effectiveness of the proposed method in the context of JOINT LEARNING .\n",
            "\n",
            "72 1000\n",
            "we present a GENERATIVE MODEL for the UNSUPERVISED LEARNING OF DEPENDENCY STRUCTURES . we also describe the multiplicative combination of this GENERATIVE MODEL with a MODEL OF LINEAR CONSTITUENCY . the GENERATIVE MODEL outperforms both components on their respective EVALUATION METRICS , giving the best published figures for UN-SUPERVISED DEPENDENCY PARSING and UNSUPERVISED CONSTITUENCY PARSING . we also demonstrate that the combined GENERATIVE MODEL works and is robust <unk> , being able to exploit either attachment or DISTRIBUTIONAL REGULARITIES that are salient in the data . \n",
            "in this paper , we propose a novel GENERATIVE MODEL for UN-SUPERVISED DEPENDENCY PARSING and UN-SUPERVISED DEPENDENCY PARSING . the proposed GENERATIVE MODEL is based on the use of a GENERATIVE MODEL , which is able to deal with DISTRIBUTIONAL REGULARITIES . the proposed GENERATIVE MODEL is based on the use of a GENERATIVE MODEL , which is able to deal with DISTRIBUTIONAL REGULARITIES . the proposed GENERATIVE MODEL is applied to the task of UN-SUPERVISED DEPENDENCY PARSING and UNSUPERVISED CONSTITUENCY PARSING . the experimental results show that the proposed GENERATIVE MODEL is able to significantly improve the performance of the proposed GENERATIVE MODEL .\n",
            "\n",
            "73 1000\n",
            "compressed sensing is a novel technique where one can recover SPARSE SIGNALS from the UNDERSAMPLED MEASUREMENTS . in this paper , a K × N MEASUREMENT MATRIX for COMPRESSED SENSING is deterministically constructed via ADDITIVE CHARACTER SEQUENCES . the WEIL BOUND is then used to show that the DETERMINISTIC SENSING MATRIX has ASYMPTOTICALLY OPTIMAL COHERENCE for n = k 2 , and that it is a tight frame . a sparse recovery guarantee for the INCOHERENT TIGHT FRAME is also discussed . numerical results show that the DETERMINISTIC SENSING MATRIX guarantees empirically reliable recovery performance via an L 1-MINIMIZATION METHOD for NOISELESS MEASUREMENTS . \n",
            "this paper proposes a novel L 1-MINIMIZATION METHOD for COMPRESSED SENSING . the proposed L 1-MINIMIZATION METHOD is based on a K × N MEASUREMENT MATRIX for SPARSE SIGNALS . the proposed L 1-MINIMIZATION METHOD is based on a K × N MEASUREMENT MATRIX , which is based on a K × N MEASUREMENT MATRIX . the proposed L 1-MINIMIZATION METHOD is based on a K × N MEASUREMENT MATRIX , which is based on a K × N MEASUREMENT MATRIX . the proposed L 1-MINIMIZATION METHOD is applied to ADDITIVE CHARACTER SEQUENCES , and the experimental results show that the proposed L 1-MINIMIZATION METHOD is effective in reducing the number of UNDERSAMPLED MEASUREMENTS in the presence of SPARSE SIGNALS .\n",
            "\n",
            "74 1000\n",
            "the substantial overhead of performing GLOBAL INTERNET MONITORING motivates techniques for inferring SPATIALLY LOCALIZED INFORMATION about performance using only HOST-BASED , END-TO-END MEASUREMENTS . in this paper , we present a novel methodology for INFERRING QUEUING DELAY DISTRIBUTIONS across internal links in the network based solely on <unk> , end-to-end measurements . a key feature of our new approach is that it is nonparametric , meaning that no a PRIORI LIMIT is placed on the number of UNKNOWN PARAMETERS used to model the DELAY DISTRIBUTIONS . the nonparametric approach is required in order to accurately estimate the wide variety of INTERNAL DELAY DISTRIBUTIONS . the methodology is formulated according to a recently proposed NONPARAMETRIC , WAVELET-BASED DENSITY ESTIMATION METHOD in combination with an EXPECTATION-MAXIMIZATION OPTIMIZATION ALGORITHM that employs a novel FAST FOURIER TRANSFORM IMPLEMENTATION . we perform NETWORK LEVEL NS SIMULATIONS to verify the ACCURACY of the ESTIMATION PROCEDURE . \n",
            "this paper addresses the problem of INFERRING QUEUING DELAY DISTRIBUTIONS for INFERRING QUEUING DELAY DISTRIBUTIONS . in particular , we propose a novel ESTIMATION PROCEDURE based on GLOBAL INTERNET MONITORING . the proposed NONPARAMETRIC , WAVELET-BASED DENSITY ESTIMATION METHOD is based on a FAST FOURIER TRANSFORM IMPLEMENTATION , which is based on a NONPARAMETRIC , WAVELET-BASED DENSITY ESTIMATION METHOD . the proposed ESTIMATION PROCEDURE is based on the FAST FOURIER TRANSFORM IMPLEMENTATION , which is based on the EXPECTATION-MAXIMIZATION OPTIMIZATION ALGORITHM . the proposed ESTIMATION PROCEDURE is evaluated on the NETWORK LEVEL NS SIMULATIONS and the NETWORK LEVEL NS SIMULATIONS is evaluated on the NETWORK LEVEL NS SIMULATIONS . the experimental results show that the proposed ESTIMATION PROCEDURE is effective in INFERRING QUEUING DELAY DISTRIBUTIONS . the proposed ESTIMATION PROCEDURE is evaluated in terms of both ACCURACY and ACCURACY .\n",
            "\n",
            "75 1000\n",
            "distributed constraint optimization -lrb- DISTRIBUTED CONSTRAINT OPTIMIZATION -rrb- is an important framework for COORDINATED MULTIAGENT DECISION MAKING . we address a practically useful variant of DISTRIBUTED CONSTRAINT OPTIMIZATION , called DISTRIBUTED CONSTRAINT OPTIMIZATION , which takes into account agents ' consumption of shared limited resources . we present a promising new class of algorithm for DISTRIBUTED CONSTRAINT OPTIMIZATION by translating the underlying COORDINATION PROBLEM to PROBABILISTIC INFERENCE . using INFERENCE TECHNIQUES such as EXPECTATION-MAXIMIZATION AND CONVEX OPTIMIZATION MACHINERY , we develop a novel CONVERGENT MESSAGE-PASSING ALGORITHM for DISTRIBUTED CONSTRAINT OPTIMIZATION . experiments on standard benchmarks show that our approach provides better quality than previous best DCOP ALGORITHMS and has much lower FAILURE RATE . comparisons against an efficient CENTRALIZED SOLVER show that our approach provides NEAR-OPTIMAL SOLUTIONS , and is significantly faster on larger instances . \n",
            "this paper presents a novel CONVERGENT MESSAGE-PASSING ALGORITHM for COORDINATED MULTIAGENT DECISION MAKING in COORDINATED MULTIAGENT DECISION MAKING . the proposed CONVERGENT MESSAGE-PASSING ALGORITHM is based on the use of EXPECTATION-MAXIMIZATION AND CONVEX OPTIMIZATION MACHINERY , which is a generalization of the CONVERGENT MESSAGE-PASSING ALGORITHM . the proposed CONVERGENT MESSAGE-PASSING ALGORITHM is based on a CONVERGENT MESSAGE-PASSING ALGORITHM , which is a generalization of the standard CONVERGENT MESSAGE-PASSING ALGORITHM . the proposed CONVERGENT MESSAGE-PASSING ALGORITHM is based on the EXPECTATION-MAXIMIZATION AND CONVEX OPTIMIZATION MACHINERY , which is a generalization of the CONVERGENT MESSAGE-PASSING ALGORITHM . the proposed CONVERGENT MESSAGE-PASSING ALGORITHM is applied to the problem of COORDINATED MULTIAGENT DECISION MAKING . the experimental results show that the proposed CONVERGENT MESSAGE-PASSING ALGORITHM is effective in reducing the FAILURE RATE and FAILURE RATE of the proposed DCOP ALGORITHMS .\n",
            "\n",
            "76 1000\n",
            "this paper proposes a framework for training CONDITIONAL RANDOM FIELDS to optimize MULTIVARIATE EVALUATION MEASURES , including NON-LINEAR MEASURES such as F-SCORE . our proposed framework is derived from an ERROR MINIMIZATION APPROACH that provides a simple solution for directly optimizing any EVALUATION MEASURE . specifically focusing on SEQUENTIAL SEGMENTATION TASKS , i.e. TEXT CHUNKING and NAMED ENTITY RECOGNITION , we introduce a LOSS FUNCTION that closely reflects the TARGET EVALUATION MEASURE for these SEQUENTIAL SEGMENTATION TASKS , namely , SEGMENTATION F-SCORE . our experiments show that our method performs better than standard CRF TRAINING . \n",
            "in this paper , we propose a novel approach to NAMED ENTITY RECOGNITION and NAMED ENTITY RECOGNITION . the proposed approach is based on the use of CONDITIONAL RANDOM FIELDS , which is a generalization of the traditional ERROR MINIMIZATION APPROACH . the proposed approach is based on the use of CONDITIONAL RANDOM FIELDS , which is a generalization of the ERROR MINIMIZATION APPROACH . the proposed approach is based on the use of CONDITIONAL RANDOM FIELDS , which is a generalization of the ERROR MINIMIZATION APPROACH . the proposed method is evaluated on a variety of SEQUENTIAL SEGMENTATION TASKS including NAMED ENTITY RECOGNITION and NAMED ENTITY RECOGNITION . the experimental results show that the proposed method achieves better performance than the state-of-the-art methods .\n",
            "\n",
            "77 1000\n",
            "various representations and INFERENCE METHODS have been proposed for LIFTED PROBABILISTIC INFERENCE in RELA-TIONAL MODELS . many of these INFERENCE METHODS choose an order to eliminate -lrb- or branch on -rrb- the PARAMETERIZED RANDOM VARIABLES . similar to such INFERENCE METHODS for NON-RELATIONAL PROBABILISTIC INFERENCE , the order of elimination has a significant role in the performance of the INFERENCE METHODS . since finding the best order is NP-COMPLETE even for RELATIONAL MODELS , heuristics have been proposed to find good orderings in the RELATIONAL MODELS . in this paper , we show that these heuristics are inefficient for RELATIONAL MODELS , because they fail to consider the POPULATION SIZES associated with LOGICAL VARIABLES . we extend existing heuristics for RELATIONAL MODELS and propose new heuristics for RELATIONAL MODELS . we evaluate the existing and new heuristics on a range of GENERATED RELATIONAL GRAPHS . \n",
            "this paper addresses the problem of NON-RELATIONAL PROBABILISTIC INFERENCE in RELATIONAL MODELS . we propose a new approach to the problem of NON-RELATIONAL PROBABILISTIC INFERENCE , where the goal is to maximize the number of LOGICAL VARIABLES in a given set of LOGICAL VARIABLES . we show that this problem can be solved efficiently using INFERENCE METHODS for NON-RELATIONAL PROBABILISTIC INFERENCE . we show that our algorithm can be applied to NON-RELATIONAL PROBABILISTIC INFERENCE , and we show that it is possible to efficiently solve the problem of NON-RELATIONAL PROBABILISTIC INFERENCE . we also show that our algorithm can be applied to NON-RELATIONAL PROBABILISTIC INFERENCE for NON-RELATIONAL PROBABILISTIC INFERENCE .\n",
            "\n",
            "78 1000\n",
            "partially observable markov decision processes -lrb- pomdps -rrb- are often used to model planning problems under uncertainty . the goal in RISK-SENSITIVE POMDPS is to find a POLICY that maximizes the probability that the CUMULATIVE COST is within some USER-DEFINED COST THRESHOLD . in this paper , unlike existing RISK-SENSITIVE POMDPS , we distinguish between the two cases of whether costs can or can not be observed and show the empirical impact of COST OBSERVATIONS . we also introduce a new SEARCH-BASED ALGORITHM to solve RISK-SENSITIVE POMDPS and show that SEARCH-BASED ALGORITHM is faster and more scalable than existing approaches in two SYNTHETIC DOMAINS and a TAXI DOMAIN generated with REAL-WORLD DATA . \n",
            "in this paper , we propose a novel SEARCH-BASED ALGORITHM for PARTIALLY OBSERVABLE MARKOV DECISION PROCESSES . the proposed SEARCH-BASED ALGORITHM is based on the use of a set of COST OBSERVATIONS , each of which is a set of COST OBSERVATIONS . the proposed SEARCH-BASED ALGORITHM is applied to the problem of PARTIALLY OBSERVABLE MARKOV DECISION PROCESSES . the proposed SEARCH-BASED ALGORITHM is applied to the problem of PARTIALLY OBSERVABLE MARKOV DECISION PROCESSES . the proposed SEARCH-BASED ALGORITHM is evaluated on a variety of SYNTHETIC DOMAINS and REAL-WORLD DATA . the experimental results show that the proposed SEARCH-BASED ALGORITHM outperforms the conventional SEARCH-BASED ALGORITHM in terms of the CUMULATIVE COST and the TAXI DOMAIN .\n",
            "\n",
            "79 1000\n",
            "we study a PHASE RETRIEVAL PROBLEM in the POISSON NOISE MODEL . motivated by the PHASELIFT APPROACH , we approximate the MAXIMUM-LIKELIHOOD ESTIMATOR by solving a CONVEX PROGRAM with a NUCLEAR NORM CONSTRAINT . while the FRANK-WOLFE ALGORITHM , together with the LANCZOS METHOD , can efficiently deal with NUCLEAR NORM CONSTRAINTS , our OBJECTIVE FUNCTION does not have a LIPSCHITZ CONTINUOUS GRADIENT , and hence existing CONVERGENCE GUARANTEES for the FRANK-WOLFE ALGORITHM do not apply . in this paper , we show that the FRANK-WOLFE ALGORITHM works for the POISSON PHASE RETRIEVAL PROBLEM , and has a GLOBAL CONVERGENCE RATE of o -lrb- 1/t -rrb- , where t is the ITERATION COUNTER . we provide rigorous theoretical guarantee and illustrating numerical results . \n",
            "this paper proposes a novel PHASELIFT APPROACH to the POISSON PHASE RETRIEVAL PROBLEM . the proposed PHASELIFT APPROACH is based on the NUCLEAR NORM CONSTRAINT , which is a POISSON PHASE RETRIEVAL PROBLEM . the proposed PHASELIFT APPROACH is based on the NUCLEAR NORM CONSTRAINT , which is a POISSON PHASE RETRIEVAL PROBLEM . the proposed PHASELIFT APPROACH is based on the NUCLEAR NORM CONSTRAINT , which is a POISSON PHASE RETRIEVAL PROBLEM . the proposed PHASELIFT APPROACH is applied to the problem of PHASE RETRIEVAL PROBLEM in a PHASE RETRIEVAL PROBLEM . the proposed PHASELIFT APPROACH is compared to the standard PHASELIFT APPROACH in terms of both GLOBAL CONVERGENCE RATE and GLOBAL CONVERGENCE RATE .\n",
            "\n",
            "80 1000\n",
            "to ensure high enough RECOGNITION performance from the <unk> of usage of the SPEECH RECOGNITION SYSTEM , prior development of highly precise ACOUSTIC MODEL LIBRARY is necessary . the analysis of HMM ACOUSTIC MODELS expressed with GAUSSIAN DISTRIBUTIONS OF MULTIDIMENSIONAL VECTORS is typically a difficult task . the <unk> -lrb- acoustic space map of sound -rrb- method featuring the visualization of distributions of the HMM ACOUSTIC MODELS in a two dimensional space by utilizing MULTIDIMENSIONAL SCALING TECHNIQUE is proposed in order to support the analysis through capability of HUMAN VISUAL PERCEPTION . the effectiveness of the proposed technique is reviewed based on an analysis on SPEAKING STYLES . the MARGINAL REGION within the TWO-DIMENSIONAL VISUAL MAP -lrb- called COSMOS MAP -rrb- obtained by the proposed method the contains HMM ACOUSTIC MODELS with lower RECOGNITION performance . it is possible to improve RECOGNITION performance by dividing the MARGINAL REGION into several smaller zones in which separate HMM ACOUSTIC MODELS is trained and provided to the speakers belonging to the same zone . \n",
            "in this paper , we propose a novel approach to HUMAN VISUAL PERCEPTION based on a MULTIDIMENSIONAL SCALING TECHNIQUE . the proposed approach is based on the use of a GAUSSIAN DISTRIBUTIONS OF MULTIDIMENSIONAL VECTORS for RECOGNITION . the proposed approach is based on the use of a GAUSSIAN DISTRIBUTIONS OF MULTIDIMENSIONAL VECTORS and a MULTIDIMENSIONAL SCALING TECHNIQUE . the proposed method is based on the use of a GAUSSIAN DISTRIBUTIONS OF MULTIDIMENSIONAL VECTORS and a MULTIDIMENSIONAL SCALING TECHNIQUE to estimate the MARGINAL REGION . the proposed method is evaluated on a SPEECH RECOGNITION SYSTEM and compared to the state-of-the-art methods . the proposed method is evaluated in terms of RECOGNITION and RECOGNITION performance .\n",
            "\n",
            "81 1000\n",
            "this paper discusses the application of the EXPECTATION-MAXIMIZATION CLUSTERING ALGORITHM to the task of CHINESE VERB SENSE DISCRIMINATION . the EXPECTATION-MAXIMIZATION CLUSTERING ALGORITHM utilized RICH LINGUISTIC FEATURES that capture PREDICATE-ARGUMENT STRUCTURE INFORMATION of the target verbs . a SEMANTIC TAXONOMY for CHINESE NOUNS , which was built semi-automatically based on two ELECTRONIC CHINESE SEMANTIC DICTIONARIES , was used to provide SEMANTIC FEATURES for the EXPECTATION-MAXIMIZATION CLUSTERING ALGORITHM . purity and NORMALIZED MUTUAL INFORMATION were used to evaluate the clustering performance on 12 CHINESE VERBS . the experimental results show that the EXPECTATION-MAXIMIZATION CLUSTERING ALGORITHM can learn sense or sense group distinctions for most of the verbs successfully . we further enhanced the EXPECTATION-MAXIMIZATION CLUSTERING ALGORITHM with certain FINE-GRAINED SEMANTIC CATEGORIES called LEXICAL SETS . our results indicate that these LEXICAL SETS improve the EXPECTATION-MAXIMIZATION CLUSTERING ALGORITHM 's performance for the three most challenging verbs chosen from the first set of experiments . \n",
            "in this paper , we propose a novel EXPECTATION-MAXIMIZATION CLUSTERING ALGORITHM based on ELECTRONIC CHINESE SEMANTIC DICTIONARIES . the proposed EXPECTATION-MAXIMIZATION CLUSTERING ALGORITHM is based on the use of RICH LINGUISTIC FEATURES extracted from the ELECTRONIC CHINESE SEMANTIC DICTIONARIES . the proposed EXPECTATION-MAXIMIZATION CLUSTERING ALGORITHM is based on the use of RICH LINGUISTIC FEATURES extracted from the ELECTRONIC CHINESE SEMANTIC DICTIONARIES . the proposed EXPECTATION-MAXIMIZATION CLUSTERING ALGORITHM is based on the use of RICH LINGUISTIC FEATURES extracted from the LEXICAL SETS . the proposed EXPECTATION-MAXIMIZATION CLUSTERING ALGORITHM is applied to the ELECTRONIC CHINESE SEMANTIC DICTIONARIES , and the results show that the proposed EXPECTATION-MAXIMIZATION CLUSTERING ALGORITHM is effective in CHINESE VERB SENSE DISCRIMINATION .\n",
            "\n",
            "82 1000\n",
            "<unk> speech synthesis is necessary in some applications such as AUTOMATIC FILM DUBBING or SPOKEN TRANSLATION . this paper presents a model for the generation of SYNTHETIC DISFLUENT SPEECH based on inserting each element of a disfluency in a context where they can be considered fluent . PROSODY obtained by the application of standard techniques on these new sentences is used for the synthesis of the <unk> sentence . in addition , LOCAL MODIFICATIONS are applied to SEGMENTAL UNITS adjacent to DISFLUENCY ELEMENTS . experiments evidence that duration follows this behavior , what supports the feasibility of the model . \n",
            "this paper presents a novel approach to SPOKEN TRANSLATION and SPOKEN TRANSLATION . the proposed approach is based on the use of LOCAL MODIFICATIONS for SPOKEN TRANSLATION and SPOKEN TRANSLATION . the proposed approach is based on the use of LOCAL MODIFICATIONS for SPOKEN TRANSLATION and SPOKEN TRANSLATION . the experimental results show that the proposed method is effective in improving the performance of SPOKEN TRANSLATION and SPOKEN TRANSLATION .\n",
            "\n",
            "83 1000\n",
            "an original concept for computing instantaneous 3D POSE and 3d velocity of fast moving objects using a single view is proposed , implemented and validated . it takes advantage of the IMAGE DEFORMATIONS induced by ROLLING SHUTTER in CMOS IMAGE SENSORS . first of all , after analysing the ROLLING SHUTTER PHENOMENON , we introduce an original model of the IMAGE FORMATION when using such a camera , based on a general model of moving rigid sets of 3d points . using 2D-3D POINT CORRESPONDENCES , we derive two complementary methods , compensating for the ROLLING SHUTTER DEFORMATIONS to deliver an accurate 3D POSE and exploiting them to also estimate the FULL 3D VELOCITY . the first solution is a general one based on NON-LINEAR OPTIMIZATION and BUNDLE ADJUSTMENT , usable for any object , while the second one is a CLOSED-FORM LINEAR SOLUTION valid for PLANAR OBJECTS . the resulting CLOSED-FORM LINEAR SOLUTION enable us to transform a cmos low cost and low power camera into an innovative and powerful VELOCITY SENSOR . finally , experimental results with real data confirm the relevance and ACCURACY of the CLOSED-FORM LINEAR SOLUTION . \n",
            "in this paper , we propose a novel approach to IMAGE FORMATION based on IMAGE FORMATION . the proposed method is based on a CLOSED-FORM LINEAR SOLUTION that exploits 2D-3D POINT CORRESPONDENCES and BUNDLE ADJUSTMENT . the proposed method is based on a CLOSED-FORM LINEAR SOLUTION that exploits 2D-3D POINT CORRESPONDENCES and BUNDLE ADJUSTMENT . the proposed method is based on a NON-LINEAR OPTIMIZATION that exploits 2D-3D POINT CORRESPONDENCES and BUNDLE ADJUSTMENT . the proposed method is based on the use of 2D-3D POINT CORRESPONDENCES and BUNDLE ADJUSTMENT . the experimental results show that the proposed method outperforms the state-of-the-art methods in terms of ACCURACY and BUNDLE ADJUSTMENT .\n",
            "\n",
            "84 1000\n",
            "maximum margin criterion is a well-known method for FEATURE EXTRACTION and DIMENSIONALITY REDUCTION . in this paper , we propose a novel FEATURE EXTRACTION method , namely TWO DIMENSIONAL MAXIMUM MARGIN CRITERION , specifically for MATRIX REPRESENTATION DATA , e.g. IMAGES . 2DMMC aims to find two ORTHOGONAL PROJECTION MATRICES to project the original matrices to a LOW DIMENSIONAL MATRIX SUBSPACE , in which a sample is close to those in the same class but far from those in different classes . both THEORETICAL ANALYSIS and experiments on BENCHMARK FACE RECOGNITION DATA SETS illustrate that the proposed method is very effective and efficient . \n",
            "this paper addresses the problem of DIMENSIONALITY REDUCTION in IMAGES . we propose a novel method to learn ORTHOGONAL PROJECTION MATRICES for IMAGES . the proposed method is based on the MAXIMUM MARGIN CRITERION , which is a MAXIMUM MARGIN CRITERION for FEATURE EXTRACTION . the proposed method is based on the MAXIMUM MARGIN CRITERION , which is based on the MAXIMUM MARGIN CRITERION . the proposed method is based on the use of ORTHOGONAL PROJECTION MATRICES , which is able to deal with IMAGES . the proposed method is evaluated on the BENCHMARK FACE RECOGNITION DATA SETS , and the results show that the proposed method is effective in improving the DIMENSIONALITY REDUCTION performance .\n",
            "\n",
            "85 1000\n",
            "this study investigates the ACOUSTIC CHARACTERISTICS of / t h / <unk> in CONVERSATIONAL SPEECH OF BRUNEI MANDARIN , a variety of MANDARIN CHINESE . based on data from 20 CHINESE BRUNEIANS , / t h / <unk> was found in the <unk> pronoun <unk> / t h a / , which is frequently pronounced as <unk> -lsb- ha -rsb- . PERCEPTUAL JUDGMENTS , SPECTROGRAPHIC ANALYSIS and ACOUSTIC CHARACTERISTICS were conducted to examine the FEATURES of this SOUND CHANGE . in comparison with the PERCEPTUAL JUDGMENTS , it was found that the SPECTROGRAPHIC INSPECTION yielded <unk> % CORRECT CLASSIFICATION of -lsb- t h -rsb- and -lsb- h -rsb- for FEMALE SPEAKERS and <unk> % for male speakers , indicating there is reasonably high reliability in IDENTIFICATION in terms of SPECTRAL PROPERTIES . results of the ACOUSTIC CHARACTERISTICS showed that there is an increase in high frequency intensity after the release of the CLOSURE for -lsb- t h -rsb- while there is little change in intensity during the FRICATION for -lsb- h -rsb- . the results showed that the lack of BURST and little increase in intensity are reasonably reliable cues for stop <unk> . \n",
            "this paper presents a novel approach to CONVERSATIONAL SPEECH OF BRUNEI MANDARIN in CONVERSATIONAL SPEECH OF BRUNEI MANDARIN . the proposed method is based on the use of FEATURES and SPECTRAL PROPERTIES in the CONVERSATIONAL SPEECH OF BRUNEI MANDARIN . the proposed method consists of two steps : -lrb- 1 -rrb- the SPECTRAL PROPERTIES in the ACOUSTIC CHARACTERISTICS and 2 -rrb- the ACOUSTIC CHARACTERISTICS , 2 -rrb- the ACOUSTIC CHARACTERISTICS , and 3 -rrb- the ACOUSTIC CHARACTERISTICS and the ACOUSTIC CHARACTERISTICS . the proposed method is based on the use of FEATURES and ACOUSTIC CHARACTERISTICS . the proposed method is evaluated on a variety of MANDARIN CHINESE . the results show that the proposed method achieves better performance than the state-of-the-art methods .\n",
            "\n",
            "86 1000\n",
            "comparative news summarization aims to highlight the <unk> and differences between two comparable NEWS TOPICS . in this study , we propose a novel approach to generating COMPARATIVE NEWS SUMMARIES . we formulate the task as an OPTIMIZATION PROBLEM of selecting proper sentences to maximize the <unk> within the summary and the representativeness to both NEWS TOPICS . we consider SEMANTIC-RELATED CROSS-TOPIC CONCEPT PAIRS as comparative evidences , and consider TOPIC-RELATED CONCEPTS as representative evidences . the OPTIMIZATION PROBLEM is addressed by using a LINEAR PROGRAMMING MODEL . the experimental results demonstrate the effectiveness of our proposed model . \n",
            "this paper addresses the problem of SEMANTIC-RELATED CROSS-TOPIC CONCEPT PAIRS in the presence of NEWS TOPICS . we propose a novel method to solve the problem of COMPARATIVE NEWS SUMMARIES . the proposed method is based on a novel LINEAR PROGRAMMING MODEL , which is based on a LINEAR PROGRAMMING MODEL . the proposed method is based on the LINEAR PROGRAMMING MODEL , which is based on a LINEAR PROGRAMMING MODEL . the experimental results show that the proposed method outperforms the existing methods in terms of both COMPARATIVE NEWS SUMMARIES .\n",
            "\n",
            "87 1000\n",
            "stochastic optimization arising from PRECODING in a MULTI-ANTENNA FADING CHANNEL with channel mean feedback to maximize data rates is important but challenging . the use of RELAYING further complicates the situation , as it may induce a NONCONVEX STRUCTURE in the OBJECTIVE FUNCTION , thereby excluding the use of existing approaches which require convexity or concavity . to deal with challenges as such , this paper presents a new framework for solving a class of STOCHASTIC OPTIMIZATION PROBLEMS . the analysis here involves the comparison of two NONNEGATIVE RANDOM VARIABLES in the LAPLACE TRANSFORM ORDER . our framework is <unk> to optimal PRECODING for MAXIMUM ERGODIC OR EFFECTIVE CAPACITY in MULTI-ANTENNA CHANNELS with or without RELAYING assuming channel mean feedback , where the objectives may or may not have convexity or concavity . the application to STOCHASTIC POWER ALLOCATION is also discussed . \n",
            "this paper addresses the problem of STOCHASTIC POWER ALLOCATION in a MULTI-ANTENNA FADING CHANNEL . in particular , we consider the problem of STOCHASTIC POWER ALLOCATION in a MULTI-ANTENNA FADING CHANNEL , where the NONCONVEX STRUCTURE is a MULTI-ANTENNA FADING CHANNEL . we show that the LAPLACE TRANSFORM ORDER of the LAPLACE TRANSFORM ORDER can be reduced to the LAPLACE TRANSFORM ORDER of the LAPLACE TRANSFORM ORDER . we show that the LAPLACE TRANSFORM ORDER of the LAPLACE TRANSFORM ORDER can be reduced to the LAPLACE TRANSFORM ORDER of the LAPLACE TRANSFORM ORDER . we also show that the LAPLACE TRANSFORM ORDER of the LAPLACE TRANSFORM ORDER can be reduced to the LAPLACE TRANSFORM ORDER of the LAPLACE TRANSFORM ORDER .\n",
            "\n",
            "88 1000\n",
            "<unk> <unk> multiple-access -lrb- <unk> -rrb- has been adopted in the UPLINK of the LTE STANDARD due to its lower PEAK-TO-AVERAGE-POWER RATIO compared to ORTHOGONAL FREQUENCY-DIVISION MULTIPLE ACCESS . recent activities in the LTE-ADVANCED STANDARDIZATION have focused on effective UPLINK TRANSMIT DIVERSITY SCHEMES with low <unk> -lsb- 1 -rsb- but without considering the PERFORMANCE DEGRADATION due to DOPPLER despite the fact that ORTHOGONAL FREQUENCY-DIVISION MULTIPLE ACCESS is required to support high mobility . in this paper , we present an OPEN-LOOP SPACE-FREQUENCY BLOCK CODING SCHEME for the SC-FDMA UPLINK and demonstrate its ROBUSTNESS to high DOPPLER and large multipath delay spread while enjoying FULL SPATIAL DIVERSITY , low <unk> and PRACTICAL DECODING COMPLEXITY by a suitable design of the frequency span of each SFBC CODEWORD . in this paper , we study the DESIGN TRADEOFFS involved in SINGLE-CARRIER FREQUENCY-DIVISION MULTIPLE-ACCESS for MOBILE SC-FDMA . \n",
            "this paper presents a novel approach to ORTHOGONAL FREQUENCY-DIVISION MULTIPLE ACCESS for SINGLE-CARRIER FREQUENCY-DIVISION MULTIPLE-ACCESS . the proposed approach is based on a LTE-ADVANCED STANDARDIZATION for SINGLE-CARRIER FREQUENCY-DIVISION MULTIPLE-ACCESS , which is based on a LTE-ADVANCED STANDARDIZATION . the proposed OPEN-LOOP SPACE-FREQUENCY BLOCK CODING SCHEME is based on a LTE-ADVANCED STANDARDIZATION . the proposed OPEN-LOOP SPACE-FREQUENCY BLOCK CODING SCHEME is based on a LTE-ADVANCED STANDARDIZATION and is able to deal with FULL SPATIAL DIVERSITY . the proposed method is evaluated in terms of ROBUSTNESS and PRACTICAL DECODING COMPLEXITY . the proposed method is evaluated in terms of ROBUSTNESS and PRACTICAL DECODING COMPLEXITY . the results show that the proposed method can reduce the PERFORMANCE DEGRADATION by up to 50 % .\n",
            "\n",
            "89 1000\n",
            "this article deals with FACIAL SEGMENTATION and LIPTRACKING with FEEDBACK CONTROL by FACE MODEL SYNTHESIS . on this topic , the search community is divided into two parts : analysis and synthesis . we want to use all the knowledge to create a GLOBAL ANALYSIS/SYNTHESIS CHAIN where the IMAGE ANALYSIS needs the 3D SYNTHESIS and conversely . as it happens , applications like FACE TRACKING or AUGMENTED REALITY need a rapid , robust and <unk> solution . our solution is based on a TWO STEP APPROACH : the first step is a REAL-TIME FACIAL SEGMENTATION with ACTIVE CONTOUR MODELS and the second step recovers a 3D-FACE MODEL in order to extract more precise parameters to adjust the first step . the contribution of this paper is to couple two research fields for creating a REAL TIME APPLICATION . the results obtained show rapid and robust performances which could be exploited in a more GLOBAL REAL-TIME FACE TRACKING APPLICATION . \n",
            "this paper addresses the problem of FACE TRACKING and FACIAL SEGMENTATION . we propose a novel approach to the problem of REAL-TIME FACIAL SEGMENTATION . the proposed approach is based on the use of ACTIVE CONTOUR MODELS , FEEDBACK CONTROL , FEEDBACK CONTROL , and FEEDBACK CONTROL . the proposed approach is based on the use of ACTIVE CONTOUR MODELS , FEEDBACK CONTROL , and LIPTRACKING . the proposed approach is evaluated on a GLOBAL REAL-TIME FACE TRACKING APPLICATION and a GLOBAL REAL-TIME FACE TRACKING APPLICATION . the experimental results show that the proposed method outperforms the state-of-the-art methods in terms of both FACIAL SEGMENTATION and FACIAL SEGMENTATION .\n",
            "\n",
            "90 1000\n",
            "we have previously introduced a method of WORD SENSE DISAMBIGUATION that computes the intended sense of a target word , using WORDNET-BASED MEASURES OF SEMANTIC RELATEDNESS -lrb- <unk> et al. , 2003 -rrb- . SENSERELATE : : TARGETWORD is a PERL PACKAGE that implements this algorithm . the WORD SENSE DISAMBIGUATION is carried out by selecting that sense of the target word which is most related to the CONTEXT WORDS . relatedness between WORD SENSES is measured using the SENSERELATE : : SIMILARITY PERL MODULES . \n",
            "this paper describes a new PERL PACKAGE called TARGETWORD , which is a PERL PACKAGE for WORD SENSE DISAMBIGUATION . the proposed SIMILARITY PERL MODULES is a PERL PACKAGE , called SENSERELATE , which is a PERL PACKAGE for WORD SENSE DISAMBIGUATION . the proposed SIMILARITY PERL MODULES is a PERL PACKAGE , called TARGETWORD , which is a PERL PACKAGE . the proposed PERL PACKAGE is a PERL PACKAGE , called TARGETWORD , which is a PERL PACKAGE , which is able to deal with WORD SENSES . we show that the proposed algorithm is able to detect WORD SENSES in a wide range of WORD SENSES , and can be applied to any PERL PACKAGE such as SENSERELATE .\n",
            "\n",
            "91 1000\n",
            "recently , we have presented a TRANSFER-FUNCTION GENERALIZED SIDELOBE CANCELER BEAMFORMER in the short time fourier transform domain , which relies on a convolutive transfer function approximation of RELATIVE TRANSFER FUNCTIONS between distinct sensors . in this paper , we combine a DELAY-AND-SUM BEAMFORMER with the TF-GSC STRUCTURE in order to suppress the SPEECH SIGNAL REFLECTIONS captured at the sensors in REVERBERANT ENVIRONMENTS . we demonstrate the performance of the proposed TRANSFER-FUNCTION GENERALIZED SIDELOBE CANCELER BEAMFORMER and compare TRANSFER-FUNCTION GENERALIZED SIDELOBE CANCELER BEAMFORMER with the TF-GSC . we show that the proposed TRANSFER-FUNCTION GENERALIZED SIDELOBE CANCELER BEAMFORMER enables SUPPRESSION OF REVERBERATIONS and further NOISE REDUCTION compared with the TF-GSC BEAMFORMER . \n",
            "this paper presents a novel TRANSFER-FUNCTION GENERALIZED SIDELOBE CANCELER BEAMFORMER for NOISE REDUCTION and NOISE REDUCTION in REVERBERANT ENVIRONMENTS . the proposed TRANSFER-FUNCTION GENERALIZED SIDELOBE CANCELER BEAMFORMER is based on a TRANSFER-FUNCTION GENERALIZED SIDELOBE CANCELER BEAMFORMER . the proposed TRANSFER-FUNCTION GENERALIZED SIDELOBE CANCELER BEAMFORMER is based on a TRANSFER-FUNCTION GENERALIZED SIDELOBE CANCELER BEAMFORMER . the proposed TRANSFER-FUNCTION GENERALIZED SIDELOBE CANCELER BEAMFORMER is based on the use of a TRANSFER-FUNCTION GENERALIZED SIDELOBE CANCELER BEAMFORMER to estimate the RELATIVE TRANSFER FUNCTIONS . the proposed TRANSFER-FUNCTION GENERALIZED SIDELOBE CANCELER BEAMFORMER is applied to SPEECH SIGNAL REFLECTIONS and NOISE REDUCTION . the experimental results show that the proposed TRANSFER-FUNCTION GENERALIZED SIDELOBE CANCELER BEAMFORMER outperforms the conventional TF-GSC in terms of NOISE REDUCTION and NOISE REDUCTION .\n",
            "\n",
            "92 1000\n",
            "this paper presents a REPRESENTATION THEORY for PERMUTATION-VALUED FUNCTIONS , which in their general form can also be called LISTWISE RANKING FUNCTIONS . POINT-WISE RANKING FUNCTIONS assign a score to each object independently , without taking into account the other objects under consideration ; whereas LISTWISE LOSS FUNCTIONS evaluate the set of scores assigned to all objects as a whole . in many SUPERVISED LEARNING to RANK TASKS , it might be of interest to use LISTWISE RANKING FUNCTIONS instead ; in particular , the BAYES OPTIMAL RANKING FUNCTIONS might themselves be <unk> , especially if the LOSS FUNCTION is <unk> . a key <unk> to using LIST-WISE RANKING FUNCTIONS has been the lack of an appropriate REPRESENTATION THEORY for such functions . we show that a NATURAL SYMMETRICITY ASSUMPTION that we call exchangeability allows us to explicitly characterize the set of such EXCHANGEABLE LISTWISE RANKING FUNCTIONS . our analysis draws from the theories of TENSOR ANALYSIS , FUNCTIONAL ANALYSIS and DE FINETTI THEOREMS . we also present experiments using a novel RERANKING METHOD motivated by our REPRESENTATION THEORY . \n",
            "this paper addresses the problem of BAYES OPTIMAL RANKING FUNCTIONS for RANK TASKS . in particular , we focus on the problem of EXCHANGEABLE LISTWISE RANKING FUNCTIONS , and propose a method to estimate the parameters of the BAYES OPTIMAL RANKING FUNCTIONS . the proposed approach is based on the use of REPRESENTATION THEORY , FUNCTIONAL ANALYSIS , and DE FINETTI THEOREMS . the proposed approach is based on the use of REPRESENTATION THEORY and DE FINETTI THEOREMS . the proposed approach is based on the use of PERMUTATION-VALUED FUNCTIONS , FUNCTIONAL ANALYSIS , and DE FINETTI THEOREMS . the proposed method is evaluated on a variety of RANK TASKS , including TENSOR ANALYSIS , TENSOR ANALYSIS , and DE FINETTI THEOREMS .\n",
            "\n",
            "93 1000\n",
            "in DIALOGUE SYSTEMS , SPEECH RECOGNITION ERRORS force the user to repeat information resulting in more turns , lower DIALOGUE EFFICIENCY and maybe complete failure . HIGHER LEVEL INFORMATION such as HISTORY , EXPECTATION , DISCOURSE KNOWLEDGE and PRAGMATICS can improve performance but are hard to quantify and effectively include in the RECOGNITION PROCESS . in this paper , DIALOGUE EXPECTATION is used to improve RECOGNITION . by permitting the DIALOGUE MANAGER to guide the interaction it is possible to track the DIALOGUE STATE and thus estimate the expected semantic content of the user 's response . the DIALOGUE MANAGER is allowed to process a large number of sentences provided by the DECODER . EXPECTATION is used as an effective criterion for selecting among competing hypotheses . this approach was tested with a simple FLIGHT RESERVATION TASK and results show improvement in CONCEPT RECOGNITION without adding significant computation . \n",
            "this paper presents a novel approach to CONCEPT RECOGNITION in DIALOGUE SYSTEMS . the proposed approach is based on the use of a DIALOGUE MANAGER , a DECODER , and a DIALOGUE MANAGER for CONCEPT RECOGNITION . the proposed method is based on the use of a DIALOGUE MANAGER , a DECODER , and a DECODER . the proposed method is based on a DIALOGUE MANAGER that uses a DIALOGUE MANAGER to estimate the DIALOGUE STATE of the HISTORY . the proposed approach is evaluated on a FLIGHT RESERVATION TASK and a FLIGHT RESERVATION TASK . the results show that the proposed method improves the RECOGNITION performance in terms of RECOGNITION and RECOGNITION performance .\n",
            "\n",
            "94 1000\n",
            "in this paper we present a novel POINT SET REGISTRATION ALGORITHM based on the ROBUST GAUSSIAN MIXTURE MODEL . we take advantage of a robust estimation to weigh the NOISE COMPONENT in ROBUST GAUSSIAN MIXTURE MODEL . moreover , a BIDIRECTIONAL EM PROCESS is introduced to model OUTLIERS in both POINT SETS in contrast to traditional methods . the performance of the POINT SET REGISTRATION ALGORITHM is demonstrated and validated in carefully designed SYNTHETIC DATA and POINT SETS extracted from MEDICAL IMAGES . results show that the proposed POINT SET REGISTRATION ALGORITHM can improve the ROBUSTNESS and ACCURACY as compared to the traditional REGISTRATION TECHNIQUES . \n",
            "this paper presents a novel POINT SET REGISTRATION ALGORITHM for MEDICAL IMAGES . the proposed ROBUST GAUSSIAN MIXTURE MODEL is based on a ROBUST GAUSSIAN MIXTURE MODEL to the BIDIRECTIONAL EM PROCESS . the proposed ROBUST GAUSSIAN MIXTURE MODEL is based on a ROBUST GAUSSIAN MIXTURE MODEL , which is able to deal with OUTLIERS and OUTLIERS . the proposed ROBUST GAUSSIAN MIXTURE MODEL is evaluated on both SYNTHETIC DATA and MEDICAL IMAGES . the performance of the proposed POINT SET REGISTRATION ALGORITHM is evaluated on both SYNTHETIC DATA and POINT SETS . the experimental results show that the proposed ROBUST GAUSSIAN MIXTURE MODEL is effective in improving the ACCURACY of the POINT SET REGISTRATION ALGORITHM in terms of both ACCURACY and ROBUSTNESS .\n",
            "\n",
            "95 1000\n",
            "in this paper we propose a simple but efficient IMAGE REPRESENTATION for solving the SCENE CLASSIFICATION PROBLEM . our new IMAGE REPRESENTATION combines the benefits of SPATIAL PYRAMID REPRESENTATION using NONLINEAR FEATURE CODING and LATENT SUPPORT VECTOR MACHINE to train a set of LATENT PYRAMIDAL REGIONS . each of our LATENT PYRAMIDAL REGIONS captures a DISCRIMINATIVE CHARACTERISTIC OF THE SCENES and is trained by searching over all possible <unk> of the images in a LATENT SVM TRAINING PROCEDURE . each LATENT PYRAMIDAL REGIONS is represented in a SPATIAL PYRAMID and uses NON-LINEAR LOCALITY CONSTRAINT CODING for learning both shape and texture patterns of the scene . the final response of the LATENT PYRAMIDAL REGIONS form a single feature vector which we call the IMAGE REPRESENTATION and can be used for the SCENE CLASSIFICATION PROBLEM . we tested our proposed SCENE REPRESENTATION MODEL in three datasets which contain a variety of scene categories -lrb- <unk> , UIUC-SPORTS and <unk> -rrb- . our IMAGE REPRESENTATION obtains state-of-the-art results on all these datasets which shows that IMAGE REPRESENTATION can simultaneously model the GLOBAL AND LOCAL SCENE CHARACTERISTICS in a single framework and is general enough to be used for both INDOOR AND OUTDOOR SCENE CLASSIFICATION . \n",
            "in this paper , we propose a novel approach to INDOOR AND OUTDOOR SCENE CLASSIFICATION for INDOOR AND OUTDOOR SCENE CLASSIFICATION . the proposed SCENE REPRESENTATION MODEL is based on NON-LINEAR LOCALITY CONSTRAINT CODING and LATENT SUPPORT VECTOR MACHINE . the proposed SCENE REPRESENTATION MODEL is based on the LATENT SUPPORT VECTOR MACHINE and the LATENT SUPPORT VECTOR MACHINE . the proposed SCENE REPRESENTATION MODEL is based on a LATENT SUPPORT VECTOR MACHINE and a LATENT SUPPORT VECTOR MACHINE . the proposed SCENE REPRESENTATION MODEL is applied to the SCENE CLASSIFICATION PROBLEM of the IMAGE REPRESENTATION and the LATENT SUPPORT VECTOR MACHINE . the experimental results show that the proposed SCENE REPRESENTATION MODEL is effective in improving the INDOOR AND OUTDOOR SCENE CLASSIFICATION performance .\n",
            "\n",
            "96 1000\n",
            "in many MACHINE LEARNING APPLICATIONS , one has access , not only to training data , but also to some high-level a PRIORI KNOWLEDGE about the desired behavior of the system . for example , it is known in advance that the output of a CHARACTER RECOGNIZER should be invariant with respect to small spatial distortions of the input images -lrb- translations , ROTATIONS , SCALE CHANGES , <unk> -rrb- . we have implemented a scheme that allows a network to learn the derivative of its outputs with respect to DISTORTION OPERATORS of our choosing . this not only reduces the LEARNING TIME and the amount of training data , but also provides a powerful language for specifying what generalizations we wish the network to perform . \n",
            "this paper addresses the problem of MACHINE LEARNING APPLICATIONS in the presence of ROTATIONS , SCALE CHANGES and SCALE CHANGES . we propose a method to estimate the SCALE CHANGES of a set of DISTORTION OPERATORS , which are then used to estimate the DISTORTION OPERATORS . the proposed algorithm is based on the use of DISTORTION OPERATORS . the proposed algorithm is tested on a variety of MACHINE LEARNING APPLICATIONS and MACHINE LEARNING APPLICATIONS . the experimental results show that the proposed method outperforms the existing methods in terms of LEARNING TIME and SCALE CHANGES .\n",
            "\n",
            "97 1000\n",
            "blind channel estimation is a promising technique to reduce the PILOT OVERHEAD . unfortunately , most existing algorithms suffer from the SCALAR AMBIGUITY PROBLEM , and hence only achieve SEMI-BLIND IDENTIFICATION . in this paper , we show that with the INFORMATION OF SOURCE CONSTELLATION , the phase of the AMBIGUOUS SCALAR can be divided into a FRACTIONAL PART and an INTEGER PART . then we propose a MULTIPLE-CONSTELLATION SCHEME enabling totally BLIND IDENTIFICATION regardless of CONSTELLATION TYPE for OFDM SYSTEMS . the necessary and sufficient condition for eliminating the SCALAR AMBIGUITY is given . an application example shows that our MULTIPLE-CONSTELLATION SCHEME can help other algorithms circumvent the ANNOYING AMBIGUITY . \n",
            "this paper addresses the problem of BLIND CHANNEL ESTIMATION in OFDM SYSTEMS . in particular , we focus on the problem of BLIND CHANNEL ESTIMATION in the presence of SCALAR AMBIGUITY . we propose a MULTIPLE-CONSTELLATION SCHEME for the INFORMATION OF SOURCE CONSTELLATION and the CONSTELLATION TYPE . the proposed MULTIPLE-CONSTELLATION SCHEME is based on a MULTIPLE-CONSTELLATION SCHEME for the INFORMATION OF SOURCE CONSTELLATION and the CONSTELLATION TYPE . the proposed MULTIPLE-CONSTELLATION SCHEME is applied to the problem of BLIND CHANNEL ESTIMATION in OFDM SYSTEMS . the proposed MULTIPLE-CONSTELLATION SCHEME is applied to the problem of BLIND CHANNEL ESTIMATION in the presence of ANNOYING AMBIGUITY and the CONSTELLATION TYPE .\n",
            "\n",
            "98 1000\n",
            "a general framework for MINIMISATION-BASED BELIEF CHANGE is presented . a problem instance is made up of an UNDIRECTED GRAPH , where a formula is associated with each vertex . for example , VERTICES may represent SPATIAL LOCATIONS , points in time , or some other notion of locality . information is shared between VERTICES via a process of VERTICES over the GRAPH . we give equivalent SEMANTIC AND SYNTACTIC CHARACTERISATIONS of this VERTICES . we also show that this approach is general enough to capture existing MINIMISATION-BASED APPROACHES to BELIEF MERGING , BELIEF REVISION , and -lrb- temporal -rrb- <unk> operators . while we focus on a SET-THEORETIC NOTION OF MINIMISATION , we also consider other approaches , such as CARDINALITY-BASED AND PRIORITY-BASED MINIMISATION . \n",
            "this paper addresses the problem of MINIMISATION-BASED BELIEF CHANGE and BELIEF REVISION in a GRAPH . we propose a method to estimate the SPATIAL LOCATIONS of a GRAPH with SPATIAL LOCATIONS . the proposed method is based on SET-THEORETIC NOTION OF MINIMISATION and BELIEF REVISION . the proposed method is based on the SET-THEORETIC NOTION OF MINIMISATION and BELIEF REVISION . the proposed method is based on the SET-THEORETIC NOTION OF MINIMISATION and BELIEF REVISION . the experimental results show that the proposed method outperforms the existing methods in terms of both CARDINALITY-BASED AND PRIORITY-BASED MINIMISATION and BELIEF REVISION .\n",
            "\n",
            "99 1000\n",
            "we propose a FAST REGRESSION MODEL for practical SINGLE IMAGE SUPER-RESOLUTION based on IN-PLACE EXAMPLES , by leveraging two fundamental SUPER-RESOLUTION APPROACHES -- learning from an EXTERNAL DATABASE and learning from <unk> . our IN-PLACE SELF-SIMILARITY refines the recently proposed LOCAL SELF-SIMILARITY by proving that a PATCH in the UPPER SCALE IMAGE have good matches around its ORIGIN LOCATION in the lower scale image . based on the IN-PLACE EXAMPLES , a FIRST-ORDER APPROXIMATION of the NONLINEAR MAPPING FUNCTION from LOW-TO HIGH-RESOLUTION IMAGE PATCHES is learned . extensive experiments on BENCHMARK AND REAL-WORLD IMAGES demonstrate that our FAST REGRESSION MODEL can produce <unk> results with sharp edges and preserved fine details , while the current state-of-the-art algorithms are prone to VISUAL ARTIFACTS . furthermore , our FAST REGRESSION MODEL can easily extend to deal with NOISE by combining the regression results on multiple IN-PLACE EXAMPLES for ROBUST ESTIMATION . the FAST REGRESSION MODEL runs fast and is particularly useful for practical applications , where the INPUT IMAGES typically contain DIVERSE TEXTURES and they are potentially contaminated by NOISE or compression artifacts . \n",
            "in this paper , we propose a novel FAST REGRESSION MODEL for SINGLE IMAGE SUPER-RESOLUTION from INPUT IMAGES . the proposed FAST REGRESSION MODEL is based on the use of LOW-TO HIGH-RESOLUTION IMAGE PATCHES for ROBUST ESTIMATION . the proposed FAST REGRESSION MODEL is based on the use of LOW-TO HIGH-RESOLUTION IMAGE PATCHES in the form of LOW-TO HIGH-RESOLUTION IMAGE PATCHES . the proposed FAST REGRESSION MODEL is based on the use of LOW-TO HIGH-RESOLUTION IMAGE PATCHES to estimate the ORIGIN LOCATION and the ORIGIN LOCATION . the proposed FAST REGRESSION MODEL is based on the use of LOW-TO HIGH-RESOLUTION IMAGE PATCHES to estimate the ORIGIN LOCATION . the proposed FAST REGRESSION MODEL is evaluated on the BENCHMARK AND REAL-WORLD IMAGES . the experimental results on BENCHMARK AND REAL-WORLD IMAGES show that the proposed FAST REGRESSION MODEL is effective and robust to NOISE .\n",
            "\n",
            "100 1000\n",
            "essence is a new FORMAL LANGUAGE for specifying COMBINATORIAL PROBLEMS in a manner similar to NATURAL RIGOROUS SPECIFICATIONS that use a mixture of NATURAL LANGUAGE and DISCRETE MATHEMATICS . ESSENCE provides a high level of ABSTRACTION , much of which is the consequence of the provision of DECISION VARIABLES whose values can be COMBINA-TORIAL OBJECTS , such as TUPLES , sets , MULTISETS , RELATIONS , PARTITIONS and functions . ESSENCE also allows these COMBINATORIAL PROBLEMS to be nested to ARBITRARY DEPTH , thus providing , for example , sets of PARTITIONS , sets of sets of PARTITIONS , and so forth . therefore , a problem that requires finding a COMPLEX COMBINATORIAL OBJECT can be directly specified by using a DECISION VARIABLE whose type is precisely that COMBINATORIAL OBJECT . \n",
            "this paper presents a novel approach to COMBINATORIAL PROBLEMS in NATURAL LANGUAGE . the approach is based on the use of a DECISION VARIABLE , TUPLES , TUPLES , TUPLES , TUPLES , and TUPLES . the proposed approach is based on the use of a set of PARTITIONS , TUPLES , TUPLES , TUPLES , and TUPLES . the proposed approach is based on the use of a set of PARTITIONS , which are then used to estimate the DECISION VARIABLES . the experimental results show that the proposed method outperforms the state-of-the-art methods .\n",
            "\n",
            "101 1000\n",
            "we introduce an UNSUPERVISED LEARNING ALGORITHM that combines PROBABILISTIC MODELING with SOLVER-BASED TECHNIQUES for PROGRAM SYNTHESIS . we apply our techniques to both a VISUAL LEARNING DOMAIN and a LANGUAGE LEARNING PROBLEM , showing that our UNSUPERVISED LEARNING ALGORITHM can learn many VISUAL CONCEPTS from only a few examples and that UNSUPERVISED LEARNING ALGORITHM can recover some ENGLISH INFLECTIONAL MORPHOLOGY . taken together , these results give both a new approach to UNSUPERVISED LEARNING OF SYMBOLIC COMPOSI-TIONAL STRUCTURES , and a technique for applying PROGRAM SYNTHESIS TOOLS to NOISY DATA . \n",
            "this paper addresses the problem of UNSUPERVISED LEARNING OF SYMBOLIC COMPOSI-TIONAL STRUCTURES from NOISY DATA . we propose a novel UNSUPERVISED LEARNING ALGORITHM for UNSUPERVISED LEARNING OF SYMBOLIC COMPOSI-TIONAL STRUCTURES from NOISY DATA . the proposed UNSUPERVISED LEARNING ALGORITHM is based on the use of PROBABILISTIC MODELING and SOLVER-BASED TECHNIQUES to solve the LANGUAGE LEARNING PROBLEM . the proposed UNSUPERVISED LEARNING ALGORITHM is based on the use of PROBABILISTIC MODELING and SOLVER-BASED TECHNIQUES to solve the LANGUAGE LEARNING PROBLEM . experimental results show that the proposed UNSUPERVISED LEARNING ALGORITHM can significantly improve the performance of the proposed UNSUPERVISED LEARNING ALGORITHM .\n",
            "\n",
            "102 1000\n",
            "we study the problem of estimating the POSITION and orientation of a CALIBRATED CAMERA from an IMAGE OF A KNOWN SCENE . a common problem in CAMERA POSE ESTIMATION is the existence of false correspondences between IMAGE FEATURES and modeled 3d points . existing techniques such as RANSAC to handle OUTLIERS have no guarantee of <unk> . in contrast , we work with a natural extension of the L ∞ NORM to the outlier case . using a simple result from CLASSICAL GEOMETRY , we derive necessary conditions for L ∞ OPTIMALITY and show how to use L ∞ OPTIMALITY in a BRANCH AND BOUND SETTING to find the optimum and to detect OUTLIERS . the algorithm has been evaluated on synthetic as well as REAL DATA showing good empirical performance . in addition , for cases with no OUTLIERS , we demonstrate shorter execution times than existing OPTIMAL ALGORITHMS . \n",
            "in this paper , we propose a novel approach to CAMERA POSE ESTIMATION in the presence of OUTLIERS . the proposed method is based on the idea of using a CALIBRATED CAMERA to estimate the POSITION of the IMAGE FEATURES . the proposed method is based on a BRANCH AND BOUND SETTING , which is based on the L ∞ NORM of the IMAGE FEATURES . the proposed method is based on the use of RANSAC to estimate the POSITION of the IMAGE FEATURES . the proposed method is evaluated on REAL DATA and REAL DATA . the results show that the proposed method is robust to OUTLIERS , and is robust to OUTLIERS .\n",
            "\n",
            "103 1000\n",
            "we study CONFIDENTIALITY ENFORCEMENT in ontologies under the CONTROLLED QUERY EVALUATION FRAMEWORK , where a policy specifies the sensitive information and a <unk> ensures that query answers that may compromise the policy are not returned . we focus on CENSORS that ensure <unk> while <unk> information access , and consider both <unk> and the OWL 2 PROFILES as ONTOLOGY LANGUAGES . \n",
            "this paper presents a novel CONTROLLED QUERY EVALUATION FRAMEWORK for CONFIDENTIALITY ENFORCEMENT . the proposed CONTROLLED QUERY EVALUATION FRAMEWORK is based on a CONTROLLED QUERY EVALUATION FRAMEWORK for CONFIDENTIALITY ENFORCEMENT . the proposed CONTROLLED QUERY EVALUATION FRAMEWORK is based on a CONTROLLED QUERY EVALUATION FRAMEWORK for CONFIDENTIALITY ENFORCEMENT . the performance of the proposed algorithm is demonstrated on a variety of ONTOLOGY LANGUAGES . the experimental results show that the proposed algorithm is able to detect CENSORS in a wide range of ONTOLOGY LANGUAGES .\n",
            "\n",
            "104 1000\n",
            "we present a PRIVACY-PRESERVING SYSTEM for estimating the size of INHOMOGENEOUS CROWDS , composed of pedestrians that travel in different directions , without using EXPLICIT OBJECT SEGMENTATION or TRACKING . first , the CROWD is segmented into components of HOMOGENEOUS MOTION , using the mixture of DYNAMIC TEXTURES MOTION MODEL . second , a set of simple HOLISTIC FEATURES is extracted from each SEGMENTED REGION , and the correspondence between FEATURES and the number of people per segment is learned with GAUSSIAN PROCESS REGRESSION . we validate both the CROWD SEGMENTATION ALGORITHM , and the CROWD COUNTING SYSTEM , on a LARGE PEDESTRIAN DATASET -lrb- 2000 frames of video , containing <unk> total pedestrian instances -rrb- . finally , we present results of the PRIVACY-PRESERVING SYSTEM running on a full hour of video . \n",
            "this paper addresses the problem of EXPLICIT OBJECT SEGMENTATION in a LARGE PEDESTRIAN DATASET . we propose a novel approach to the problem of EXPLICIT OBJECT SEGMENTATION , which is based on the use of a DYNAMIC TEXTURES MOTION MODEL and a PRIVACY-PRESERVING SYSTEM . the proposed approach is based on the use of a DYNAMIC TEXTURES MOTION MODEL and a PRIVACY-PRESERVING SYSTEM . the proposed approach is based on the use of a DYNAMIC TEXTURES MOTION MODEL and a PRIVACY-PRESERVING SYSTEM . the proposed approach is evaluated on a LARGE PEDESTRIAN DATASET and a LARGE PEDESTRIAN DATASET . the results show that the proposed method is effective in improving the TRACKING performance in TRACKING . the proposed method is evaluated on a LARGE PEDESTRIAN DATASET and a LARGE PEDESTRIAN DATASET .\n",
            "\n",
            "105 1000\n",
            "we introduce a TWO-LAYER UNDIRECTED GRAPHICAL MODEL , called a '' replicated <unk> '' , that can be used to TWO-LAYER UNDIRECTED GRAPHICAL MODEL and automatically extract LOW-DIMENSIONAL LATENT SEMANTIC REPRESENTATIONS from a large UNSTRUCTURED COLLECTION OF DOCUMENTS . we present efficient LEARNING AND INFERENCE ALGORITHMS for this TWO-LAYER UNDIRECTED GRAPHICAL MODEL , and show how a TWO-LAYER UNDIRECTED GRAPHICAL MODEL , TWO-LAYER UNDIRECTED GRAPHICAL MODEL , can be used to produce an accurate estimate of the <unk> the TWO-LAYER UNDIRECTED GRAPHICAL MODEL assigns to test data . this allows us to demonstrate that the proposed TWO-LAYER UNDIRECTED GRAPHICAL MODEL is able to generalize much better compared to LATENT DIRICHLET ALLOCATION in terms of both the <unk> of HELD-OUT DOCUMENTS and the RETRIEVAL ACCURACY . \n",
            "this paper presents a novel TWO-LAYER UNDIRECTED GRAPHICAL MODEL , called TWO-LAYER UNDIRECTED GRAPHICAL MODEL , to address the problem of UNSTRUCTURED COLLECTION OF DOCUMENTS . the proposed TWO-LAYER UNDIRECTED GRAPHICAL MODEL is based on a novel TWO-LAYER UNDIRECTED GRAPHICAL MODEL , called TWO-LAYER UNDIRECTED GRAPHICAL MODEL , for UNSTRUCTURED COLLECTION OF DOCUMENTS . the proposed TWO-LAYER UNDIRECTED GRAPHICAL MODEL is based on a novel TWO-LAYER UNDIRECTED GRAPHICAL MODEL , called TWO-LAYER UNDIRECTED GRAPHICAL MODEL , for UNSTRUCTURED COLLECTION OF DOCUMENTS . the proposed TWO-LAYER UNDIRECTED GRAPHICAL MODEL is compared to the state-of-the-art LATENT DIRICHLET ALLOCATION , and the results show that the proposed TWO-LAYER UNDIRECTED GRAPHICAL MODEL significantly improves the performance of the LATENT DIRICHLET ALLOCATION .\n",
            "\n",
            "106 1000\n",
            "several early GAME-PLAYING COMPUTER PROGRAMS used FORWARD PRUNING -lrb- i.e. , the practice of deliberately ignoring nodes that are believed unlikely to affect a GAME TREE 's minimax value -rrb- , but this technique did not seem to result in good decision-making . the poor performance of FORWARD PRUNING presents a major puzzle for ai research on GAME PLAYING , because some version of FORWARD PRUNING seems to be '' what people do , '' and the best CHESS-PLAYING PROGRAMS still do not play as well as the best humans . as a step toward deeper understanding of FORWARD PRUNING , we have set up models of FORWARD PRUNING on two different kinds of GAME TREES , and used these models to investigate how FORWARD PRUNING affects the probability of choosing the correct move . in our studies , FORWARD PRUNING did better than <unk> when there was a high correlation among the MINIMAX VALUES OF SIBLING NODES in a GAME TREE . this result suggests that FORWARD PRUNING may possibly be a useful DECISION-MAKING TECHNIQUE in certain kinds of games . in particular , we believe that bridge may be such a game . \n",
            "in this paper , we propose a novel approach to GAME PLAYING in GAME TREES . the proposed approach is based on the use of FORWARD PRUNING , which is a generalization of the GAME TREE to the GAME TREE . the proposed approach is based on the use of FORWARD PRUNING to estimate the MINIMAX VALUES OF SIBLING NODES of the target signal . the proposed method is based on the use of FORWARD PRUNING to estimate the MINIMAX VALUES OF SIBLING NODES of the source and target speaker . the performance of the proposed method is demonstrated on several benchmark datasets .\n",
            "\n",
            "107 1000\n",
            "we propose MODULE GRAPHICAL LASSO , an aggressive dimensionality reduction and NETWORK ESTIMATION TECHNIQUE for a HIGH-DIMENSIONAL GAUSSIAN GRAPHICAL MODEL . MODULE GRAPHICAL LASSO achieves SCALABILITY , interpretability and RO-BUSTNESS by exploiting the modularity property of many REAL-WORLD NETWORKS . VARIABLES are organized into tightly coupled modules and a GRAPH STRUCTURE is estimated to determine the conditional <unk> among modules . MODULE GRAPHICAL LASSO iteratively learns the MODULE ASSIGNMENT OF VARIABLES , the LATENT VARIABLES , each corresponding to a MODULE , and the parameters of the GGM of the LATENT VARIABLES . in synthetic data experiments , MODULE GRAPHICAL LASSO outperforms the standard GRAPHICAL LASSO and three other methods that incorporate LATENT VARIABLES into GGM . when applied to GENE EXPRESSION DATA from OVARIAN CANCER , MODULE GRAPHICAL LASSO out-performs standard CLUSTERING ALGORITHMS in identifying FUNCTIONALLY COHERENT GENE SETS and PREDICTING SURVIVAL TIME OF PATIENTS . the learned modules and their dependencies provide novel insights into CANCER BIOLOGY as well as identifying possible novel drug targets . \n",
            "in this paper , we propose a novel HIGH-DIMENSIONAL GAUSSIAN GRAPHICAL MODEL to PREDICTING SURVIVAL TIME OF PATIENTS and PREDICTING SURVIVAL TIME OF PATIENTS . the proposed HIGH-DIMENSIONAL GAUSSIAN GRAPHICAL MODEL is based on the use of LATENT VARIABLES in the form of a HIGH-DIMENSIONAL GAUSSIAN GRAPHICAL MODEL . the proposed HIGH-DIMENSIONAL GAUSSIAN GRAPHICAL MODEL is based on the use of LATENT VARIABLES in the form of LATENT VARIABLES . the proposed HIGH-DIMENSIONAL GAUSSIAN GRAPHICAL MODEL is based on the use of LATENT VARIABLES in the form of a HIGH-DIMENSIONAL GAUSSIAN GRAPHICAL MODEL . the proposed HIGH-DIMENSIONAL GAUSSIAN GRAPHICAL MODEL is applied to PREDICTING SURVIVAL TIME OF PATIENTS and PREDICTING SURVIVAL TIME OF PATIENTS . the experimental results show that the proposed HIGH-DIMENSIONAL GAUSSIAN GRAPHICAL MODEL significantly improves the RO-BUSTNESS and RO-BUSTNESS of the MODULE GRAPHICAL LASSO .\n",
            "\n",
            "108 1000\n",
            "the EARTH MOVER 'S DISTANCE -lsb- 19 -rsb- is an important perceptually meaningful metric for comparing HISTOGRAMS , but EARTH MOVER 'S DISTANCE suffers from high -lrb- o -lrb- n 3 log n -rrb- -rrb- COMPUTATIONAL COMPLEXITY . we present a novel LINEAR TIME ALGORITHM for approximating the EARTH MOVER 'S DISTANCE for LOW DIMENSIONAL HIS-TOGRAMS using the sum of absolute values of the WEIGHTED WAVELET COEFFICIENTS of the DIFFERENCE HISTOGRAM . EMD COMPUTATION is a special case of the KANTOROVICH-RUBINSTEIN TRANSSHIPMENT PROBLEM , and we exploit the HÖLDER CONTINUITY CONSTRAINT in its DUAL FORM to convert EARTH MOVER 'S DISTANCE into a simple OPTIMIZATION PROBLEM with an explicit solution in the WAVELET DOMAIN . we prove that the resulting WAVELET EMD METRIC is equivalent to EARTH MOVER 'S DISTANCE , i.e. the ratio of the two is bounded . we also provide estimates for the bounds . the WEIGHTED WAVELET TRANSFORM can be computed in time linear in the number of HISTOGRAM BINS , while the comparison is about as fast as for NORMAL EUCLIDEAN DISTANCE or <unk> 2 statistic . we experimentally show that WAVELET EMD METRIC is a good approximation to EARTH MOVER 'S DISTANCE , has similar performance , but requires much less computation . \n",
            "this paper presents a novel LINEAR TIME ALGORITHM , called EARTH MOVER 'S DISTANCE , for LOW DIMENSIONAL HIS-TOGRAMS . the EARTH MOVER 'S DISTANCE is based on a WEIGHTED WAVELET TRANSFORM , which is a KANTOROVICH-RUBINSTEIN TRANSSHIPMENT PROBLEM with a NORMAL EUCLIDEAN DISTANCE . the proposed EARTH MOVER 'S DISTANCE is based on a LINEAR TIME ALGORITHM , which is a KANTOROVICH-RUBINSTEIN TRANSSHIPMENT PROBLEM with NORMAL EUCLIDEAN DISTANCE . the proposed EARTH MOVER 'S DISTANCE is based on a LINEAR TIME ALGORITHM , which is a generalization of the EARTH MOVER 'S DISTANCE to the WAVELET DOMAIN . the proposed EARTH MOVER 'S DISTANCE is applied to the KANTOROVICH-RUBINSTEIN TRANSSHIPMENT PROBLEM in a KANTOROVICH-RUBINSTEIN TRANSSHIPMENT PROBLEM . the proposed EARTH MOVER 'S DISTANCE is applied to the KANTOROVICH-RUBINSTEIN TRANSSHIPMENT PROBLEM in a KANTOROVICH-RUBINSTEIN TRANSSHIPMENT PROBLEM . the experimental results show that the proposed EARTH MOVER 'S DISTANCE is effective in reducing the COMPUTATIONAL COMPLEXITY of the EARTH MOVER 'S DISTANCE .\n",
            "\n",
            "109 1000\n",
            "we present a novel precoding or modulation scheme -lrb- matrix modulation -rrb- that allows PARALLEL TRANSMISSION of several DATA SIGNALS over an unknown multiple-input multiple-output -lrb- mimo -rrb- channel . we first present a theorem on unique SIGNAL DEMODULATION and an efficient ITERATIVE DEMODULATION ALGORITHM for TRANSMISSION over an UNKNOWN INSTANTANEOUS-MIXTURE CHANNEL . we then generalize our results to an UNKNOWN MIMO CHANNEL with memory . \n",
            "this paper presents a novel ITERATIVE DEMODULATION ALGORITHM for TRANSMISSION . the proposed ITERATIVE DEMODULATION ALGORITHM is based on a ITERATIVE DEMODULATION ALGORITHM , which is based on a ITERATIVE DEMODULATION ALGORITHM . the proposed ITERATIVE DEMODULATION ALGORITHM is based on a ITERATIVE DEMODULATION ALGORITHM that exploits the UNKNOWN MIMO CHANNEL of the DATA SIGNALS . the proposed ITERATIVE DEMODULATION ALGORITHM is applied to the problem of DATA SIGNALS , and the results show that the proposed algorithm is able to recover the UNKNOWN MIMO CHANNEL of the DATA SIGNALS .\n",
            "\n",
            "110 1000\n",
            "most SPEECH ENHANCEMENT ALGORITHMS heavily depend on the NOISE POWER SPECTRAL DENSITY . because this quantity is unknown in practice , estimation from the NOISY DATA is necessary . we present a LOW COMPLEXITY METHOD for NOISE PSD ESTIMATION . the LOW COMPLEXITY METHOD is based on a MINIMUM MEAN-SQUARED ERROR ESTIMA-TOR of the NOISE MAGNITUDE-SQUARED DFT COEFFICIENTS . compared to MINIMUM STATISTICS BASED NOISE TRACKING , SEGMENTAL SNR and PESQ are improved for NON-STATIONARY NOISE SOURCES with 1 db and 0.25 mos points , respectively . compared to recently published algorithms , similar good NOISE TRACKING performance is obtained , but at a COMPUTATIONAL COMPLEXITY that is in the order of a factor 40 lower . \n",
            "this paper presents a novel LOW COMPLEXITY METHOD for NOISE TRACKING . the proposed LOW COMPLEXITY METHOD is based on a MINIMUM MEAN-SQUARED ERROR ESTIMA-TOR . the proposed LOW COMPLEXITY METHOD is based on a MINIMUM MEAN-SQUARED ERROR ESTIMA-TOR , which is based on a MINIMUM MEAN-SQUARED ERROR ESTIMA-TOR . the proposed LOW COMPLEXITY METHOD is based on a MINIMUM MEAN-SQUARED ERROR ESTIMA-TOR , which is based on the MINIMUM MEAN-SQUARED ERROR ESTIMA-TOR . the proposed LOW COMPLEXITY METHOD is applied to NOISY DATA , and the experimental results show that the proposed method is effective in improving the COMPUTATIONAL COMPLEXITY and COMPUTATIONAL COMPLEXITY . the proposed method is robust to NON-STATIONARY NOISE SOURCES , and is robust to NON-STATIONARY NOISE SOURCES . the proposed method is evaluated in terms of COMPUTATIONAL COMPLEXITY and PESQ . the experimental results show that the proposed method is effective in improving the COMPUTATIONAL COMPLEXITY and COMPUTATIONAL COMPLEXITY .\n",
            "\n",
            "111 1000\n",
            "a new SPEECH ENHANCEMENT SYSTEM , which is based on a TIME-FREQUENCY ADAPTIVE WAVELET SOFT THRESHOLDING , is presented in this paper . the SPEECH ENHANCEMENT SYSTEM utilises a BARK-SCALED WAVELET PACKET DECOMPOSITION integrated into a MODIFIED WEINER FILTERING TECHNIQUE using a novel THRESHOLD ESTIMATION METHOD based on a MAGNITUDE DECISION-DIRECTED APPROACH . first , a BARK-SCALED WAVELET PACKET transform is used to decompose the SPEECH SIGNAL into critical bands . THRESHOLD ESTIMATION is then performed for each WAVELET BAND according to an ADAPTIVE NOISE LEVEL-TRACKING ALGORITHM . finally , the speech is estimated by incorporating the COMPUTED THRESHOLD into a WIENER FILTERING PROCESS , using the MAGNITUDE DECISION-DIRECTED APPROACH . the proposed SPEECH ENHANCEMENT TECHNIQUE has been tested with various STATIONARY AND NON-STATIONARY NOISE CASES . reported results show that the SPEECH ENHANCEMENT SYSTEM is capable of a high-level of NOISE SUPPRESSION while preserving the intelligibility and naturalness of the speech . \n",
            "this paper presents a novel SPEECH ENHANCEMENT TECHNIQUE based on TIME-FREQUENCY ADAPTIVE WAVELET SOFT THRESHOLDING . the proposed SPEECH ENHANCEMENT TECHNIQUE is based on a MODIFIED WEINER FILTERING TECHNIQUE for NOISE SUPPRESSION . the proposed ADAPTIVE NOISE LEVEL-TRACKING ALGORITHM is based on a MODIFIED WEINER FILTERING TECHNIQUE based on TIME-FREQUENCY ADAPTIVE WAVELET SOFT THRESHOLDING . the proposed ADAPTIVE NOISE LEVEL-TRACKING ALGORITHM is based on a MODIFIED WEINER FILTERING TECHNIQUE based on a MODIFIED WEINER FILTERING TECHNIQUE . the proposed ADAPTIVE NOISE LEVEL-TRACKING ALGORITHM is based on a MODIFIED WEINER FILTERING TECHNIQUE , which is based on a MODIFIED WEINER FILTERING TECHNIQUE . the proposed ADAPTIVE NOISE LEVEL-TRACKING ALGORITHM is based on a MODIFIED WEINER FILTERING TECHNIQUE , which is based on a MODIFIED WEINER FILTERING TECHNIQUE . the proposed ADAPTIVE NOISE LEVEL-TRACKING ALGORITHM is applied to the STATIONARY AND NON-STATIONARY NOISE CASES in the STATIONARY AND NON-STATIONARY NOISE CASES . experimental results show that the proposed ADAPTIVE NOISE LEVEL-TRACKING ALGORITHM is effective in improving the NOISE SUPPRESSION performance of the SPEECH ENHANCEMENT SYSTEM .\n",
            "\n",
            "112 1000\n",
            "this paper proposes a DECENTRALIZED STATE ESTIMATION SCHEME via NETWORK GOSSIPING with applications in SMART GRID WIDE-AREA MONITORING . the proposed DECENTRALIZED STATE ESTIMATION SCHEME allows DISTRIBUTED CONTROL AREAS to solve for an accurate GLOBAL STATE ESTIMATE collaboratively using the proposed GOSSIP-BASED GAUSS-NEWTON ALGORITHM . furthermore , the proposed DECENTRALIZED STATE ESTIMATION SCHEME mitigates the influence of BAD DATA by adap-tively updating the NOISE VARIANCES and re-weighting the contributions of the most recent measurements for STATE ESTIMATION . compared with other DISTRIBUTED TECHNIQUES , our DECENTRALIZED STATE ESTIMATION SCHEME via GOSSIPING is more flexible and resilient in case of NETWORK RECONFIGURATIONS and failures . we further prove that the POWER FLOW EQUATIONS satisfy the sufficient condition for the GOSSIP-BASED GAUSS-NEWTON ALGORITHM to converge to the desired solution . simulations of the IEEE-118 SYSTEM show that the proposed DECENTRALIZED STATE ESTIMATION SCHEME estimates and tracks the global state robustly , and degrades gracefully when there are RANDOM FAILURES and BAD DATA . \n",
            "this paper presents a novel DECENTRALIZED STATE ESTIMATION SCHEME based on NETWORK GOSSIPING . the proposed DECENTRALIZED STATE ESTIMATION SCHEME is based on a GLOBAL STATE ESTIMATE and is able to deal with BAD DATA and BAD DATA . the proposed DECENTRALIZED STATE ESTIMATION SCHEME is based on the use of a GLOBAL STATE ESTIMATE and a GLOBAL STATE ESTIMATE . the proposed DECENTRALIZED STATE ESTIMATION SCHEME is based on the use of BAD DATA and BAD DATA . the experimental results show that the proposed DECENTRALIZED STATE ESTIMATION SCHEME is effective in improving the STATE ESTIMATION performance in DISTRIBUTED CONTROL AREAS and BAD DATA .\n",
            "\n",
            "113 1000\n",
            "integration is affected by the curse of dimen-sionality and quickly becomes intractable as the dimensionality of the problem grows . we propose a RANDOMIZED ALGORITHM that , with high probability , gives a <unk> approximation of a general discrete integral defined over an exponentially large set . this RANDOMIZED ALGORITHM relies on solving only a small number of instances of a DISCRETE COMBINATORIAL OPTIMIZATION PROBLEM subject to RANDOMLY GENERATED PARITY CONSTRAINTS used as a HASH FUNCTION . as an DISCRETE COMBINATORIAL OPTIMIZATION PROBLEM , we demonstrate that with a small number of MAP QUERIES we can efficiently approximate the PARTITION FUNCTION of DISCRETE GRAPHICAL MODELS , which can in turn be used , for instance , for MARGINAL COMPUTATION or MODEL SELECTION . \n",
            "this paper addresses the problem of MODEL SELECTION and MODEL SELECTION in DISCRETE GRAPHICAL MODELS . in particular , we focus on the problem of MODEL SELECTION and MODEL SELECTION . in particular , we consider the problem of MODEL SELECTION in the presence of MAP QUERIES . we show that the PARTITION FUNCTION can be approximated by a RANDOMIZED ALGORITHM , which can be solved efficiently using RANDOMLY GENERATED PARITY CONSTRAINTS . we show that the proposed algorithm can be applied to a DISCRETE COMBINATORIAL OPTIMIZATION PROBLEM with RANDOMLY GENERATED PARITY CONSTRAINTS .\n",
            "\n",
            "114 1000\n",
            "we present a robust approach to modeling VOICED SPEECH using a family of minimum variance distortionless response -lrb- mvdr -rrb- spectral estimates . the method exploits the fact that for a SXED MODEL ORDER , for a SINUSOIDAL SIGNAL in noise , the MVDR ESTIMATE at the SINUSOIDAL FREQUENCY is approximately related to the SINUSOIDAL AND NOISE POWER in a simple LINEAR MANNER with the COEFSCIENTS being dependent on the MODEL ORDER . MODELING VOICED SPEECH as a sum of harmonic signals , we then use the aforementioned relationship along with a LEAST SQUARES APPROACH to combine a family of MVDR ESTIMATES -LRB- MVDR ESTIMATES of different orders -rrb- and develop a robust approach for modeling VOICED SPEECH . experimental results of SPECTRAL ESTIMATION OF SINUSOIDS , SYNTHETIC VOWELS , and actual speech signals at MVDR ESTIMATES -LRB- MVDR ESTIMATES of 0 db and 5 db using this approach indicate an increased resolution in the ESTIMATED MVDR SPECTRA . the MVDR ESTIMATES -LRB- MVDR ESTIMATES computed from the MVDR ESTIMATE using this approach are also used for speaker <unk> experiments on the TIMIT DATABASE at various MVDR ESTIMATES -LRB- MVDR ESTIMATES . the results indicate a reasonable improvement in RECOGNITION performance when compared to the MVDR ESTIMATES -LRB- MVDR ESTIMATES and the SXED ORDER MVDR-MFCC . \n",
            "in this paper , we propose a novel method for SPECTRAL ESTIMATION OF SINUSOIDS in VOICED SPEECH . the proposed method is based on a LEAST SQUARES APPROACH , which is a MVDR ESTIMATE for MODELING VOICED SPEECH and SPECTRAL ESTIMATION OF SINUSOIDS . the proposed method is based on a LEAST SQUARES APPROACH , which is based on a LEAST SQUARES APPROACH with a MVDR ESTIMATE . the proposed method is based on a LEAST SQUARES APPROACH , which is based on a LEAST SQUARES APPROACH with MVDR ESTIMATES -LRB- MVDR ESTIMATES . the proposed method is based on a LEAST SQUARES APPROACH with MVDR ESTIMATES -LRB- MVDR ESTIMATES and SXED ORDER MVDR-MFCC . experimental results on SYNTHETIC VOWELS show that the proposed method outperforms the state-of-the-art methods in terms of both SINUSOIDAL AND NOISE POWER and SXED ORDER MVDR-MFCC .\n",
            "\n",
            "115 1000\n",
            "clipping or SATURATION in AUDIO SIGNALS is a very common problem in SIGNAL PROCESSING , for which , in the severe case , there is still no satisfactory solution . in such case , there is a tremendous loss of information , and traditional methods fail to appropriately recover the signal . we propose a novel approach for this SIGNAL RESTORATION PROBLEM based on the framework of ITERATIVE HARD THRESHOLDING . this approach , which enforces the consistency of the RECONSTRUCTED SIGNAL with the CLIPPED OBSERVATIONS , shows superior performance in comparison to the state-of-the-art DECLIPPING ALGORITHMS . this is confirmed on synthetic and on actual HIGH-DIMENSIONAL AUDIO DATA PROCESSING , both on SNR and on SUBJECTIVE USER LISTENING EVALUATIONS . \n",
            "this paper presents a novel approach to HIGH-DIMENSIONAL AUDIO DATA PROCESSING and SIGNAL PROCESSING . the proposed approach is based on the use of CLIPPED OBSERVATIONS and SIGNAL PROCESSING . the proposed approach is based on the use of CLIPPED OBSERVATIONS and SIGNAL PROCESSING . the proposed approach is based on the use of CLIPPED OBSERVATIONS and SIGNAL PROCESSING . the proposed approach is evaluated on a variety of SUBJECTIVE USER LISTENING EVALUATIONS and HIGH-DIMENSIONAL AUDIO DATA PROCESSING . the experimental results show that the proposed method outperforms the state-of-the-art methods in terms of SNR and SIGNAL PROCESSING .\n",
            "\n",
            "116 1000\n",
            "consider a MIMO HETEROGENEOUS NETWORK with multiple transmitters -lrb- including <unk> , <unk> and <unk> base stations -rrb- and many receivers -lrb- mobile users -rrb- . the users are to be assigned to the base stations which then optimize their LINEAR TRANSMIT BEAMFORMERS accordingly . in this work , we consider the problem of JOINT BASE STATION ASSIGNMENT and LINEAR BEAM-FORMER DESIGN to maximize a SYSTEM WIDE UTILITY . we first establish the np-hardness of the resulting OPTIMIZATION PROBLEM for a large family of Α-FAIRNESS UTILITY FUNCTIONS . then , we propose an efficient algorithm to approximately solve this problem for the special case of SUM RATE MAXIMIZATION . the simulation results show that the algorithm improves the SUM RATE . \n",
            "this paper addresses the problem of SYSTEM WIDE UTILITY and LINEAR BEAM-FORMER DESIGN . in particular , we consider the problem of SUM RATE MAXIMIZATION and LINEAR BEAM-FORMER DESIGN in a MIMO HETEROGENEOUS NETWORK . in particular , we show that the SUM RATE of the MIMO HETEROGENEOUS NETWORK can be reduced to the SUM RATE of the JOINT BASE STATION ASSIGNMENT . we also show that the SUM RATE of the MIMO HETEROGENEOUS NETWORK can be reduced to the SUM RATE of the MIMO HETEROGENEOUS NETWORK . we also show that the SUM RATE of the algorithm is much faster than the SUM RATE of the MIMO HETEROGENEOUS NETWORK .\n",
            "\n",
            "117 1000\n",
            "blind channel identification using LINEAR REDUNDANT FILTERBANK PRECODERS has been studied extensively in the literature . most methods are proposed based on the assumption that BLOCK SYNCHRONIZATION is perfect . in practice , a BLIND BLOCK SYNCHRONIZATION ALGORITHM must be used to justify this assumption . this paper studies the BLIND BLOCK SYNCHRONIZATION PROBLEM in systems using a ZERO-PADDING PRECODER . a previously reported method is reviewed and a new approach for the BLIND BLOCK SYNCHRONIZATION PROBLEM is proposed . GENERALIZED VERSIONS of both approaches are then developed using a PARAMETER called REPETITION INDEX . simulation results show that when the REPETITION INDEX is chosen to be greater than unity , the BLOCK SYNCHRONIZATION ERROR RATE performance of the proposed BLIND BLOCK SYNCHRONIZATION ALGORITHM has a significant improvement over the previously reported method . ' \n",
            "in this paper , we propose a novel BLIND BLOCK SYNCHRONIZATION ALGORITHM for BLIND CHANNEL IDENTIFICATION . the proposed BLIND BLOCK SYNCHRONIZATION ALGORITHM is based on a LINEAR REDUNDANT FILTERBANK PRECODERS . the proposed BLIND BLOCK SYNCHRONIZATION ALGORITHM is based on a LINEAR REDUNDANT FILTERBANK PRECODERS , which is based on a LINEAR REDUNDANT FILTERBANK PRECODERS . the proposed BLIND BLOCK SYNCHRONIZATION ALGORITHM is based on a LINEAR REDUNDANT FILTERBANK PRECODERS . the proposed BLIND BLOCK SYNCHRONIZATION ALGORITHM is applied to the problem of BLIND CHANNEL IDENTIFICATION . the experimental results show that the proposed BLIND BLOCK SYNCHRONIZATION ALGORITHM is effective in improving the BLOCK SYNCHRONIZATION ERROR RATE of the proposed BLIND BLOCK SYNCHRONIZATION ALGORITHM . moreover , the proposed BLIND BLOCK SYNCHRONIZATION ALGORITHM can also be applied to the problem of BLIND CHANNEL IDENTIFICATION .\n",
            "\n",
            "118 1000\n",
            "<unk> dependency -lrb- pd - -rrb- grammars are proposed as a means of efficient treatment of DISCONTINUOUS CONSTRUCTIONS . <unk> describe two kinds of DEPENDENCIES : LOCAL , explicitly derived by the RULES , and long , implicitly specified by NEGATIVE AND POSITIVE VA-LENCIES OF WORDS . if in a <unk> the number of NON-SATURATED VALENCIES in DERIVED STRUCTURES is bounded by a constant , then it is weakly equivalent to a CF-GRAMMAR and has a cents ¡ $ # ¦ ¥ ¨ § - time parsing algorithm . it happens that such BOUNDED PD-GRAMMARS are strong enough to express such phenomena as UNBOUNDED RAISING , EXTRACTION and EX-TRAPOSITION . \n",
            "this paper presents a novel approach to the problem of POLARIZED DEPENDENCY -LRB- PD - -RRB- GRAMMARS . the method is based on the use of a set of RULES , a set of RULES , and a BOUNDED PD-GRAMMARS . the method is based on the use of a set of RULES , each of which is a set of RULES , and a BOUNDED PD-GRAMMARS is used to estimate the DEPENDENCIES . the proposed method is evaluated on a variety of DISCONTINUOUS CONSTRUCTIONS , including EX-TRAPOSITION , EXTRACTION , EXTRACTION and CF-GRAMMAR . the results show that the proposed method is effective in reducing the number of RULES , and to the best of our method .\n",
            "\n",
            "119 1000\n",
            "we use well-established results in BIOLOGICAL VISION to construct a novel VISION MODEL for HANDWRITTEN DIGIT RECOGNITION . we show empirically that the FEATURES extracted by our VISION MODEL are linearly separable over a large training set -lrb- mnist -rrb- . using only a LINEAR CLASSIFIER on these FEATURES , our VISION MODEL is relatively simple yet outperforms other models on the same data set . \n",
            "this paper presents a novel VISION MODEL for BIOLOGICAL VISION . the proposed VISION MODEL is based on a LINEAR CLASSIFIER for HANDWRITTEN DIGIT RECOGNITION . the proposed VISION MODEL is based on a LINEAR CLASSIFIER , which is based on a LINEAR CLASSIFIER . the proposed VISION MODEL is applied to the task of HANDWRITTEN DIGIT RECOGNITION for HANDWRITTEN DIGIT RECOGNITION . the experimental results show that the proposed VISION MODEL is effective in improving the HANDWRITTEN DIGIT RECOGNITION performance .\n",
            "\n",
            "120 1000\n",
            "developing individualized head related transfer functions -lrb- HRTF -rrb- is an essential requirement for accurate VIRTUALIZATION OF SOUND . however it is time consuming and complicated for both the subject and the developer . obtaining the SPECTRAL NOTCHES which are the most prominent features of HRTF is very important to reconstruct the head related impulse response -lrb- HRTF -rrb- accurately . in this paper , a method suitable for fast computation of the frequencies of SPECTRAL NOTCHES is proposed . the LINEAR PREDICTION RESIDUAL CEPSTRUM is used to compute the SPECTRAL NOTCHES with a high degree of ACCURACY in this work . subsequent use of BATTEAUS REFLECTION MODEL to <unk> the SPECTRAL NOTCHES on the PINNA IMAGES indicate that the proposed method is able to provide finer contours . experiments on reconstruction of the HRTF indicates that the method performs better than other methods . \n",
            "this paper presents a novel approach to the VIRTUALIZATION OF SOUND . the proposed method is based on the BATTEAUS REFLECTION MODEL , which is based on a LINEAR PREDICTION RESIDUAL CEPSTRUM . the proposed method is based on the BATTEAUS REFLECTION MODEL , which is based on the BATTEAUS REFLECTION MODEL . the proposed method is based on a LINEAR PREDICTION RESIDUAL CEPSTRUM , which is based on the LINEAR PREDICTION RESIDUAL CEPSTRUM . the proposed method is evaluated on a variety of PINNA IMAGES . the experimental results show that the proposed method is effective in reducing the ACCURACY and the ACCURACY of the proposed method .\n",
            "\n",
            "121 1000\n",
            "description logics , and in particular the WEB ONTOLOGY LANGUAGE OWL has been proposed as an appropriate basis for computing matches between STRUCTURED OBJECTS for the sake of INFORMATION INTEGRATION and SERVICE DISCOVERY . a drawback of the direct use of SUBSUMPTION as a MATCHING CRITERION is the inability to compute PARTIAL MATCHES and <unk> the degree of MISMATCH . in this paper , we describe a method for overcoming these problems that is based on APPROXIMATE LOGICAL REASONING . in particular , we approximate the SUBSUMPTION RELATION by defining the notion of SUBSUMPTION with respect to a certain subset of the concept and relation names . we present the formal semantics of this relation , describe a SOUND AND COMPLETE ALGORITHM for COMPUTING APPROXIMATE SUBSUMPTION and discuss its application to MATCHING TASKS . \n",
            "this paper addresses the problem of COMPUTING APPROXIMATE SUBSUMPTION and SERVICE DISCOVERY . we propose a novel approach to the problem of COMPUTING APPROXIMATE SUBSUMPTION and SERVICE DISCOVERY . the proposed approach is based on a novel SOUND AND COMPLETE ALGORITHM , which is based on a SOUND AND COMPLETE ALGORITHM . the proposed approach is based on a SOUND AND COMPLETE ALGORITHM , which is able to deal with PARTIAL MATCHES . the proposed method is based on a SOUND AND COMPLETE ALGORITHM , which is able to deal with PARTIAL MATCHES . the proposed approach is evaluated on a variety of MATCHING TASKS and SERVICE DISCOVERY . the experimental results show that the proposed approach is able to detect and track objects in complex scenes with PARTIAL MATCHES , and COMPUTING APPROXIMATE SUBSUMPTION and SERVICE DISCOVERY .\n",
            "\n",
            "122 1000\n",
            "in this work <unk> 's law is followed for designing a PERCEPTUALLY-SHAPED SIDE-INFORMED DATA HIDING SCHEME . the resulting PERCEPTUALLY-SHAPED SIDE-INFORMED DATA HIDING SCHEME is a GENERALIZED VERSION of a LOGARITHMIC QUANTIZATION ALGORITHM previously proposed by the authors . closed formulas for analyzing the EMBEDDING POWER and decoding error probability of this new PERCEPTUALLY-SHAPED SIDE-INFORMED DATA HIDING SCHEME are provided , and experimental results showing its good behavior against severe attacks are reported . \n",
            "in this paper , we propose a novel PERCEPTUALLY-SHAPED SIDE-INFORMED DATA HIDING SCHEME , called PERCEPTUALLY-SHAPED SIDE-INFORMED DATA HIDING SCHEME , which is an extension of the LOGARITHMIC QUANTIZATION ALGORITHM . the proposed PERCEPTUALLY-SHAPED SIDE-INFORMED DATA HIDING SCHEME is a LOGARITHMIC QUANTIZATION ALGORITHM , which is based on a LOGARITHMIC QUANTIZATION ALGORITHM , called the PERCEPTUALLY-SHAPED SIDE-INFORMED DATA HIDING SCHEME . the proposed PERCEPTUALLY-SHAPED SIDE-INFORMED DATA HIDING SCHEME , called the PERCEPTUALLY-SHAPED SIDE-INFORMED DATA HIDING SCHEME , is based on a new PERCEPTUALLY-SHAPED SIDE-INFORMED DATA HIDING SCHEME called the PERCEPTUALLY-SHAPED SIDE-INFORMED DATA HIDING SCHEME . the proposed PERCEPTUALLY-SHAPED SIDE-INFORMED DATA HIDING SCHEME , called the PERCEPTUALLY-SHAPED SIDE-INFORMED DATA HIDING SCHEME , is an extension of the well-known LOGARITHMIC QUANTIZATION ALGORITHM , which is a LOGARITHMIC QUANTIZATION ALGORITHM . experimental results show that the proposed PERCEPTUALLY-SHAPED SIDE-INFORMED DATA HIDING SCHEME is able to significantly improve the performance of the proposed PERCEPTUALLY-SHAPED SIDE-INFORMED DATA HIDING SCHEME .\n",
            "\n",
            "123 1000\n",
            "many REAL WORLD DATA SETS can be viewed as points in a HIGHER-DIMENSIONAL SPACE that lie concentrated around a LOWER-DIMENSIONAL MANIFOLD STRUCTURE . we propose a new MULTISCALE REPRESENTATION for such POINT CLOUDS based on LIFTING and perfect matching . the result is an adaptive wavelet transform that decomposes a POINT CLOUD into MANIFOLD APPROXIMATIONS and details at multiple scales . we illustrate with several examples that the transform can extract an UNKNOWN SMOOTH MANIFOLD from NOISY POINT CLOUD SAMPLES using simple WAVELET THRESHOLDING IDEAS . \n",
            "in this paper , we propose a novel MULTISCALE REPRESENTATION for POINT CLOUDS in POINT CLOUDS . the proposed MULTISCALE REPRESENTATION is based on the use of LIFTING to estimate the LOWER-DIMENSIONAL MANIFOLD STRUCTURE . the proposed MULTISCALE REPRESENTATION is based on the use of LIFTING to estimate the LOWER-DIMENSIONAL MANIFOLD STRUCTURE . the proposed MULTISCALE REPRESENTATION is based on the use of LIFTING to estimate the LOWER-DIMENSIONAL MANIFOLD STRUCTURE of the LIFTING . the proposed MULTISCALE REPRESENTATION is applied to the problem of POINT CLOUDS in POINT CLOUDS . the proposed method is evaluated on two REAL WORLD DATA SETS with NOISY POINT CLOUD SAMPLES .\n",
            "\n",
            "124 1000\n",
            "in this work we propose an ERROR-RESILIENT SCHEME that allows enhancing the ROBUSTNESS of a VIDEO STREAM . based on DISTRIBUTED SOURCE CODING PRINCIPLES , an AUXILIARY STREAM is sent in parallel to the MAIN STREAM as a redundant representation of the sequence that is used to correct errors at the DECODER , thus reducing the impact of drift . in order to perform an optimal bit allocation in the AUXILIARY STREAM , the ENCODER needs to compute a reliable estimate of the expected video distortion observed at the DECODER SIDE due to CHANNEL LOSS . this paper proposes an algorithm to calculate the expected distortion of decoded <unk> -lrb- dubbed <unk> -rrb- and its application to the BIT ALLOCATION PROBLEM in a DSC BASED AUXILIARY STREAM . \n",
            "this paper addresses the problem of recovering a VIDEO STREAM from a VIDEO STREAM . we propose a method to estimate the CHANNEL LOSS of a VIDEO STREAM using a DSC BASED AUXILIARY STREAM . the proposed method is based on a DSC BASED AUXILIARY STREAM , which is based on a DSC BASED AUXILIARY STREAM . the proposed method is based on a DSC BASED AUXILIARY STREAM . the proposed method is based on a DSC BASED AUXILIARY STREAM . the proposed method is based on a DSC BASED AUXILIARY STREAM , which is based on a DSC BASED AUXILIARY STREAM . the experimental results show that the proposed method is effective in ROBUSTNESS and ROBUSTNESS .\n",
            "\n",
            "125 1000\n",
            "this paper reports on experimental results obtained from a performance comparison of FEATURE COMBINATIONS STRATEGIES in CONTENT BASED IMAGE RETRIEVAL . the use of SUPPORT VECTOR MACHINES is compared to COMBMIN , <unk> , <unk> and BORDAFUSE COMBINATION STRATEGIES , all of which are evaluated on a carefully compiled set of COREL IMAGES and the trecvid 2003 search task collection . \n",
            "in this paper , we propose a novel approach to CONTENT BASED IMAGE RETRIEVAL in the context of CONTENT BASED IMAGE RETRIEVAL . the proposed approach is based on the use of FEATURE COMBINATIONS STRATEGIES for CONTENT BASED IMAGE RETRIEVAL . the proposed approach is based on the use of FEATURE COMBINATIONS STRATEGIES for CONTENT BASED IMAGE RETRIEVAL . the experimental results show that the proposed method is effective in improving the performance of CONTENT BASED IMAGE RETRIEVAL .\n",
            "\n",
            "126 1000\n",
            "sound zones are two or more regions within a LISTENING SPACE where listeners are provided with PERSONAL AUDIO . ACOUSTIC CONTRAST CONTROL is a SOUND ZONING METHOD that maximizes the AVERAGE SQUARED SOUND PRESSURE in one zone constrained to constant pressure in other zones . state-of-the-art time domain broadband acoustic contrast control -lrb- <unk> -rrb- SOUND ZONING METHOD are designed for ANECHOIC ENVIRONMENTS . these SOUND ZONING METHOD are not able to realize a FLAT FREQUENCY RESPONSE in a limited frequency range within a REVERBERANT ENVIRONMENT . SOUND FIELD CONTROL in a limited frequency range is a requirement to accommodate the effective working range of the loudspeakers . in this paper , a new BACC METHOD is proposed which results in an implementation realizing a FLAT FREQUENCY RESPONSE in the target zone . this BACC METHOD is applied in a BANDLIMITED LOW-FREQUENCY SCENARIO where the LOUDSPEAKER LAYOUT <unk> two CONTROLLED ZONES . the performance is verified with experimental results in an ACOUSTI-CALLY DAMPED ROOM . \n",
            "this paper describes a novel SOUND ZONING METHOD , called SOUND FIELD CONTROL , for SOUND FIELD CONTROL . the proposed SOUND ZONING METHOD is a SOUND ZONING METHOD , a SOUND ZONING METHOD , and a BANDLIMITED LOW-FREQUENCY SCENARIO for the LOUDSPEAKER LAYOUT . the proposed SOUND ZONING METHOD is based on a BACC METHOD , called ACOUSTIC CONTRAST CONTROL , to estimate the LOUDSPEAKER LAYOUT of a scene from a BANDLIMITED LOW-FREQUENCY SCENARIO . the proposed SOUND ZONING METHOD is based on a BACC METHOD , called ACOUSTIC CONTRAST CONTROL , to estimate the LOUDSPEAKER LAYOUT of the LOUDSPEAKER LAYOUT . the proposed SOUND ZONING METHOD is a BANDLIMITED LOW-FREQUENCY SCENARIO , which is based on a BACC METHOD . the proposed SOUND ZONING METHOD is applied to the problem of PERSONAL AUDIO in a BANDLIMITED LOW-FREQUENCY SCENARIO , and the results show that the proposed algorithm is able to accurately recover the LOUDSPEAKER LAYOUT of the SOUND ZONES .\n",
            "\n",
            "127 1000\n",
            "many HARD COMPUTATIONAL SOCIAL CHOICE PROBLEMS are known to become tractable when voters ' preferences belong to a RESTRICTED DOMAIN , such as those of SINGLE-PEAKED OR SINGLE-CROSSING PREFERENCES . however , to date , all algorithmic results of this type have been obtained for the setting where each voter 's preference list is a total order of candidates . the goal of this paper is to extend this line of research to the setting where voters ' preferences are <unk> , i.e. , each voter <unk> a subset of candidates and <unk> the remaining candidates . we propose several analogues of the notions of SINGLE-PEAKED AND SINGLE-CROSSING PREFERENCES for DICHOTOMOUS PROFILES and investigate the relationships among them . we then demonstrate that for some of these notions the respective RESTRICTED DOMAINS admit efficient algorithms for COMPUTATIONALLY HARD APPROVAL-BASED MULTI-WINNER RULES . \n",
            "this paper addresses the problem of HARD COMPUTATIONAL SOCIAL CHOICE PROBLEMS from RESTRICTED DOMAINS . we propose a novel approach to the problem of HARD COMPUTATIONAL SOCIAL CHOICE PROBLEMS from RESTRICTED DOMAINS . the proposed approach is based on the use of DICHOTOMOUS PROFILES , which is a generalization of the existing COMPUTATIONALLY HARD APPROVAL-BASED MULTI-WINNER RULES . the proposed approach is based on the use of RESTRICTED DOMAINS , which are more robust to SINGLE-PEAKED OR SINGLE-CROSSING PREFERENCES . experimental results demonstrate the effectiveness of the proposed approach .\n",
            "\n",
            "128 1000\n",
            "we present a novel algorithm for SIMULTANEOUS COLOR AND DEPTH INPAINTING . the algorithm takes STEREO IMAGES and estimated disparity maps as input and fills in MISSING COLOR and depth information introduced by OCCLUSIONS or OBJECT REMOVAL . we first complete the disparities for the OCCLUSION REGIONS using a SEGMENTATION-BASED APPROACH . the completed disparities can be used to facilitate the user in labeling objects to be removed . since part of the removed regions in one image is visible in the other , we mutually complete the two images through 3D WARPING . finally , we complete the remaining unknown regions using a DEPTH-ASSISTED TEXTURE SYNTHESIS TECHNIQUE , which simultaneously fills in both color and depth . we demonstrate the effectiveness of the proposed algorithm on several challenging DATA SETS . \n",
            "this paper addresses the problem of OBJECT REMOVAL in STEREO IMAGES . we propose a novel SEGMENTATION-BASED APPROACH for SIMULTANEOUS COLOR AND DEPTH INPAINTING and OBJECT REMOVAL . the proposed SEGMENTATION-BASED APPROACH is based on a SEGMENTATION-BASED APPROACH for SIMULTANEOUS COLOR AND DEPTH INPAINTING and OBJECT REMOVAL . the SEGMENTATION-BASED APPROACH is based on a SEGMENTATION-BASED APPROACH , which is able to deal with OCCLUSIONS and OCCLUSIONS . we demonstrate the effectiveness of the proposed method in the context of SIMULTANEOUS COLOR AND DEPTH INPAINTING and OBJECT REMOVAL .\n",
            "\n",
            "129 1000\n",
            "we present an approach to RECONSTRUCTION OF DETAILED SCENE GEOMETRY from range video . RANGE DATA produced by COMMODITY HANDHELD CAMERAS suffers from HIGH-FREQUENCY ERRORS and LOW-FREQUENCY DISTORTION . our approach deals with both sources of error by reconstructing LOCALLY SMOOTH SCENE FRAGMENTS and letting these fragments deform in order to align to each other . we develop a volumetric registration formulation that leverages the smoothness of the deformation to make OPTIMIZATION practical for large scenes . experimental results demonstrate that our approach substantially increases the fidelity of COMPLEX SCENE GEOMETRY reconstructed with COMMODITY HANDHELD CAMERAS . \n",
            "this paper addresses the problem of RECONSTRUCTION OF DETAILED SCENE GEOMETRY in COMMODITY HANDHELD CAMERAS with COMPLEX SCENE GEOMETRY . we propose a method to estimate the RECONSTRUCTION OF DETAILED SCENE GEOMETRY from RANGE DATA with HIGH-FREQUENCY ERRORS and LOW-FREQUENCY DISTORTION . the proposed algorithm is based on the use of COMMODITY HANDHELD CAMERAS with HIGH-FREQUENCY ERRORS and LOW-FREQUENCY DISTORTION . the proposed approach is evaluated on a variety of COMMODITY HANDHELD CAMERAS with HIGH-FREQUENCY ERRORS and LOW-FREQUENCY DISTORTION .\n",
            "\n",
            "130 1000\n",
            "we present a novel VARIATIONAL APPROACH to TOP-DOWN IMAGE SEGMENTATION , which accounts for significant projective transformations between a single prior image and the image to be segmented . the proposed VARIATIONAL APPROACH is coupled with reliable estimation of the TRANSFORMATION PARAMETERS , without using POINT CORRESPONDENCES . the prior shape is represented by a GENERALIZED CONE that is based on the CONTOUR of the REFERENCE OBJECT . its UNLEVEL SECTIONS correspond to possible instances of the VISIBLE CONTOUR under PERSPECTIVE DISTORTION and SCALING . we extend the CHAN-VESE ENERGY FUNCTIONAL by adding a SHAPE TERM . this term measures the distance between the currently estimated section of the GENERALIZED CONE and the region bounded by the ZERO-CROSSING OF THE EVOLVING LEVEL SET FUNCTION . promising segmentation results are obtained for images of rotated , translated , corrupted and partly occluded objects . the RECOVERED TRANSFORMATION PARAMETERS are compatible with the GROUND TRUTH . \n",
            "in this paper , we propose a novel VARIATIONAL APPROACH for TOP-DOWN IMAGE SEGMENTATION . the proposed VARIATIONAL APPROACH is based on a VARIATIONAL APPROACH to the VISIBLE CONTOUR of the CONTOUR . the proposed VARIATIONAL APPROACH is based on a VARIATIONAL APPROACH of the VISIBLE CONTOUR of the CONTOUR . the proposed VARIATIONAL APPROACH is based on the ZERO-CROSSING OF THE EVOLVING LEVEL SET FUNCTION of the VISIBLE CONTOUR and the TRANSFORMATION PARAMETERS of the CONTOUR . the proposed VARIATIONAL APPROACH is applied to the VISIBLE CONTOUR of the VISIBLE CONTOUR , and the RECOVERED TRANSFORMATION PARAMETERS of the CONTOUR is estimated . the performance of the proposed VARIATIONAL APPROACH is demonstrated on a variety of UNLEVEL SECTIONS and GROUND TRUTH .\n",
            "\n",
            "131 1000\n",
            "the task of REVIEW RATING PREDICTION can be well addressed by using REGRESSION ALGORITHMS if there is a reliable training set of reviews with HUMAN RATINGS . in this paper , we aim to investigate a more challenging task of CROSS-LANGUAGE REVIEW RATING PREDICTION , which makes use of only rated reviews in a source language -lrb- e.g. english -rrb- to predict the rating scores of <unk> reviews in a target language -lrb- e.g. GERMAN -rrb- . we propose a new CO-REGRESSION ALGORITHM to address this task by leveraging UNLABELED REVIEWS . evaluation results on several datasets show that our proposed CO-REGRESSION ALGORITHM can consistently improve the prediction results . \n",
            "this paper addresses the problem of CROSS-LANGUAGE REVIEW RATING PREDICTION in the presence of HUMAN RATINGS . we propose a novel CO-REGRESSION ALGORITHM based on UNLABELED REVIEWS . the proposed CO-REGRESSION ALGORITHM is based on the use of UNLABELED REVIEWS in the form of UNLABELED REVIEWS . the proposed CO-REGRESSION ALGORITHM is applied to the problem of CROSS-LANGUAGE REVIEW RATING PREDICTION . the experimental results show that the proposed CO-REGRESSION ALGORITHM is effective in CROSS-LANGUAGE REVIEW RATING PREDICTION . the performance of the proposed CO-REGRESSION ALGORITHM is demonstrated on a variety of UNLABELED REVIEWS .\n",
            "\n",
            "132 1000\n",
            "in this paper we present a PROBABILISTIC ALGORITHM that extracts a MAPPING between two subspaces by representing each SUBSPACE as a collection of states . an arbitrary increase in number of states results in over-fitting the training data without exploring the underlying structure of the map . this paper suggests a method to impose SPARSITY CONSTRAINTS on the STATE MAP by using ENTROPIC PRIORS . this PROBABILISTIC ALGORITHM is applied to the problem of ARTIFICIAL BANDWIDTH EXPANSION that involves estimating the MISSING FREQUENCY COMPONENTS -lrb- 3.7 -- 8 khz and 0 -- 0.3 khz -rrb- of speech given the NAR-ROWBAND SPEECH SIGNAL -lrb- 0.3 -- 3.7 khz -rrb- . \n",
            "in this paper , we propose a novel PROBABILISTIC ALGORITHM for ARTIFICIAL BANDWIDTH EXPANSION . the proposed PROBABILISTIC ALGORITHM is based on a PROBABILISTIC ALGORITHM for ARTIFICIAL BANDWIDTH EXPANSION . the proposed PROBABILISTIC ALGORITHM is based on a PROBABILISTIC ALGORITHM for ARTIFICIAL BANDWIDTH EXPANSION . the proposed PROBABILISTIC ALGORITHM is based on a PROBABILISTIC ALGORITHM for ARTIFICIAL BANDWIDTH EXPANSION . the proposed PROBABILISTIC ALGORITHM is applied to the problem of ARTIFICIAL BANDWIDTH EXPANSION . the experimental results show that the proposed PROBABILISTIC ALGORITHM is effective in ARTIFICIAL BANDWIDTH EXPANSION .\n",
            "\n",
            "133 1000\n",
            "we optimize over the set of CORRECTED LAPLACIANS associated with a WEIGHTED GRAPH to improve the AVERAGE CASE NORMALIZED CUT of a GRAPH PARTITIONING . unlike EDGE-RELAXATION SDPS , optimizing over the set cl naturally exploits the MATRIX SPARSITY by operating solely on the diagonal . this structure is critical to IMAGE SEGMENTATION APPLICATIONS because the number of vertices is generally proportional to the number of pixels in the image . CL OPTIMIZATION provides a guiding principle for improving the COMBINATORIAL SOLUTION over the SPECTRAL RELAXATION , which is important because small improvements in the CUT COST often result in significant improvements in the PERCEPTUAL RELEVANCE of the segmenta-tion . we develop an OPTIMIZATION PROCEDURE to accommodate PRIOR INFORMATION in the form of STATISTICAL SHAPE MODELS , resulting in a SEGMENTATION METHOD that produces FOREGROUND REGIONS which are consistent with a PARAMETERIZED FAMILY OF SHAPES . we validate our OPTIMIZATION PROCEDURE with GROUND TRUTH on MRI MEDICAL IMAGES , providing a QUANTITATIVE COMPARISON against results produced by current SPECTRAL RELAXATION APPROACHES to GRAPH PARTITIONING . \n",
            "this paper presents a novel SEGMENTATION METHOD for STATISTICAL SHAPE MODELS . the proposed SEGMENTATION METHOD is based on a QUANTITATIVE COMPARISON , which is a WEIGHTED GRAPH of a WEIGHTED GRAPH . the proposed SEGMENTATION METHOD is based on the use of a WEIGHTED GRAPH for GRAPH PARTITIONING . the proposed SEGMENTATION METHOD is based on the use of a WEIGHTED GRAPH to estimate the CORRECTED LAPLACIANS of the FOREGROUND REGIONS . the proposed SEGMENTATION METHOD is applied to the PARAMETERIZED FAMILY OF SHAPES of the WEIGHTED GRAPH , and the SEGMENTATION METHOD is used to estimate the FOREGROUND REGIONS . the proposed SEGMENTATION METHOD is evaluated on the AVERAGE CASE NORMALIZED CUT and compared to the AVERAGE CASE NORMALIZED CUT of the GROUND TRUTH . the proposed SEGMENTATION METHOD is compared with the AVERAGE CASE NORMALIZED CUT and the GROUND TRUTH . the proposed SEGMENTATION METHOD is compared to the AVERAGE CASE NORMALIZED CUT and the GROUND TRUTH .\n",
            "\n",
            "134 1000\n",
            "real world systems often have PARAMETERIZED CONTROLLERS which can be tuned to improve performance . BAYESIAN OPTIMIZATION METHODS provide for efficient optimization of these REAL WORLD SYSTEMS , so as to reduce the number of required experiments on the expensive PHYSICAL SYSTEM . in this paper we address BAYESIAN OPTIMIZATION in the setting where performance is only observed through a STOCHASTIC BINARY OUTCOME -- success or failure of the experiment . unlike BANDIT PROBLEMS , the goal is to maximize the REAL WORLD SYSTEMS performance after this OFFLINE TRAINING PHASE rather than minimize regret during training . in this work we define the STOCHASTIC BINARY OPTIMIZATION PROBLEM and propose an approach using an adaptation of GAUSSIAN PROCESSES for STOCHASTIC BINARY OPTIMIZATION PROBLEM that presents a BAYESIAN OPTIMIZATION FRAMEWORK for this STOCHASTIC BINARY OPTIMIZATION PROBLEM . we propose an EXPERIMENT SELECTION METRIC for this setting based on expected improvement . we demonstrate the algorithm 's performance on SYNTHETIC PROBLEMS and on a real snake robot learning to move over an obstacle . \n",
            "this paper addresses the problem of BANDIT PROBLEMS in REAL WORLD SYSTEMS . in particular , we consider the problem of BAYESIAN OPTIMIZATION in a BAYESIAN OPTIMIZATION FRAMEWORK . we propose a novel BAYESIAN OPTIMIZATION FRAMEWORK to the problem of BAYESIAN OPTIMIZATION . the proposed BAYESIAN OPTIMIZATION FRAMEWORK is based on the use of GAUSSIAN PROCESSES to estimate the STOCHASTIC BINARY OUTCOME . the proposed BAYESIAN OPTIMIZATION FRAMEWORK is based on the use of GAUSSIAN PROCESSES for BAYESIAN OPTIMIZATION . the proposed BAYESIAN OPTIMIZATION FRAMEWORK is applied to the problem of BANDIT PROBLEMS in REAL WORLD SYSTEMS . the experimental results demonstrate the effectiveness of the proposed BAYESIAN OPTIMIZATION FRAMEWORK .\n",
            "\n",
            "135 1000\n",
            "the MAP -lrb- maximum a POSTERIORI HYPOTHESIS -RRB- PROBLEM in BAYESIAN NETWORKS is to find the most likely states of a set of variables given partial evidence on the complement of that set . standard STRUCTURE-BASED INFERENCE METHODS for finding exact solutions to MAP , such as VARIABLE ELIMINATION and JOIN-TREE ALGORITHMS , have complexities that are exponential in the CONSTRAINED TREEWIDTH OF THE NETWORK . a more recent algorithm , proposed by <unk> and <unk> , is exponential only in the TREEWIDTH and has been shown to handle networks whose CONSTRAINED TREEWIDTH is quite high . in this paper we present a new algorithm for exact MAP that is not necessarily limited in scalability even by the TREEWIDTH . this is achieved by leverag-ing recent advances in COMPILATION OF BAYESIAN NETWORKS into ARITHMETIC CIRCUITS , which can circumvent TREEWIDTH-IMPOSED LIMITS by exploiting the LOCAL STRUCTURE present in the BAYESIAN NETWORKS . specifically , we implement a BRANCH-AND-BOUND SEARCH where the BOUNDS are computed using LINEAR-TIME OPERATIONS on the COMPILED ARITHMETIC CIRCUIT . on networks with LOCAL STRUCTURE , we observe orders-of-magnitude improvements over the algorithm of <unk> and <unk> . in particular , we are able to efficiently solve many problems where the latter algorithm runs out of memory because of high TREEWIDTH . \n",
            "in this paper , we propose a novel method for COMPILATION OF BAYESIAN NETWORKS based on BAYESIAN NETWORKS . the proposed method is based on the use of BAYESIAN NETWORKS and STRUCTURE-BASED INFERENCE METHODS . the proposed method is based on the use of VARIABLE ELIMINATION and STRUCTURE-BASED INFERENCE METHODS to estimate the LOCAL STRUCTURE of the MAP . the proposed method is based on the use of BAYESIAN NETWORKS and STRUCTURE-BASED INFERENCE METHODS . the proposed method is based on the use of BAYESIAN NETWORKS and STRUCTURE-BASED INFERENCE METHODS to estimate the LOCAL STRUCTURE of the MAP . the proposed method can be applied to ARITHMETIC CIRCUITS and STRUCTURE-BASED INFERENCE METHODS . the performance of the proposed method is demonstrated on a variety of ARITHMETIC CIRCUITS and STRUCTURE-BASED INFERENCE METHODS . the experimental results show that the proposed method is effective in reducing the number of TREEWIDTH in the COMPILED ARITHMETIC CIRCUIT .\n",
            "\n",
            "136 1000\n",
            "a key factor of high quality WORD SEGMENTA-TION for JAPANESE is a HIGH-COVERAGE DICTIONARY , but it is costly to manually build such a LEXICAL RESOURCE . although EXTERNAL LEXICAL RESOURCES for human readers are potentially good knowledge sources , EXTERNAL LEXICAL RESOURCES have not been utilized due to differences in SEGMENTATION CRITERIA . to supplement a MORPHOLOGICAL DICTIONARY with these resources , we propose a new task of JAPANESE NOUN PHRASE SEGMENTATION . we apply NON-PARAMETRIC BAYESIAN LANGUAGE MODELS to segment each noun phrase in these resources according to the statistical behavior of its supposed constituents in text . for INFERENCE , we propose a novel BLOCK SAMPLING PROCEDURE named HYBRID TYPE-BASED SAMPLING , which has the ability to directly escape a LOCAL OPTIMUM that is not too distant from the GLOBAL OPTIMUM . experiments show that the proposed BLOCK SAMPLING PROCEDURE efficiently corrects the initial segmentation given by a MORPHOLOGICAL ANA-LYZER . \n",
            "this paper presents a novel approach to JAPANESE NOUN PHRASE SEGMENTATION in JAPANESE . the proposed approach is based on the use of a HIGH-COVERAGE DICTIONARY and a HIGH-COVERAGE DICTIONARY for INFERENCE . the proposed method is based on the use of a HIGH-COVERAGE DICTIONARY and a HIGH-COVERAGE DICTIONARY . the proposed method is based on the use of a HIGH-COVERAGE DICTIONARY as a HIGH-COVERAGE DICTIONARY . the proposed approach is based on the use of a HIGH-COVERAGE DICTIONARY and a HIGH-COVERAGE DICTIONARY for INFERENCE . the proposed method is evaluated on a JAPANESE NOUN PHRASE SEGMENTATION . the experimental results show that the proposed method achieves better performance than the state-of-the-art methods .\n",
            "\n",
            "137 1000\n",
            "this paper presents a new algorithm for PLAN RECOGNITION called <unk> -lrb- engine for LEXICALIZED INTENT RECOGNITION -RRB- . <unk> represents the plans to be recognized with a GRAMMATICAL FORMALISM called COMBINATORY CATEGORIAL GRAMMAR . we show that representing plans with COMBINATORY CATEGORIAL GRAMMAR can allow us to prevent early commitment to PLAN GOALS and thereby reduce runtime . \n",
            "in this paper , we present a novel approach to the problem of PLAN RECOGNITION . the proposed approach is based on a COMBINATORY CATEGORIAL GRAMMAR , a COMBINATORY CATEGORIAL GRAMMAR , a COMBINATORY CATEGORIAL GRAMMAR , a COMBINATORY CATEGORIAL GRAMMAR , and a COMBINATORY CATEGORIAL GRAMMAR . we show that the proposed method can be applied to a wide range of PLAN GOALS .\n",
            "\n",
            "138 1000\n",
            "mention pair MENTION PAIR MODELS that predict whether or not two mentions are coreferent have historically been very effective for COREF-ERENCE RESOLUTION , but do not make use of ENTITY-LEVEL INFORMATION . however , we show that the scores produced by such MENTION PAIR MODELS can be aggregated to define powerful ENTITY-LEVEL FEATURES between clusters of mentions . using these FEATURES , we train an ENTITY-CENTRIC COREFERENCE SYSTEM that learns an effective POLICY for building up COREFERENCE CHAINS incrementally . the MENTION PAIR SCORES are also used to prune the SEARCH SPACE the ENTITY-CENTRIC COREFERENCE SYSTEM works in , allowing for efficient training with an exact LOSS FUNCTION . we evaluate our ENTITY-CENTRIC COREFERENCE SYSTEM on the ENGLISH PORTION of the 2012 CONLL SHARED TASK DATASET and show that ENTITY-CENTRIC COREFERENCE SYSTEM improves over the current state of the art . \n",
            "in this paper , we propose a novel ENTITY-CENTRIC COREFERENCE SYSTEM for COREFERENCE CHAINS in a ENTITY-CENTRIC COREFERENCE SYSTEM . the proposed ENTITY-CENTRIC COREFERENCE SYSTEM is based on the use of a set of ENTITY-LEVEL FEATURES that are extracted from the MENTION PAIR SCORES of the MENTION PAIR MODELS . the proposed ENTITY-CENTRIC COREFERENCE SYSTEM consists of a set of FEATURES that are used to estimate the MENTION PAIR SCORES of the MENTION PAIR MODELS . the proposed ENTITY-CENTRIC COREFERENCE SYSTEM is applied to the CONLL SHARED TASK DATASET , which is based on a LOSS FUNCTION . the proposed ENTITY-CENTRIC COREFERENCE SYSTEM is evaluated on the CONLL SHARED TASK DATASET , and the results show that the proposed method is effective in improving the COREF-ERENCE RESOLUTION performance of the ENTITY-CENTRIC COREFERENCE SYSTEM . the proposed method is evaluated on the CONLL SHARED TASK DATASET and the results show that the proposed method is effective in improving the COREF-ERENCE RESOLUTION performance .\n",
            "\n",
            "139 1000\n",
            "this paper describes an experiment comparing the effect of two different approaches to INFORMATION PRESENTATION on ITEM RECALL . the results show that using DISCOURSE CUES facilitates <unk> the presented information . \n",
            "in this paper , we present a novel method for INFORMATION PRESENTATION based on INFORMATION PRESENTATION . the proposed approach is based on the use of INFORMATION PRESENTATION to estimate the INFORMATION PRESENTATION . the proposed method is based on the use of DISCOURSE CUES in order to estimate the DISCOURSE CUES of the source and target speaker . the experimental results show that the proposed method is effective in improving the ITEM RECALL performance .\n",
            "\n",
            "140 1000\n",
            "a HOMOGRAPHY MATRIX is used in COMPUTER VISION FIELD to solve the CORRESPONDENCE PROBLEM between a pair of STEREO IMAGES . RANSAC ALGORITHM is often used to calculate the HOMOGRAPHY MATRIX by randomly selecting a set of FEATURES iteratively . RANSAC ALGORITHM in this paper converts RANSAC ALGORITHM into TWO-LAYERS . the first layer is addressing SAMPLING PROBLEM which we can describe our knowledge about DEGENERATE FEATURES by mean of CONSTRAINT SATISFACTION PROBLEMS . by dividing the input IMAGE into a GRID and making FEATURE POINTS into discrete domains , we can model the IMAGE into the CONSTRAINT SATISFACTION PROBLEMS to efficiently filter out DEGENERATE FEATURES . by expressing the knowledge about degenerate feature samples using CONSTRAINT SATISFACTION PROBLEMS in the first layer , so that computer has knowledge about how to skip computing the HOMOGRAPHY MATRIX in the MODEL ESTIMATION STEP for the second layer . the experimental results show that the proposed RANSAC ALGORITHM can outperform the most of variants of RANSAC without sacrificing its EXECUTION TIME . \n",
            "in this paper , we propose a novel RANSAC ALGORITHM for the CORRESPONDENCE PROBLEM from STEREO IMAGES . the proposed RANSAC ALGORITHM is based on a RANSAC ALGORITHM to the CORRESPONDENCE PROBLEM . the proposed RANSAC ALGORITHM is based on the use of a HOMOGRAPHY MATRIX to estimate the HOMOGRAPHY MATRIX . the proposed RANSAC ALGORITHM is based on the use of a HOMOGRAPHY MATRIX to estimate the HOMOGRAPHY MATRIX . the proposed RANSAC ALGORITHM is applied to the CORRESPONDENCE PROBLEM in a MODEL ESTIMATION STEP , and the results show that the proposed RANSAC ALGORITHM is more robust to EXECUTION TIME than the conventional RANSAC . the proposed RANSAC ALGORITHM is more robust to EXECUTION TIME than conventional RANSAC .\n",
            "\n",
            "141 1000\n",
            "image sequence processing IMAGE SEQUENCE PROCESSING TECHNIQUES are used to study EXCHANGE , GROWTH , AND TRANSPORT PROCESSES and to tackle key questions in ENVIRONMENTAL PHYSICS and BIOLOGY . these applications require high ACCURACY for the ESTIMATION OF THE MOTION FIELD since the most interesting parameters of the DYNAMICAL PROCESSES studied are contained in FIRST-ORDER DERIVATIVES OF THE MOTION FIELD or in DYNAMICAL CHANGES OF THE MOVING OBJECTS . therefore the performance and OPTIMIZATION OF LOW-LEVEL MOTION ESTIMATORS is discussed . a TENSOR METHOD tuned with carefully optimized DERIVATIVE FILTERS yields reliable and dense displacement vector fields -lrb- <unk> -rrb- with an ACCURACY of up to a few <unk> PIXELS/FRAME for REAL-WORLD IMAGES . the ACCURACY of the TENSOR METHOD is verified with COMPUTER-GENERATED SEQUENCES and a CALIBRATED IMAGE SEQUENCE . with the improvements in ACCURACY the MOTION ESTIMATION is now rather limited by imperfections in the CCD SENSORS , especially the SPATIAL NONUNI-FORMITY in the RESPONSIVITY . with a simple TWO-POINT CALIBRATION , these effects can efficiently be suppressed . the application of the IMAGE SEQUENCE PROCESSING TECHNIQUES to the ANALYSIS OF PLANT GROWTH , to OCEAN SURFACE MICROTURBULENCE IN IR IMAGE SEQUENCES , and to SEDIMENT TRANSPORT is demonstrated . \n",
            "this paper addresses the problem of MOTION ESTIMATION from REAL-WORLD IMAGES and OCEAN SURFACE MICROTURBULENCE IN IR IMAGE SEQUENCES . we propose a novel TENSOR METHOD based on DYNAMICAL PROCESSES for MOTION ESTIMATION and ANALYSIS OF PLANT GROWTH . the proposed TENSOR METHOD is based on a FIRST-ORDER DERIVATIVES OF THE MOTION FIELD , which is a FIRST-ORDER DERIVATIVES OF THE MOTION FIELD with a FIRST-ORDER DERIVATIVES OF THE MOTION FIELD . the proposed TENSOR METHOD is based on a novel TENSOR METHOD , which is able to capture DYNAMICAL CHANGES OF THE MOVING OBJECTS and DYNAMICAL CHANGES OF THE MOVING OBJECTS . the proposed TENSOR METHOD is evaluated on both REAL-WORLD IMAGES and REAL-WORLD IMAGES . the experimental results show that the proposed TENSOR METHOD significantly improves the ACCURACY and ACCURACY of the proposed TENSOR METHOD in terms of ACCURACY and ANALYSIS OF PLANT GROWTH .\n",
            "\n",
            "142 1000\n",
            "| MULTIPLE SOURCE SIGNALS impinging on an ANTENNA ARRAY can be separated by TIME-FREQUENCY SYNTHESIS TECHNIQUES . averaging of the TIME-FREQUENCY DISTRIBUTIONS of the data across the array permits the SPATIAL SIGNATURES OF SOURCES to play a fundamental role in improving the synthesis performance . array a VERAGING introduces a WEIGHING FUNCTION in the TIME-FREQUENCY DOMAIN that decreases the NOISE LEVELS , reduces the interactions of the source signals , and mitigates the CROSSTERMS . this is achieved independent of the temporal characteristics of the source signals and without causing any SMEARING OF THE SIGNAL TERMS . the WEIGHING FUNCTION may take NON-INTEGER VALUES , which are determined by the COMMUNICATION CHANNEL , the SOURCE POSITIONS and their ANGULAR SEPARATIONS . unlike the recently devised BLIND SOURCE SEPARATION METHODS using SPATIAL TIME-FREQUENCY DISTRIBUTIONS , the proposed method does not require WHITENING or retrieval of the source directional matrix . the paper evaluates the proposed method in terms of performance and computations relative t o the existing SOURCE SEPARATION TECHNIQUES based on QUADRATIC T-F DISTRIBUTIONS . \n",
            "in this paper , we propose a novel approach to MULTIPLE SOURCE SIGNALS based on TIME-FREQUENCY SYNTHESIS TECHNIQUES . the proposed approach is based on the use of a WEIGHING FUNCTION , a WEIGHING FUNCTION , and a WEIGHING FUNCTION for SMEARING OF THE SIGNAL TERMS . the proposed approach is based on the use of a WEIGHING FUNCTION , a WEIGHING FUNCTION , and a WEIGHING FUNCTION for SMEARING OF THE SIGNAL TERMS . the proposed approach is based on the use of QUADRATIC T-F DISTRIBUTIONS and ANGULAR SEPARATIONS . the proposed method is evaluated on a variety of NOISE LEVELS and NOISE LEVELS . the experimental results show that the proposed method outperforms the state-of-the-art methods in terms of WHITENING and WHITENING .\n",
            "\n",
            "143 1000\n",
            "we propose a novel method for inferring whether x causes y or vice versa from joint observations of x and y . the basic idea is to model the observed data using PROBABILISTIC LATENT VARIABLE MODELS , which incorporate the effects of UNOBSERVED NOISE . to this end , we consider the HYPOTHETICAL EFFECT VARIABLE to be a function of the HYPOTHETICAL CAUSE VARIABLE and an independent noise term -lrb- not necessarily additive -rrb- . an important novel aspect of our work is that we do not restrict the MODEL CLASS , but instead put general non-parametric priors on this function and on the distribution of the cause . the CAUSAL DIRECTION can then be inferred by using standard BAYESIAN MODEL SELECTION . we evaluate our approach on SYNTHETIC DATA and REAL-WORLD DATA and report encouraging results . \n",
            "in this paper , we propose a novel approach to BAYESIAN MODEL SELECTION based on BAYESIAN MODEL SELECTION . the proposed approach is based on the use of PROBABILISTIC LATENT VARIABLE MODELS to estimate the CAUSAL DIRECTION of the MODEL CLASS . the proposed method is based on the use of PROBABILISTIC LATENT VARIABLE MODELS to estimate the CAUSAL DIRECTION . the proposed method is based on the use of PROBABILISTIC LATENT VARIABLE MODELS to estimate the CAUSAL DIRECTION . the proposed method is evaluated on SYNTHETIC DATA , and the results show that the proposed method outperforms the state-of-the-art methods .\n",
            "\n",
            "144 1000\n",
            "this article introduces AUTOMATIC RECOGNITION OF SPEECH without any AUDIO INFORMATION . movements of the tongue , lips , and jaw are tracked by an ELECTROMAGNETIC ARTICULOGRA-PHY DEVICE and are used as FEATURES to create HIDDEN MARKOV MODELS and conduct AUTOMATIC SPEECH RECOGNITION in a conventional way . the results obtained are promising , which confirm that PHONETIC FEATURES characterizing ARTICULATION are as discriminating as those characterizing ACOUSTICS -lrb- except for voicing -rrb- . the results also show that using TONGUE PARAMETERS result in a higher ACCURACY compared with the LIP PARAMETERS . \n",
            "in this paper , we present a novel approach to AUTOMATIC RECOGNITION OF SPEECH based on HIDDEN MARKOV MODELS . the proposed approach is based on the use of a HIDDEN MARKOV MODELS , which is able to capture the AUDIO INFORMATION of the PHONETIC FEATURES . the proposed method is based on the use of a set of PHONETIC FEATURES extracted from the ELECTROMAGNETIC ARTICULOGRA-PHY DEVICE . the experimental results show that the proposed method outperforms the conventional FEATURES in terms of both ACCURACY and ACCURACY .\n",
            "\n",
            "145 1000\n",
            "in this paper , we introduce a new IMAGE DESCRIPTOR for BROAD IMAGE CATEGORIZATION , the PROGRESSIVE RANDOMIZA-TION , that uses PERTURBATIONS on the values of the least significant bits -lrb- <unk> -rrb- of IMAGES . we show that different classes of IMAGES have a distinct behavior under our IMAGE DESCRIPTOR , and that using STATISTICAL DESCRIPTORS OF LSB OCCURRENCES and enough TRAINING EXAMPLES , the IMAGE DESCRIPTOR already performs as well or better than comparable existing techniques in the literature . with few TRAINING EXAMPLES , pr still has good separability , and its ACCURACY increases with the size of the training set . we validate our IMAGE DESCRIPTOR using four IMAGE DATABASES with different categories . \n",
            "this paper presents a novel approach to BROAD IMAGE CATEGORIZATION based on STATISTICAL DESCRIPTORS OF LSB OCCURRENCES . the proposed IMAGE DESCRIPTOR is based on the use of a set of PERTURBATIONS extracted from the IMAGE DATABASES , which are then used to estimate the IMAGE DESCRIPTOR . the proposed IMAGE DESCRIPTOR is applied to the problem of BROAD IMAGE CATEGORIZATION from IMAGE DATABASES . the experimental results show that the proposed method outperforms the state-of-the-art methods in terms of ACCURACY and ACCURACY .\n",
            "\n",
            "146 1000\n",
            "in this paper , we introduce a TUNED EIGENSPACE TECHNIQUE so as to classify HUMAN MOTION . the TUNED EIGENSPACE TECHNIQUE presented here overcomes those problems related to ARTICULATED MOTION and <unk> texture effects by learning various HUMAN MOTIONS in terms of their SEQUENTIAL POSTURES in an eigenspace . in order to cope with the variability inherent to ARTICULATED MOTION , we propose a TUNED EIGENSPACE TECHNIQUE to tune the set of SEQUENTIAL EIGENSPACES . once the learnt TUNED EIGENSPACES are at hand , the RECOGNITION TASK then becomes a nearest-neighbor search over the <unk> . we show how our TUNED EIGENSPACE TECHNIQUE can be used for purposes of REAL-WORLD AND SYNTHETIC POSE RECOGNITION . we also discuss and overcome the problem related to CLOTHING TEXTURE that occurs in REAL-WORLD DATA , and propose a BACKGROUND SUBTRACTION METHOD to employ the TUNED EIGENSPACE TECHNIQUE in OUTDOOR ENVIRONMENT . we provide results on SYNTHETIC IMAGERY for a number of HUMAN POSES and illustrate the utility of the TUNED EIGENSPACE TECHNIQUE for the purposes of HUMAN MOTION RECOGNITION . \n",
            "this paper presents a novel BACKGROUND SUBTRACTION METHOD for HUMAN MOTION RECOGNITION . the proposed BACKGROUND SUBTRACTION METHOD is based on a TUNED EIGENSPACE TECHNIQUE for HUMAN MOTION RECOGNITION . the proposed TUNED EIGENSPACE TECHNIQUE is based on a TUNED EIGENSPACE TECHNIQUE to the RECOGNITION TASK . the proposed TUNED EIGENSPACE TECHNIQUE is based on the TUNED EIGENSPACE TECHNIQUE , which is based on the TUNED EIGENSPACE TECHNIQUE . the proposed TUNED EIGENSPACE TECHNIQUE is applied to the RECOGNITION TASK in a RECOGNITION TASK . the experimental results show that the proposed TUNED EIGENSPACE TECHNIQUE is effective in improving the RECOGNITION TASK performance on SYNTHETIC IMAGERY .\n",
            "\n",
            "147 1000\n",
            "we present a CONTINUOUS OPTIMIZATION FRAMEWORK for INTERACTIVE TRACKING OF 2D GENERIC OBJECTS in a single video stream . the user begins with specifying the locations of a target object in a small set of keyframes ; the CONTINUOUS OPTIMIZATION FRAMEWORK then automatically tracks locations of the objects by combining USER CONSTRAINTS with VISUAL MEASUREMENTS across the entire sequence . we formulate the problem in a SPACETIME OPTIMIZATION FRAMEWORK that optimizes over the whole sequence simultaneously . the resulting CONTINUOUS OPTIMIZATION FRAMEWORK is consistent with VISUAL MEASUREMENTS across the entire sequence while satisfying USER CONSTRAINTS . we also introduce prior terms to reduce TRACKING AMBIGUITY . we demonstrate the power of our CONTINUOUS OPTIMIZATION FRAMEWORK on TRACKING AMBIGUITY with significant occlusions , SCALE and orientation changes , ILLUMINATION CHANGES , SUDDEN MOVEMENT OF OBJECTS , and also SIMULTANEOUS TRACKING OF MULTIPLE OBJECTS . we compare the performance of our CONTINUOUS OPTIMIZATION FRAMEWORK with alternative methods . \n",
            "this paper presents a novel CONTINUOUS OPTIMIZATION FRAMEWORK for SIMULTANEOUS TRACKING OF MULTIPLE OBJECTS . the proposed CONTINUOUS OPTIMIZATION FRAMEWORK is based on a CONTINUOUS OPTIMIZATION FRAMEWORK for SIMULTANEOUS TRACKING OF MULTIPLE OBJECTS and SIMULTANEOUS TRACKING OF MULTIPLE OBJECTS . the proposed CONTINUOUS OPTIMIZATION FRAMEWORK is based on a CONTINUOUS OPTIMIZATION FRAMEWORK for SIMULTANEOUS TRACKING OF MULTIPLE OBJECTS and SIMULTANEOUS TRACKING OF MULTIPLE OBJECTS . the proposed CONTINUOUS OPTIMIZATION FRAMEWORK is applied to SIMULTANEOUS TRACKING OF MULTIPLE OBJECTS and SIMULTANEOUS TRACKING OF MULTIPLE OBJECTS . the proposed CONTINUOUS OPTIMIZATION FRAMEWORK is applied to SIMULTANEOUS TRACKING OF MULTIPLE OBJECTS and SIMULTANEOUS TRACKING OF MULTIPLE OBJECTS . the experimental results demonstrate the effectiveness of the proposed CONTINUOUS OPTIMIZATION FRAMEWORK in comparison to the state of the art .\n",
            "\n",
            "148 1000\n",
            "we consider the problem of PARAMETER ESTIMATION for signals characterized by sums of PARAMETERIZED FUNCTIONS . we present a DYNAMIC DICTIONARY SUBSET SELECTION APPROACH to PARAMETER ESTIMATION where we iteratively select a small number of DICTIONARY ELEMENTS and then alter the parameters of these DICTIONARY ELEMENTS to achieve better signal model fit . the proposed DYNAMIC DICTIONARY SUBSET SELECTION APPROACH avoids the use of highly oversampled -lrb- and highly correlated -rrb- DICTIONARY ELEMENTS , which are needed in FIXED DICTIONARY APPROACHES to reduce PARAMETER BIAS associated with DICTIONARY QUANTIZATION . we demonstrate estimation performance on a sinusoidal signal estimation example . \n",
            "in this paper , we propose a novel DYNAMIC DICTIONARY SUBSET SELECTION APPROACH for PARAMETER ESTIMATION . the proposed DYNAMIC DICTIONARY SUBSET SELECTION APPROACH is based on the use of FIXED DICTIONARY APPROACHES for PARAMETER ESTIMATION . the proposed DYNAMIC DICTIONARY SUBSET SELECTION APPROACH is based on the idea of DICTIONARY QUANTIZATION , which is a generalization of the existing DYNAMIC DICTIONARY SUBSET SELECTION APPROACH . the proposed DYNAMIC DICTIONARY SUBSET SELECTION APPROACH is based on the idea of DICTIONARY QUANTIZATION , which is a function of the PARAMETER BIAS . the proposed DYNAMIC DICTIONARY SUBSET SELECTION APPROACH is applied to the problem of PARAMETER ESTIMATION . the experimental results show that the proposed DYNAMIC DICTIONARY SUBSET SELECTION APPROACH is effective in improving the performance of the proposed DYNAMIC DICTIONARY SUBSET SELECTION APPROACH .\n",
            "\n",
            "149 1000\n",
            "cluster identification is introduced as the process of jointly evaluating clustering and labelling schemes for CLUSTER-LABELLING SCHEME SELECTION . normalized <unk> and BBN METRICS for comparing clustering performances across varied clustering and labelling schemes are presented . the merits of the BBN METRICS are evaluated and applied for SPEAKER-ENVIRONMENT TRACKING in BROADCAST NEWS . \n",
            "this paper addresses the problem of CLUSTER IDENTIFICATION in the context of BROADCAST NEWS . in particular , we propose a novel approach to CLUSTER IDENTIFICATION based on BBN METRICS . the proposed approach is based on the use of BBN METRICS for CLUSTER IDENTIFICATION . the proposed approach is evaluated on a variety of BROADCAST NEWS . the experimental results show that the proposed method is effective in improving the performance of SPEAKER-ENVIRONMENT TRACKING .\n",
            "\n",
            "150 1000\n",
            "we recently proposed a family of ROBUST LINEAR AND NONLIN-EAR ESTIMATION TECHNIQUES for recognizing the three EMOTION PRIMITIVES -- VALENCE , ACTIVATION , and DOMINANCE -- from SPEECH . these were based on both LOCAL AND GLOBAL SPEECH DURATION , ENERGY , MFCC and pitch FEATURES . this paper aims to study the RELATIVE IMPORTANCE of these four categories of ACOUSTIC FEATURES in this EMOTION ESTIMATION CONTEXT . three measures are considered : the number of FEATURES from each category when all FEATURES are used in selection , the MEAN ABSOLUTE ERROR when each category is used separately , and the MEAN ABSOLUTE ERROR when a category is excluded from FEATURE SELECTION . we find that the RELATIVE IMPORTANCE is in the order of MFCC > energy ≈ pitch > duration . additionally , ESTIMATOR FUSION almost always improves performance , and locally weighted fusion always outperforms AVERAGE FUSION regardless of the number of FEATURES used . \n",
            "in this paper , we propose a novel approach to FEATURE SELECTION based on ROBUST LINEAR AND NONLIN-EAR ESTIMATION TECHNIQUES . the proposed approach is based on the use of ROBUST LINEAR AND NONLIN-EAR ESTIMATION TECHNIQUES , which is able to deal with LOCAL AND GLOBAL SPEECH DURATION , such as VALENCE , VALENCE , DOMINANCE , and VALENCE . the proposed approach is based on the use of ROBUST LINEAR AND NONLIN-EAR ESTIMATION TECHNIQUES , which is able to deal with LOCAL AND GLOBAL SPEECH DURATION , such as VALENCE , DOMINANCE , and VALENCE . the proposed method is evaluated in terms of the MEAN ABSOLUTE ERROR and the MEAN ABSOLUTE ERROR . experimental results show that the proposed method can achieve better performance than the state-of-the-art methods .\n",
            "\n",
            "151 1000\n",
            "this paper extends the EXPECTATION-MAXIMIZATION ALGORITHM to estimate not only OPTIMAL ACOUSTIC MODEL PARAMETERS , but also optimal center frequencies and bandwidths of the FILTER BANK used in CEPSTRAL FEATURE EXTRACTION for BIRD CALL CLASSIFICATION . the search is done using the GRADIENT ASCENT METHOD . filter bank and model parameters are optimized iteratively . experiments are conducted on a large noisy corpus containing ANTBIRD CALLS from 5 species . it is shown that FEATURES extracted using the optimized FILTER BANK result in a lower CLASSIFICATION ERROR RATE than those extracted using a MEL-SCALED FILTER BANK . \n",
            "in this paper , we propose a novel method for BIRD CALL CLASSIFICATION based on a FILTER BANK . the proposed approach is based on the use of a FILTER BANK to estimate the OPTIMAL ACOUSTIC MODEL PARAMETERS . the proposed method is based on the use of a FILTER BANK to estimate the OPTIMAL ACOUSTIC MODEL PARAMETERS . the proposed method is based on the use of a FILTER BANK . the proposed FEATURES is applied to the OPTIMAL ACOUSTIC MODEL PARAMETERS of the FILTER BANK . experimental results show that the proposed FEATURES can improve the CLASSIFICATION ERROR RATE of the FEATURES by the proposed method .\n",
            "\n",
            "152 1000\n",
            "we present UTILE SUFFIX MEMORY , a REINFORCEMENT LEARNING ALGORITHM that uses SHORT-TERM MEMORY to overcome the STATE ALIASING that results from HIDDEN STATE . by combining the advantages of previous work in <unk> -lrb- or '' <unk> '' -rrb- learning and previous work with STATISTICAL TESTS for separating noise from task structure , the REINFORCEMENT LEARNING ALGORITHM learns quickly , creates only as much memory as needed for the task at hand , and handles noise well . UTILE SUFFIX MEMORY uses a TREE-STRUCTURED REPRESENTATION , and is related to work on <unk> \n",
            "this paper proposes a novel REINFORCEMENT LEARNING ALGORITHM called UTILE SUFFIX MEMORY . the proposed REINFORCEMENT LEARNING ALGORITHM is based on a TREE-STRUCTURED REPRESENTATION , called UTILE SUFFIX MEMORY . the proposed REINFORCEMENT LEARNING ALGORITHM , called UTILE SUFFIX MEMORY , is based on a TREE-STRUCTURED REPRESENTATION , called UTILE SUFFIX MEMORY . the proposed REINFORCEMENT LEARNING ALGORITHM , called UTILE SUFFIX MEMORY , is based on UTILE SUFFIX MEMORY . the proposed REINFORCEMENT LEARNING ALGORITHM is based on the UTILE SUFFIX MEMORY , which is based on UTILE SUFFIX MEMORY . the experimental results show that the proposed REINFORCEMENT LEARNING ALGORITHM can significantly improve the performance of the proposed REINFORCEMENT LEARNING ALGORITHM .\n",
            "\n",
            "153 1000\n",
            "the DOMINANT ACOUSTIC MODELING METHODOLOGY based on HIDDEN MARKOV MODELS is known to have certain weaknesses . partial solutions to these flaws have been presented , but the fundamental problem remains : compression of the data to a compact HMM discards useful information such as TIME DEPENDENCIES and SPEAKER INFORMATION . in this paper , we look at PURE EXAMPLE BASED RECOGNITION as a solution to this problem . by replacing the HMM with the underlying examples , all information in the training data is retained . we show how information about SPEAKER AND ENVIRONMENT can be used , introducing a new interpretation of ADAPTATION . the basis for the PURE EXAMPLE BASED RECOGNITION is the well-known DTW ALGORITHM , which has often been used for small tasks . however , LARGE VOCABULARY SPEECH RECOGNITION introduces new demands , resulting in an explosion of the SEARCH SPACE . we show how this problem can be tackled using a DATA DRIVEN APPROACH which selects appropriate SPEECH EXAMPLES as candidates for DTW-ALIGNMENT . \n",
            "this paper proposes a novel DATA DRIVEN APPROACH for LARGE VOCABULARY SPEECH RECOGNITION . the proposed DATA DRIVEN APPROACH is based on a DATA DRIVEN APPROACH , which is based on the DOMINANT ACOUSTIC MODELING METHODOLOGY . the proposed DATA DRIVEN APPROACH is based on a DTW ALGORITHM , which is robust to SPEAKER INFORMATION and SPEAKER INFORMATION . the proposed DATA DRIVEN APPROACH is based on a DTW ALGORITHM , which is able to deal with TIME DEPENDENCIES and SPEAKER INFORMATION . the proposed DATA DRIVEN APPROACH is applied to the SPEAKER AND ENVIRONMENT and the TIME DEPENDENCIES and the TIME DEPENDENCIES . experimental results show that the proposed DATA DRIVEN APPROACH is effective in improving the ADAPTATION performance in LARGE VOCABULARY SPEECH RECOGNITION .\n",
            "\n",
            "154 1000\n",
            "we consider a MULTIPLE-INPUT MULTIPLE-OUTPUT WIRELESS COMMUNICATION SCENARIO in which the channel follows a general SPATIALLY-CORRELATED COMPLEX GAUSSIAN DISTRIBUTION with NON-ZERO MEAN . we derive an explicit characterization of the optimal input covariance from an ERGODIC RATE PERSPECTIVE for systems that operate at LOW SNRS . this characterization is in terms of the <unk> decomposition of a matrix that depends on the mean and the covariance of the channel , and typically results in a BEAMFORMING STRATEGY along the principal eigenvector of that matrix . simulation results show the potential impact of -lrb- jointly -rrb- exploiting the mean and the covariance of the channel on the ERGODIC ACHIEVABLE RATE at both low and <unk> snrs . \n",
            "this paper addresses the problem of estimating the ERGODIC ACHIEVABLE RATE of a MULTIPLE-INPUT MULTIPLE-OUTPUT WIRELESS COMMUNICATION SCENARIO in a MULTIPLE-INPUT MULTIPLE-OUTPUT WIRELESS COMMUNICATION SCENARIO . the proposed approach is based on a SPATIALLY-CORRELATED COMPLEX GAUSSIAN DISTRIBUTION with a SPATIALLY-CORRELATED COMPLEX GAUSSIAN DISTRIBUTION . the proposed algorithm is based on a SPATIALLY-CORRELATED COMPLEX GAUSSIAN DISTRIBUTION with a SPATIALLY-CORRELATED COMPLEX GAUSSIAN DISTRIBUTION . the proposed algorithm is based on a SPATIALLY-CORRELATED COMPLEX GAUSSIAN DISTRIBUTION of the SPATIALLY-CORRELATED COMPLEX GAUSSIAN DISTRIBUTION . the performance of the proposed algorithm is evaluated in terms of the ERGODIC ACHIEVABLE RATE and the ERGODIC ACHIEVABLE RATE of the BEAMFORMING STRATEGY .\n",
            "\n",
            "155 1000\n",
            "we discuss a few new MOTION DEBLURRING PROBLEMS that are significant to KERNEL ESTIMATION and NON-BLIND DECONVOLUTION . we found that strong EDGES do not always PROFIT KERNEL ESTIMATION , but instead under certain circumstance degrade it . this finding leads to a new metric to measure the usefulness of IMAGE EDGES in MOTION DEBLURRING and a GRADIENT SELECTION PROCESS to mitigate their possible adverse effect . we also propose an efficient and high-quality KERNEL ESTIMATION method based on using the SPATIAL PRIOR and the ITERATIVE SUPPORT DETECTION KERNEL REFINEMENT , which avoids HARD THRESHOLD of the kernel elements to enforce SPARSITY . we employ the TV-1 DECONVOLUTION MODEL , solved with a new VARIABLE SUBSTITUTION SCHEME to robustly suppress NOISE . \n",
            "this paper addresses the problem of MOTION DEBLURRING and MOTION DEBLURRING in MOTION DEBLURRING PROBLEMS . the proposed TV-1 DECONVOLUTION MODEL is based on the GRADIENT SELECTION PROCESS and the GRADIENT SELECTION PROCESS . the proposed TV-1 DECONVOLUTION MODEL is based on the GRADIENT SELECTION PROCESS and the GRADIENT SELECTION PROCESS . the proposed TV-1 DECONVOLUTION MODEL is based on the GRADIENT SELECTION PROCESS and the GRADIENT SELECTION PROCESS . the proposed TV-1 DECONVOLUTION MODEL is based on the GRADIENT SELECTION PROCESS and the GRADIENT SELECTION PROCESS . the performance of the proposed TV-1 DECONVOLUTION MODEL is demonstrated on a variety of MOTION DEBLURRING PROBLEMS and MOTION DEBLURRING . the performance of the proposed algorithm is demonstrated on a variety of MOTION DEBLURRING PROBLEMS and MOTION DEBLURRING .\n",
            "\n",
            "156 1000\n",
            "in an extended IMAGE SEQUENCE of an outdoor scene , one observes changes in color induced by variations in the SPECTRAL COMPOSITION OF DAYLIGHT . this paper proposes a model for these TEMPORAL COLOR CHANGES and explores its use for the ANALYSIS OF OUTDOOR SCENES from TIME-LAPSE VIDEO DATA . we show that the time-varying changes in DIRECT SUNLIGHT and AMBIENT SKYLIGHT can be recovered with this model , and that an IMAGE SEQUENCE can be decomposed into two corresponding components . the decomposition provides access to both RADIOMETRIC AND GEOMETRIC INFORMATION about a scene , and we demonstrate how this can be exploited for a variety of VISUAL TASKS , including <unk> , BACKGROUND SUBTRACTION , SHADOW DETECTION , SCENE RECONSTRUCTION , and CAMERA GEO-LOCATION . \n",
            "in this paper , we present a novel approach to the ANALYSIS OF OUTDOOR SCENES from TIME-LAPSE VIDEO DATA . the proposed approach is based on the use of a SPECTRAL COMPOSITION OF DAYLIGHT and a SPECTRAL COMPOSITION OF DAYLIGHT . the proposed approach is based on the use of a SPECTRAL COMPOSITION OF DAYLIGHT and a SPECTRAL COMPOSITION OF DAYLIGHT . the proposed approach is based on the use of TIME-LAPSE VIDEO DATA , which is based on the RADIOMETRIC AND GEOMETRIC INFORMATION and the AMBIENT SKYLIGHT . the proposed method is evaluated on a variety of VISUAL TASKS including SHADOW DETECTION , SHADOW DETECTION , SCENE RECONSTRUCTION and BACKGROUND SUBTRACTION .\n",
            "\n",
            "157 1000\n",
            "this paper describes a method for the AUTOMATIC ALIGNMENT OF PARALLEL TEXTS at CLAUSE LEVEL . the method features STATISTICAL TECHNIQUES coupled with SHALLOW LINGUISTIC PROCESSING . it <unk> a PARALLEL BILINGUAL CORPUS and identifies alignments between the clauses of the source and target language sides of the corpus . PARALLEL TEXTS are first statistically aligned at sentence level and then tagged with their PART-OF-SPEECH CATEGORIES . REGULAR GRAMMARS functioning on tags , recognize clauses on both sides of the PARALLEL TEXT . a PROBABILISTIC MODEL is applied next , operating on the basis of WORD OCCURRENCE and CO-OCCURRENCE PROBABILITIES and CHARACTER LENGTHS . depending on SENTENCE SIZE , possible alignments arc fed into a DYNAMIC PROGRANUNING FRAMEWORK or a SIMULATED ANNEALING SYSTEM in order to find or <unk> ~ te the best alignment . 1HE METHOD has been tested on a SMALL ENG ~ LISH-GREEK CORPUS consisting of texts relevant to SOFTWARE SYSTEMS and has produced promising results in terms of correctly identified CLAUSE ALIGNMENTS . \n",
            "this paper addresses the problem of AUTOMATIC ALIGNMENT OF PARALLEL TEXTS in SOFTWARE SYSTEMS . we propose a novel 1HE METHOD for AUTOMATIC ALIGNMENT OF PARALLEL TEXTS and SHALLOW LINGUISTIC PROCESSING . the key idea is to use a PROBABILISTIC MODEL to estimate the CO-OCCURRENCE PROBABILITIES , CO-OCCURRENCE PROBABILITIES , CO-OCCURRENCE PROBABILITIES , and CLAUSE ALIGNMENTS . the proposed approach is based on a novel PROBABILISTIC MODEL , which is able to learn a PROBABILISTIC MODEL from a set of PARALLEL TEXTS . the proposed approach is evaluated on a SMALL ENG ~ LISH-GREEK CORPUS and a SMALL ENG ~ LISH-GREEK CORPUS . the results show that the proposed 1HE METHOD is effective in reducing the number of PART-OF-SPEECH CATEGORIES and CHARACTER LENGTHS .\n",
            "\n",
            "158 1000\n",
            "we investigate a general SEMI-MARKOV DECISION PROCESS FRAMEWORK for modeling CONCURRENT DECISION MAKING , where agents learn optimal plans over CONCURRENT TEMPORALLY EXTENDED ACTIONS . we introduce three types of PARALLEL TERMINATION SCHEMES -- all , any and continue -- and theoretically and experimentally compare them . \n",
            "this paper proposes a novel SEMI-MARKOV DECISION PROCESS FRAMEWORK for CONCURRENT DECISION MAKING . the proposed SEMI-MARKOV DECISION PROCESS FRAMEWORK is based on a SEMI-MARKOV DECISION PROCESS FRAMEWORK for CONCURRENT DECISION MAKING . the proposed SEMI-MARKOV DECISION PROCESS FRAMEWORK is based on a SEMI-MARKOV DECISION PROCESS FRAMEWORK for CONCURRENT TEMPORALLY EXTENDED ACTIONS . the proposed SEMI-MARKOV DECISION PROCESS FRAMEWORK is applied to the problem of CONCURRENT TEMPORALLY EXTENDED ACTIONS , and the experimental results show that the proposed SEMI-MARKOV DECISION PROCESS FRAMEWORK is effective in reducing the number of sources .\n",
            "\n",
            "159 1000\n",
            "this paper investigates the problem of how to partition UNKNOWN SPEECH UTTERANCES into CLUSTERS , such that the overall WITHIN-CLUSTER HOMOGENEITY of speakers ' voice characteristics can be maximized . the WITHIN-CLUSTER HOMOGENEITY is characterized by the LIKELIHOOD PROBABILITY that a CLUSTER MODEL , trained using all the utterances within a CLUSTER , matches each of the WITHIN-CLUSTER UTTERANCES . such probability is then maximized by using a GENETIC ALGORITHM , which determines the best CLUSTER where each utterance should be located . for greater COMPUTATIONAL EFFICIENCY , also proposed is an alternative solution that approximates the LIKELIHOOD PROBABILITY with a DIVERGENCE-BASED MODEL SIMILARITY . the method is further designed to estimate the optimal number of CLUSTERS automatically . \n",
            "this paper addresses the problem of DIVERGENCE-BASED MODEL SIMILARITY in the presence of UNKNOWN SPEECH UTTERANCES . in particular , we propose a method to estimate the LIKELIHOOD PROBABILITY of a CLUSTER MODEL . the proposed method is based on a GENETIC ALGORITHM of the LIKELIHOOD PROBABILITY of the CLUSTERS of the CLUSTER . the proposed method is based on a GENETIC ALGORITHM of the LIKELIHOOD PROBABILITY of the CLUSTER MODEL . the proposed method is applied to the problem of UNKNOWN SPEECH UTTERANCES in the presence of UNKNOWN SPEECH UTTERANCES . experimental results show that the proposed method is effective in reducing the number of CLUSTERS in the presence of UNKNOWN SPEECH UTTERANCES .\n",
            "\n",
            "160 1000\n",
            "we describe our implementation of a PARALLEL DEPTH RECOVERY SCHEME for a FOUR-CAMERA MULTIBASELINE STEREO in a CONVER-GENT CONFIGURATION . our PARALLEL DEPTH RECOVERY SCHEME is capable of IMAGE CAPTURE at VIDEO RATE . this is critical in applications that require THREE-DIMENSIONAL TRACKING . we obtain DENSE STEREO DEPTH DATA by projecting a light pattern of frequency modulated <unk> varying intensity onto the scene , thus increasing the LOCAL DISCRIMINABILITY at each pixel and facilitating matches . in addition , we make most of the CAMERA VIEW AREAS by converging them at a volume of interest . results show that we are able to extract STEREO DEPTH DATA that are , on the average , less than 1 mm in error at distances between 1.5 to 3.5 m away from the cameras . \n",
            "this paper presents a novel PARALLEL DEPTH RECOVERY SCHEME for THREE-DIMENSIONAL TRACKING with DENSE STEREO DEPTH DATA . the proposed PARALLEL DEPTH RECOVERY SCHEME is based on the use of a set of STEREO DEPTH DATA and a CONVER-GENT CONFIGURATION . the proposed PARALLEL DEPTH RECOVERY SCHEME is applied to the STEREO DEPTH DATA in the presence of LOCAL DISCRIMINABILITY . the proposed PARALLEL DEPTH RECOVERY SCHEME is evaluated on a variety of STEREO DEPTH DATA . the experimental results show that the proposed PARALLEL DEPTH RECOVERY SCHEME significantly improves the performance of THREE-DIMENSIONAL TRACKING in THREE-DIMENSIONAL TRACKING .\n",
            "\n",
            "161 1000\n",
            "practical schemes for DISTRIBUTED VIDEO CODING with SIDE INFORMATION at the DECODER need to consider NON-STANDARD CORRELATION MODELS in order to take NON-STATIONARITIES into account . in this paper we introduce two CORRELATION MODELS for GAUSSIAN SOURCES , the <unk> -lrb- <unk> -rrb- and the GAUSSIAN-ERASURE MODELS , and evaluate LOWER AND UPPER BOUNDS on their RATE-DISTORTION FUNCTIONS . provided that the probability of IMPULSE NOISE or of ERASURES remains small , these bounds remain close to the RATE-DISTORTION FUNCTION for GAUSSIAN CORRELATION . two practical schemes for the GE CORRELATION MODEL are also presented , with performance about 1.5 db away from the upper bound . \n",
            "in this paper , we propose a novel method for DISTRIBUTED VIDEO CODING in DISTRIBUTED VIDEO CODING . the proposed method is based on the use of NON-STANDARD CORRELATION MODELS to estimate the SIDE INFORMATION of the target signal . the proposed method is based on the use of NON-STANDARD CORRELATION MODELS for DISTRIBUTED VIDEO CODING . the proposed method is based on the use of the GE CORRELATION MODEL to estimate the SIDE INFORMATION of the target signal . the proposed method is based on the use of the SIDE INFORMATION in the DECODER . the proposed method is based on the use of NON-STANDARD CORRELATION MODELS for DISTRIBUTED VIDEO CODING . the experimental results show that the proposed method is effective in reducing the number of GAUSSIAN SOURCES in the presence of IMPULSE NOISE .\n",
            "\n",
            "162 1000\n",
            "our work deals with the important problem of globally characterizing truthful mechanisms where players have <unk> , ADDITIVE VALUATIONS , like scheduling unrelated machines or ADDITIVE COMBINATORIAL AUCTIONS . very few mechanisms are known for these settings and the question is : can we prove that no other truthful mechanisms exist ? we characterize truthful mechanisms for n players and 2 tasks or items , as either TASK-INDEPENDENT , or a PLAYER-GROUPING MINIMIZER , a new class of mechanisms we discover , which generalizes AFFINE MIN-IMIZERS . we assume <unk> , strong <unk> and that the truthful payments 1 are continuous functions of players ' bids . \n",
            "this paper addresses the problem of ADDITIVE COMBINATORIAL AUCTIONS in the presence of ADDITIVE VALUATIONS . we propose a method to estimate the parameters of the TASK-INDEPENDENT and the PLAYER-GROUPING MINIMIZER . the algorithm is based on the use of a PLAYER-GROUPING MINIMIZER and a PLAYER-GROUPING MINIMIZER . the algorithm is based on a PLAYER-GROUPING MINIMIZER and the PLAYER-GROUPING MINIMIZER . the performance of the proposed algorithm is evaluated on a number of examples .\n",
            "\n",
            "163 1000\n",
            "sparse signal approximation can be used to design efficient LOW BIT-RATE CODING SCHEMES . SPARSE SIGNAL APPROXIMATION heavily relies on the ability to design appropriate dictionaries and corresponding DECOMPOSITION ALGORITHMS . the size of the dictionary , and therefore its resolution , is a key parameter that handles the tradeoff between SPARSITY and tractability . this work proposes the use of a non adaptive random sequence of <unk> in a GREEDY DECOMPOSITION PROCESS , thus browsing a larger DICTIONARY SPACE in a PROBABILISTIC FASHION with no additional projection cost nor PARAMETER ESTIMATION . this SPARSE SIGNAL APPROXIMATION leads to very SPARSE DECOMPOSITIONS , at a CONTROLLED COMPUTATIONAL COMPLEXITY . experimental evaluation is provided as proof of concept for LOW BIT RATE COMPRESSION OF AUDIO SIGNALS . \n",
            "this paper addresses the problem of LOW BIT RATE COMPRESSION OF AUDIO SIGNALS in the presence of LOW BIT RATE COMPRESSION OF AUDIO SIGNALS . we propose a novel approach to PARAMETER ESTIMATION based on SPARSE SIGNAL APPROXIMATION . the proposed method is based on the GREEDY DECOMPOSITION PROCESS , which is based on a GREEDY DECOMPOSITION PROCESS . the proposed method is based on the GREEDY DECOMPOSITION PROCESS , which is based on the GREEDY DECOMPOSITION PROCESS . the proposed method is based on the GREEDY DECOMPOSITION PROCESS , which is a GREEDY DECOMPOSITION PROCESS . the proposed method is evaluated on a variety of LOW BIT RATE COMPRESSION OF AUDIO SIGNALS . the experimental results show that the proposed method outperforms the existing methods in terms of CONTROLLED COMPUTATIONAL COMPLEXITY and CONTROLLED COMPUTATIONAL COMPLEXITY .\n",
            "\n",
            "164 1000\n",
            "the goal of SPEECH EMOTION RECOGNITION is to identify the emotional or physical state of a human being from his or her voice . one of the most important things in a SPEECH EMOTION RECOGNITION is to extract and select relevant SPEECH FEATURES with which most emotions could be recognized . in this paper , we present a SMOOTHED NONLINEAR ENERGY OPERATOR - based amplitude modulation cepstral coefficients -lrb- <unk> -rrb- feature for RECOGNIZING EMOTIONS from SPEECH SIGNALS . SMOOTHED NONLINEAR ENERGY OPERATOR estimates the energy required to produce the AM-FM SIGNAL , and then the estimated energy is separated into its AMPLITUDE AND FREQUENCY COMPONENTS using an ENERGY SEPARATION ALGORITHM . AMCC FEATURES are obtained by first decomposing a SPEECH SIGNAL using a C-CHANNEL GAMMATONE FILTERBANK , computing the AM POWER SPECTRUM , and taking a DISCRETE COSINE TRANSFORM of the ROOT COMPRESSED AM POWER SPECTRUM . conventional MFCC -lrb- mel-frequency cepstral coefficients -rrb- and <unk> dft -lrb- discrete fourier transform -rrb- spectrum based cepstral coefficients -lrb- <unk> -rrb- FEATURES are used for comparing the RECOGNITION performances of the proposed FEATURES . EMOTION RECOGNITION experiments are conducted on the FAU AIBO SPONTANEOUS EMOTION CORPUS . it is observed from the experimental results that the AMCC FEATURES provide a relative improvement of approximately 3.5 % over the baseline MFCC . \n",
            "this paper presents a novel method for RECOGNIZING EMOTIONS from SPEECH SIGNALS . the proposed method is based on a C-CHANNEL GAMMATONE FILTERBANK that uses a SMOOTHED NONLINEAR ENERGY OPERATOR to estimate the AMPLITUDE AND FREQUENCY COMPONENTS . the proposed method is based on a SMOOTHED NONLINEAR ENERGY OPERATOR that uses a SMOOTHED NONLINEAR ENERGY OPERATOR to estimate the AMPLITUDE AND FREQUENCY COMPONENTS . the proposed method is based on a C-CHANNEL GAMMATONE FILTERBANK that uses a SMOOTHED NONLINEAR ENERGY OPERATOR to estimate the AMPLITUDE AND FREQUENCY COMPONENTS . the proposed method is evaluated on the FAU AIBO SPONTANEOUS EMOTION CORPUS and the results show that the proposed method is effective in improving the RECOGNITION performance . the proposed method is evaluated on the FAU AIBO SPONTANEOUS EMOTION CORPUS , and the results show that the proposed method is effective in improving the RECOGNITION performance .\n",
            "\n",
            "165 1000\n",
            "this paper describes our recent work in developing an UNIFIED DUTCH AND GERMAN SPEECH RECOGNITION SYSTEM in the SPEECHDAT DOMAIN . the ACOUSTIC COMPONENT of the UNIFIED DUTCH AND GERMAN SPEECH RECOGNITION SYSTEM is accomplished through sharing COMMON PHONEMES without preserving any information about the languages . we propose a more robust MCE-BASED TRAINING ALGORITHM , where only the LANGUAGE DEPENDENT PHONEME MODELS are allowed to be adjusted , according to the type of training data . experimental results on DUTCH AND GERMAN SUBWORD RECOGNITION TASKS clearly show an OVERALL STRING ERROR RATE REDUCTION of about 7 % and 13 % obtained by the newly trained UNIFIED DUTCH AND GERMAN SPEECH RECOGNITION SYSTEM in comparison with the conventional MCE-TRAINED MULTILINGUAL SYSTEM . \n",
            "this paper presents a UNIFIED DUTCH AND GERMAN SPEECH RECOGNITION SYSTEM for DUTCH AND GERMAN SUBWORD RECOGNITION TASKS . the proposed MCE-TRAINED MULTILINGUAL SYSTEM is based on the use of a set of COMMON PHONEMES . the proposed MCE-TRAINED MULTILINGUAL SYSTEM is evaluated on the DUTCH AND GERMAN SUBWORD RECOGNITION TASKS , and the results show that the proposed MCE-TRAINED MULTILINGUAL SYSTEM is effective in improving the OVERALL STRING ERROR RATE REDUCTION of the MCE-TRAINED MULTILINGUAL SYSTEM . the proposed MCE-BASED TRAINING ALGORITHM is evaluated on the DUTCH AND GERMAN SUBWORD RECOGNITION TASKS , and the results show that the proposed MCE-TRAINED MULTILINGUAL SYSTEM is effective in improving the OVERALL STRING ERROR RATE REDUCTION of the MCE-TRAINED MULTILINGUAL SYSTEM .\n",
            "\n",
            "166 1000\n",
            "spectral EMBEDDING based on the SINGULAR VALUE DECOMPOSITION is a widely used '' preprocessing '' step in many LEARNING TASKS , typically leading to DI-MENSIONALITY REDUCTION by projecting onto a number of DOMINANT SINGULAR VECTORS and <unk> the COORDINATE AXES -lrb- by a PREDEFINED FUNCTION of the singular value -rrb- . however , the number of such vectors required to capture PROBLEM STRUCTURE grows with PROBLEM SIZE , and even PARTIAL SVD COMPUTATION becomes a bottleneck . in this paper , we propose a LOW-COMPLEXITY COMPRESSIVE SPECTRAL EMBEDDING ALGORITHM , which employs RANDOM PROJECTIONS and FINITE ORDER POLYNOMIAL EXPANSIONS to compute approximations to SVD-BASED EMBEDDING . for an M × N MATRIX with T NON-ZEROS , its TIME COMPLEXITY is o -lrb- -lrb- t + m + n -rrb- log -lrb- m + n -rrb- -rrb- , and the EMBEDDING DIMENSION is o -lrb- log -lrb- m + n -rrb- -rrb- , both of which are independent of the number of SINGULAR VECTORS whose effect we wish to capture . to the best of our knowledge , this is the first work to circumvent this dependence on the number of SINGULAR VECTORS for GENERAL SVD-BASED EMBEDDINGS . the key to <unk> the SINGULAR VALUE DECOMPOSITION is the observation that , for DOWNSTREAM INFERENCE TASKS such as CLUSTERING and classification , we are only interested in using the resulting EMBEDDING to evaluate PAIRWISE SIMILARITY METRICS derived from the ℓ 2-norm , rather than capturing the effect of the underlying MATRIX on ARBITRARY VECTORS as a partial SINGULAR VALUE DECOMPOSITION tries to do . our numerical results on NETWORK DATASETS demonstrate the efficacy of the proposed LOW-COMPLEXITY COMPRESSIVE SPECTRAL EMBEDDING ALGORITHM , and motivate further exploration of its application to DOWNSTREAM INFERENCE TASKS . \n",
            "in this paper , we propose a novel LOW-COMPLEXITY COMPRESSIVE SPECTRAL EMBEDDING ALGORITHM based on SINGULAR VALUE DECOMPOSITION and FINITE ORDER POLYNOMIAL EXPANSIONS . the proposed LOW-COMPLEXITY COMPRESSIVE SPECTRAL EMBEDDING ALGORITHM is based on the use of RANDOM PROJECTIONS and FINITE ORDER POLYNOMIAL EXPANSIONS . the proposed LOW-COMPLEXITY COMPRESSIVE SPECTRAL EMBEDDING ALGORITHM is based on the SINGULAR VALUE DECOMPOSITION , which is based on the SINGULAR VALUE DECOMPOSITION . the proposed LOW-COMPLEXITY COMPRESSIVE SPECTRAL EMBEDDING ALGORITHM is based on the use of RANDOM PROJECTIONS and DOMINANT SINGULAR VECTORS . the proposed LOW-COMPLEXITY COMPRESSIVE SPECTRAL EMBEDDING ALGORITHM is based on the use of RANDOM PROJECTIONS and FINITE ORDER POLYNOMIAL EXPANSIONS . the proposed LOW-COMPLEXITY COMPRESSIVE SPECTRAL EMBEDDING ALGORITHM is evaluated on two NETWORK DATASETS . the experimental results show that the proposed LOW-COMPLEXITY COMPRESSIVE SPECTRAL EMBEDDING ALGORITHM is effective for DOWNSTREAM INFERENCE TASKS such as CLUSTERING and CLUSTERING . the proposed LOW-COMPLEXITY COMPRESSIVE SPECTRAL EMBEDDING ALGORITHM can be applied to DOWNSTREAM INFERENCE TASKS such as CLUSTERING and CLUSTERING . the experimental results on NETWORK DATASETS demonstrate the effectiveness of the proposed LOW-COMPLEXITY COMPRESSIVE SPECTRAL EMBEDDING ALGORITHM .\n",
            "\n",
            "167 1000\n",
            "this paper describes a DATA-DRIVEN TECHNIQUE for optimizing the ACOUSTIC MODELS for SPEECH RECOGNITION SYSTEMS that target COMMERCIAL APPLICATIONS over <unk> . FRAME-AVERAGED FOREGROUND LOG-LIKELIHOODS -LRB- FOREGROUND SCORES -rrb- correlate to RECOGNITION ERRORS . these scores are used together with gender to optimize DATA WEIGHTING for the ACOUSTIC MODELS . this process is interpreted as increasing the PRIORS and associated parameters for POORLY MODELED DATA . the SCORE-BASED OPTIMIZATION leads to about 7 % fewer SEMANTIC ERRORS on a LIVE EVALUATION SET collected after the last data used to estimate the ACOUSTIC MODELS . \n",
            "this paper presents a novel DATA-DRIVEN TECHNIQUE for SPEECH RECOGNITION SYSTEMS based on a DATA-DRIVEN TECHNIQUE for SPEECH RECOGNITION SYSTEMS . the proposed ACOUSTIC MODELS is based on the use of PRIORS extracted from the ACOUSTIC MODELS . the proposed DATA-DRIVEN TECHNIQUE is based on the use of PRIORS and PRIORS . the proposed DATA-DRIVEN TECHNIQUE is evaluated on a LIVE EVALUATION SET and on a LIVE EVALUATION SET . the results show that the proposed DATA-DRIVEN TECHNIQUE is effective in improving the RECOGNITION ERRORS performance in COMMERCIAL APPLICATIONS . the proposed DATA-DRIVEN TECHNIQUE is evaluated on the LIVE EVALUATION SET , and the results show that the proposed DATA-DRIVEN TECHNIQUE is effective in improving the RECOGNITION ERRORS performance .\n",
            "\n",
            "168 1000\n",
            "a STATISTICAL PARAMETRIC APPROACH to SINGING VOICE SYNTHESIS based on HIDDEN MARKOV MODELS has been growing in popularity over the last few years . the spectrum , <unk> , VIBRATO , and duration of the singing voice in this STATISTICAL PARAMETRIC APPROACH are simultaneously modeled with CONTEXT-DEPENDENT HMMS and waveforms are generated from the HMMS themselves . since HMM-BASED SINGING VOICE SYNTHESIS SYSTEMS are '' corpus-based , '' the HMMS corresponding to CONTEXTUAL FACTORS that rarely appear in the training data can not be <unk> . however , it may be difficult to prepare a large enough quantity of SINGING VOICE DATA sung by one singer . furthermore , the pitch included in each song is imbalanced , and there is the vocal range of the singer . in this paper , we propose '' SINGER ADAPTIVE TRAINING '' which can solve the DATA SPARSE-NESS PROBLEM . experimental results demonstrated that the proposed STATISTICAL PARAMETRIC APPROACH improved the quality of the SYNTHESIZED SINGING VOICES . \n",
            "this paper presents a novel STATISTICAL PARAMETRIC APPROACH for SINGING VOICE SYNTHESIS . the proposed STATISTICAL PARAMETRIC APPROACH is based on a STATISTICAL PARAMETRIC APPROACH for SINGING VOICE SYNTHESIS . the proposed STATISTICAL PARAMETRIC APPROACH is based on a STATISTICAL PARAMETRIC APPROACH for SINGING VOICE SYNTHESIS . the proposed STATISTICAL PARAMETRIC APPROACH is based on the use of HIDDEN MARKOV MODELS for SINGING VOICE SYNTHESIS . the proposed STATISTICAL PARAMETRIC APPROACH is applied to SINGING VOICE DATA , and the experimental results demonstrate the effectiveness of the proposed STATISTICAL PARAMETRIC APPROACH .\n",
            "\n",
            "169 1000\n",
            "this paper presents a method for constructing DE-TERMINISTIC PROLOG PARSERS from CORPORA OF PARSED SENTENCES . our approach uses recent MACHINE LEARNING METHODS for inducing PROLOG RULES from examples -lrb- INDUCTIVE LOGIC PROGRAMMING -rrb- . we discuss several advantages of this method compared to recent STATISTICAL METHODS and present results on learning complete PARSERS from portions of the ATIS CORPUS . \n",
            "this paper addresses the problem of INDUCTIVE LOGIC PROGRAMMING in the context of DE-TERMINISTIC PROLOG PARSERS . we propose a novel approach to the problem of INDUCTIVE LOGIC PROGRAMMING , which is based on the use of STATISTICAL METHODS . the proposed approach is based on the use of STATISTICAL METHODS to model the CORPORA OF PARSED SENTENCES . the proposed approach is based on the use of STATISTICAL METHODS , which can be used in conjunction with a DE-TERMINISTIC PROLOG PARSERS . experimental results on the ATIS CORPUS show that the proposed approach is able to achieve the same performance as the current state of the art .\n",
            "\n",
            "170 1000\n",
            "we pose UNSEEN VIEW SYNTHESIS as a PROBABILISTIC TENSOR COMPLETION PROBLEM . given images of people organized by their ROUGH VIEWPOINT , we form a 3D APPEARANCE TENSOR indexed by images -lrb- pose examples -rrb- , VIEWPOINTS , and IMAGE POSITIONS . after discovering the LOW-DIMENSIONAL LATENT FACTORS that approximate that tensor , we can impute its missing entries . in this way , we generate novel SYNTHETIC VIEWS OF PEOPLE -- even when they are observed from just one CAMERA VIEWPOINT . we show that the INFERRED VIEWS are both visually and quantitatively accurate . furthermore , we demonstrate their value for recognizing actions in UNSEEN VIEWS and ESTIMATING VIEWPOINT in novel images . while existing methods are often forced to choose between data that is either realistic or multi-view , our VIRTUAL VIEWS offer both , thereby allowing greater ROBUSTNESS to viewpoint in novel images . \n",
            "this paper addresses the problem of ESTIMATING VIEWPOINT in the presence of VIEWPOINTS and IMAGE POSITIONS . we propose a method to estimate the IMAGE POSITIONS of a scene from a single CAMERA VIEWPOINT . the PROBABILISTIC TENSOR COMPLETION PROBLEM is formulated as a PROBABILISTIC TENSOR COMPLETION PROBLEM and solved using a PROBABILISTIC TENSOR COMPLETION PROBLEM . the proposed method is based on a PROBABILISTIC TENSOR COMPLETION PROBLEM and the PROBABILISTIC TENSOR COMPLETION PROBLEM is formulated as a PROBABILISTIC TENSOR COMPLETION PROBLEM . the proposed approach is based on a PROBABILISTIC TENSOR COMPLETION PROBLEM and is able to deal with UNSEEN VIEWS and VIEWPOINTS . the proposed approach is evaluated on a variety of SYNTHETIC VIEWS OF PEOPLE and VIEWPOINTS .\n",
            "\n",
            "171 1000\n",
            "in this paper , we present a KERNEL-BASED APPROACH to the CLUSTERING OF DIFFUSION TENSORS and FIBER TRACTS . we propose to use a MERCER KERNEL over the TENSOR SPACE where both SPATIAL AND DIFFUSION INFORMATION are taken into account . this MERCER KERNEL highlights implicitly the connectivity along FIBER TRACTS . TENSOR SEGMENTATION is performed using KERNEL-PCA compounded with a LANDMARK-ISOMAP EMBEDDING and K-MEANS CLUSTERING . based on a SOFT FIBER REPRESENTATION , we extend the TENSOR KERNEL to deal with FIBER TRACTS using the MULTI-INSTANCE KERNEL that reflects not only interactions between points along FIBER TRACTS , but also the interactions between DIFFUSION TENSORS . this KERNEL-BASED APPROACH is further extended by way of an ATLAS-BASED REGISTRATION OF DIFFUSION-FREE IMAGES , followed by a CLASSIFICATION OF FIBERS based on NONLINEAR KERNEL SUPPORT VECTOR MACHINES . promising experimental results of tensor and <unk> classification of the human skeletal muscle over a significant set of HEALTHY AND DISEASED SUBJECTS demonstrate the potential of our KERNEL-BASED APPROACH . \n",
            "this paper proposes a novel KERNEL-BASED APPROACH for ATLAS-BASED REGISTRATION OF DIFFUSION-FREE IMAGES . the proposed KERNEL-BASED APPROACH is based on the use of a MULTI-INSTANCE KERNEL and K-MEANS CLUSTERING . the proposed KERNEL-BASED APPROACH is based on the use of a MULTI-INSTANCE KERNEL and K-MEANS CLUSTERING . the proposed KERNEL-BASED APPROACH is based on the use of NONLINEAR KERNEL SUPPORT VECTOR MACHINES and K-MEANS CLUSTERING to estimate the FIBER TRACTS . the proposed KERNEL-BASED APPROACH is based on the use of NONLINEAR KERNEL SUPPORT VECTOR MACHINES and K-MEANS CLUSTERING . the experimental results show that the proposed KERNEL-BASED APPROACH is robust to FIBER TRACTS and FIBER TRACTS . the proposed KERNEL-BASED APPROACH is applied to the ATLAS-BASED REGISTRATION OF DIFFUSION-FREE IMAGES , and the results show that the proposed KERNEL-BASED APPROACH is effective in ATLAS-BASED REGISTRATION OF DIFFUSION-FREE IMAGES .\n",
            "\n",
            "172 1000\n",
            "a QUERY SPELLER is crucial to SEARCH ENGINE in improving WEB SEARCH RELEVANCE . this paper describes novel methods for use of DISTRIBUTIONAL SIMILARITY estimated from QUERY LOGS in learning improved QUERY SPELLING CORRECTION MODELS . the key to our methods is the property of DIS-TRIBUTIONAL SIMILARITY between two terms : it is high between a frequently occurring <unk> and its CORRECTION , and low between two irrelevant terms only with similar spellings . we present two models that are able to take advantage of this property . experimental results demonstrate that the DISTRIBUTIONAL SIMILARITY BASED MODELS can significantly outper-form their baseline systems in the WEB QUERY SPELLING CORRECTION TASK . \n",
            "in this paper , we propose a novel approach to CORRECTION based on QUERY LOGS . the proposed approach is based on the use of a QUERY SPELLER , which is able to deal with QUERY LOGS . the proposed approach is based on the use of QUERY LOGS in the form of a SEARCH ENGINE . the proposed approach is based on the use of QUERY LOGS for CORRECTION . the proposed approach is evaluated on a WEB QUERY SPELLING CORRECTION TASK . the experimental results show that the proposed method outperforms the state-of-the-art methods in terms of CORRECTION and CORRECTION .\n",
            "\n",
            "173 1000\n",
            "sparse learning framework , which is very popular in the field of NATURE LANGUAGE PROCESSING recently due to the advantages of efficiency and GENERALIZABILITY , can be applied to CONDITIONAL RANDOM FIELDS with L1 REGULARIZATION METHOD . STOCHASTIC GRADIENT DESCENT METHOD has been used in training L1-REGULARIZED CRFS , because STOCHASTIC GRADIENT DESCENT METHOD often requires much less TRAINING TIME than the BATCH TRAINING ALGORITHM like QUASI-NEWTON METHOD in practice . nevertheless , STOCHASTIC GRADIENT DESCENT METHOD sometimes fails to converge to the optimum , and STOCHASTIC GRADIENT DESCENT METHOD can be very sensitive to the LEARNING RATE PARAMETER SETTINGS . we present a TWO-STAGE TRAINING ALGORITHM which guarantees the CONVERGENCE , and use HEURIS-TIC LINE SEARCH STRATEGY to make the first stage of STOCHASTIC GRADIENT DESCENT METHOD more robust and stable . experimental evaluations on CHINESE WORD SEGMENTATION and NAME ENTITY RECOGNITION TASKS demonstrate that our TWO-STAGE TRAINING ALGORITHM can produce more accurate and compact model with less TRAINING TIME for L1 REGULARIZATION . \n",
            "this paper addresses the problem of CHINESE WORD SEGMENTATION and NAME ENTITY RECOGNITION TASKS . in this paper , we propose a novel TWO-STAGE TRAINING ALGORITHM for CHINESE WORD SEGMENTATION . the proposed SPARSE LEARNING FRAMEWORK is based on a STOCHASTIC GRADIENT DESCENT METHOD , which is a generalization of the STOCHASTIC GRADIENT DESCENT METHOD to the NATURE LANGUAGE PROCESSING . the proposed STOCHASTIC GRADIENT DESCENT METHOD is based on a STOCHASTIC GRADIENT DESCENT METHOD , which is a generalization of the standard BATCH TRAINING ALGORITHM , which is based on the STOCHASTIC GRADIENT DESCENT METHOD . the proposed STOCHASTIC GRADIENT DESCENT METHOD is applied to CHINESE WORD SEGMENTATION and NAME ENTITY RECOGNITION TASKS . experimental results on CHINESE WORD SEGMENTATION and NAME ENTITY RECOGNITION TASKS show that the proposed TWO-STAGE TRAINING ALGORITHM is effective in improving the performance of CHINESE WORD SEGMENTATION and NAME ENTITY RECOGNITION TASKS .\n",
            "\n",
            "174 1000\n",
            "in this paper we investigate the problem of CHANNEL TRACKING and DETECTION for MIMO-OFDM SYSTEMS over fast varying channels obeying a GAUSS-MARKOV MODEL . we consider TIME DOMAIN TRACKING of the CHANNEL MATRIX TAPS with kalman ¿ <unk> , whereas SYMBOLS DETECTION is carried out by a ZERO-FORCING SOFT DETECTOR . a key assumption of the theory of KALMAN ¿ LTER is that the GAUSS-MARKOV MODEL is perfectly known , while COMMUNICATION SYSTEMS make use of the DETECTED SYMBOLS as an input to the KALMAN ¿ LTER in order to form a suitable GAUSS-MARKOV MODEL . this gives rise to ERROR PROPAGATION due to MIS-DETECTED SYMBOLS -lrb- model mismatch -rrb- and is usually solved by using frequently inserted pilot symbols , resulting in a REDUCED SPECTRAL EF ¿ CIENCY . to overcome this problem , we suggest a novel approach to mitigate the ERROR PROPAGATION due to MIS-DETECTIONS without using frequent pilot symbols . in particular , we consider the reliability of the detections based on the SOFT DETECTOR and use only those outputs that have robust reliability to track the CHANNEL MATRIX TAPS , minimizing the effect of KALMAN ¿ LTER MISMODELING . this method can signi ¿ cantly reduce the ERROR PROPAGATION EFFECT , leading to an improved BIT ERROR PROBABILITY . \n",
            "in this paper , we propose a novel approach to CHANNEL TRACKING in MIMO-OFDM SYSTEMS . the proposed approach is based on the use of a GAUSS-MARKOV MODEL , which is a GAUSS-MARKOV MODEL for MIMO-OFDM SYSTEMS . the proposed approach is based on the use of a GAUSS-MARKOV MODEL to estimate the CHANNEL MATRIX TAPS of the DETECTED SYMBOLS . the proposed method is based on a GAUSS-MARKOV MODEL , which is based on a GAUSS-MARKOV MODEL . the proposed method is based on a GAUSS-MARKOV MODEL , which is based on a GAUSS-MARKOV MODEL . the proposed method is applied to MIMO-OFDM SYSTEMS , which is based on a GAUSS-MARKOV MODEL . the proposed method is evaluated in terms of the BIT ERROR PROBABILITY , and the performance of the proposed method is evaluated on a REDUCED SPECTRAL EF ¿ CIENCY . the results show that the proposed method is effective in reducing the ERROR PROPAGATION EFFECT and the ERROR PROPAGATION EFFECT of the proposed method .\n",
            "\n",
            "175 1000\n",
            "we introduce a new CLASS OF DISTINGUISHED REGIONS based on detecting the most salient CONVEX LOCAL ARRANGEMENTS OF CONTOURS in the image . the regions are used in a similar way to the LOCAL INTEREST POINTS extracted from GRAY-LEVEL IMAGES , but they capture shape rather than texture . LOCAL CONVEXITY is characterized by measuring the extent to which the detected image contours support circle or <unk> local structures at each position and scale in the image . our CLASS OF DISTINGUISHED REGIONS combines two COST FUNCTIONS defined on the TANGENTIAL EDGES near the circle : a TANGENTIAL-GRADIENT ENERGY TERM , and an ENTROPY TERM that ensures LOCAL SUPPORT from a wide range of ANGULAR POSITIONS around the circle . the DETECTED REGIONS are invariant to SCALE CHANGES and ROTATIONS , and robust against CLUTTER , OCCLUSIONS and SPURIOUS EDGE DETECTIONS . experimental results show very good performance for both SHAPE MATCHING and recognition of object categories . \n",
            "this paper addresses the problem of SHAPE MATCHING in GRAY-LEVEL IMAGES . we propose a method to estimate the ANGULAR POSITIONS of a scene from a set of LOCAL INTEREST POINTS . the proposed method consists of two steps : -lrb- 1 -rrb- a set of LOCAL INTEREST POINTS , and -lrb- 2 -rrb- a set of COST FUNCTIONS to estimate the ANGULAR POSITIONS . the proposed method is based on the use of LOCAL CONVEXITY and SPURIOUS EDGE DETECTIONS . the proposed approach is based on the use of LOCAL CONVEXITY and SPURIOUS EDGE DETECTIONS . the proposed method is evaluated on a variety of GRAY-LEVEL IMAGES and GRAY-LEVEL IMAGES . the results show that the proposed method is robust to OCCLUSIONS , OCCLUSIONS , and SCALE CHANGES .\n",
            "\n",
            "176 1000\n",
            "large-scale 1-regularized loss minimization problems arise in HIGH-DIMENSIONAL APPLICATIONS such as COMPRESSED SENSING and HIGH-DIMENSIONAL SUPERVISED LEARNING , including CLASSIFICATION and REGRESSION PROBLEMS . HIGH-PERFORMANCE ALGORITHMS and implementations are critical to efficiently solving these problems . building upon previous work on COORDINATE DESCENT ALGORITHMS for 1-REGULARIZED PROBLEMS , we introduce a novel family of algorithms called BLOCK-GREEDY COORDINATE DESCENT that includes , as special cases , several existing algorithms such as SCD , GREEDY CD , SHOTGUN , and THREAD-GREEDY . we give a UNIFIED CONVERGENCE ANALYSIS for the family of BLOCK-GREEDY ALGORITHMS . the analysis suggests that BLOCK-GREEDY COORDINATE DESCENT can better exploit PARALLELISM if FEATURES are clustered so that the MAXIMUM INNER PRODUCT between FEATURES in different blocks is small . our THEORETICAL CONVERGENCE ANALYSIS is supported with experimental results using data from diverse REAL-WORLD APPLICATIONS . we hope that algorithmic approaches and CONVERGENCE ANALYSIS we provide will not only advance the field , but will also encourage researchers to systematically explore the design space of algorithms for solving LARGE-SCALE 1-REGULARIZATION PROBLEMS . \n",
            "many LARGE-SCALE 1-REGULARIZATION PROBLEMS such as COMPRESSED SENSING , SHOTGUN , SHOTGUN , and COMPRESSED SENSING , have been widely used in many applications , such as COMPRESSED SENSING , SHOTGUN , SHOTGUN , and LARGE-SCALE 1-REGULARIZATION PROBLEMS . in this paper , we propose a novel approach to LARGE-SCALE 1-REGULARIZED LOSS MINIMIZATION PROBLEMS , which is based on the idea of BLOCK-GREEDY COORDINATE DESCENT , which is a generalization of the existing BLOCK-GREEDY ALGORITHMS . we show that the proposed THEORETICAL CONVERGENCE ANALYSIS can be applied to LARGE-SCALE 1-REGULARIZED LOSS MINIMIZATION PROBLEMS , such as BLOCK-GREEDY COORDINATE DESCENT , SHOTGUN , and COMPRESSED SENSING . we show that the proposed algorithm can be applied to LARGE-SCALE 1-REGULARIZED LOSS MINIMIZATION PROBLEMS , including SHOTGUN , SHOTGUN , SHOTGUN , and SHOTGUN .\n",
            "\n",
            "177 1000\n",
            "the social nature of laughter <unk> people to <unk> together . this JOINT VOCAL ACTION often results in OVERLAPPING LAUGHTER . in this paper , we show that the ACOUSTICS OF OVERLAPPING LAUGHS are different from NON-OVERLAPPING LAUGHS . we found that overlapping <unk> are stronger prosodically marked than NON-OVERLAPPING ONES , in terms of higher values for duration , mean f0 , mean and MAXIMUM INTENSITY , and the amount of voicing . this effect is <unk> by the number of people joining in the LAUGHTER EVENT , which suggests that ENTRAINMENT is at work . we also found that GROUP SIZE affects the number of overlapping <unk> which illustrates the CONTAGIOUS NATURE OF LAUGHTER . finally , people appear to join laughter simultaneously at a delay of approximately 500 ms ; a delay that must be considered when developing SPOKEN DIALOGUE SYSTEMS that are able to respond to users ' <unk> . \n",
            "in this paper , we present a novel approach to the problem of OVERLAPPING LAUGHTER in SPOKEN DIALOGUE SYSTEMS . the proposed approach is based on a ACOUSTICS OF OVERLAPPING LAUGHS , which is based on a ACOUSTICS OF OVERLAPPING LAUGHS . the proposed approach is based on a ACOUSTICS OF OVERLAPPING LAUGHS , which is based on a ACOUSTICS OF OVERLAPPING LAUGHS . the proposed method is based on a ACOUSTICS OF OVERLAPPING LAUGHS , which is based on a ACOUSTICS OF OVERLAPPING LAUGHS . the proposed method is based on a ACOUSTICS OF OVERLAPPING LAUGHS , which is based on a ACOUSTICS OF OVERLAPPING LAUGHS . the proposed method is evaluated on a variety of SPOKEN DIALOGUE SYSTEMS . the results show that the proposed method is robust to OVERLAPPING LAUGHTER , and is robust to OVERLAPPING LAUGHTER .\n",
            "\n",
            "178 1000\n",
            "a HALF-DUPLEX DISTRIBUTED BEAMFORMING TECHNIQUE for RELAY NETWORKS with FREQUENCY SELECTIVE FADING CHANNELS is developed . the NETWORK RELAYS use the FILTER-AND-FORWARD STRATEGY to compensate for the TRANSMITTER-TO-RELAY AND RELAY-TO-DESTINATION CHANNELS using FINITE IMPULSE RESPONSE FILTERS . with the channel state information -lrb- csi -rrb- being available at the RECEIVER , the TRANSMIT RELAY POWER is minimized subject to the DESTINATION QUALITY-OF-SERVICE CONSTRAINT . this DISTRIBUTED BEAMFORMING PROBLEM is shown to have a CLOSED-FORM SOLUTION . simulation results demonstrate substantial improvements in terms of the RELAY TRANSMITTED POWER and feasibility of the DESTINATION QOS CONSTRAINT as compared to AMPLIFY-AND-FORWARD DISTRIBUTED BEAMFORMING TECHNIQUES . \n",
            "this paper presents a novel HALF-DUPLEX DISTRIBUTED BEAMFORMING TECHNIQUE for RELAY NETWORKS . the proposed HALF-DUPLEX DISTRIBUTED BEAMFORMING TECHNIQUE is based on a FILTER-AND-FORWARD STRATEGY to the DISTRIBUTED BEAMFORMING PROBLEM . the proposed HALF-DUPLEX DISTRIBUTED BEAMFORMING TECHNIQUE is based on a FILTER-AND-FORWARD STRATEGY to the DISTRIBUTED BEAMFORMING PROBLEM . the proposed HALF-DUPLEX DISTRIBUTED BEAMFORMING TECHNIQUE is based on a FILTER-AND-FORWARD STRATEGY , which is based on the DESTINATION QOS CONSTRAINT of the NETWORK RELAYS . the proposed HALF-DUPLEX DISTRIBUTED BEAMFORMING TECHNIQUE is based on the DESTINATION QOS CONSTRAINT of the NETWORK RELAYS . the proposed HALF-DUPLEX DISTRIBUTED BEAMFORMING TECHNIQUE is applied to the TRANSMITTER-TO-RELAY AND RELAY-TO-DESTINATION CHANNELS , and the results show that the proposed HALF-DUPLEX DISTRIBUTED BEAMFORMING TECHNIQUE is effective in reducing the RELAY TRANSMITTED POWER .\n",
            "\n",
            "179 1000\n",
            "our goal in this work was to develop an accurate method to identify LAUGHTER SEGMENTS , ultimately for the purpose of SPEAKER RECOGNITION . our previous work used MLPS to perform FRAME LEVEL DETECTION OF LAUGHTER using SHORT-TERM FEATURES , including MFCCS and pitch , and achieved a 7.9 % EER on our test set . we improved upon our previous results by including HIGH-LEVEL AND LONG-TERM FEATURES , MEDIAN FILTERING , and performing segmentation via a HYBRID MLP/HMM SYSTEM with VITERBI DECODING . upon including the LONG-TERM FEATURES and MEDIAN FILTERING , our results improved to 5.4 % EER on our test set and 2.7 % EER on an EQUAL-PRIOR TEST SET used by others . after attaining segmentation results by incorporating the HYBRID MLP/HMM SYSTEM and VITERBI DECODING , we had a <unk> % PRECISION RATE and <unk> % RECALL RATE on our test set . to our knowledge these are the best known LAUGHTER DETECTION results on the ICSI MEETING RECORDER CORPUS to date . \n",
            "in this paper , we propose a novel approach to SPEAKER RECOGNITION in a HYBRID MLP/HMM SYSTEM . the proposed approach is based on the use of SHORT-TERM FEATURES extracted from the MLPS . the proposed approach is based on the use of SHORT-TERM FEATURES extracted from the MLPS . the proposed method is evaluated on the ICSI MEETING RECORDER CORPUS , and the results show that the proposed method is effective in improving PRECISION RATE and RECALL RATE . the proposed method is evaluated on the ICSI MEETING RECORDER CORPUS , and the results show that the proposed method is effective in reducing the PRECISION RATE and RECALL RATE . the proposed method is evaluated on the ICSI MEETING RECORDER CORPUS , and the results show that the proposed method is effective in improving the PRECISION RATE and RECALL RATE .\n",
            "\n",
            "180 1000\n",
            "we propose a simple NEURAL ARCHITECTURE for NATURAL LANGUAGE INFERENCE . our NEURAL ARCHITECTURE uses attention to decompose the problem into <unk> that can be solved separately , thus making it trivially parallelizable . on the STANFORD NATURAL LANGUAGE INFERENCE DATASET , we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any WORD-ORDER INFORMATION . adding INTRA-SENTENCE ATTENTION that takes a minimum amount of order into account yields further improvements . \n",
            "this paper presents a novel NEURAL ARCHITECTURE for NATURAL LANGUAGE INFERENCE . the proposed NEURAL ARCHITECTURE is based on the use of a set of WORD-ORDER INFORMATION , which are then used to estimate the WORD-ORDER INFORMATION . the performance of the proposed NEURAL ARCHITECTURE is evaluated on the STANFORD NATURAL LANGUAGE INFERENCE DATASET , and the results show that the proposed method is effective in improving the performance of NATURAL LANGUAGE INFERENCE .\n",
            "\n",
            "181 1000\n",
            "we identify and study two types of '' ACCIDENTAL '' IMAGES that can be formed in scenes . the first is an ACCIDENTAL PIN-HOLE CAMERA IMAGE . these images are often mistaken for SHADOWS , but can reveal structures outside a room , or the unseen shape of the light APERTURE into the room . the second class of ACCIDENTAL '' IMAGES are '' inverse '' <unk> camera images , formed by subtracting an image with a small <unk> present from a REFERENCE IMAGE without the <unk> . the REFERENCE IMAGE can be an earlier frame of a VIDEO SEQUENCE . both types of ACCIDENTAL '' IMAGES happen in a variety of different situations -lrb- an INDOOR SCENE illuminated by NATURAL LIGHT , a street with a person walking under the shadow of a building , etc. -rrb- . ACCIDENTAL CAMERAS can reveal information about the scene outside the image , the LIGHTING CONDITIONS , or the APERTURE by which light enters the scene . \n",
            "this paper presents a novel method for ACCIDENTAL '' IMAGES in the presence of SHADOWS . the method is based on the use of a ACCIDENTAL PIN-HOLE CAMERA IMAGE and a ACCIDENTAL PIN-HOLE CAMERA IMAGE to estimate the APERTURE of the REFERENCE IMAGE . the method is based on the use of a ACCIDENTAL PIN-HOLE CAMERA IMAGE and a set of ACCIDENTAL '' IMAGES . the results show that the proposed method is able to recover the APERTURE of a scene from the REFERENCE IMAGE .\n",
            "\n",
            "182 1000\n",
            "we address the DETECTION OF VEHICLES in a VIDEO STREAM obtained from a MOVING AIRBORNE PLATFORM . our approach is based on robust OPTICAL FLOW ALGORITHM applied on STABILIZED FRAMES . STABILIZATION OF THE FRAMES compensates for GROSS AFFINE BACKGROUND MOTION prior to running robust optical flow to compute DENSE RESIDUAL FLOW . based on the flow and the previous BACKGROUND APPEARANCE MODEL , the new frame is separated into background and foreground oc-clusion layers using an EM-BASED MOTION SEGMENTATION . the proposed framework shows that GROUND VEHICLES can be detected and segmented from AIRBORNE VIDEO SEQUENCES while building a MOSAIC OF THE BACKGROUND LAYER . \n",
            "this paper presents a novel method for DETECTION OF VEHICLES from AIRBORNE VIDEO SEQUENCES . the proposed method is based on a MOSAIC OF THE BACKGROUND LAYER , which is based on a BACKGROUND APPEARANCE MODEL . the proposed method is based on a MOSAIC OF THE BACKGROUND LAYER , which is based on a BACKGROUND APPEARANCE MODEL . the proposed method is based on a MOSAIC OF THE BACKGROUND LAYER , which is based on the BACKGROUND APPEARANCE MODEL . the proposed method is based on a MOSAIC OF THE BACKGROUND LAYER , which is based on a BACKGROUND APPEARANCE MODEL . the proposed method is based on a MOSAIC OF THE BACKGROUND LAYER , which is based on the EM-BASED MOTION SEGMENTATION . the proposed method is evaluated using a MOVING AIRBORNE PLATFORM . the results show that the proposed method is able to detect and track moving objects in a VIDEO STREAM .\n",
            "\n",
            "183 1000\n",
            "the traditional PARTIAL-MODEL SELECTION SEARCHING METHOD for MODEL-ORDER SELECTION in LINEAR REGRESSION is a NESTED FULL-PARAMETERS-SET SEARCHING PROCEDURE over the desired orders , which we call FULL-MODEL ORDER SELECTION . on the other hand , a method for MODEL-SELECTION SEARCHES for the best <unk> within each order . in this paper , we propose using the PARTIAL-MODEL SELECTION SEARCHING METHOD for MODEL-ORDER SELECTION , which we call FULL-MODEL ORDER SELECTION . we show by simulations that the proposed PARTIAL-MODEL SELECTION SEARCHING METHOD gives better ACCURACIES than the traditional one , especially for LOW SIGNAL-TO-NOISE RATIOS over a wide range of MODEL-ORDER SELECTION CRITERIA -lrb- both information <unk> and <unk> -rrb- . also , we show that for some models the performance of the BOOTSTRAP-BASED CRITERION improves significantly by using the proposed PARTIAL-MODEL SELECTION SEARCHING METHOD . \n",
            "this paper presents a novel PARTIAL-MODEL SELECTION SEARCHING METHOD for LINEAR REGRESSION . the proposed PARTIAL-MODEL SELECTION SEARCHING METHOD is based on the use of a set of MODEL-ORDER SELECTION CRITERIA , each of which is a set of MODEL-ORDER SELECTION CRITERIA . the proposed PARTIAL-MODEL SELECTION SEARCHING METHOD is applied to the problem of MODEL-ORDER SELECTION . the proposed PARTIAL-MODEL SELECTION SEARCHING METHOD is applied to the problem of MODEL-ORDER SELECTION , and the results show that the proposed PARTIAL-MODEL SELECTION SEARCHING METHOD is effective in improving the ACCURACIES of the proposed PARTIAL-MODEL SELECTION SEARCHING METHOD .\n",
            "\n",
            "184 1000\n",
            "many REAL-WORLD DECISION-THEORETIC PLANNING PROBLEMS are naturally modeled using both CONTINUOUS STATE AND ACTION SPACES , yet little work has provided exact solutions for the case of continuous actions . in this work , we propose a SYMBOLIC DYNAMIC PROGRAMMING SOLUTION to obtain the optimal CLOSED-FORM VALUE FUNCTION and POLICY for CSA-MDPS with MUL-TIVARIATE CONTINUOUS STATE AND ACTIONS , DISCRETE NOISE , PIECEWISE LINEAR DYNAMICS , and piecewise linear -lrb- or RESTRICTED PIECEWISE QUADRATIC -RRB- REWARD . our key contribution over previous SDP work is to show how the CONTINUOUS ACTION MAXIMIZATION STEP in the DYNAMIC PROGRAMMING BACKUP can be evaluated optimally and <unk> -- a task which amounts to SYMBOLIC CONSTRAINED OPTIMIZATION subject to UNKNOWN STATE PARAMETERS ; we further integrate this technique to work with an efficient and compact data structure for SDP -- the extended algebraic decision diagram -lrb- <unk> -rrb- . we demonstrate empirical results on a DIDACTIC NONLINEAR PLANNING EXAMPLE and two domains from operations research to show the first AUTOMATED EXACT SOLUTION to these problems . \n",
            "this paper addresses the problem of MUL-TIVARIATE CONTINUOUS STATE AND ACTIONS in a CONTINUOUS ACTION MAXIMIZATION STEP . in particular , we consider the problem of MUL-TIVARIATE CONTINUOUS STATE AND ACTIONS , MUL-TIVARIATE CONTINUOUS STATE AND ACTIONS , MUL-TIVARIATE CONTINUOUS STATE AND ACTIONS , and RESTRICTED PIECEWISE QUADRATIC -RRB- REWARD . in this paper , we propose a novel method to estimate the UNKNOWN STATE PARAMETERS of the SDP , which is based on a DYNAMIC PROGRAMMING BACKUP . the proposed SYMBOLIC DYNAMIC PROGRAMMING SOLUTION is based on a DYNAMIC PROGRAMMING BACKUP . the proposed SYMBOLIC DYNAMIC PROGRAMMING SOLUTION is based on a DYNAMIC PROGRAMMING BACKUP . the proposed AUTOMATED EXACT SOLUTION is based on a DYNAMIC PROGRAMMING BACKUP . the proposed SYMBOLIC DYNAMIC PROGRAMMING SOLUTION is applied to the problem of MUL-TIVARIATE CONTINUOUS STATE AND ACTIONS , MUL-TIVARIATE CONTINUOUS STATE AND ACTIONS , and RESTRICTED PIECEWISE QUADRATIC -RRB- REWARD .\n",
            "\n",
            "185 1000\n",
            "we enrich a <unk> resource of COMMON-SENSE KNOWLEDGE by formulating the problem as one of KNOWLEDGE BASE COMPLETION . most work in KNOWLEDGE BASE COMPLETION focuses on KNOWLEDGE BASES like FREEBASE that relate entities drawn from a fixed set . however , the tuples in CONCEPTNET -lrb- <unk> and <unk> , 2012 -rrb- define relations between an unbounded set of phrases . we develop NEURAL NETWORK MODELS for scoring tuples on arbitrary phrases and evaluate NEURAL NETWORK MODELS by their ability to distinguish TRUE HELD-OUT TUPLES from false ones . we find strong performance from a BILINEAR MODEL using a simple ADDITIVE ARCHITECTURE to BILINEAR MODEL phrases . we manually evaluate our trained BILINEAR MODEL 's ability to assign quality scores to novel tuples , finding that BILINEAR MODEL can propose TU-PLES at the same quality level as MEDIUM-CONFIDENCE TUPLES from CONCEPTNET . \n",
            "in this paper , we propose a novel BILINEAR MODEL for KNOWLEDGE BASE COMPLETION . the proposed BILINEAR MODEL is based on the use of a BILINEAR MODEL for KNOWLEDGE BASE COMPLETION . the proposed BILINEAR MODEL is based on the use of a BILINEAR MODEL , which is a BILINEAR MODEL of the BILINEAR MODEL . the proposed BILINEAR MODEL is based on the use of a BILINEAR MODEL , which is a BILINEAR MODEL of the BILINEAR MODEL . the proposed BILINEAR MODEL is applied to the problem of KNOWLEDGE BASE COMPLETION . the experimental results show that the proposed BILINEAR MODEL is effective in reducing the number of KNOWLEDGE BASES , and to the best of our knowledge .\n",
            "\n",
            "186 1000\n",
            "using a recently developed REAL TIME AUDIO CAMERA , that uses the output of a SPHERICAL MICROPHONE ARRAY BEAMFORMER steered in all directions to create CENTRAL PROJECTION to create ACOUSTIC INTENSITY IMAGES , we present a technique to measure the ACOUSTICS OF ROOMS AND HALLS . a PANORAMIC MOSAICED VISUAL IMAGE OF THE SPACE is also create . since both the VISUAL AND THE AUDIO CAMERA IMAGES are CENTRAL PROJECTION , REGISTRATION of the acquired AUDIO AND VIDEO IMAGES can be performed using standard COMPUTER VISION TECHNIQUES . we describe the technique , and apply it to the examine the relation between ACOUSTICAL FEATURES and architectural details of the <unk> concert hall at the <unk> smith performing arts center in <unk> <unk> , <unk> . \n",
            "this paper addresses the problem of REGISTRATION from a REAL TIME AUDIO CAMERA . we propose a novel approach to REGISTRATION based on a SPHERICAL MICROPHONE ARRAY BEAMFORMER . the proposed approach is based on a SPHERICAL MICROPHONE ARRAY BEAMFORMER , which is based on a SPHERICAL MICROPHONE ARRAY BEAMFORMER . the proposed approach is based on a SPHERICAL MICROPHONE ARRAY BEAMFORMER , which is based on a SPHERICAL MICROPHONE ARRAY BEAMFORMER . the proposed approach is evaluated on a REAL TIME AUDIO CAMERA , and the results show that the proposed method is effective in improving the ACOUSTICS OF ROOMS AND HALLS .\n",
            "\n",
            "187 1000\n",
            "<unk> audio IMMERSIVE AUDIO SYSTEMS are being envisioned for applications that include TELECONFERENCING and <unk> ; AUGMENTED AND VIRTUAL REALITY for MANUFACTURING AND ENTERTAINMENT ; AIR TRAFFIC CONTROL , PILOT WARNING , and GUIDANCE SYSTEMS ; DISPLAYS for the <unk> ; DISTANCE LEARNING ; and professional sound and picture editing for television and film . the principal function of such IMMERSIVE AUDIO SYSTEMS is to synthesize , manipulate and render SOUND FIELDS in real time . in this paper we examine several SIGNAL PROCESSING CONSIDERATIONS in SPATIAL SOUND RENDERING over loudspeakers . we propose two methods that can be used to implement the necessary filters for generating VIRTUAL SOUND SOURCES based on SYNTHETIC HEAD-RELATED TRANSFER FUNCTIONS with the same SPECTRAL CHARACTERISTICS as those of the REAL SOURCE . \n",
            "this paper presents a novel approach to SPATIAL SOUND RENDERING in IMMERSIVE AUDIO SYSTEMS that combines AUGMENTED AND VIRTUAL REALITY and IMMERSIVE AUDIO SYSTEMS . the proposed approach is based on the use of SYNTHETIC HEAD-RELATED TRANSFER FUNCTIONS , PILOT WARNING , DISTANCE LEARNING and GUIDANCE SYSTEMS . the proposed approach is based on the use of SYNTHETIC HEAD-RELATED TRANSFER FUNCTIONS , PILOT WARNING , DISTANCE LEARNING , and GUIDANCE SYSTEMS . the proposed approach is evaluated on a REAL SOURCE and a REAL SOURCE . the results show that the proposed method is able to detect and track moving objects in a scene from a REAL SOURCE .\n",
            "\n",
            "188 1000\n",
            "though both DOCUMENT SUMMARIZATION and KEYWORD EXTRACTION aim to extract concise representations from documents , these two tasks have usually been investigated independently . this paper proposes a novel ITERATIVE REINFORCEMENT APPROACH to simultaneously extracting SUMMARY AND KEYWORDS from SINGLE DOCUMENT under the assumption that the SUMMARY AND KEYWORDS of a document can be mutually boosted . the ITERATIVE REINFORCEMENT APPROACH can naturally make full use of the reinforcement between sentences and KEYWORDS by fusing three kinds of relationships between sentences and words , either homogeneous or heterogeneous . experimental results show the effectiveness of the proposed ITERATIVE REINFORCEMENT APPROACH for both tasks . the ITERATIVE REINFORCEMENT APPROACH is validated to work almost as well as the KNOWLEDGE-BASED APPROACH for COMPUTING WORD SEMANTICS . \n",
            "this paper presents a novel ITERATIVE REINFORCEMENT APPROACH for DOCUMENT SUMMARIZATION and KEYWORD EXTRACTION . the proposed ITERATIVE REINFORCEMENT APPROACH is based on a KNOWLEDGE-BASED APPROACH for KEYWORD EXTRACTION and KEYWORD EXTRACTION . the proposed ITERATIVE REINFORCEMENT APPROACH is based on a KNOWLEDGE-BASED APPROACH for COMPUTING WORD SEMANTICS and KEYWORD EXTRACTION . the proposed ITERATIVE REINFORCEMENT APPROACH is compared to a KNOWLEDGE-BASED APPROACH , and the results show that the proposed ITERATIVE REINFORCEMENT APPROACH is able to detect and track moving objects in a scene .\n",
            "\n",
            "189 1000\n",
            "eeg connectivity measures could provide a new type of FEATURE SPACE for inferring a subject 's intention in BRAIN-COMPUTER INTERFACES . however , very little is known on EEG CONNECTIVITY PATTERNS for BRAIN-COMPUTER INTERFACES . in this study , EEG CONNECTIVITY MEASURES during MOTOR IMAGERY of the left and right is investigated in a broad frequency range across the whole scalp by combining BEAMFORMING with TRANSFER ENTROPY and taking into account possible volume <unk> effects . observed <unk> patterns indicate that MODULATION intentionally induced by MOTOR IMAGERY is strongest in the Γ-BAND , i.e. , above 35 hz . furthermore , MODULATION between MOTOR IMAGERY and rest is found to be more pronounced than between MOTOR IMAGERY of different hands . this is in contrast to results on MOTOR IMAGERY obtained with BANDPOWER FEATURES , and might provide an explanation for the so far only moderate success of CONNECTIVITY FEATURES in BRAIN-COMPUTER INTERFACES . it is concluded that future studies on CONNECTIVITY BASED BCIS should focus on HIGH FREQUENCY BANDS and consider experimental paradigms that maximally vary COGNITIVE DEMANDS between conditions . \n",
            "in this paper , we propose a novel approach to BRAIN-COMPUTER INTERFACES in MOTOR IMAGERY . the proposed method is based on the use of CONNECTIVITY FEATURES , which are used to represent the EEG CONNECTIVITY PATTERNS . the proposed method is based on the use of BANDPOWER FEATURES , MODULATION , MODULATION , MODULATION , and TRANSFER ENTROPY . the proposed method is based on the use of BANDPOWER FEATURES in the FEATURE SPACE . the proposed method is evaluated on a variety of MOTOR IMAGERY .\n",
            "\n",
            "190 1000\n",
            "symbolic bidirectional <unk> SEARCH is a prominent technique for COST-OPTIMAL PLANNING . thus , the question whether it can be further improved by making use of HEURISTIC FUNCTIONS raises naturally . however , the use of HEURISTICS in BIDI-RECTIONAL SEARCH does not always improve its performance . we propose a novel way to use ABSTRACTION HEURISTICS in SYMBOLIC BIDIRECTIONAL SEARCH in which the SEARCH only <unk> to HEURISTICS when it becomes unfeasible . we adapt the definition of PARTIAL AND PERIMETER ABSTRACTIONS to BIDIRECTIONAL SEARCH , where a ⇤ is used to traverse the ABSTRACT STATE SPACES AND/OR generate the PERIMETER . the results show that ABSTRACTION HEURISTICS can further improve SYMBOLIC BIDIRECTIONAL SEARCH in some domains . in fact , the resulting planner , SYMBA ⇤ , was the winner of the <unk> of the last IPC . \n",
            "in this paper , we propose a novel method for COST-OPTIMAL PLANNING . the proposed method is based on the use of PARTIAL AND PERIMETER ABSTRACTIONS for COST-OPTIMAL PLANNING . the proposed approach is based on the use of PARTIAL AND PERIMETER ABSTRACTIONS for BIDIRECTIONAL SEARCH . the proposed approach is based on the use of PARTIAL AND PERIMETER ABSTRACTIONS for COST-OPTIMAL PLANNING . the proposed method is based on the use of PARTIAL AND PERIMETER ABSTRACTIONS for COST-OPTIMAL PLANNING . the proposed method is applied to the problem of COST-OPTIMAL PLANNING . the experimental results show that the proposed method outperforms the existing methods in terms of SEARCH .\n",
            "\n",
            "191 1000\n",
            "we present an UNSUPERVISED ALGORITHM for the discovery of words and WORD-LIKE FRAGMENTS from the SPEECH SIGNAL , without using an UPFRONT DEFINED LEXICON or ACOUSTIC PHONE MODELS . the UNSUPERVISED ALGORITHM is based on a combination of ACOUSTIC PATTERN DISCOVERY , CLUSTERING , and TEMPORAL SEQUENCE LEARNING . UNSUPERVISED ALGORITHM exploits the ACOUSTIC SIMILARITY between multiple acoustic tokens of the same words or WORD-LIKE FRAGMENTS . in its current form , the UNSUPERVISED ALGORITHM is able to discover words in speech with low <unk> -lrb- connected digits -rrb- . although its performance still falls off compared to MAINSTREAM ASR APPROACHES , the value of the UNSUPERVISED ALGORITHM is its potential to serve as a COMPUTATIONAL MODEL in two research directions . first , the UNSUPERVISED ALGORITHM may lead to an approach for SPEECH RECOGNITION that is fundamentally <unk> from the MODELLING CONSTRAINTS in conventional ASR . second , the proposed UNSUPERVISED ALGORITHM can be interpreted as a COMPUTATIONAL MODEL of LANGUAGE ACQUISITION that takes actual speech as input and is able to find words as 'em <unk> ' properties from RAW INPUT . \n",
            "this paper presents a novel UNSUPERVISED ALGORITHM based on ACOUSTIC PATTERN DISCOVERY and TEMPORAL SEQUENCE LEARNING . the proposed COMPUTATIONAL MODEL is based on the use of TEMPORAL SEQUENCE LEARNING and TEMPORAL SEQUENCE LEARNING . the proposed UNSUPERVISED ALGORITHM is based on the use of TEMPORAL SEQUENCE LEARNING and TEMPORAL SEQUENCE LEARNING . the proposed UNSUPERVISED ALGORITHM is based on the use of TEMPORAL SEQUENCE LEARNING and TEMPORAL SEQUENCE LEARNING . the proposed UNSUPERVISED ALGORITHM is based on the use of TEMPORAL SEQUENCE LEARNING and TEMPORAL SEQUENCE LEARNING . the proposed UNSUPERVISED ALGORITHM is applied to SPEECH RECOGNITION , CLUSTERING , and ACOUSTIC PHONE MODELS . the experimental results show that the proposed COMPUTATIONAL MODEL is effective in improving the SPEECH RECOGNITION performance .\n",
            "\n",
            "192 1000\n",
            "we present a novel approach for unsu-pervised induction of a REORDERING GRAMMAR using a modified form of PERMUTATION TREES -lrb- zhang and <unk> , 2007 -rrb- , which we apply to PREORDERING in PHRASE-BASED MACHINE TRANSLATION . unlike previous approaches , we induce in one step both the HIERARCHICAL STRUCTURE and the TRANSDUCTION FUNCTION over it from WORD-ALIGNED PARALLEL CORPORA . furthermore , our model -lrb- 1 -rrb- handles NON-ITG REORDERING PATTERNS -lrb- up to <unk> branching -rrb- , -lrb- 2 -rrb- is learned from all derivations by treating not only LABELING but also BRACKETING as LATENT VARIABLE , -lrb- 3 -rrb- is entirely <unk> at the level of REORDERING RULES , and -lrb- 4 -rrb- requires no LINGUISTIC ANNOTATION . our model is evaluated both for ACCURACY in PREDICTING TARGET ORDER , and for its impact on TRANSLATION QUALITY . we report significant performance gains over PHRASE REORDERING , and over two known PREORDERING BASELINES for ENGLISH-JAPANESE . \n",
            "this paper presents a novel approach to PHRASE-BASED MACHINE TRANSLATION in PHRASE-BASED MACHINE TRANSLATION . the approach is based on the use of a REORDERING GRAMMAR and a REORDERING GRAMMAR . the proposed approach is based on the use of a REORDERING GRAMMAR and a REORDERING GRAMMAR . the proposed approach is based on the use of a REORDERING GRAMMAR and a REORDERING GRAMMAR . the proposed method is based on the use of a REORDERING GRAMMAR and a REORDERING GRAMMAR . the proposed method is evaluated in terms of ACCURACY and ACCURACY . the performance of the proposed method is evaluated on a variety of WORD-ALIGNED PARALLEL CORPORA . the results show that the proposed approach is effective in reducing the TRANSLATION QUALITY and ACCURACY of the proposed method .\n",
            "\n",
            "193 1000\n",
            "this paper describes some improvements in SPEECH RECOGNITION OF BROADCAST NEWS COMMENTARY in JAPANESE . since NEWS COMMENTARY SPEECH has different LINGUISTIC AND ACOUSTIC FEATURES from READ SPEECH , it gives lower WORD RECOGNITION ACCURACY . in this paper we apply to news <unk> some RULES which represent the LINGUISTIC FEATURES OF NEWS COMMENTARIES , and generate WORD SEQUENCES for LANGUAGE MODEL ADAPTATION . we also use a large volume of transcriptions of NEWS PROGRAMS as training texts . ACOUSTIC MODELS are <unk> and their structures are changed so as to recognize relatively short phonemes , because we found the SPEECH RATE of NEWS COMMENTARY is sometimes much faster than that of READ SPEECH . furthermore , by using a DECODER that can handle CROSSWORD TRIPHONE MODELS , we reduced the WORD ERROR RATE by 32 % . \n",
            "this paper presents a novel approach to SPEECH RECOGNITION OF BROADCAST NEWS COMMENTARY based on LANGUAGE MODEL ADAPTATION . the proposed approach is based on the use of RULES extracted from WORD SEQUENCES extracted from WORD SEQUENCES . the proposed approach is based on the use of RULES extracted from WORD SEQUENCES extracted from the LINGUISTIC FEATURES OF NEWS COMMENTARIES . the proposed method is evaluated on a SPEECH RECOGNITION OF BROADCAST NEWS COMMENTARY and compared to a standard DECODER . the results show that the proposed method is effective in improving WORD RECOGNITION ACCURACY compared to READ SPEECH .\n",
            "\n",
            "194 1000\n",
            "this study is a contribution to the field of VISUAL SPEECH PROCESSING . it focuses on the AUTOMATIC EXTRACTION OF SPEECH LIP FEATURES from NATURAL LIPS . the method is based on the direct prediction of these FEATURES from predictors derived from an adequate transformation of the pixels of the lip region of interest . the transformation is made of a 2-D DISCRETE COSINE TRANSFORM combined with a PRINCIPAL COMPONENT ANALYSIS applied to a subset of the DCT COEFFICIENTS corresponding to about 1 % of the total <unk> . the results show the possibility to estimate the GEOMETRIC LIP FEATURES with a good ACCURACY -lrb- a root mean square of 1 to 1.4 mm for the LIP APERTURE and the lip width -rrb- using a reduce set of predictors derived from the PCA . \n",
            "in this paper , we propose a novel method for AUTOMATIC EXTRACTION OF SPEECH LIP FEATURES based on PRINCIPAL COMPONENT ANALYSIS . the proposed approach is based on a 2-D DISCRETE COSINE TRANSFORM , which is based on PRINCIPAL COMPONENT ANALYSIS . the proposed method is based on a 2-D DISCRETE COSINE TRANSFORM , which is based on a 2-D DISCRETE COSINE TRANSFORM . the proposed method is based on the use of a 2-D DISCRETE COSINE TRANSFORM , which is based on PRINCIPAL COMPONENT ANALYSIS . the proposed method is evaluated in terms of ACCURACY and ACCURACY . the experimental results show that the proposed method outperforms the state-of-the-art methods in terms of ACCURACY and ACCURACY .\n",
            "\n",
            "195 1000\n",
            "descriptor learning has recently drawn increasing attention in COMPUTER VISION , existing algorithms are mainly developed for CLASSIFICATION rather than for REGRESSION which however has recently emerged as a powerful tool to solve a broad range of problems , e.g. , HEAD POSE ESTIMATION . in this paper , we propose a novel SUPERVISED DESCRIPTOR LEARNING ALGORITHM to establish a DISCRIMINATIVE AND COMPACT FEATURE REPRESENTATION for MULTI-OUTPUT REGRESSION . by formulating as GENERALIZED LOW-RANK APPROXIMATIONS OF MATRICES with a SUPERVISED MANIFOLD REGULARIZATION , the SUPERVISED DESCRIPTOR LEARNING ALGORITHM removes irrelevant and redundant information from RAW FEATURES by transforming into a LOW-DIMENSIONAL SPACE under the SUPERVISION OF MULTIVARIATE TARGETS . the obtained discriminative while compact descriptor largely reduces the variability and AMBIGUITY in MULTI-OUTPUT REGRESSION , and therefore enables more accurate and efficient MULTIVARIATE ESTIMATION . we demonstrate the effectiveness of the proposed SUPERVISED DESCRIPTOR LEARNING ALGORITHM on a REPRESENTATIVE MULTI-OUTPUT REGRESSION TASK : HEAD POSE ESTIMATION using the BENCHMARK POINTING '04 DATASET . experimental results show that the SUPERVISED DESCRIPTOR LEARNING ALGORITHM can achieve high pose ESTIMATION ACCURACY and significantly outperforms state-of-the-art algorithms by an ERROR REDUCTION up to <unk> % . the proposed SUPERVISED DESCRIPTOR LEARNING ALGORITHM provides a general DESCRIPTOR LEARNING FRAMEWORK in a supervised way for MULTI-OUTPUT REGRESSION which can largely boost the performance of existing MULTI-OUTPUT REGRESSION TASKS . \n",
            "this paper presents a novel SUPERVISED DESCRIPTOR LEARNING ALGORITHM for MULTIVARIATE ESTIMATION in COMPUTER VISION . the proposed DESCRIPTOR LEARNING FRAMEWORK is based on the use of SUPERVISED MANIFOLD REGULARIZATION for MULTIVARIATE ESTIMATION . the proposed DESCRIPTOR LEARNING FRAMEWORK is based on the use of SUPERVISED MANIFOLD REGULARIZATION for MULTIVARIATE ESTIMATION . the proposed DESCRIPTOR LEARNING FRAMEWORK is based on the use of SUPERVISED MANIFOLD REGULARIZATION , which is a DISCRIMINATIVE AND COMPACT FEATURE REPRESENTATION for MULTIVARIATE ESTIMATION . the proposed DESCRIPTOR LEARNING FRAMEWORK is evaluated on a BENCHMARK POINTING '04 DATASET and a BENCHMARK POINTING '04 DATASET . the experimental results show that the proposed SUPERVISED DESCRIPTOR LEARNING ALGORITHM is effective in improving the ESTIMATION ACCURACY of the DESCRIPTOR LEARNING FRAMEWORK in terms of ESTIMATION ACCURACY and AMBIGUITY . the proposed SUPERVISED DESCRIPTOR LEARNING ALGORITHM is evaluated on the BENCHMARK POINTING '04 DATASET , and the results show that the proposed SUPERVISED DESCRIPTOR LEARNING ALGORITHM is effective in improving the ESTIMATION ACCURACY of the SUPERVISED DESCRIPTOR LEARNING ALGORITHM .\n",
            "\n",
            "196 1000\n",
            "<unk> an audio mixture containing multiple SIMULTANEOUS BIRD SOUNDS is a challenging task . however , BIRDSONG often contains RAPID PITCH MODULATIONS , and these RAPID PITCH MODULATIONS carry information which may be of use in AUTOMATIC RECOGNITION . in this paper we demonstrate that an improved SPEC-TROGRAM REPRESENTATION , based on the DISTRIBUTION DERIVATIVE METHOD , leads to improved performance of a SEGREGATION ALGORITHM which uses a MARKOV RENEWAL PROCESS MODEL to track VOCALISATION PATTERNS consisting of singing and <unk> . \n",
            "this paper presents a novel SEGREGATION ALGORITHM for SIMULTANEOUS BIRD SOUNDS based on a MARKOV RENEWAL PROCESS MODEL . the proposed SEGREGATION ALGORITHM is based on a MARKOV RENEWAL PROCESS MODEL for SIMULTANEOUS BIRD SOUNDS . the proposed SEGREGATION ALGORITHM is based on the use of a MARKOV RENEWAL PROCESS MODEL for RAPID PITCH MODULATIONS . the proposed SEGREGATION ALGORITHM is based on a MARKOV RENEWAL PROCESS MODEL , which is based on a MARKOV RENEWAL PROCESS MODEL . the proposed SEGREGATION ALGORITHM is applied to SIMULTANEOUS BIRD SOUNDS , and the experimental results show that the proposed SEGREGATION ALGORITHM is effective in improving the AUTOMATIC RECOGNITION performance .\n",
            "\n",
            "197 1000\n",
            "we present a novel approach to represent TRANSIENTS using SPECTRAL-DOMAIN AMPLITUDE-MODULATED/FREQUENCY-MODULATED FUNCTIONS . the model is applied to the real and imaginary parts of the fourier transform -lrb- <unk> -rrb- of the transient . the suitability of the model lies in the observation that since TRANSIENTS are <unk> in time , the real and imaginary parts of the FOURIER SPECTRUM have a MODULATION STRUCTURE . the SPECTRAL AM is the ENVELOPE and the SPECTRAL FM is the GROUP DELAY FUNCTION . the GROUP DELAY is estimated using SPECTRAL ZERO-CROSSINGS and the SPECTRAL ENVELOPE is estimated using a COHERENT DEMODULATOR . we show that the proposed technique is robust to ADDITIVE NOISE . we present applications of the proposed technique to <unk> and <unk> in speech . \n",
            "in this paper , we propose a novel method to estimate the MODULATION STRUCTURE of a scene from a single image . the proposed method consists of two steps : -lrb- 1 -rrb- a GROUP DELAY FUNCTION , a COHERENT DEMODULATOR , and a COHERENT DEMODULATOR ; -lrb- 2 -rrb- a COHERENT DEMODULATOR to estimate the MODULATION STRUCTURE and the SPECTRAL ENVELOPE , and -lrb- 3 -rrb- a COHERENT DEMODULATOR to estimate the SPECTRAL ENVELOPE and the SPECTRAL ENVELOPE . the proposed method is based on a COHERENT DEMODULATOR , which is based on the SPECTRAL ZERO-CROSSINGS and the SPECTRAL ENVELOPE . the proposed method is applied to the problem of ADDITIVE NOISE , and the results show that the proposed method is robust to ADDITIVE NOISE and ADDITIVE NOISE .\n",
            "\n",
            "198 1000\n",
            "mixed probabilistic and deterministic graphical models are ubiquitous in REAL-WORLD APPLICATIONS . unfortunately , GIBBS SAMPLING , a popular MCMC TECHNIQUE , does not converge to the correct answers in presence of DETERMINISM and therefore can not be used for INFERENCE in such models . in this paper , we propose to remedy this problem by combining GIBBS SAMPLING with SAMPLESEARCH , an advanced IMPORTANCE SAMPLING TECHNIQUE which leverages complete SAT/CSP SOLVERS to generate high quality samples from HARD DETERMINISTIC SPACES . we call the resulting algorithm , SAMPLESEARCH . unlike GIBBS SAMPLING which yields UNWEIGHTED SAMPLES , SAMPLESEARCH yields weighted samples . computing these weights exactly can be computationally expensive and therefore we propose several approximations . we show that our APPROXIMATE WEIGHTING SCHEMES yield consistent estimates and demonstrate experimentally that SAMPLESEARCH is competitive in terms of ACCURACY with state-of-the-art algorithms such as SAMPLESEARCH , MC-SAT and BELIEF PROPAGATION . \n",
            "this paper addresses the problem of MIXED PROBABILISTIC AND DETERMINISTIC GRAPHICAL MODELS in REAL-WORLD APPLICATIONS . we propose a new algorithm , called SAMPLESEARCH , for MIXED PROBABILISTIC AND DETERMINISTIC GRAPHICAL MODELS , which is a generalization of the standard MCMC TECHNIQUE . the algorithm is based on the use of SAMPLESEARCH , SAMPLESEARCH , SAMPLESEARCH , and BELIEF PROPAGATION . the algorithm is based on the use of GIBBS SAMPLING , SAMPLESEARCH , SAMPLESEARCH , and BELIEF PROPAGATION . the algorithm is based on the use of GIBBS SAMPLING , SAMPLESEARCH , SAMPLESEARCH , and BELIEF PROPAGATION . the performance of the proposed algorithm is demonstrated on REAL-WORLD APPLICATIONS and REAL-WORLD APPLICATIONS .\n",
            "\n",
            "199 1000\n",
            "the goal of LOW-LEVEL VISION is to estimate an underlying scene , given an OBSERVED IMAGE . REAL-WORLD SCENES -lrb- eg , <unk> or shapes -rrb- can be very complex , conventionally requiring HIGH DIMENSIONAL REPRESENTATIONS which are hard to estimate and store . we propose a LOW-DIMENSIONAL REPRESENTATION , called a SCENE RECIPE , that relies on the image itself to describe the COMPLEX SCENE CONFIGURATIONS . SHAPE RECIPES are an example : these are the REGRESSION COEFFICIENTS that predict the BANDPASSED SHAPE from IMAGE DATA . we describe the benefits of this LOW-DIMENSIONAL REPRESENTATION , and show two uses illustrating their properties : -lrb- 1 -rrb- we improve STEREO SHAPE ESTIMATES by learning shape recipes at low resolution and applying STEREO SHAPE ESTIMATES at full resolution ; -lrb- 2 -rrb- SHAPE RECIPES implicitly contain information about lighting and materials and we use STEREO SHAPE ESTIMATES for MATERIAL SEGMENTATION . \n",
            "in this paper , we propose a novel method for MATERIAL SEGMENTATION from IMAGE DATA . the proposed approach is based on the use of STEREO SHAPE ESTIMATES to estimate the BANDPASSED SHAPE of a scene from the OBSERVED IMAGE . the proposed method is based on the use of STEREO SHAPE ESTIMATES to estimate the BANDPASSED SHAPE of the OBSERVED IMAGE . the proposed method is based on the use of STEREO SHAPE ESTIMATES to estimate the BANDPASSED SHAPE of the OBSERVED IMAGE . the proposed approach is evaluated on a variety of REAL-WORLD SCENES . the results show that the proposed method is able to estimate the BANDPASSED SHAPE of a scene from a single image .\n",
            "\n",
            "200 1000\n",
            "the GUITAR FEEDBACK EFFECT , or HOWLING , is well known to the general public and identified with many <unk> music genres and it is the only case of ACOUSTIC FEEDBACK employed for musical purposes . VIRTUAL ACOUSTIC FEEDBACK , is regarded as the extension of this phenomenon to any INSTRUMENT OR SOUND SOURCE by means of VIRTUAL ACOUSTICS and is meant to enrich the SOUND PALETTE of a musician . the study of the ACOUSTIC FEEDBACK as a MUSICAL TOOL and COMPUTATIONAL TECHNIQUES for its <unk> have been <unk> addressed in literature . in this paper a NONLINEAR FEEDBACK OSCILLATOR is proposed and its properties derived . the NONLINEAR FEEDBACK OSCILLATOR does not necessarily need to be connected to a VIRTUAL INSTRUMENT , thus enables to process any kind of PITCHED REAL-TIME INPUT . \n",
            "this paper presents a novel approach to VIRTUAL ACOUSTIC FEEDBACK and COMPUTATIONAL TECHNIQUES . the proposed approach is based on the use of VIRTUAL ACOUSTIC FEEDBACK and COMPUTATIONAL TECHNIQUES . the proposed approach is based on the use of VIRTUAL ACOUSTIC FEEDBACK and COMPUTATIONAL TECHNIQUES . the proposed approach is based on a NONLINEAR FEEDBACK OSCILLATOR , which is based on a NONLINEAR FEEDBACK OSCILLATOR . the method is based on the use of a NONLINEAR FEEDBACK OSCILLATOR and the COMPUTATIONAL TECHNIQUES . the proposed method can be applied to a wide range of COMPUTATIONAL TECHNIQUES and COMPUTATIONAL TECHNIQUES . the performance of the proposed method is demonstrated on a variety of VIRTUAL ACOUSTICS and COMPUTATIONAL TECHNIQUES .\n",
            "\n",
            "201 1000\n",
            "the ACCURACY of PARSING has <unk> 90 % recently , but this is not high enough to use PARSING results practically in NATURAL LANGUAGE PROCESSING APPLICATIONS such as PARAPHRASE ACQUISITION and RELATION EXTRACTION . we present a method for detecting reliable parses out of the outputs of a single DEPENDENCY PARSER . this technique is also applied to DOMAIN ADAPTATION OF DEPENDENCY PARSING . our goal was to improve the performance of a state-of-the-art DEPENDENCY PARSER on the data set of the domain adaptation track of the CONLL 2007 SHARED TASK , a formidable challenge . \n",
            "this paper addresses the problem of DOMAIN ADAPTATION OF DEPENDENCY PARSING in the presence of PARAPHRASE ACQUISITION and DOMAIN ADAPTATION OF DEPENDENCY PARSING . in this paper , we propose a novel approach to DOMAIN ADAPTATION OF DEPENDENCY PARSING based on RELATION EXTRACTION and DOMAIN ADAPTATION OF DEPENDENCY PARSING . the proposed approach is based on the use of PARSING and DOMAIN ADAPTATION OF DEPENDENCY PARSING . the experimental results on the CONLL 2007 SHARED TASK demonstrate the effectiveness of the proposed DEPENDENCY PARSER .\n",
            "\n",
            "202 1000\n",
            "deep convolutional neural networks -lrb- CNNS -rrb- are successfully used in a number of applications . however , their STORAGE AND COMPUTATIONAL REQUIREMENTS have largely prevented their widespread use on MOBILE DEVICES . here we present an effective CNN COMPRESSION APPROACH in the FREQUENCY DOMAIN , which focuses not only on smaller weights but on all the weights and their underlying connections . by treating CONVOLUTIONAL FILTERS as IMAGES , we decompose their representations in the FREQUENCY DOMAIN as common parts -lrb- i.e. , CLUSTER CENTERS -rrb- shared by other similar filters and their individual private parts -lrb- i.e. , individual residuals -rrb- . a large number of LOW-ENERGY FREQUENCY COEFFICIENTS in both parts can be discarded to produce HIGH COMPRESSION without significantly compromising ACCURACY . we relax the computational burden of CONVOLUTION OPERATIONS in CNNS by linearly combining the convolution responses of DISCRETE COSINE TRANSFORM BASES . the COMPRESSION AND SPEED-UP RATIOS of the proposed CNN COMPRESSION APPROACH are thoroughly analyzed and evaluated on BENCHMARK IMAGE DATASETS to demonstrate its superiority over state-of-the-art methods . \n",
            "this paper presents a novel CNN COMPRESSION APPROACH for HIGH COMPRESSION in MOBILE DEVICES . the proposed CNN COMPRESSION APPROACH is based on a CNN COMPRESSION APPROACH of the DISCRETE COSINE TRANSFORM BASES of the CONVOLUTIONAL FILTERS . the proposed CNN COMPRESSION APPROACH consists of two steps : -lrb- 1 -rrb- the LOW-ENERGY FREQUENCY COEFFICIENTS in the FREQUENCY DOMAIN ; -lrb- 2 -rrb- the use of CONVOLUTIONAL FILTERS in the form of CONVOLUTIONAL FILTERS , and -lrb- 3 -rrb- a CNN COMPRESSION APPROACH for HIGH COMPRESSION . the proposed CNN COMPRESSION APPROACH is evaluated on BENCHMARK IMAGE DATASETS , and the results show that the proposed CNN COMPRESSION APPROACH significantly improves the ACCURACY of the CNN COMPRESSION APPROACH in terms of ACCURACY and ACCURACY .\n",
            "\n",
            "203 1000\n",
            "-- TRANSMIT-REFERENCE ULTRA-WIDEBAND SYSTEMS are attractive due to their relatively low COMPLEXITY at both the TRANSMITTER and the receiver . partly , this is achieved by making RESTRICTIVE ASSUMPTIONS such as a FRAME LENGTH which should be much larger than the CHANNEL LENGTH . this limits their use to low data rate applications . in this paper , we lift this restriction and allow INTER-FRAME INTERFERENCE to occur . we propose a suitable SIGNAL PROCESSING DATA MODEL and corresponding RECEIVER ALGORITHMS which take the INTER-FRAME INTERFERENCE into account . the performance of the SIGNAL PROCESSING DATA MODEL are verified using simulations . \n",
            "in this paper , we present a novel approach to the problem of TRANSMIT-REFERENCE ULTRA-WIDEBAND SYSTEMS in the presence of INTER-FRAME INTERFERENCE . the proposed approach is based on the use of the SIGNAL PROCESSING DATA MODEL to estimate the CHANNEL LENGTH of the signal . the proposed method is based on a SIGNAL PROCESSING DATA MODEL of the SIGNAL PROCESSING DATA MODEL . the proposed method is based on the use of a SIGNAL PROCESSING DATA MODEL which is robust to INTER-FRAME INTERFERENCE such as FRAME LENGTH and FRAME LENGTH . the performance of the proposed method is evaluated in terms of the COMPLEXITY and the COMPLEXITY of the proposed method .\n",
            "\n",
            "204 1000\n",
            "we introduce a new application of ONLINE DIALOGUE ANALYSIS : supporting PEDAGOGICAL ASSESSMENT OF ONLINE Q&A DISCUSSIONS . extending the existing SPEECH ACT FRAMEWORK , we capture common EMOTIONAL EXPRESSIONS that often appear in STUDENT DISCUSSIONS , such as frustration and degree of certainty , and present a viable approach for the CLASSIFICATION . we demonstrate how such dialogue information can be used in analyzing STUDENT DISCUSSIONS and identifying difficulties . in particular , the DIFFICULTY EXPRESSIONS are aligned to DISCUSSION PATTERNS and student performance . we found that frustration occurs more frequently in longer discussions . the students who frequently express frustration tend to get lower <unk> than others . on the other hand , frequency of HIGH CERTAINTY EXPRESSIONS is positively correlated with the performance . we expect such dialogue analyses can become a powerful ASSESSMENT TOOL for instructors and education researchers . \n",
            "this paper presents a novel approach to PEDAGOGICAL ASSESSMENT OF ONLINE Q&A DISCUSSIONS in STUDENT DISCUSSIONS . the proposed approach is based on the use of a SPEECH ACT FRAMEWORK to estimate the DIFFICULTY EXPRESSIONS of the EMOTIONAL EXPRESSIONS . the proposed approach is based on the use of the SPEECH ACT FRAMEWORK to estimate the DIFFICULTY EXPRESSIONS and the DIFFICULTY EXPRESSIONS . the proposed method is based on the use of a SPEECH ACT FRAMEWORK to estimate the DIFFICULTY EXPRESSIONS and the DIFFICULTY EXPRESSIONS . the proposed approach is evaluated on a variety of EMOTIONAL EXPRESSIONS .\n",
            "\n",
            "205 1000\n",
            "we present a novel TRANSLATION MODEL , which simultaneously exploits the CONSTITUENCY AND DEPENDENCY TREES on the source side , to combine the advantages of two types of trees . we take HEAD-DEPENDENTS RELATIONS OF DEPENDENCY TREES as backbone and incorporate PHRASAL NODES OF CONSTITUENCY TREES as the source side of our TRANSLATION RULES , and the target side as strings . our RULES hold the property of long distance reorderings and the compatibility with phrases . large-scale experimental results show that our TRANSLATION MODEL achieves significantly improvements over the <unk> -lrb- <unk> BLEU on average -rrb- and <unk> -lrb- <unk> BLEU on average -rrb- models , which only employ single type of trees , and significantly outperforms the state-of-the-art HIERARCHICAL PHRASE-BASED MODEL -lrb- <unk> BLEU on average -rrb- , on three CHINESE-ENGLISH NIST TEST SETS . \n",
            "this paper presents a novel TRANSLATION MODEL for HEAD-DEPENDENTS RELATIONS OF DEPENDENCY TREES . the proposed TRANSLATION MODEL is based on the use of a set of PHRASAL NODES OF CONSTITUENCY TREES , each of which is a set of TRANSLATION RULES . the proposed TRANSLATION MODEL is evaluated on the CHINESE-ENGLISH NIST TEST SETS , and the results show that the proposed TRANSLATION MODEL is effective in improving the BLEU of the TRANSLATION MODEL . moreover , the proposed TRANSLATION MODEL is much more efficient than the traditional TRANSLATION MODEL . the proposed TRANSLATION MODEL is evaluated on both CHINESE-ENGLISH NIST TEST SETS . the experimental results on the CHINESE-ENGLISH NIST TEST SETS show that the proposed TRANSLATION MODEL outperforms the HIERARCHICAL PHRASE-BASED MODEL in terms of both BLEU and BLEU .\n",
            "\n",
            "206 1000\n",
            "the AUTOMATIC CLASSIFICATION OF ENVIRONMENTAL NOISE SOURCES from their ACOUSTIC SIGNATURES recorded at the microphone of a NOISE MONITORING SYSTEM -lrb- <unk> -rrb- is an active subject of research nowadays . this paper shows how HIDDEN MARKOV MODELS -LRB- HMM 'S -RRB- can be used to build an ENVIRONMENTAL NOISE RECOGNITION SYSTEM based on a TIME-FREQUENCY ANALYSIS of the noise signal . the performance of the proposed HMM-BASED APPROACH is evaluated experimentally for the classification of five types of NOISE EVENTS -lrb- car , TRUCK , MOPED , AIRCRAFT , train -rrb- . the HMM-BASED APPROACH is found to outperform previously proposed CLASSIFIERS based on the AVERAGE SPECTRUM OF NOISE EVENT with more than 95 % of correct classifications . for comparison , a CLASSIFICATION TEST is performed with HUMAN LISTENERS for the same data which shows that the best HMM-BASED APPROACH outper-forms the '' average '' human listener who achieves only <unk> % of correct classification for the same task . \n",
            "in this paper , we propose a novel HMM-BASED APPROACH for AUTOMATIC CLASSIFICATION OF ENVIRONMENTAL NOISE SOURCES . the proposed HMM-BASED APPROACH is based on the use of ACOUSTIC SIGNATURES and TRUCK . the proposed HMM-BASED APPROACH is based on the use of ACOUSTIC SIGNATURES and the ACOUSTIC SIGNATURES . the proposed HMM-BASED APPROACH is based on the use of the ACOUSTIC SIGNATURES and the ACOUSTIC SIGNATURES . the experimental results show that the proposed HMM-BASED APPROACH significantly outperforms the conventional HMM-BASED APPROACH in terms of both MOPED and AIRCRAFT . the performance of the proposed HMM-BASED APPROACH is evaluated on the AVERAGE SPECTRUM OF NOISE EVENT , and the results show that the proposed HMM-BASED APPROACH is effective in improving the AVERAGE SPECTRUM OF NOISE EVENT of the NOISE MONITORING SYSTEM .\n",
            "\n",
            "207 1000\n",
            "in this paper , we propose a novel framework to integrate ARTIC-ULATORY FEATURES into HMM-BASED ASR SYSTEM . this is achieved by using POSTERIOR PROBABILITIES of different AFS -lrb- estimated by MULTILAYER PERCEPTRONS -rrb- directly as OBSERVATION FEATURES in KULLBACK-LEIBLER DIVERGENCE BASED HMM SYSTEM . on the TIMIT PHONEME RECOGNITION TASK , the proposed framework yields a PHONEME RECOGNITION ACCURACY of <unk> % which is comparable to KULLBACK-LEIBLER DIVERGENCE BASED HMM SYSTEM using POSTERIOR PROBABILITIES OF PHONEMES as FEATURES -lrb- <unk> % -rrb- . furthermore , a best performance of <unk> % PHONEME RECOGNITION ACCURACY is achieved by jointly modeling AF PROBABILITIES and PHONEME PROBABILITIES as FEATURES . this shows the efficacy and flexibility of the proposed approach . \n",
            "in this paper , we propose a novel method to incorporate ARTIC-ULATORY FEATURES into the HMM-BASED ASR SYSTEM . the proposed HMM-BASED ASR SYSTEM consists of two steps : -lrb- 1 -rrb- a set of FEATURES that can be extracted from the OBSERVATION FEATURES , and -lrb- 2 -rrb- a KULLBACK-LEIBLER DIVERGENCE BASED HMM SYSTEM based on POSTERIOR PROBABILITIES and POSTERIOR PROBABILITIES . in the proposed HMM-BASED ASR SYSTEM , a KULLBACK-LEIBLER DIVERGENCE BASED HMM SYSTEM is employed to select the most appropriate FEATURES for each AFS . the proposed KULLBACK-LEIBLER DIVERGENCE BASED HMM SYSTEM is evaluated on the TIMIT PHONEME RECOGNITION TASK using the TIMIT PHONEME RECOGNITION TASK . the experimental results show that the proposed KULLBACK-LEIBLER DIVERGENCE BASED HMM SYSTEM achieves better performance in terms of PHONEME RECOGNITION ACCURACY on the TIMIT PHONEME RECOGNITION TASK .\n",
            "\n",
            "208 1000\n",
            "we propose a new VECTOR ENCODING SCHEME -LRB- TREE QUAN-TIZATION -RRB- that obtains LOSSY COMPACT CODES for HIGH-DIMENSIONAL VECTORS via TREE-BASED DYNAMIC PROGRAMMING . similarly to several previous schemes such as PRODUCT QUANTIZATION , these PRODUCT QUANTIZATION correspond to CODEWORD NUMBERS within multiple codebooks . we propose an INTEGER PROGRAMMING-BASED OPTIMIZATION that jointly recovers the CODING TREE STRUCTURE and the codebooks by minimizing the COMPRESSION ERROR on a TRAINING DATASET . in the experiments with diverse visual descriptors -lrb- sift , NEURAL CODES , FISHER VECTORS -rrb- , TREE QUANTIZATION is shown to combine FAST ENCODING and state-of-the-art ACCURACY in terms of the COMPRESSION ERROR , the RETRIEVAL PERFORMANCE , and the IMAGE CLASSIFICATION ERROR . \n",
            "in this paper , we propose a novel approach to FAST ENCODING , which is based on a VECTOR ENCODING SCHEME -LRB- TREE QUAN-TIZATION -RRB- . the proposed method is based on a VECTOR ENCODING SCHEME -LRB- TREE QUAN-TIZATION -RRB- , which is based on a VECTOR ENCODING SCHEME -LRB- TREE QUAN-TIZATION -RRB- . the proposed method is based on a VECTOR ENCODING SCHEME -LRB- TREE QUAN-TIZATION -RRB- , which is based on a VECTOR ENCODING SCHEME -LRB- TREE QUAN-TIZATION -RRB- . the proposed method is based on a VECTOR ENCODING SCHEME -LRB- TREE QUAN-TIZATION -RRB- , which is based on a VECTOR ENCODING SCHEME -LRB- TREE QUAN-TIZATION -RRB- . the experimental results show that the proposed method is effective in improving the RETRIEVAL PERFORMANCE and COMPRESSION ERROR . the proposed method is evaluated in terms of RETRIEVAL PERFORMANCE and COMPRESSION ERROR . the performance of the proposed method is evaluated on the TRAINING DATASET , and the results show that the proposed method is effective in improving the ACCURACY and RETRIEVAL PERFORMANCE .\n",
            "\n",
            "209 1000\n",
            "this paper proposes a novel technique of incorporating FACTOR-ANALYSIS-BASED INTER-SESSION VARIABILITY MODELLING in SPEAKER VERIFICATION SYSTEMS that employ CONTINUOUS PROGRESSIVE SPEAKER MODEL ADAPTATION . CONTINUOUS MODEL ADAPTATION involves the use of all encountered trials in the ADAPTATION PROCESS through the assignment of CONFIDENCE MEASURES . the proposed approach incorporates these CONFIDENCE MEASURES in the general statistics used in the ISV MODELLING PROCESS . PROGRESSIVE SVM-BASED CLASSIFICATION was integrated into the system through the <unk> of GMM MEAN SUPERVECTORS . the proposed system demonstrated a gain of 50 % over baseline results when <unk> on the NIST 2005 SRE CORPUS . ADAPTATIVE SCORE NORMALISATION TECHNIQUES were found to be beneficial to both GMM AND SVM CONFIGURATIONS alleviating the detrimental effects of SCORE SHIFT in PROGRESSIVE SYSTEMS . \n",
            "this paper addresses the problem of FACTOR-ANALYSIS-BASED INTER-SESSION VARIABILITY MODELLING for SPEAKER VERIFICATION SYSTEMS based on CONTINUOUS PROGRESSIVE SPEAKER MODEL ADAPTATION . in particular , we focus on the problem of CONTINUOUS MODEL ADAPTATION in the context of SPEAKER VERIFICATION SYSTEMS . in particular , we propose a novel approach to CONTINUOUS MODEL ADAPTATION based on CONTINUOUS PROGRESSIVE SPEAKER MODEL ADAPTATION . the proposed approach is based on the use of ADAPTATIVE SCORE NORMALISATION TECHNIQUES , which is a generalization of the ISV MODELLING PROCESS to the ADAPTATION PROCESS . the proposed approach is evaluated on the NIST 2005 SRE CORPUS , and the results show that the proposed method is effective in improving the performance of SPEAKER VERIFICATION SYSTEMS .\n",
            "\n",
            "210 1000\n",
            "we propose to include SYNCHRONY EFFECTS , known to exist in the AUDITORY SYSTEM , to represent VOICED PARTS OF THE SPEECH SIGNAL in a robust way . the system decomposes the input signal by means of a BAND-PASS FILTER BANK , and utilizes a bank of phase <unk> loops -lrb- <unk> -rrb- to obtain information on the frequencies present at a specific time . this information about the FREQUENCY DISTRIBUTION is transformed into a SPECTRAL-LIKE REPRESENTATION based on SYNCHRONY EFFECTS . NOISY SPEECH RECOGNITION experiments are performed using this SYNCHRONY-BASED SPECTRUM , which is transformed into a small set of COEFFICIENTS by using a transformation similar to that utilized for MEL CEPSTRUM FEATURES . we show that RECOGNITION performance compared to MEL CEPSTRUM FEATURES is advantageous , when measured over a range of SNR CONDITIONS , especially in the HIGH NOISE LEVEL CASE . \n",
            "in this paper , we propose a novel method for NOISY SPEECH RECOGNITION in NOISY SPEECH RECOGNITION . the proposed method is based on the use of MEL CEPSTRUM FEATURES in the AUDITORY SYSTEM . the proposed method consists of two steps : -lrb- 1 -rrb- a BAND-PASS FILTER BANK based on a BAND-PASS FILTER BANK , and -lrb- 2 -rrb- a BAND-PASS FILTER BANK for RECOGNITION . the proposed method is based on the use of MEL CEPSTRUM FEATURES in the SPECTRAL-LIKE REPRESENTATION . the proposed method is applied to NOISY SPEECH RECOGNITION . the experimental results show that the proposed method is effective in improving the RECOGNITION performance in SNR CONDITIONS .\n",
            "\n",
            "211 1000\n",
            "we consider the problem of quickly localizing multiple targets by asking questions of the form '' how many targets are within this set '' while obtaining noisy answers . this setting is a generalization to multiple targets of the game of 20 questions in which only a single target is queried . we assume that the targets are points on the real line , or in a two dimensional plane for the experiments , drawn independently from a KNOWN DISTRIBUTION . we evaluate the performance of a policy using the expected entropy of the POSTERIOR DISTRIBUTION after a fixed number of questions with noisy answers . we derive a lower bound for the value of this problem and study a specific policy , named the DYADIC POLICY . we show that this policy achieves a value which is no more than twice this lower bound when answers are <unk> , and show a more general CONSTANT FACTOR APPROXIMATION GUARANTEE for the NOISY SETTING . we present an empirical evaluation of this policy on SIMULATED DATA for the problem of detecting multiple instances of the same object in an image . finally , we present experiments on LOCALIZING MULTIPLE FACES simultaneously on REAL IMAGES . \n",
            "this paper addresses the problem of LOCALIZING MULTIPLE FACES from a single image . in this paper , we propose a novel method to estimate the POSTERIOR DISTRIBUTION of a scene from a single image . the proposed method is based on a CONSTANT FACTOR APPROXIMATION GUARANTEE , which is a CONSTANT FACTOR APPROXIMATION GUARANTEE . the proposed method is based on the CONSTANT FACTOR APPROXIMATION GUARANTEE , which is a CONSTANT FACTOR APPROXIMATION GUARANTEE . the proposed method is applied to the problem of LOCALIZING MULTIPLE FACES from REAL IMAGES . experimental results on SIMULATED DATA demonstrate the effectiveness of the proposed method .\n",
            "\n",
            "212 1000\n",
            "in this paper , we address the problem of precisely recovering the 3-D POSE OF 3-D SHAPE FIDUCIALS from IMAGES obtained by means of an UNCALIBRATED COMPUTED TOMOGRA-PHY IMAGING DEVICE . the main goal in this work is to model and estimate the GEOMETRIC TRANSFORMATION relating line <unk> to their projections in CROSS-SECTIONAL IMAGES . to do so , we propose techniques which solve the points to lines correspondence using CLOSED-FORM AND NUMERICAL ALGORITHMS . a GEOMETRIC TRANSFORMATION with eight degrees of FREEDOM -LRB- ROTATION , TRANSLATION and ANISOTROPIC SCALING -rrb- is used to model both a RIGID-BODY TRANSFORMATION and a SCALING TRANSFORMATION ACCOUNTING for CT SCAN INTRINSIC PARAMETERS . furthermore , an ESTIMATION OF ERROR BOUNDS in space is given when IMAGE DATA are affected by NOISE . real experiments show that the proposed method provides good results on a set of CT IMAGES from many viewpoints . \n",
            "in this paper , we propose a novel method for 3-D POSE OF 3-D SHAPE FIDUCIALS in CT IMAGES . the proposed method is based on the use of ANISOTROPIC SCALING and SCALING TRANSFORMATION ACCOUNTING to estimate the FREEDOM -LRB- ROTATION . the proposed method is based on the SCALING TRANSFORMATION ACCOUNTING and the SCALING TRANSFORMATION ACCOUNTING . the proposed method is based on the SCALING TRANSFORMATION ACCOUNTING and SCALING TRANSFORMATION ACCOUNTING . the proposed method is based on the SCALING TRANSFORMATION ACCOUNTING and the SCALING TRANSFORMATION ACCOUNTING . the proposed method is evaluated on a variety of CT IMAGES . the results show that the proposed method is robust to NOISE and NOISE , especially in the presence of NOISE and NOISE .\n",
            "\n",
            "213 1000\n",
            "the phonetic translation of <unk> speech -lrb- cs -rrb- gestures needs to mix the MANUAL CS INFORMATION together with the lips , taking into account the desynchronization delay -lrb- <unk> et al. -lsb- 2 -rsb- , <unk> et al. -lsb- 4 -rsb- -rrb- between these two flows of information . this contribution focuses on the LIP FLOW MODELING in the case of FRENCH VOWELS . previously , CLASSIFICATION MODELS have been developed for a PROFESSIONAL NORMAL-HEARING CS SPEAKER -lrb- <unk> et al. , -lsb- 7 -rsb- -rrb- . these CLASSIFICATION MODELS are used as a reference . in this study , we process the case of a DEAF CS SPEAKER and discuss the possibilities of CLASSIFICATION . the best performance -lrb- <unk> % -rrb- is obtained with the adaptation of the DEAF DATA to the REFERENCE MODELS . \n",
            "this paper addresses the problem of LIP FLOW MODELING in the context of a PROFESSIONAL NORMAL-HEARING CS SPEAKER . we propose a novel approach to the problem of LIP FLOW MODELING , which is based on the use of DEAF DATA for CLASSIFICATION . the proposed approach is based on the use of REFERENCE MODELS , which is based on a LIP FLOW MODELING . the proposed approach is based on the use of REFERENCE MODELS , which is based on the use of REFERENCE MODELS . the proposed approach is evaluated on a variety of FRENCH VOWELS . the results show that the proposed approach is effective in improving the CLASSIFICATION performance .\n",
            "\n",
            "214 1000\n",
            "the classical way of ENCODING PREFERENCES in DECISION THEORY is by means of UTILITY OR VALUE FUNCTIONS . however agents are not always able to deliver such a function directly . in this paper , we relate three different ways of specifying preferences , namely by means of a set of particular types of CONSTRAINTS on the UTILITY FUNCTION , by means of an ordered set of PRIORITIZED GOALS expressed by LOGICAL PROPOSITIONS , and by means of an ordered set of subsets of possible candidates reaching the same level of satisfaction . these different EXPRESSION MODES can be handled in a WEIGHTED LOGICAL SETTING , here the one of POSSIBILISTIC LOGIC . the AGGREGATION OF PREFERENCES pertaining to different criteria can then be handled by fusing sets of PRIORITIZED GOALS . apart from a better EXPRESSIVITY , the benefits of a LOGICAL REPRESENTATION OF PREFERENCES are to put them in a suitable format for REASONING PURPOSES , or for modifying or revising them . \n",
            "this paper addresses the problem of REASONING PURPOSES in a WEIGHTED LOGICAL SETTING . we propose a novel approach to REASONING PURPOSES based on LOGICAL REPRESENTATION OF PREFERENCES . the proposed approach is based on the use of UTILITY OR VALUE FUNCTIONS in the form of a UTILITY FUNCTION to the UTILITY FUNCTION . the proposed method consists of two steps : -lrb- 1 -rrb- a set of CONSTRAINTS in the form of CONSTRAINTS ; -lrb- 2 -rrb- a LOGICAL REPRESENTATION OF PREFERENCES , and -lrb- 2 -rrb- a UTILITY FUNCTION that captures the AGGREGATION OF PREFERENCES . the proposed approach is based on the use of UTILITY OR VALUE FUNCTIONS in the DECISION THEORY . the proposed approach is evaluated on a variety of EXPRESSION MODES , and the experimental results show that the proposed method outperforms the state-of-the-art methods .\n",
            "\n",
            "215 1000\n",
            "we consider the problem of designing a LINEAR TRANSFORMATION such as to achieve MINIMUM BAYES ERROR -lrb- or probability of misclassification -rrb- . two avenues will be explored : the first is to maximize the cents - average divergence between the CLASS DENSITIES and the second is to minimize the union <unk> bound in the range of cents . while both approaches yield similar performance in practice , they out-perform standard LDA FEATURES and show a 10 % relative improvement in the WORD ERROR RATE over state-of-the-art CEPSTRAL FEATURES on a large vocabulary telephony speech recognition task . \n",
            "in this paper , we propose a novel approach to the problem of LINEAR TRANSFORMATION . the proposed method is based on the use of a set of LDA FEATURES , which are used to estimate the CLASS DENSITIES of the CLASS DENSITIES . the proposed method is based on the use of a set of LDA FEATURES , which are then used to estimate the CLASS DENSITIES . the experimental results show that the proposed method is effective in improving the WORD ERROR RATE of the proposed method .\n",
            "\n",
            "216 1000\n",
            "we address the problem of MINIMUM MEAN-SQUARED ERROR ESTIMATION where the ESTIMATOR is constrained to belong to a <unk> set of functions . we derive a simple CLOSED FORM FORMULA that reveals the structure of the RESTRICTED ESTIMATOR for a wide class of constraints . using this CLOSED FORM FORMULA we study various types of CONSTRAINED ESTIMATION PROBLEMS that arise commonly in the ELDS OF SIGNAL PROCESSING and communication . \n",
            "this paper addresses the problem of learning a RESTRICTED ESTIMATOR from a given set of CONSTRAINED ESTIMATION PROBLEMS . in this paper , we consider the problem of MINIMUM MEAN-SQUARED ERROR ESTIMATION , where the goal is to minimize the total number of sources . we show that the problem of MINIMUM MEAN-SQUARED ERROR ESTIMATION can be formulated as a MINIMUM MEAN-SQUARED ERROR ESTIMATION . we show that the problem of MINIMUM MEAN-SQUARED ERROR ESTIMATION can be solved by a CLOSED FORM FORMULA . we show that the proposed algorithm is able to solve the problem of MINIMUM MEAN-SQUARED ERROR ESTIMATION .\n",
            "\n",
            "217 1000\n",
            "an important problem in IMAGE LABELING concerns learning with IMAGES labeled at varying levels of SPECIFICITY . we propose an approach that can incorporate IMAGES with labels drawn from a SEMANTIC HIERARCHY , and can also readily cope with MISSING LABELS , and ROUGHLY-SPECIFIED OBJECT BOUNDARIES . we introduce a new form of LATENT TOPIC MODEL , learning a novel CONTEXT REPRESENTATION in the JOINT LABEL-AND-IMAGE SPACE by capturing CO-OCCURRING PATTERNS within and between IMAGE FEATURES and object labels . given a topic , the LATENT TOPIC MODEL generates the INPUT DATA , as well as a TOPIC-DEPENDENT PROBABILISTIC CLASSIFIER to predict labels for IMAGE REGIONS . we present results on two REAL-WORLD DATASETS , demonstrating significant improvements gained by including the COARSELY LABELED IMAGES . \n",
            "this paper addresses the problem of IMAGE LABELING in IMAGES . we propose a novel approach to IMAGE LABELING in IMAGES . the proposed approach is based on a novel LATENT TOPIC MODEL , which is able to deal with MISSING LABELS , MISSING LABELS , and MISSING LABELS . the proposed approach is based on a LATENT TOPIC MODEL , which is able to deal with MISSING LABELS , MISSING LABELS and MISSING LABELS . the proposed approach is based on the use of a LATENT TOPIC MODEL to estimate the MISSING LABELS and the MISSING LABELS . experimental results on REAL-WORLD DATASETS demonstrate the effectiveness of the proposed method .\n",
            "\n",
            "218 1000\n",
            "in this paper , we present a complete platform for the SEMIAUTOMATIC GENERATION OF HUMAN-MACHINE DIALOG SYSTEMS , that using as input a description of the database of the service , a FLOW MODEL with the different states of the final application and a GUIDED INTERACTION step by step with the DESIGNER 'S INTERVENTION , generates dialogs to access the SERVICE DATA in different languages and two modalities , SPEECH AND WEB , simultaneously . we describe in detail several strategies that have been followed to reduce the time needed to do the design using the mentioned information . we also address important issues in DIALOG APPLICATIONS as mixed initiative and OVERANSWERING DIALOGS , CONFIRMATION HANDLING and how to provide the user long lists of information . \n",
            "this paper addresses the problem of SEMIAUTOMATIC GENERATION OF HUMAN-MACHINE DIALOG SYSTEMS in the presence of SERVICE DATA . in particular , we propose a novel approach to the problem of SEMIAUTOMATIC GENERATION OF HUMAN-MACHINE DIALOG SYSTEMS and CONFIRMATION HANDLING . the proposed approach is based on the use of OVERANSWERING DIALOGS and CONFIRMATION HANDLING . the proposed approach is based on the use of OVERANSWERING DIALOGS and CONFIRMATION HANDLING . the proposed approach is evaluated on a variety of DIALOG APPLICATIONS including OVERANSWERING DIALOGS , OVERANSWERING DIALOGS , and CONFIRMATION HANDLING .\n",
            "\n",
            "219 1000\n",
            "a NON-PARAMETRIC BAYESIAN MODEL is proposed for processing multiple images . the analysis employs IMAGE FEATURES and , when present , the words associated with accompanying annotations . the NON-PARAMETRIC BAYESIAN MODEL clusters the images into classes , and each image is segmented into a set of objects , also allowing the opportunity to assign a word to each object -lrb- localized labeling -rrb- . each object is assumed to be represented as a HETEROGENEOUS MIX OF COMPONENTS , with this realized via MIXTURE MODELS linking IMAGE FEATURES to OBJECT TYPES . the number of IMAGE CLASSES , number of OBJECT TYPES , and the characteristics of the OBJECT-FEATURE MIXTURE MODELS are inferred nonparametrically . to constitute SPATIALLY CONTIGUOUS OBJECTS , a new LOGISTIC STICK-BREAKING PROCESS is developed . INFERENCE is performed efficiently via VARIATIONAL BAYESIAN ANALYSIS , with example results presented on two IMAGE DATABASES . \n",
            "this paper addresses the problem of INFERENCE from IMAGE DATABASES . we propose a novel method for INFERENCE based on VARIATIONAL BAYESIAN ANALYSIS . given a set of IMAGE CLASSES , a set of IMAGE FEATURES are extracted from the IMAGE CLASSES of the IMAGE CLASSES . the proposed approach is based on the LOGISTIC STICK-BREAKING PROCESS , which is based on the LOGISTIC STICK-BREAKING PROCESS . the proposed NON-PARAMETRIC BAYESIAN MODEL is based on the LOGISTIC STICK-BREAKING PROCESS , which is based on the LOGISTIC STICK-BREAKING PROCESS . the proposed approach is evaluated on a variety of IMAGE DATABASES . the experimental results show that the proposed method outperforms the existing methods in terms of INFERENCE and INFERENCE .\n",
            "\n",
            "220 1000\n",
            "this paper deals with NOISY PHASE MONOCOMPONENT SIGNALS in ADDITIVE NOISE . this model is more appropriate for REAL WORLD APPLICATIONS in particular for RADAR AND COMMUNICATIONS . the problem is introduced and a MAXIMUM LIKELIHOOD SOLUTION is proposed . specifically , the CRAMÈR-RAO BOUND is explicitly derived and compared to the case of NOISE FREE PHASE . \n",
            "this paper addresses the problem of NOISY PHASE MONOCOMPONENT SIGNALS in REAL WORLD APPLICATIONS . in particular , we consider the problem of NOISY PHASE MONOCOMPONENT SIGNALS in the presence of ADDITIVE NOISE . in particular , we consider the problem of NOISY PHASE MONOCOMPONENT SIGNALS in the presence of ADDITIVE NOISE . in particular , we consider the problem of NOISY PHASE MONOCOMPONENT SIGNALS in the presence of ADDITIVE NOISE . we show that this problem can be solved efficiently using a MAXIMUM LIKELIHOOD SOLUTION . we show that the proposed algorithm is able to recover the MAXIMUM LIKELIHOOD SOLUTION under a variety of conditions .\n",
            "\n",
            "221 1000\n",
            "localization and synchronization are critical challenges for a WIRELESS NETWORK , which are conventionally solved independently . recently , various estimators have been proposed to jointly synchronize and localize a NODE in a STATIC NETWORK based on two way communication . in this paper , we present a novel and generic model based on two way communication between nodes , which are in RELATIVE MOTION with respect to each other . furthermore , for the entire STATIC NETWORK we propose a closed form extended global least squares -lrb- <unk> -rrb- solution to solve for all the UNKNOWN CLOCK SKEWS , CLOCK OFFSETS , INITIAL PAIRWISE DISTANCES and RELATIVE RADIAL VELOCITIES using a SINGLE CLOCK REFERENCE within the STATIC NETWORK . a new CRAMER RAO BOUND is derived and the proposed fusion center based extended global least squares -lrb- <unk> -rrb- solution is shown to be asymptotically optimal . \n",
            "this paper addresses the problem of LOCALIZATION AND SYNCHRONIZATION in the presence of CLOCK OFFSETS , CLOCK OFFSETS , CLOCK OFFSETS , and INITIAL PAIRWISE DISTANCES . we propose a method to estimate the RELATIVE MOTION of a WIRELESS NETWORK . the proposed method consists of two steps . first , we estimate the RELATIVE MOTION of the NODE of the WIRELESS NETWORK . then , a STATIC NETWORK is proposed to estimate the INITIAL PAIRWISE DISTANCES , CLOCK OFFSETS , INITIAL PAIRWISE DISTANCES , and INITIAL PAIRWISE DISTANCES . experimental results demonstrate the effectiveness of the proposed method .\n",
            "\n",
            "222 1000\n",
            "the COMPACT DESCRIPTION OF A VIDEO SEQUENCE through a single IMAGE MAP and a DOMINANT MOTION has applications in several domains , including VIDEO BROWSING AND RETRIEVAL , COMPRESSION , MOSAICING , and VISUAL SUMMARIZATION . building such a COMPACT DESCRIPTION OF A VIDEO SEQUENCE requires the capability to register all the frames with respect to the dominant object in the scene , a task which has been , in the past , addressed through TEMPORALLY LOCALIZED MOTION ESTIMATES . in this paper , we show how the lack of TEMPORAL CONSISTENCY associated with such estimates can <unk> the validity of the DOMINANT MOTION ASSUMPTION , leading to <unk> between different scene interpretations and poor registration . to avoid this <unk> , we augment the MOTION MODEL with a GENERIC TEMPORAL CONSTRAINT which increases the ROBUSTNESS against competing interpretations , leading to more meaningful CONTENT SUMMARIZATION . \n",
            "this paper addresses the problem of CONTENT SUMMARIZATION and CONTENT SUMMARIZATION . in this paper , we propose a novel MOTION MODEL for COMPACT DESCRIPTION OF A VIDEO SEQUENCE and CONTENT SUMMARIZATION . the proposed GENERIC TEMPORAL CONSTRAINT is based on the DOMINANT MOTION ASSUMPTION and the DOMINANT MOTION between the IMAGE MAP and the DOMINANT MOTION . the proposed MOTION MODEL is based on the DOMINANT MOTION ASSUMPTION and the DOMINANT MOTION ASSUMPTION between the IMAGE MAP and the DOMINANT MOTION . the proposed MOTION MODEL is applied to VIDEO BROWSING AND RETRIEVAL and CONTENT SUMMARIZATION . the experimental results show that the proposed MOTION MODEL is effective in improving the ROBUSTNESS of the proposed MOTION MODEL .\n",
            "\n",
            "223 1000\n",
            "expert finding for QUESTION ANSWERING is a challenging QUESTION ANSWERING in COMMUNITY-BASED QUESTION ANSWERING SITE , arising in many applications such as QUESTION ROUTING and the identification of best answers . in order to provide high-quality experts , many existing approaches learn the USER MODEL mainly from their past QUESTION-ANSWERING ACTIVITIES in cqa sites , which suffer from the spar-sity QUESTION ANSWERING of CQA DATA . in this paper , we consider the QUESTION ANSWERING of expert finding from the viewpoint of LEARNING RANKING METRIC EMBEDDING . we propose a novel RANKING METRIC NETWORK LEARNING FRAMEWORK for expert finding by exploiting both users ' RELATIVE QUALITY RANK to given questions and their SOCIAL RELATIONS . we then develop a RANDOM-WALK BASED LEARNING METHOD with RECURRENT NEURAL NETWORKS for RANKING METRIC NETWORK EMBEDDING . the extensive experiments on a LARGE-SCALE DATASET from a real world cqa site show that our RANDOM-WALK BASED LEARNING METHOD achieves better performance than other state-of-the-art solutions to the QUESTION ANSWERING . \n",
            "this paper presents a novel RANKING METRIC NETWORK LEARNING FRAMEWORK for QUESTION ANSWERING . the proposed RANKING METRIC NETWORK LEARNING FRAMEWORK is based on RECURRENT NEURAL NETWORKS , a RANKING METRIC NETWORK LEARNING FRAMEWORK for LEARNING RANKING METRIC EMBEDDING . the proposed RANKING METRIC NETWORK LEARNING FRAMEWORK is based on RECURRENT NEURAL NETWORKS , which is based on RECURRENT NEURAL NETWORKS . the proposed RANKING METRIC NETWORK LEARNING FRAMEWORK is based on the use of RECURRENT NEURAL NETWORKS to estimate the RELATIVE QUALITY RANK . the proposed RANKING METRIC NETWORK LEARNING FRAMEWORK is evaluated on a LARGE-SCALE DATASET and a LARGE-SCALE DATASET . the experimental results show that the proposed RANDOM-WALK BASED LEARNING METHOD is effective in improving the RELATIVE QUALITY RANK of the CQA DATA . the proposed RANDOM-WALK BASED LEARNING METHOD is evaluated on the LARGE-SCALE DATASET and the results show that the proposed RANDOM-WALK BASED LEARNING METHOD is effective in improving the RELATIVE QUALITY RANK .\n",
            "\n",
            "224 1000\n",
            "in this work , the novel task of DETECTING DELETIONS within automatic speech recognition -lrb- asr -rrb- system output is investigated . DELETION-INFORMED CONFIDENCE ESTIMATION is proposed as an approach which simultaneously yields a CONFIDENCE SCORE in a word being correct , as well as a DELETION CONFIDENCE SCORE which indicates whether a DELETION is likely to occur in the output . the sequential nature of CONDITIONAL RANDOM FIELD MODELS is exploited as a means through which this can be achieved . it is shown that this SEQUENCE STRUCTURE is crucial in yielding useful DELETION DETECTION SCORES , with an equivalent NON-SEQUENTIAL MODEL proven to be unsuitable for the task . the DELETION-INFORMED CONFIDENCE ESTIMATION APPROACH is also shown to outperform one where DELETION CONFIDENCE SCORES are estimated as a CLASSIFICATION TASK separate from that of OVERALL CONFIDENCE ESTIMATION . \n",
            "in this paper , we propose a novel approach to DELETION-INFORMED CONFIDENCE ESTIMATION in CONDITIONAL RANDOM FIELD MODELS . the proposed approach is based on the use of a SEQUENCE STRUCTURE , which is a CONFIDENCE SCORE . the proposed approach is based on the use of a SEQUENCE STRUCTURE , which is based on a NON-SEQUENTIAL MODEL . the proposed approach is based on the use of a SEQUENCE STRUCTURE , which is a CONFIDENCE SCORE . the proposed method is evaluated on a CLASSIFICATION TASK and a CLASSIFICATION TASK . the experimental results show that the proposed method achieves better performance than the state-of-the-art methods .\n",
            "\n",
            "225 1000\n",
            "hybrid <unk> combine multiple types of reasoning , usually subsumption and <unk> resolution . we outline a system which combines NATURAL DEDUCTION and SUBSUMPTION REASONING using INFERENCE GRAPHS implementing a LOGIC OF ARBITRARY AND INDEFINITE OBJECTS . \n",
            "this paper addresses the problem of LOGIC OF ARBITRARY AND INDEFINITE OBJECTS and SUBSUMPTION REASONING . we propose a new approach to the problem of SUBSUMPTION REASONING and SUBSUMPTION REASONING . we show that this problem can be viewed as a special case of NATURAL DEDUCTION and SUBSUMPTION REASONING . we show that the proposed algorithm can be applied to the problem of SUBSUMPTION REASONING , and show that it is possible to efficiently solve the problem of SUBSUMPTION REASONING .\n",
            "\n",
            "226 1000\n",
            "active learning can lead to a dramatic reduction in labeling effort . however , in many practical implementations -lrb- such as CROWDSOURCING , surveys , high-throughput experimental design -rrb- , it is preferable to query labels for batches of examples to be labelled in parallel . while several HEURISTICS have been proposed for BATCH-MODE ACTIVE LEARNING , little is known about their theoretical performance . we consider BATCH MODE ACTIVE LEARNING and more general <unk> stochastic optimization problems that exhibit ADAPTIVE SUBMODULARITY , a NATURAL DIMINISHING RETURNS CONDITION . we prove that for such problems , a simple GREEDY STRATEGY is competitive with the OPTIMAL BATCH-MODE POLICY . in some cases , surprisingly , the use of batches incurs competitively low cost , even when compared to a fully SEQUENTIAL STRATEGY . we demonstrate the effectiveness of our approach on BATCH-MODE ACTIVE LEARNING TASKS , where it outperforms the state of the art , as well as the novel problem of MULTI-STAGE INFLUENCE MAXIMIZATION in SOCIAL NETWORKS . \n",
            "this paper addresses the problem of MULTI-STAGE INFLUENCE MAXIMIZATION in SOCIAL NETWORKS . in particular , we focus on the problem of MULTI-STAGE INFLUENCE MAXIMIZATION in SOCIAL NETWORKS . in particular , we consider the problem of MULTI-STAGE INFLUENCE MAXIMIZATION in SOCIAL NETWORKS . we show that the NATURAL DIMINISHING RETURNS CONDITION of the MULTI-STAGE INFLUENCE MAXIMIZATION can be reduced to the NATURAL DIMINISHING RETURNS CONDITION of the OPTIMAL BATCH-MODE POLICY . we also show that the OPTIMAL BATCH-MODE POLICY can be applied to BATCH-MODE ACTIVE LEARNING TASKS . we also show that the GREEDY STRATEGY can be applied to BATCH-MODE ACTIVE LEARNING TASKS .\n",
            "\n",
            "227 1000\n",
            "in this paper , we investigate DETECTION AND LOCALIZATION OF GENERAL 3D OBJECT CLASSES by relating LOCAL SCALE-INVARIANT FEATURES to a VIEWPOINT INVARIANT REFERENCE FRAME . this can generally be achieved by either a MULTI-VIEW REPRESENTATION , where FEATURES and REFERENCE FRAME are modeled as a collection of distinct views , or by a VIEWPOINT INVARIANT REPRESENTATION , where FEATURES and REFERENCE FRAME are mod-eled independently of VIEWPOINT . we compare MULTI-VIEW AND VIEWPOINT INVARIANT REPRESENTATIONS trained and tested on the same data , where the VIEWPOINT INVARIANT APPROACH results in fewer FALSE POSITIVE DETECTIONS and higher AVERAGE PRECISION . we present a new , ITERATIVE LEARNING ALGORITHM to determine an optimal VIEWPOINT INVARIANT REFERENCE FRAME from training images in a DATA-DRIVEN MANNER . the learned optimal REFERENCE FRAME is centrally located with respect to the 3D OBJECT CLASS and to IMAGE FEATURES in a given view , thereby minimizing REFERENCE FRAME LOCALIZATION ERROR as predicted by theory and maintaining a CONSISTENT GEOMETRICAL INTERPRETATION with respect to the underlying OBJECT CLASS . MODELING AND DETECTION based on the optimal REFERENCE FRAME improves DETECTION performance for both MULTIVIEW AND VIEWPOINT INVARIANT REPRESENTATIONS . experimentation is performed on the CLASS OF 3D FACES , using the PUBLIC COLOR FERET DATABASE for training , the CMU PROFILE DATABASE for testing and sift IMAGE FEATURES . \n",
            "this paper addresses the problem of MODELING AND DETECTION from a PUBLIC COLOR FERET DATABASE . we propose a novel VIEWPOINT INVARIANT APPROACH to the problem of MODELING AND DETECTION in a DATA-DRIVEN MANNER . the proposed approach is based on the use of a MULTI-VIEW REPRESENTATION and a REFERENCE FRAME to estimate the OBJECT CLASS and the REFERENCE FRAME . the proposed approach is based on the use of a MULTI-VIEW REPRESENTATION and a REFERENCE FRAME to estimate the 3D OBJECT CLASS and the REFERENCE FRAME . the proposed method is evaluated on a PUBLIC COLOR FERET DATABASE and a PUBLIC COLOR FERET DATABASE of the PUBLIC COLOR FERET DATABASE . the results show that the proposed VIEWPOINT INVARIANT APPROACH is robust to FALSE POSITIVE DETECTIONS and AVERAGE PRECISION , and is robust to FALSE POSITIVE DETECTIONS and AVERAGE PRECISION .\n",
            "\n",
            "228 1000\n",
            "confusion matrices have been widely used to increase the ACCURACY of SPEECH RECOGNISERS , but usually a MEAN CONFUSION MATRIX , averaged over many speakers , is used . however , analysis shows that CONFUSION MATRICES for individual speakers vary considerably , and so there is benefit in obtaining estimates of CONFUSION MATRICES for individual speakers . unfortunately , there is rarely enough data to make reliable estimates . we present a technique for estimating the elements of a SPEAKER 'S CONFUSION MATRIX given only SPARSE DATA from the speaker . it utilizes NON-NEGATIVE MATRIX FACTORISATION to find structure within CONFUSION MATRICES , and this structure is exploited to make improved estimates . results show that under certain conditions , this technique can give estimates that are as good as those obtained with twice the number of utterances available from the speaker . \n",
            "in this paper , we propose a novel method for NON-NEGATIVE MATRIX FACTORISATION . the proposed method is based on the use of CONFUSION MATRICES to estimate the SPEAKER 'S CONFUSION MATRIX of the target signal . the proposed method is based on the use of CONFUSION MATRICES to estimate the MEAN CONFUSION MATRIX of the target signal . the proposed method is based on the use of NON-NEGATIVE MATRIX FACTORISATION to estimate the SPEAKER 'S CONFUSION MATRIX of the target signal . the proposed method is evaluated in terms of ACCURACY and ACCURACY . the experimental results show that the proposed method is effective in improving the ACCURACY of the proposed method .\n",
            "\n",
            "229 1000\n",
            "this paper presents our work on '' SNACK , '' a LOW-DIMENSIONAL CONCEPT EMBEDDING ALGORITHM that combines HUMAN EXPERTISE with AUTOMATIC MACHINE SIMILARITY KERNELS . both parts are complimentary : HUMAN INSIGHT can capture relationships that are not apparent from the object 's visual similarity and the machine can help relieve the human from having to exhaustively specify many constraints . we show that our SNACK EMBEDDINGS are useful in several tasks : distinguishing PRIME AND NONPRIME NUMBERS on MNIST , discovering labeling mistakes in the CALTECH UCSD BIRDS DATASET with the help of DEEP-LEARNED FEATURES , creating TRAINING DATASETS for BIRD CLASSIFIERS , capturing SUBJECTIVE HUMAN TASTE on a new dataset of 10,000 <unk> , and qualitatively exploring an UNSTRUCTURED SET OF PICTOGRAPHIC CHARACTERS . comparisons with the state-of-the-art in these tasks show that SNACK produces better CONCEPT EMBEDDINGS that require less HUMAN SUPERVISION than the leading methods . \n",
            "this paper addresses the problem of HUMAN EXPERTISE in the presence of HUMAN EXPERTISE . we propose a novel approach to AUTOMATIC MACHINE SIMILARITY KERNELS based on SNACK . the proposed approach is based on the use of DEEP-LEARNED FEATURES , which is based on a LOW-DIMENSIONAL CONCEPT EMBEDDING ALGORITHM and a LOW-DIMENSIONAL CONCEPT EMBEDDING ALGORITHM . the proposed method is based on a LOW-DIMENSIONAL CONCEPT EMBEDDING ALGORITHM and a LOW-DIMENSIONAL CONCEPT EMBEDDING ALGORITHM with a LOW-DIMENSIONAL CONCEPT EMBEDDING ALGORITHM . the proposed method is evaluated on the CALTECH UCSD BIRDS DATASET , and the results show that the proposed method is effective in improving the performance of the BIRD CLASSIFIERS .\n",
            "\n",
            "230 1000\n",
            "we present a NEURAL NETWORK SIMULATION which we implemented on the massively parallel connection machine 2 . in contrast to previous work , this NEURAL NETWORK SIMULATION is based on BIOLOGICALLY REALISTIC NEU-RONS with NONTRIVIAL SINGLE-CELL DYNAMICS , high connectivity with a STRUCTURE modelled in agreement with BIOLOGICAL DATA , and preservation of the TEMPORAL DYNAMICS OF SPIKE INTERACTIONS . we simulate NEURAL NETWORKS of <unk> neurons coupled by about 1000 synapses per neuron , and estimate the performance for much larger systems . communication between neurons is identified as the COMPUTATION-ALLY MOST DEMANDING TASK and we present a novel method to overcome this bottleneck . the NEURAL NETWORK SIMULATION has already been used to study the PRIMARY VISUAL SYSTEM of the cat . \n",
            "this paper describes a novel approach to TEMPORAL DYNAMICS OF SPIKE INTERACTIONS in BIOLOGICAL DATA . the proposed approach is based on the use of a NEURAL NETWORK SIMULATION that allows the TEMPORAL DYNAMICS OF SPIKE INTERACTIONS to be integrated into a PRIMARY VISUAL SYSTEM . the proposed approach is based on the use of NEURAL NETWORKS , which is based on a NEURAL NETWORK SIMULATION of the PRIMARY VISUAL SYSTEM . the proposed approach is evaluated on a COMPUTATION-ALLY MOST DEMANDING TASK . the results show that the proposed approach is effective in improving the TEMPORAL DYNAMICS OF SPIKE INTERACTIONS in BIOLOGICAL DATA .\n",
            "\n",
            "231 1000\n",
            "<unk> ± RB ± ATL is an extension of RB ± ATL where it is possible to model consumption and production of several resources by a set of agents . the MODEL-CHECKING PROBLEM for RB ± ATL is known to be decidable . however the only available MODEL-CHECKING ALGORITHM for RB ± ATL uses a FORWARD SEARCH OF THE STATE SPACE , and hence does not have an efficient SYMBOLIC IMPLEMENTATION . in this paper , we consider a fragment of RB ± ATL , <unk> ± RB ± ATL , that allows only one resource type . we give a SYMBOLIC MODEL-CHECKING ALGORITHM for this fragment of RB ± ATL , and evaluate the performance of an <unk> implementation of the SYMBOLIC MODEL-CHECKING ALGORITHM on an example problem that can be scaled to large state spaces . \n",
            "this paper presents a novel MODEL-CHECKING ALGORITHM for RB ± ATL . the proposed MODEL-CHECKING ALGORITHM is based on a FORWARD SEARCH OF THE STATE SPACE for RB ± ATL . the proposed SYMBOLIC MODEL-CHECKING ALGORITHM is based on a FORWARD SEARCH OF THE STATE SPACE , which is based on the FORWARD SEARCH OF THE STATE SPACE . the proposed MODEL-CHECKING ALGORITHM is based on a FORWARD SEARCH OF THE STATE SPACE , which is based on the FORWARD SEARCH OF THE STATE SPACE . the proposed MODEL-CHECKING ALGORITHM is applied to RB ± ATL , and the experimental results demonstrate the effectiveness of the proposed MODEL-CHECKING ALGORITHM .\n",
            "\n",
            "232 1000\n",
            "this paper explores in detail the use of ERROR CORRECTING OUTPUT CODING for learning TEXT CLASSIFIERS . we show that the ACCURACY of a naive bayes classifier over TEXT CLASSIFICATION TASKS can be significantly improved by taking advantage of the error-correcting properties of the code . we also explore the use of different kinds of codes , namely ERROR-CORRECTING CODES , RANDOM CODES , and domain and <unk> codes and give experimental results for each of them . the ERROR CORRECTING OUTPUT CODING scales well to large data sets with a large number of classes . experiments on a REAL-WORLD DATA SET show a reduction in classification error by up to 66 % over the traditional naive bayes classifier . we also compare our empirical results to <unk> results and find that the two closely agree . \n",
            "this paper addresses the problem of ERROR CORRECTING OUTPUT CODING in the context of TEXT CLASSIFICATION TASKS . in particular , we propose a novel approach to the problem of ERROR CORRECTING OUTPUT CODING , which is based on the idea of ERROR CORRECTING OUTPUT CODING and RANDOM CODES . the proposed approach is based on the use of RANDOM CODES and RANDOM CODES . the proposed approach is evaluated on the REAL-WORLD DATA SET , and the results show that the proposed method is effective in improving the ACCURACY of TEXT CLASSIFICATION TASKS .\n",
            "\n",
            "233 1000\n",
            "traditional ACOUSTIC SOURCE LOCALIZATION uses a TWO-STEP PROCEDURE requiring INTERMEDIATE TIME-DELAY ESTIMATES from pairs of microphones . an alternative SINGLE-STEP APPROACH is proposed in this paper in which PARTICLE FILTERING is used to estimate the SOURCE LOCATION through STEERED BEAMFORMING . this SINGLE-STEP APPROACH is especially attractive in SPEECH ENHANCEMENT APPLICATIONS , where the LOCALIZATION ESTIMATES are typically used to steer a BEAMFORMER at a later stage . simulation results show that the SINGLE-STEP APPROACH is robust to REVERBERATION , and is able to accurately follow the SOURCE TRAJECTORY . \n",
            "in this paper , we propose a novel SINGLE-STEP APPROACH for ACOUSTIC SOURCE LOCALIZATION based on PARTICLE FILTERING . the proposed SINGLE-STEP APPROACH is based on the use of STEERED BEAMFORMING for ACOUSTIC SOURCE LOCALIZATION . the proposed SINGLE-STEP APPROACH is based on the use of STEERED BEAMFORMING for ACOUSTIC SOURCE LOCALIZATION . the proposed SINGLE-STEP APPROACH is based on the use of STEERED BEAMFORMING for ACOUSTIC SOURCE LOCALIZATION . the proposed SINGLE-STEP APPROACH is based on the use of INTERMEDIATE TIME-DELAY ESTIMATES for ACOUSTIC SOURCE LOCALIZATION . the proposed SINGLE-STEP APPROACH is applied to the ACOUSTIC SOURCE LOCALIZATION for ACOUSTIC SOURCE LOCALIZATION , and the experimental results show that the proposed SINGLE-STEP APPROACH is effective in improving the LOCALIZATION ESTIMATES .\n",
            "\n",
            "234 1000\n",
            "this paper introduces a DIAGNOSIS SCHEME of a RAILWAY INFRASTRUCTURE COMPONENT based on a combined use of EMPIRICAL MODE DECOMPOSITION and HILBERT TRANSFORM . this RAILWAY INFRASTRUCTURE COMPONENT is dedicated to TRACK/VEHICLE TRANSMISSION referred as TRACK CIRCUIT . the aim is to detect its working state from one MEASUREMENT SIGNAL which can be viewed as a superposition of several oscillations and PERIODIC PATTERNS called INTRINSIC MODE FUNCTIONS . for this application , it will be shown that PHYSICAL MEANING can be assigned to each mode that EMD tries to extract . furthermore , when the HILBERT TRANSFORM of the INTRINSIC MODE FUNCTIONS is performed , we show that the changing of INSTANTANEOUS FREQUENCY can be linked to the existence of defect . the performances are illustrated on both SIMULATED AND EXPERIMENTAL SIGNALS . \n",
            "this paper presents a novel DIAGNOSIS SCHEME for TRACK/VEHICLE TRANSMISSION . the proposed DIAGNOSIS SCHEME is based on a HILBERT TRANSFORM of the MEASUREMENT SIGNAL of the RAILWAY INFRASTRUCTURE COMPONENT . the proposed DIAGNOSIS SCHEME is based on a DIAGNOSIS SCHEME of the INSTANTANEOUS FREQUENCY of the MEASUREMENT SIGNAL . the proposed DIAGNOSIS SCHEME is based on a DIAGNOSIS SCHEME of the INSTANTANEOUS FREQUENCY of the MEASUREMENT SIGNAL . the proposed DIAGNOSIS SCHEME is based on a DIAGNOSIS SCHEME of the INSTANTANEOUS FREQUENCY of the TRACK CIRCUIT . the proposed DIAGNOSIS SCHEME is applied to the SIMULATED AND EXPERIMENTAL SIGNALS , and the results show that the proposed algorithm is able to recover the INSTANTANEOUS FREQUENCY of the signal under a wide range of PHYSICAL MEANING .\n",
            "\n",
            "235 1000\n",
            "we present a VOICE MORPHING STRATEGY that can be used to generate a CONTINUUM OF ACCENT TRANSFORMATIONS between a FOREIGN SPEAKER and a NATIVE SPEAKER . the VOICE MORPHING STRATEGY performs a cepstral decomposition of speech into SPECTRAL SLOPE and SPECTRAL DETAIL . ACCENT CONVERSIONS are then generated by combining the SPECTRAL SLOPE of the FOREIGN SPEAKER with a morph of the SPECTRAL DETAIL of the NATIVE SPEAKER . SPECTRAL MORPHING is achieved by representing the SPECTRAL DETAIL through PULSE DENSITY MODULATION and AVERAGING PULSES in a PAIR-WISE FASHION . the VOICE MORPHING STRATEGY is validated on PARALLEL RECORDINGS from two ARCTIC SPEAKERS using both objective and subjective measures of ACOUSTIC QUALITY , SPEAKER IDENTITY and FOREIGN ACCENT . \n",
            "this paper proposes a novel VOICE MORPHING STRATEGY for SPECTRAL MORPHING . the proposed VOICE MORPHING STRATEGY is based on a VOICE MORPHING STRATEGY for SPECTRAL MORPHING . the proposed VOICE MORPHING STRATEGY is based on the PULSE DENSITY MODULATION and the SPECTRAL DETAIL . the proposed VOICE MORPHING STRATEGY is based on a VOICE MORPHING STRATEGY , which is based on the PULSE DENSITY MODULATION and AVERAGING PULSES . the proposed VOICE MORPHING STRATEGY is applied to the CONTINUUM OF ACCENT TRANSFORMATIONS and the ACOUSTIC QUALITY for ARCTIC SPEAKERS and FOREIGN ACCENT . the performance of the proposed VOICE MORPHING STRATEGY is demonstrated on a variety of PARALLEL RECORDINGS . the results show that the proposed method is able to detect and track moving objects in a scene , and the ACOUSTIC QUALITY of the proposed method is comparable to the state of the art .\n",
            "\n",
            "236 1000\n",
            "problem context open problem : exact <unk> nearest neighbor search in HAMMING DISTANCE on BINARY CODES . \n",
            "this paper addresses the problem of PROBLEM CONTEXT OPEN PROBLEM in the presence of BINARY CODES . we propose a method to solve the PROBLEM CONTEXT OPEN PROBLEM with BINARY CODES . we show that this problem can be solved by solving a PROBLEM CONTEXT OPEN PROBLEM with HAMMING DISTANCE . we show that our algorithm is able to solve the problem of PROBLEM CONTEXT OPEN PROBLEM in the presence of HAMMING DISTANCE .\n",
            "\n",
            "237 1000\n",
            "for the purpose of efficient AUDIO CODING at LOW RATES , a new BIT ALLOCATION STRATEGY is proposed in this paper . the basic idea behind this BIT ALLOCATION STRATEGY is '' give bits to the band with the maximum <unk> '' or '' retrieve bits from the band with the maximum <unk> '' . the notion of '' <unk> efficiency '' is suggested and BIT ALLOCATION STRATEGY can be employed to construct a BIT ASSIGNMENT ALGORITHM operated at BAND-LEVEL as compared to the traditional FRAME-LEVEL BIT ASSIGNMENT METHODS . based on this BIT ASSIGNMENT ALGORITHM a new BIT ALLOCATION STRATEGY , called MAX-BNLR SCHEME , is designed for the MPEG-4 AAC . simulation results show that the performance of the MAX-BNLR SCHEME is significantly better than that of the MPEG-4 AAC VERIFICATION MODEL and is close to that of TB-ANMR -lsb- 3 -rsb- , which is the -lrb- nearly -rrb- optimal solution . moreover , the MAX-BNLR SCHEME has the advantages of LOW COMPUTATIONAL COMPLEXITY comparing to TB-ANMR . \n",
            "this paper presents a new MPEG-4 AAC VERIFICATION MODEL for AUDIO CODING in MPEG-4 AAC . the proposed MPEG-4 AAC VERIFICATION MODEL is based on the use of a MAX-BNLR SCHEME , a MAX-BNLR SCHEME , and a MAX-BNLR SCHEME . the proposed MPEG-4 AAC VERIFICATION MODEL is based on the use of a MAX-BNLR SCHEME , a MAX-BNLR SCHEME , a MAX-BNLR SCHEME , and a MAX-BNLR SCHEME . the proposed MAX-BNLR SCHEME is compared with the MPEG-4 AAC VERIFICATION MODEL and the MPEG-4 AAC VERIFICATION MODEL . the performance of the proposed MAX-BNLR SCHEME is evaluated in terms of the performance of the proposed MPEG-4 AAC VERIFICATION MODEL . the performance of the proposed MAX-BNLR SCHEME is evaluated on a variety of MPEG-4 AAC .\n",
            "\n",
            "238 1000\n",
            "hashing has been widely applied to approximate nearest neighbor search for LARGE-SCALE MULTIMEDIA RETRIEVAL . SUPERVISED HASHING improves the quality of HASH CODING by exploiting the SEMANTIC SIMILARITY on DATA PAIRS and has received increasing attention recently . for most existing SUPERVISED HASHING METHODS for IMAGE RETRIEVAL , an image is first represented as a vector of HAND-CRAFTED OR MACHINE-LEARNED FEATURES , then quantized by a separate QUANTIZATION STEP that generates BINARY CODES . however , SUBOPTIMAL HASH CODING may be produced , since the QUANTIZATION ERROR is not statistically minimized and the QUANTIZATION STEP is not optimally compatible with the HASH CODING . in this paper , we propose a novel DEEP QUANTIZATION NETWORK ARCHITECTURE for SUPERVISED HASHING , which learns IMAGE REPRESENTATION for HASH CODING and formally control the QUANTIZATION ERROR . the DEEP QUANTIZATION NETWORK ARCHITECTURE constitutes four key components : -lrb- 1 -rrb- a SUB-NETWORK with multiple CONVOLUTION-POOLING LAYERS to capture DEEP IMAGE REPRESENTATIONS ; -lrb- 2 -rrb- a fully connected bottleneck layer to generate DIMENSION-REDUCED REPRESENTATION optimal for HASH CODING ; -lrb- 3 -rrb- a PAIRWISE COSINE LOSS LAYER for SIMILARITY-PRESERVING LEARNING ; and -lrb- 4 -rrb- a PRODUCT QUANTIZA-TION LOSS for controlling HASHING QUALITY and the <unk> of BOTTLENECK REPRESENTATION . extensive experiments on standard IMAGE RETRIEVAL DATASETS show the proposed DEEP QUANTIZATION NETWORK ARCHITECTURE yields substantial boosts over latest state-of-the-art HASHING METHODS . \n",
            "this paper presents a novel DEEP QUANTIZATION NETWORK ARCHITECTURE for LARGE-SCALE MULTIMEDIA RETRIEVAL in LARGE-SCALE MULTIMEDIA RETRIEVAL . the proposed DEEP QUANTIZATION NETWORK ARCHITECTURE is based on the use of a PRODUCT QUANTIZA-TION LOSS and a PRODUCT QUANTIZA-TION LOSS . the proposed DEEP QUANTIZATION NETWORK ARCHITECTURE is based on the use of a PRODUCT QUANTIZA-TION LOSS and a PRODUCT QUANTIZA-TION LOSS . the proposed DEEP QUANTIZATION NETWORK ARCHITECTURE is based on the use of a PRODUCT QUANTIZA-TION LOSS and a PRODUCT QUANTIZA-TION LOSS . the proposed DEEP QUANTIZATION NETWORK ARCHITECTURE is applied to the IMAGE REPRESENTATION for LARGE-SCALE MULTIMEDIA RETRIEVAL . experimental results on the IMAGE RETRIEVAL DATASETS show that the proposed DEEP QUANTIZATION NETWORK ARCHITECTURE significantly outperforms conventional HASHING METHODS in terms of QUANTIZATION ERROR and QUANTIZATION ERROR . the performance of the proposed DEEP QUANTIZATION NETWORK ARCHITECTURE is demonstrated on a variety of IMAGE RETRIEVAL DATASETS .\n",
            "\n",
            "239 1000\n",
            "we address the problem of segmenting and RECOGNIZING OBJECTS in REAL WORLD IMAGES , focusing on challenging ARTICULATED CATEGORIES such as humans and other animals . for this purpose , we propose a novel design for REGION-BASED OBJECT DETECTORS that integrates efficiently top-down information from SCANNING-WINDOWS PART MODELS and GLOBAL APPEARANCE CUES . our detectors produce CLASS-SPECIFIC SCORES for BOTTOM-UP REGIONS , and then aggregate the votes of multiple overlapping candidates through PIXEL CLASSIFICATION . we evaluate our approach on the PASCAL SEGMENTATION CHALLENGE , and report competitive performance with respect to current leading techniques . on VOC2010 , our method obtains the best results in <unk> categories and the highest performance on ARTICULATED OBJECTS . \n",
            "this paper addresses the problem of RECOGNIZING OBJECTS in REAL WORLD IMAGES . we propose a novel approach to the PASCAL SEGMENTATION CHALLENGE from REAL WORLD IMAGES . the proposed approach is based on the use of CLASS-SPECIFIC SCORES and GLOBAL APPEARANCE CUES . the proposed approach is based on the use of CLASS-SPECIFIC SCORES , GLOBAL APPEARANCE CUES , and GLOBAL APPEARANCE CUES . the proposed method is evaluated on a PASCAL SEGMENTATION CHALLENGE and on a PASCAL SEGMENTATION CHALLENGE . the experimental results show that the proposed method outperforms the state-of-the-art methods in terms of both CLASS-SPECIFIC SCORES and GLOBAL APPEARANCE CUES .\n",
            "\n",
            "240 1000\n",
            "this paper presents a LOW-POWER BIT-SERIAL VITERBI DECODER CHIP with the CODING RATE r = 1 = 3 and the constraint length k = 9 -lrb- 256 states -rrb- . this LOW-POWER BIT-SERIAL VITERBI DECODER CHIP has been implemented using 0.5 m THREE-LAYER METAL CMOS TECHNOLOGY and is targeted for HIGH SPEED CONVOLU-TIONAL DECODING for next generation wireless applications such as WIDE-BAND CDMA MOBILE SYSTEMS and WIRELESS ATM LANS . the LOW-POWER BIT-SERIAL VITERBI DECODER CHIP is expected to operate at <unk> under 3.3 v and at <unk> under 1.8 v . the ADD-COMPARE-SELECT UNITS have been designed using BIT-SERIAL ARITHMETIC , which has made it feasible to execute 256 ACS OPERATIONS in parallel . for TRACE-BACK OPERATIONS , we have developed a novel POWER-EFFICIENT TRACE-BACK SCHEME and an APPLICATION-SPECIFIC MEMORY , which was designed considering that 256 bits should be written simultaneously for write operations but only one bit needs to be accessed for read operations . we have estimated that the LOW-POWER BIT-SERIAL VITERBI DECODER CHIP dissipates only <unk> at 2MBPS OPERATION under 1.8 v. \n",
            "this paper presents a novel POWER-EFFICIENT TRACE-BACK SCHEME for WIDE-BAND CDMA MOBILE SYSTEMS . the proposed POWER-EFFICIENT TRACE-BACK SCHEME is based on the use of a 2MBPS OPERATION and a POWER-EFFICIENT TRACE-BACK SCHEME . the proposed POWER-EFFICIENT TRACE-BACK SCHEME is based on a POWER-EFFICIENT TRACE-BACK SCHEME of the LOW-POWER BIT-SERIAL VITERBI DECODER CHIP . the proposed POWER-EFFICIENT TRACE-BACK SCHEME is based on a POWER-EFFICIENT TRACE-BACK SCHEME . the proposed POWER-EFFICIENT TRACE-BACK SCHEME is applied to the LOW-POWER BIT-SERIAL VITERBI DECODER CHIP and the LOW-POWER BIT-SERIAL VITERBI DECODER CHIP . the performance of the proposed POWER-EFFICIENT TRACE-BACK SCHEME is evaluated on a WIRELESS ATM LANS and a WIRELESS ATM LANS . the performance of the proposed POWER-EFFICIENT TRACE-BACK SCHEME is evaluated on a WIRELESS ATM LANS and a WIRELESS ATM LANS . the results show that the proposed method is effective in reducing the CODING RATE and the CODING RATE of the proposed method .\n",
            "\n",
            "241 1000\n",
            "in current SPEECH RECOGNITION SYSTEMS mainly SHORT-TIME FOURIER TRANSFORM BASED FEATURES like MFCC are applied . dropping the short-time stationarity assumption of the VOICED SPEECH , this paper introduces the NON-STATIONARY SIGNAL ANALYSIS into the ASR FRAMEWORK . we present new ACOUSTIC FEATURES extracted by a PITCH-ADAPTIVE GAMMATONE FILTER BANK . the NOISE ROBUSTNESS was proved on aurora 2 and 4 tasks , where the proposed ACOUSTIC FEATURES outperform the standard MFCC . furthermore , successful combination experiments via MFCC indicate the differences between the new ACOUSTIC FEATURES and MFCC . \n",
            "this paper presents a novel approach to NON-STATIONARY SIGNAL ANALYSIS based on a PITCH-ADAPTIVE GAMMATONE FILTER BANK . the proposed method is based on the use of a PITCH-ADAPTIVE GAMMATONE FILTER BANK , a PITCH-ADAPTIVE GAMMATONE FILTER BANK , and a PITCH-ADAPTIVE GAMMATONE FILTER BANK . the proposed method is based on a PITCH-ADAPTIVE GAMMATONE FILTER BANK , which is based on a PITCH-ADAPTIVE GAMMATONE FILTER BANK . the experimental results show that the proposed ACOUSTIC FEATURES outperform the conventional MFCC in terms of both NOISE ROBUSTNESS and MFCC . the performance of the proposed ACOUSTIC FEATURES is evaluated in terms of NOISE ROBUSTNESS and MFCC .\n",
            "\n",
            "242 1000\n",
            "statistical approaches for building NON-RIGID DEFORMABLE MODELS , such as the ACTIVE APPEARANCE MODEL , have enjoyed great popularity in recent years , but typically require tedious MANUAL ANNOTATION OF TRAINING IMAGES . in this paper , a LEARNING BASED APPROACH for the AUTOMATIC ANNOTATION OF VISUALLY DEFORMABLE OBJECTS from a single ANNOTATED FRONTAL IMAGE is presented and demonstrated on the example of AUTOMATICALLY ANNOTATING FACE IMAGES that can be used for building ACTIVE APPEARANCE MODEL for FITTING and TRACKING . this LEARNING BASED APPROACH employs the idea of initially learning the CORRESPONDENCES BETWEEN LANDMARKS in a FRONTAL IMAGE and a set of training images with a face in arbitrary poses . using this learner , VIRTUAL IMAGES OF UNSEEN FACES at any ARBITRARY POSE for which the learner was trained can be reconstructed by predicting the new LANDMARK LOCATIONS and warping the TEXTURE from the FRONTAL IMAGE . VIEW-BASED AAMS are then built from the VIRTUAL IMAGES and used for automatically annotating UNSEEN IMAGES , including images of different facial expressions , at any random pose within the MAXIMUM RANGE spanned by the virtually RECONSTRUCTED IMAGES . the LEARNING BASED APPROACH is experimentally validated by AUTOMATICALLY ANNOTATING FACE IMAGES from three different databases . \n",
            "in this paper , we propose a novel LEARNING BASED APPROACH for AUTOMATICALLY ANNOTATING FACE IMAGES and TRACKING . the proposed ACTIVE APPEARANCE MODEL is based on AUTOMATICALLY ANNOTATING FACE IMAGES and FITTING . the proposed ACTIVE APPEARANCE MODEL is based on a LEARNING BASED APPROACH that exploits the CORRESPONDENCES BETWEEN LANDMARKS between the LANDMARK LOCATIONS and the FRONTAL IMAGE . the proposed ACTIVE APPEARANCE MODEL is based on a LEARNING BASED APPROACH that exploits the CORRESPONDENCES BETWEEN LANDMARKS between the LANDMARK LOCATIONS and the LANDMARK LOCATIONS . the proposed LEARNING BASED APPROACH is applied to AUTOMATICALLY ANNOTATING FACE IMAGES and TRACKING . experimental results demonstrate the effectiveness of the proposed LEARNING BASED APPROACH in AUTOMATICALLY ANNOTATING FACE IMAGES and TRACKING .\n",
            "\n",
            "243 1000\n",
            "in this paper , we present a new method for FACIAL AGE ESTIMATION based on ORDINAL DISCRIMINATIVE FEATURE LEARNING . considering the temporally ordinal and continuous characteristic of AGING PROCESS , the proposed method not only aims at preserving the LOCAL MANIFOLD STRUCTURE OF FACIAL IMAGES , but also it wants to keep the ORDINAL INFORMATION among AGING FACES . moreover , we try to remove REDUNDANT INFORMATION from both the LOCALITY INFORMATION and ORDINAL INFORMATION as much as possible by minimizing NONLINEAR CORRELATION and RANK CORRELATION . finally , we formulate these two issues into a unified optimization problem of FEATURE SELECTION and present an efficient solution . the experiments are conducted on the PUBLIC AVAILABLE IMAGES OF GROUPS DATASET and the FG-NET DATASET , and the experimental results demonstrate the power of the proposed method against the state-of-the-art methods . \n",
            "in this paper , we propose a novel approach to FACIAL AGE ESTIMATION based on ORDINAL DISCRIMINATIVE FEATURE LEARNING . the proposed approach is based on the idea of ORDINAL DISCRIMINATIVE FEATURE LEARNING , which is based on a LOCAL MANIFOLD STRUCTURE OF FACIAL IMAGES and LOCALITY INFORMATION . the proposed approach is based on the use of ORDINAL DISCRIMINATIVE FEATURE LEARNING and RANK CORRELATION . the proposed approach is based on the use of ORDINAL DISCRIMINATIVE FEATURE LEARNING and RANK CORRELATION . the proposed approach is evaluated on the PUBLIC AVAILABLE IMAGES OF GROUPS DATASET , and the results show that the proposed method is effective in improving the FACIAL AGE ESTIMATION performance .\n",
            "\n",
            "244 1000\n",
            "we examine the SET COVERING MACHINE when SET COVERING MACHINE uses DATA-DEPENDENT HALF-SPACES for its set of FEATURES and bound its GENERALIZATION ERROR in terms of the number of training errors and the number of <unk> SET COVERING MACHINE achieves on the training data . we show that SET COVERING MACHINE provides a favorable alternative to DATA-DEPENDENT BALLS on some NATURAL DATA SETS . compared to the SUPPORT VECTOR MACHINE , the SET COVERING MACHINE with DATA-DEPENDENT HALF-SPACES produces substantially sparser clas-sifiers with comparable -lrb- and sometimes better -rrb- generalization . furthermore , we show that our bound on the GENERALIZATION ERROR provides an effective guide for MODEL SELECTION . \n",
            "this paper addresses the problem of MODEL SELECTION in a SUPPORT VECTOR MACHINE . we propose a new SET COVERING MACHINE , called SUPPORT VECTOR MACHINE , which is able to learn a SUPPORT VECTOR MACHINE from a set of NATURAL DATA SETS . the proposed SET COVERING MACHINE is based on the idea of SUPPORT VECTOR MACHINE , which is a generalization of the standard SET COVERING MACHINE . the proposed SET COVERING MACHINE is applied to the problem of MODEL SELECTION in a SET COVERING MACHINE . experimental results show that the proposed SET COVERING MACHINE outperforms the standard SET COVERING MACHINE in terms of both GENERALIZATION ERROR and GENERALIZATION ERROR .\n",
            "\n",
            "245 1000\n",
            "in the learning process of SPEECH MODELING , many choices or settings are defined '' a priori '' or are resulting from years of experimental work . in this paper , instead , a GLOBAL LEARNING SCHEME is proposed based on a DISTRIBUTED GENETIC ALGORITHM combined with a standard SPEECH-MODELING ALGORITHM . the SPEECH RECOGNITION MODELS are now created out of a pre-defined SPACE of solutions . furthermore , this GLOBAL LEARNING SCHEME enables to learn the SPEECH MODELS as well as the best FEATURE EXTRACTION MODULE . experimental validation is performed on the task of discovering the WAVELET PACKET BEST BASIS DECOMPOSITION , knowing that the '' a PRIORI '' REFERENCE is the MEL-SCALED SUBBAND DECOMPOSITION . two experiments are presented , a REFERENCE SYSTEM using a SIMULATED FITNESS and a second one that uses the SPEECH RECOGNITION performance as FITNESS VALUE . in the latter , each element of the SPACE is a CONNECTIONIST SYSTEM defined by a WAVELET TOPOLOGY and its associated NEURAL NETWORK . \n",
            "this paper presents a novel DISTRIBUTED GENETIC ALGORITHM for SPEECH MODELING and SPEECH RECOGNITION . the proposed DISTRIBUTED GENETIC ALGORITHM is based on a PRIORI '' REFERENCE , which is a PRIORI '' REFERENCE of a PRIORI '' REFERENCE . the proposed GLOBAL LEARNING SCHEME is based on a PRIORI '' REFERENCE , which is a PRIORI '' REFERENCE of a PRIORI '' REFERENCE . the proposed GLOBAL LEARNING SCHEME is based on a SPEECH-MODELING ALGORITHM , which is based on the WAVELET PACKET BEST BASIS DECOMPOSITION . the proposed GLOBAL LEARNING SCHEME is based on the use of a PRIORI '' REFERENCE , which is a PRIORI '' REFERENCE . the proposed GLOBAL LEARNING SCHEME is applied to SPEECH RECOGNITION and SPEECH RECOGNITION . the experimental results show that the proposed GLOBAL LEARNING SCHEME is effective in improving the SPEECH RECOGNITION and SPEECH RECOGNITION performance .\n",
            "\n",
            "246 1000\n",
            "in this paper we report our results concerning the study of multivariate functions of <unk> signals . in particular we show that MULTILINEAR TENSOR FORMS of the decomposed signal yield a CLASS OF LTERS that we propose to call PIECEWISE VOLTERRA FILTERS . a LTER can be viewed as a transformation of r n ! r , where n is the number of LTER TAPS . PWV LTERS PARTITION R N using a HY-PER-RECTANGULAR LATTICE , and assign a VOLTERRA LTER to each of the partition regions . at the PARTITION BOUNDARIES CONTINUITY between the MULTIVARIATE POLYNOMIALS is preserved resulting in class c 0 piecewise polynomials . PIECEWISE VOLTERRA FILTERS constitute an eecient alternative for describing some systems rich in HARD NONLINEAR STRUCTURES , especially since PARAMETER ESTIMATION remains a LINEAR PROBLEM for PWV 'S . \n",
            "this paper addresses the problem of PARAMETER ESTIMATION for PIECEWISE VOLTERRA FILTERS . we propose a PWV LTERS PARTITION R N for the PWV LTERS PARTITION R N , which is based on the PWV LTERS PARTITION R N . the proposed PWV LTERS PARTITION R N is based on the PWV LTERS PARTITION R N , which is based on a VOLTERRA LTER . the proposed PWV LTERS PARTITION R N is based on the PWV LTERS PARTITION R N , which is based on the PWV LTERS PARTITION R N . the proposed PWV LTERS PARTITION R N is based on the PWV LTERS PARTITION R N of the PWV LTERS PARTITION R N , which is a LINEAR PROBLEM . the proposed PWV LTERS PARTITION R N is applied to the PWV LTERS PARTITION R N of the HY-PER-RECTANGULAR LATTICE , and the results show that the proposed PWV LTERS PARTITION R N is effective in reducing the number of LTER TAPS in the HY-PER-RECTANGULAR LATTICE .\n",
            "\n",
            "247 1000\n",
            "semi-supervised learning , which uses UNLABELED DATA to help learn a DISCRIMINATIVE MODEL , is especially important for STRUCTURED OUTPUT PROBLEMS , as considerably more effort is needed to label its MULTI-DIMENSIONAL OUTPUTS versus standard single output problems . we propose a new MAX-MARGIN FRAMEWORK for SEMI-SUPERVISED STRUCTURED OUTPUT LEARNING , that allows the use of powerful DISCRETE OPTIMIZATION ALGORITHMS and HIGH ORDER REGULAR-IZERS defined directly on MODEL PREDICTIONS for the UNLABELED EXAMPLES . we show that our MAX-MARGIN FRAMEWORK is closely related to POSTERIOR REGULARIZA-TION , and the two MAX-MARGIN FRAMEWORK optimize special cases of the same objective . the new MAX-MARGIN FRAMEWORK is instantiated on two IMAGE SEGMENTATION TASKS , using both a GRAPH REGULARIZER and a CARDINALITY REGULARIZER . experiments also demonstrate that this MAX-MARGIN FRAMEWORK can utilize UNLABELED DATA from a different source than the LABELED DATA to significantly improve performance while saving labeling effort . \n",
            "this paper addresses the problem of SEMI-SUPERVISED STRUCTURED OUTPUT LEARNING from UNLABELED DATA . we propose a MAX-MARGIN FRAMEWORK for SEMI-SUPERVISED STRUCTURED OUTPUT LEARNING in IMAGE SEGMENTATION TASKS . the proposed DISCRIMINATIVE MODEL is based on the use of UNLABELED DATA and the CARDINALITY REGULARIZER . the proposed DISCRIMINATIVE MODEL is based on the idea of POSTERIOR REGULARIZA-TION , which is based on the CARDINALITY REGULARIZER . the proposed DISCRIMINATIVE MODEL is based on the use of UNLABELED DATA to estimate the CARDINALITY REGULARIZER . the proposed DISCRIMINATIVE MODEL is applied to IMAGE SEGMENTATION TASKS , such as the CARDINALITY REGULARIZER and the CARDINALITY REGULARIZER . the experimental results show that the proposed DISCRIMINATIVE MODEL is effective in improving the IMAGE SEGMENTATION TASKS performance of the proposed DISCRIMINATIVE MODEL .\n",
            "\n",
            "248 1000\n",
            "in this paper , we explore the problem of MULTIVIEW SUB-SPACE CLUSTERING . we introduce a LOW-RANK TENSOR CONSTRAINT to explore the COMPLEMENTARY INFORMATION from multiple views and , accordingly , establish a novel method called low-rank tensor constrained <unk> subspace clustering -lrb- <unk> -rrb- . our method regards the SUBSPACE REPRESENTATION MATRICES of different views as a LOW-RANK TENSOR CONSTRAINT , which captures <unk> the HIGH ORDER CORRELATIONS underlying MULTI-VIEW DATA . then the LOW-RANK TENSOR CONSTRAINT is equipped with a LOW-RANK CONSTRAINT , which models elegantly the cross information among different views , reduces <unk> the redundancy of the learned SUBSPACE REPRESENTATIONS , and improves the ACCURACY of CLUSTERING as well . the INFERENCE PROCESS of the AFFINITY MATRIX for CLUSTERING is formulated as a TEN-SOR NUCLEAR NORM MINIMIZATION PROBLEM , constrained with an additional ℓ 2,1-NORM REGULARIZER and some LINEAR EQUALITIES . the MINIMIZATION PROBLEM is convex and thus can be solved efficiently by an augmented <unk> alternating direction minimization -lrb- <unk> -rrb- method . extensive experimental results on four BENCHMARK IMAGE DATASETS show the effectiveness of the proposed LT-MSC METHOD . \n",
            "this paper addresses the problem of MULTIVIEW SUB-SPACE CLUSTERING in MULTI-VIEW DATA . in particular , we propose a LT-MSC METHOD to the problem of MULTIVIEW SUB-SPACE CLUSTERING . the proposed LT-MSC METHOD is based on a LOW-RANK TENSOR CONSTRAINT , which is a LOW-RANK CONSTRAINT of the AFFINITY MATRIX . the proposed LT-MSC METHOD is based on a LOW-RANK TENSOR CONSTRAINT , which is a LOW-RANK CONSTRAINT of the AFFINITY MATRIX . the proposed LT-MSC METHOD is applied to the problem of CLUSTERING , which is a TEN-SOR NUCLEAR NORM MINIMIZATION PROBLEM . the proposed LT-MSC METHOD is evaluated on two BENCHMARK IMAGE DATASETS . the experimental results on BENCHMARK IMAGE DATASETS show that the proposed LT-MSC METHOD is effective in improving the ACCURACY of the proposed LT-MSC METHOD .\n",
            "\n",
            "249 1000\n",
            "planning landmarks are facts that must be true at some point in every SOLUTION PLAN . previous work has very successfully exploited PLANNING LANDMARKS in SATISFICING PLANNING . we propose a methodology for deriving ADMISSIBLE HEURISTIC ESTIMATES for COST-OPTIMAL PLANNING from a set of PLANNING LANDMARKS . the resulting HEURISTICS fall into a novel class of MULTI-PATH DEPENDENT HEURISTICS , and we present a simple BEST-FIRST SEARCH PROCEDURE exploiting such HEURISTICS . our empirical evaluation shows that this framework favorably competes with the state-of-the-art of COST-OPTIMAL HEURISTIC SEARCH . \n",
            "this paper addresses the problem of COST-OPTIMAL PLANNING in SATISFICING PLANNING . we propose a novel method for COST-OPTIMAL PLANNING based on HEURISTICS . the proposed approach is based on the use of HEURISTICS for COST-OPTIMAL PLANNING . the proposed approach is based on the use of HEURISTICS , which is a generalization of the standard BEST-FIRST SEARCH PROCEDURE . the proposed approach is based on the use of HEURISTICS to improve the performance of COST-OPTIMAL PLANNING . the proposed method is evaluated on a variety of PLANNING LANDMARKS , and the experimental results demonstrate the effectiveness of the proposed method .\n",
            "\n",
            "250 1000\n",
            "this paper considers the BLIND SEPARATION OF NONSTATIONARY SOURCES in the UNDERDETERMINED CONVOLUTIVE MIXTURE CASE . we introduce two methods based on the sparsity assumption of the sources in the TIME-FREQUENCY DOMAIN . the first one assumes that the sources are DISJOINT in the TF DOMAIN ; i.e. there is at most one source signal present at a given point in the TF DOMAIN . in the second method , we relax this assumption by allowing the sources to be TF-NONDISJOINT to a certain extent . in particular , the number of sources present -lrb- active -rrb- at a TF POINT should be strictly less than the number of sensors . in that case , the BLIND SEPARATION OF NONSTATIONARY SOURCES can be achieved thanks to SUBSPACE PROJECTION which allows us to identify the ACTIVE SOURCES and to estimate their corresponding TIME-FREQUENCY DISTRIBUTION VALUES . \n",
            "this paper addresses the problem of BLIND SEPARATION OF NONSTATIONARY SOURCES from a UNDERDETERMINED CONVOLUTIVE MIXTURE CASE . in particular , we consider the problem of BLIND SEPARATION OF NONSTATIONARY SOURCES in a TIME-FREQUENCY DOMAIN . we propose a method to estimate the TIME-FREQUENCY DISTRIBUTION VALUES of the ACTIVE SOURCES using a SUBSPACE PROJECTION . we show that this problem can be solved using a SUBSPACE PROJECTION . we show that the proposed algorithm is able to recover the TF POINT of the ACTIVE SOURCES , and is able to recover the TF POINT from the TF DOMAIN .\n",
            "\n",
            "251 1000\n",
            "in this paper , we propose a new RATE CONTROL SCHEME designed for the <unk> high efficiency video coding -lrb- <unk> -rrb- standard , and aimed at enhancing the quality of regions of interest -lrb- roi -rrb- . our RATE CONTROL SCHEME allocates a higher bit rate to the region of interest while keeping the GLOBAL BIT RATE close to the assigned target value . this RATE CONTROL SCHEME is developed for a VIDEOCONFERENCING SYSTEM , where the ROIS -lrb- typically , faces -rrb- are automatically detected and each CODING UNIT is classified in a region of the interest map . this map is given as input to the RATE CONTROL ALGORITHM and the BIT ALLOCATION is made accordingly . experimental results show that the proposed RATE CONTROL SCHEME achieves accurate TARGET BIT RATES and provides an improvement in the REGION OF INTEREST QUALITY , both in OBJECTIVE METRICS and based on SUBJECTIVE QUALITY EVALUATION . \n",
            "in this paper , we propose a novel RATE CONTROL ALGORITHM for VIDEOCONFERENCING SYSTEM . the proposed RATE CONTROL SCHEME is based on a RATE CONTROL ALGORITHM for BIT ALLOCATION . the proposed RATE CONTROL SCHEME is based on a RATE CONTROL ALGORITHM that uses a RATE CONTROL ALGORITHM to estimate the TARGET BIT RATES of the VIDEOCONFERENCING SYSTEM . the performance of the proposed RATE CONTROL ALGORITHM is evaluated on the SUBJECTIVE QUALITY EVALUATION and the SUBJECTIVE QUALITY EVALUATION of the VIDEOCONFERENCING SYSTEM . the performance of the proposed VIDEOCONFERENCING SYSTEM is evaluated in terms of SUBJECTIVE QUALITY EVALUATION and TARGET BIT RATES .\n",
            "\n",
            "252 1000\n",
            "<unk> object BOUNDING BOX is a simple and popular INTERACTION PARADIGM considered by many existing INTERACTIVE IMAGE SEGMENTATION FRAMEWORKS . however , these frameworks tend to exploit the provided BOUNDING BOX merely to exclude its <unk> from consideration and sometimes to initialize the ENERGY MINIMIZATION . in this paper , we discuss how the BOUNDING BOX can be further used to impose a powerful TOPOLOGICAL PRIOR , which prevents the solution from EXCESSIVE SHRINKING and ensures that the USER-PROVIDED BOX bounds the segmentation in a sufficiently tight way . the TOPOLOGICAL PRIOR is expressed using HARD CONSTRAINTS incorporated into the GLOBAL ENERGY MINIMIZATION FRAMEWORK leading to an NP-HARD INTEGER PROGRAM . we then investigate the possible OPTIMIZATION STRATEGIES including LINEAR RELAXATION as well as a new GRAPH CUT ALGORITHM called PINPOINTING . the latter can be used either as a ROUNDING METHOD for the FRACTIONAL LP SOLUTION , which is provably better than THRESHOLDING-BASED ROUNDING , or as a fast STANDALONE HEURISTIC . we evaluate the proposed algorithms on a PUBLICLY AVAILABLE DATASET , and demonstrate the practical benefits of the new TOPOLOGICAL PRIOR both qualitatively and quantitatively . \n",
            "in this paper , we propose a novel GRAPH CUT ALGORITHM for INTERACTIVE IMAGE SEGMENTATION FRAMEWORKS . the proposed GRAPH CUT ALGORITHM is based on a GRAPH CUT ALGORITHM , which is a GLOBAL ENERGY MINIMIZATION FRAMEWORK with HARD CONSTRAINTS . the proposed GRAPH CUT ALGORITHM is based on a GRAPH CUT ALGORITHM , which is based on the INTERACTION PARADIGM . the proposed GRAPH CUT ALGORITHM is based on a GRAPH CUT ALGORITHM , called LINEAR RELAXATION , to estimate the TOPOLOGICAL PRIOR of the FRACTIONAL LP SOLUTION . the proposed GLOBAL ENERGY MINIMIZATION FRAMEWORK is based on a GRAPH CUT ALGORITHM , which is a LINEAR RELAXATION with HARD CONSTRAINTS . the proposed GRAPH CUT ALGORITHM is compared to THRESHOLDING-BASED ROUNDING , which is based on the INTERACTION PARADIGM . the proposed ROUNDING METHOD is compared to THRESHOLDING-BASED ROUNDING , which is based on a PUBLICLY AVAILABLE DATASET . the experimental results show that the proposed FRACTIONAL LP SOLUTION can significantly improve the performance of the proposed FRACTIONAL LP SOLUTION .\n",
            "\n",
            "253 1000\n",
            "in this paper , the application of the SINUSOIDAL MODEL for AUDIO/SPEECH SIGNALS to the WATERMARKING TASK is proposed . the basic idea is that adequate MODULATION of medium rank partials -lrb- frequency -rrb- trajectories is not perceptible and thus this MODULATION may contain the data to be embedded in the signal . the MODULATION -lrb- encoding -rrb- and estimation -lrb- decoding -rrb- of the message are described in this paper and preliminary promising results are given in the case of SPEECH SIGNALS . \n",
            "this paper presents a novel SINUSOIDAL MODEL for SPEECH SIGNALS . the proposed SINUSOIDAL MODEL is based on a SINUSOIDAL MODEL for the WATERMARKING TASK . the proposed SINUSOIDAL MODEL is based on the use of the SINUSOIDAL MODEL to estimate the MODULATION . the proposed SINUSOIDAL MODEL is applied to the problem of AUDIO/SPEECH SIGNALS from SPEECH SIGNALS . the experimental results show that the proposed SINUSOIDAL MODEL is effective in improving the performance of the proposed SINUSOIDAL MODEL .\n",
            "\n",
            "254 1000\n",
            "* <unk> -rrb- <unk> in this paper , a new FILTER that is performing COLOR IMAGE ENHANCEMENT is presented . the FILTER is achieving this through the minimization of a WEIGHTED COST FUNCTION . the weights are determined using potential functions which are calculated in such a way as to convey SPATIAL INFORMATION . application of the proposed FILTER on a REAL BLURRED AND NOISY COLOR IMAGE is performed to verify its ENHANCEMENT CAPABILITIES . \n",
            "in this paper , we propose a novel approach to COLOR IMAGE ENHANCEMENT based on a FILTER for COLOR IMAGE ENHANCEMENT . the proposed FILTER is based on the FILTER , which is based on a FILTER for COLOR IMAGE ENHANCEMENT . the proposed FILTER is applied to the problem of COLOR IMAGE ENHANCEMENT . the experimental results show that the proposed method outperforms the existing methods in terms of ENHANCEMENT CAPABILITIES .\n",
            "\n",
            "255 1000\n",
            "we address two open theoretical questions in POLICY GRADIENT REINFORCEMENT LEARNING . the first concerns the efficacy of using FUNCTION APPROXIMATION to represent the STATE ACTION VALUE FUNCTION , <unk> theory is presented showing that LINEAR FUNCTION APPROXIMATION REPRESENTATIONS OF Q can degrade the rate of convergence of PERFORMANCE GRADIENT ESTIMATES by a factor of o -lrb- m l -rrb- relative to when no FUNCTION APPROXIMATION of q is used , where m is the number of possible actions and l is the number of BASIS FUNCTIONS in the FUNCTION APPROXIMATION REPRESENTATION . the second concerns the use of a BIAS TERM in estimating the STATE ACTION VALUE FUNCTION . theory is presented showing that a NON-ZERO BIAS TERM can improve the rate of convergence of PERFORMANCE GRADIENT ESTIMATES by o -lrb- 1 − -lrb- <unk> -rrb- -rrb- , where m is the number of possible actions . experimental evidence is presented showing that these theoretical results lead to significant improvement in the convergence properties of POLICY GRADIENT REINFORCEMENT LEARNING ALGORITHMS . \n",
            "in this paper , we propose a novel method to estimate the STATE ACTION VALUE FUNCTION of the STATE ACTION VALUE FUNCTION . the proposed POLICY GRADIENT REINFORCEMENT LEARNING ALGORITHMS is based on the BIAS TERM of the STATE ACTION VALUE FUNCTION of the STATE ACTION VALUE FUNCTION . the proposed method is based on a LINEAR FUNCTION APPROXIMATION REPRESENTATIONS OF Q , which is based on the LINEAR FUNCTION APPROXIMATION REPRESENTATIONS OF Q . the proposed method is based on the use of a BIAS TERM to estimate the STATE ACTION VALUE FUNCTION of the STATE ACTION VALUE FUNCTION . the PERFORMANCE GRADIENT ESTIMATES of the proposed method is compared with the results obtained using the proposed method . the experimental results show that the proposed method is effective in improving the PERFORMANCE GRADIENT ESTIMATES of the proposed method .\n",
            "\n",
            "256 1000\n",
            "good DIALOGUE STRATEGIES in SPOKEN DIALOGUE SYSTEMS help to ensure and maintain MUTUAL UNDERSTANDING and thus play a crucial role in robust CONVERSATIONAL INTERACTION . we focus on CLARIFICATION STRATEGIES and build USER SIMULATIONS which are critical for REINFORCEMENT LEARNING , which is a cheap and principled way to automatically optimise DIALOGUE MANAGEMENT . in this paper we present a novel CLUSTER-BASED TECHNIQUE for building USER SIMULATIONS which show varying , but complete and consistent behaviour with respect to real users . we use this CLUSTER-BASED TECHNIQUE to build USER SIMULATIONS and we also introduce the CLUSTER-BASED USER SIMULATION TECHNIQUE which allows us to evaluate USER SIMULATIONS with respect to these desiderata . we show that the CLUSTER-BASED USER SIMULATION TECHNIQUE performs significantly better -lrb- at p < 0.01 -rrb- than decisions made using either the one most likely action or a RANDOM BASE-LINE . the CLUSTER-BASED USER SIMULATIONS reduce the average error of these other CLUSTER-BASED USER SIMULATION TECHNIQUE by 53 % and 34 % respectively . \n",
            "this paper presents a novel CLUSTER-BASED USER SIMULATION TECHNIQUE for DIALOGUE MANAGEMENT in SPOKEN DIALOGUE SYSTEMS . the proposed CLUSTER-BASED USER SIMULATION TECHNIQUE is based on the use of CLARIFICATION STRATEGIES for DIALOGUE MANAGEMENT . the proposed CLUSTER-BASED USER SIMULATION TECHNIQUE is based on the use of CLARIFICATION STRATEGIES for DIALOGUE MANAGEMENT . the proposed CLUSTER-BASED USER SIMULATION TECHNIQUE is based on the use of CLARIFICATION STRATEGIES for DIALOGUE MANAGEMENT . the proposed CLUSTER-BASED USER SIMULATION TECHNIQUE is applied to DIALOGUE MANAGEMENT for DIALOGUE MANAGEMENT . the experimental results show that the proposed CLUSTER-BASED USER SIMULATION TECHNIQUE significantly improves the performance of the proposed CLUSTER-BASED USER SIMULATION TECHNIQUE . moreover , the proposed CLUSTER-BASED USER SIMULATION TECHNIQUE can also be applied to SPOKEN DIALOGUE SYSTEMS for DIALOGUE MANAGEMENT .\n",
            "\n",
            "257 1000\n",
            "this paper examines the effect of sensor performance on SPEAKER DIARISATION in meetings and investigates the use of more advanced BEAMFORMING TECHNIQUES , beyond the typically employed DELAY-SUM BEAMFORMER , for mitigating the effects of poorer sensor performance . we present SUPER-DIRECTIVE BEAMFORMING and investigate how different time difference of ARRIVAL SMOOTHING and BEAMFORMING TECHNIQUES influence the performance of state-of-the-art DIAR-ISATION SYSTEMS . we produced and transcribed a new CORPUS OF MEETINGS recorded in the INSTRUMENTED MEETING ROOM using a high SNR ANALOGUE and a newly developed low snr DIGITAL MEMS MICROPHONE ARRAY -lrb- <unk> .2 -rrb- . this research demonstrates that DELAY-SUM BEAMFORMER has a significant effect on the DIARISATION ERROR RATE and that simple NOISE REDUCTION and BEAMFORMING SCHEMES suffice to overcome AUDIO SIGNAL DEGRADATION due to the lower snr of modern MEMS MICROPHONES . INDEX TERMS -- speaker <unk> in meetings , DIGITAL MEMS MICROPHONE ARRAY , time difference of arrival -lrb- tdoa -rrb- , <unk> beamforming \n",
            "this paper addresses the problem of NOISE REDUCTION in DIAR-ISATION SYSTEMS . we propose a novel approach to the problem of NOISE REDUCTION in DIAR-ISATION SYSTEMS . the proposed method consists of two steps : -lrb- 1 -rrb- a DELAY-SUM BEAMFORMER of the INSTRUMENTED MEETING ROOM , and -lrb- 2 -rrb- a DELAY-SUM BEAMFORMER . the proposed method consists of two steps : -lrb- 1 -rrb- a DELAY-SUM BEAMFORMER of the DIARISATION ERROR RATE and -lrb- 2 -rrb- a DELAY-SUM BEAMFORMER . the proposed method is based on the use of ARRIVAL SMOOTHING and ARRIVAL SMOOTHING . the proposed method is evaluated in terms of DIARISATION ERROR RATE and DIARISATION ERROR RATE . the experimental results show that the proposed method achieves better NOISE REDUCTION performance in terms of DIARISATION ERROR RATE and BEAMFORMING SCHEMES .\n",
            "\n",
            "258 1000\n",
            "balancing between COMPUTATIONAL EFFICIENCY and SAMPLE EFFICIENCY is an important goal in REINFORCEMENT LEARNING . TEMPORAL DIFFERENCE LEARNING ALGORITHMS stochastically update the VALUE FUNCTION , with a LINEAR TIME COMPLEXITY in the number of FEATURES , whereas LEAST-SQUARES TEMPORAL DIFFERENCE ALGORITHMS are sample efficient but can be quadratic in the number of FEATURES . in this work , we develop an efficient INCREMENTAL LOW-RANK LSTD -LRB- -RRB- ALGORITHM that progresses towards the goal of better balancing computation and SAMPLE EFFICIENCY . the INCREMENTAL LOW-RANK LSTD -LRB- -RRB- ALGORITHM reduces the COMPUTATION AND STORAGE COMPLEXITY to the number of FEATURES times the chosen RANK PARAMETER while summarizing past samples efficiently to nearly obtain the SAMPLE EFFICIENCY of LSTD . we derive a simulation bound on the solution given by TRUNCATED LOW-RANK APPROXIMATION , illustrating a BIAS-VARIANCE TRADE-OFF dependent on the choice of RANK . we demonstrate that the INCREMENTAL LOW-RANK LSTD -LRB- -RRB- ALGORITHM effectively balances COMPUTATIONAL COMPLEXITY and SAMPLE EFFICIENCY for POLICY EVALUATION in a benchmark task and a HIGH-DIMENSIONAL ENERGY ALLOCATION DOMAIN . \n",
            "this paper presents a novel INCREMENTAL LOW-RANK LSTD -LRB- -RRB- ALGORITHM for REINFORCEMENT LEARNING . the proposed INCREMENTAL LOW-RANK LSTD -LRB- -RRB- ALGORITHM is based on a TRUNCATED LOW-RANK APPROXIMATION , which is a generalization of the INCREMENTAL LOW-RANK LSTD -LRB- -RRB- ALGORITHM to the HIGH-DIMENSIONAL ENERGY ALLOCATION DOMAIN . the proposed INCREMENTAL LOW-RANK LSTD -LRB- -RRB- ALGORITHM is based on a TRUNCATED LOW-RANK APPROXIMATION , which is a TRUNCATED LOW-RANK APPROXIMATION of the VALUE FUNCTION . the proposed INCREMENTAL LOW-RANK LSTD -LRB- -RRB- ALGORITHM is applied to the problem of REINFORCEMENT LEARNING , where the VALUE FUNCTION is modeled by a TRUNCATED LOW-RANK APPROXIMATION . the proposed INCREMENTAL LOW-RANK LSTD -LRB- -RRB- ALGORITHM is evaluated in terms of both COMPUTATION AND STORAGE COMPLEXITY and COMPUTATIONAL COMPLEXITY . the proposed INCREMENTAL LOW-RANK LSTD -LRB- -RRB- ALGORITHM is evaluated in terms of both COMPUTATION AND STORAGE COMPLEXITY and COMPUTATIONAL EFFICIENCY . the proposed INCREMENTAL LOW-RANK LSTD -LRB- -RRB- ALGORITHM is evaluated in terms of both COMPUTATION AND STORAGE COMPLEXITY and COMPUTATIONAL EFFICIENCY .\n",
            "\n",
            "259 1000\n",
            "this paper describes the application of EXPLANATION-BASED LEARNING , a MACHINE LEARNING TECHNIQUE , to the SRI CORE LANGUAGE ENGINE , a large scale general purpose natural language analysis system . the idea is to bypass normal morphological , syntactic and -lrb- partly -rrb- semantic processing , for most input sentences , instead using a set of learned MACHINE LEARNING TECHNIQUE . EXPLANATION-BASED LEARNING is used to extract the learned MACHINE LEARNING TECHNIQUE automatically from sample sentences submitted by a user and thus tune the system for that particular user . by indexing the learned MACHINE LEARNING TECHNIQUE efficiently , it is possible to achieve dramatic speed-ups . performance measurements were carried out using a training set of 1500 sentences and a separate test set of 100 sentences , all from the ATIS CORPUS . a set of <unk> learned MACHINE LEARNING TECHNIQUE was derived from the training set . these MACHINE LEARNING TECHNIQUE covered 90 percent of the test sentences and reduced the TOTAL PROCESSING TIME to a third . an overall speed-up of 50 percent was accomplished using a set of only 250 learned MACHINE LEARNING TECHNIQUE . \n",
            "this paper presents a novel MACHINE LEARNING TECHNIQUE for SRI CORE LANGUAGE ENGINE . the proposed MACHINE LEARNING TECHNIQUE is based on a MACHINE LEARNING TECHNIQUE and a MACHINE LEARNING TECHNIQUE . the proposed MACHINE LEARNING TECHNIQUE is based on the use of EXPLANATION-BASED LEARNING and MACHINE LEARNING TECHNIQUE . the proposed MACHINE LEARNING TECHNIQUE is evaluated on the ATIS CORPUS , and the results show that the proposed MACHINE LEARNING TECHNIQUE is effective in TOTAL PROCESSING TIME and TOTAL PROCESSING TIME .\n",
            "\n",
            "260 1000\n",
            "this paper addresses the problem of merging SPEECH ENHANCEMENT and coding in the context of an AUDITORY MODELING . the NOISY SIGNAL is rst processed by a fast WAVELET PACKET TRANSFORM ALGORITHM to obtain an AUDITORY SPECTRUM , from which a ROUGH MASKING MODEL is estimated . then , this model is used to <unk> a SUBTRACTIVE-TYPE ENHANCEMENT ALGORITHM . the enhanced SPEECH COECIENTS are then encoded in the same TIME-FREQUENCY TRANSFORM DOMAIN using MASKING THRESHOLD CONSTRAINTS for QUANTIZATION NOISE . the advantage of the proposed method is that both enhancement and coding are performed with the transform <unk> , without making use of the additional FFT PROCESSING . \n",
            "in this paper , we propose a novel approach to AUDITORY MODELING based on a ROUGH MASKING MODEL . the proposed approach is based on the use of MASKING THRESHOLD CONSTRAINTS in the TIME-FREQUENCY TRANSFORM DOMAIN , which is a ROUGH MASKING MODEL . the proposed method is based on the MASKING THRESHOLD CONSTRAINTS , which is a ROUGH MASKING MODEL . the proposed method is based on a ROUGH MASKING MODEL , which is based on a ROUGH MASKING MODEL . the proposed method is based on a ROUGH MASKING MODEL , which is based on a ROUGH MASKING MODEL . the proposed method is applied to SPEECH ENHANCEMENT in a TIME-FREQUENCY TRANSFORM DOMAIN . the experimental results show that the proposed method is effective in improving the SPEECH ENHANCEMENT performance in SPEECH ENHANCEMENT .\n",
            "\n",
            "261 1000\n",
            "the development of an AUTOMATIC SPEECH RECOGNITION SYSTEM for the BILINGUAL MEDIAPARL CORPUS is challenging for several reasons : -lrb- 1 -rrb- REVERBERANT RECORDINGS , -lrb- 2 -rrb- ACCENTED SPEECH , and -lrb- 3 -rrb- no prior information about the language . in that context , we employ FREQUENCY DOMAIN LINEAR PREDICTION-BASED FEATURES to reduce the effect of REVERBERATION , exploit BILINGUAL DEEP NEURAL NETWORKS applied in TANDEM AND HYBRID ACOUSTIC MODELING APPROACHES to significantly improve AUTOMATIC SPEECH RECOGNITION SYSTEM for AC-CENTED SPEECH and develop a fully AUTOMATIC SPEECH RECOGNITION SYSTEM using ENTROPY-BASED DECODING-GRAPH SELECTION . our experiments indicate that the proposed AUTOMATIC SPEECH RECOGNITION SYSTEM performs similar to a LANGUAGE-SPECIFIC ASR SYSTEM if approximately five seconds of speech are available . \n",
            "in this paper , we propose a novel AUTOMATIC SPEECH RECOGNITION SYSTEM based on BILINGUAL DEEP NEURAL NETWORKS for AC-CENTED SPEECH . the proposed AUTOMATIC SPEECH RECOGNITION SYSTEM is based on the use of BILINGUAL DEEP NEURAL NETWORKS for AC-CENTED SPEECH and AC-CENTED SPEECH . the proposed AUTOMATIC SPEECH RECOGNITION SYSTEM is based on the use of BILINGUAL DEEP NEURAL NETWORKS for ENTROPY-BASED DECODING-GRAPH SELECTION . the proposed AUTOMATIC SPEECH RECOGNITION SYSTEM is based on the use of BILINGUAL DEEP NEURAL NETWORKS for ENTROPY-BASED DECODING-GRAPH SELECTION . the experimental results show that the proposed AUTOMATIC SPEECH RECOGNITION SYSTEM outperforms the conventional AUTOMATIC SPEECH RECOGNITION SYSTEM in terms of both REVERBERATION and ACCENTED SPEECH .\n",
            "\n",
            "262 1000\n",
            "the recent proliferation of LARGE MULTIMEDIA COLLECTIONS has gathered immense attention from the speech research community , because SPEECH RECOGNITION enables the transcription and indexing of such LARGE MULTIMEDIA COLLECTIONS . TOPICALITY INFORMATION can be used to improve TRANSCRIPTION QUALITY and enable CONTENT NAVIGATION . in this paper , we give a novel QUALITY MEASURE for TOPIC SEGMEN-TATION ALGORITHMS that improves over previously used measures . our QUALITY MEASURE takes into account not only the presence or absence of TOPIC BOUNDARIES but also the content of the text or SPEECH SEGMENTS labeled as <unk> . additionally , we demonstrate that TOPIC SEGMENTATION QUALITY of SPOKEN LANGUAGE can be improved using SPEECH RECOGNITION LATTICES . using LATTICES , improvements over the BASELINE ONE-BEST TOPIC MODEL are observed when measured with the previously existing TOPIC SEGMENTATION QUALITY MEASURE , as well as the new QUALITY MEASURE proposed in this paper -lrb- 9.4 % and <unk> % RELATIVE ERROR REDUCTION , respectively -rrb- . \n",
            "this paper presents a novel TOPIC SEGMENTATION QUALITY MEASURE for SPEECH RECOGNITION based on LATTICES . the proposed QUALITY MEASURE is based on the use of LATTICES extracted from the LATTICES . the proposed TOPIC SEGMENTATION QUALITY MEASURE is evaluated on the BASELINE ONE-BEST TOPIC MODEL and compared with the conventional BASELINE ONE-BEST TOPIC MODEL . the proposed TOPIC SEGMENTATION QUALITY MEASURE is evaluated on the TOPIC SEGMENTATION QUALITY MEASURE and compared to the BASELINE ONE-BEST TOPIC MODEL . the TRANSCRIPTION QUALITY of the proposed TOPIC SEGMENTATION QUALITY MEASURE is achieved by the proposed TOPIC SEGMENTATION QUALITY MEASURE in comparison with the conventional BASELINE ONE-BEST TOPIC MODEL . the TRANSCRIPTION QUALITY of the proposed TOPIC SEGMENTATION QUALITY MEASURE is evaluated on the TOPIC SEGMENTATION QUALITY MEASURE and the TRANSCRIPTION QUALITY of the proposed TOPIC SEGMENTATION QUALITY MEASURE .\n",
            "\n",
            "263 1000\n",
            "we present a method for REGION IDENTIFICATION in multiple images . a set of regions in different images and the correspondences on their boundaries can be thought of as a boundary in the MULTI-DIMENSIONAL SPACE formed by the product of the individual image domains . we minimize an ENERGY FUNCTIONAL on the space of such boundaries , thereby identifying simultaneously both the optimal regions in each image and the optimal correspondences on their boundaries . we use a RATIO FORM for the ENERGY FUNCTIONAL , thus enabling the GLOBAL MINIMIZATION OF THE ENERGY FUNCTIONAL using a POLYNOMIAL TIME GRAPH ALGORITHM , among other desirable properties . we choose a simple form for this energy that <unk> boundaries that lie on HIGH INTENSITY GRADIENTS in each image , while encouraging correspondences between boundaries in different images that match INTENSITY VALUES . the latter tendency is weighted by a novel HEURISTIC ENERGY that encourages the boundaries to lie on disparity or OPTICAL FLOW DISCONTINUITIES , although no DENSE OPTICAL FLOW or DISPARITY MAP is computed . \n",
            "in this paper , we propose a novel POLYNOMIAL TIME GRAPH ALGORITHM for REGION IDENTIFICATION . the proposed POLYNOMIAL TIME GRAPH ALGORITHM is based on a GLOBAL MINIMIZATION OF THE ENERGY FUNCTIONAL and is solved using a POLYNOMIAL TIME GRAPH ALGORITHM . the proposed POLYNOMIAL TIME GRAPH ALGORITHM is based on a GLOBAL MINIMIZATION OF THE ENERGY FUNCTIONAL and the GLOBAL MINIMIZATION OF THE ENERGY FUNCTIONAL is solved by a POLYNOMIAL TIME GRAPH ALGORITHM . the proposed POLYNOMIAL TIME GRAPH ALGORITHM is based on a GLOBAL MINIMIZATION OF THE ENERGY FUNCTIONAL and the GLOBAL MINIMIZATION OF THE ENERGY FUNCTIONAL is solved using a POLYNOMIAL TIME GRAPH ALGORITHM . the proposed POLYNOMIAL TIME GRAPH ALGORITHM is applied to the problem of REGION IDENTIFICATION and REGION IDENTIFICATION . the experimental results show that the proposed method outperforms the existing methods in terms of REGION IDENTIFICATION and REGION IDENTIFICATION .\n",
            "\n",
            "264 1000\n",
            "assessing similarity between FEATURES is a key step in OBJECT RECOGNITION and SCENE CATEGORIZATION TASKS . we argue that knowledge on the DISTRIBUTION OF DISTANCES generated by SIMILARITY FUNCTIONS is crucial in deciding whether FEATURES are similar or not . intuitively one would expect that similarities between FEATURES could arise from any DISTRIBUTION . in this paper , we will derive the contrary , and report the theoretical result that L P-NORMS -- a class of commonly applied distance metrics -- from one FEATURE VECTOR to other vectors are <unk> if the FEATURE VALUES are correlated and <unk> distributed . besides these assumptions being realistic for IMAGES , we experimentally show them to hold for various popular FEATURE EXTRACTION ALGORITHMS , for a diverse range of IMAGES . this fundamental insight opens new directions in the assessment of FEATURE SIMILARITY , with projected improvements in OBJECT AND SCENE RECOGNITION ALGORITHMS . \n",
            "in this paper , we propose a novel approach to OBJECT RECOGNITION and SCENE CATEGORIZATION TASKS . the proposed approach is based on the use of SIMILARITY FUNCTIONS extracted from the FEATURES of the DISTRIBUTION . the proposed approach is based on the use of SIMILARITY FUNCTIONS extracted from the DISTRIBUTION of the FEATURES . the proposed method is based on the use of SIMILARITY FUNCTIONS extracted from the DISTRIBUTION of the DISTRIBUTION . the proposed method is evaluated on a variety of SCENE CATEGORIZATION TASKS and SCENE CATEGORIZATION TASKS . the experimental results show that the proposed method outperforms the state-of-the-art methods in terms of OBJECT RECOGNITION and SCENE CATEGORIZATION TASKS .\n",
            "\n",
            "265 1000\n",
            "we present a new algorithm for COMPUTING UPPER BOUNDS for an optimization version of the E-MAJSAT PROBLEM called FUNCTIONAL E-MAJSAT . the algorithm utilizes the COMPILATION LANGUAGE D-DNNF which underlies several state-of-the-art algorithms for solving related problems . this bound computation can be used in a BRANCH-AND-BOUND SOLVER for solving FUNCTIONAL E-MAJSAT . we then present a technique for pruning values from the BRANCH-AND-BOUND SEARCH TREE based on the information available after each bound computation . we evaluated the proposed techniques in a MAP SOLVER and a PROBABILISTIC CONFORMANT PLANNER . in both cases , our experiments showed that the new techniques improved the efficiency of state-of-the-art solvers by orders of magnitude . \n",
            "this paper addresses the problem of COMPILATION LANGUAGE D-DNNF for a E-MAJSAT PROBLEM . the E-MAJSAT PROBLEM is based on a BRANCH-AND-BOUND SOLVER and a BRANCH-AND-BOUND SOLVER . the E-MAJSAT PROBLEM is based on a BRANCH-AND-BOUND SOLVER and a BRANCH-AND-BOUND SOLVER . the proposed PROBABILISTIC CONFORMANT PLANNER is based on a BRANCH-AND-BOUND SOLVER and a BRANCH-AND-BOUND SOLVER . the performance of the proposed algorithm is demonstrated on a variety of COMPILATION LANGUAGE D-DNNF and a BRANCH-AND-BOUND SOLVER .\n",
            "\n",
            "266 1000\n",
            "the problem of assigning m points in the <unk> real space r n to k clusters is formulated as that of determining k centers in r n such that the sum of distances of each point to the nearest center is minimized . if a POLYHEDRAL DISTANCE is used , the problem can be formulated as that of minimizing a PIECEWISE-LINEAR CONCAVE FUNCTION on a POLYHEDRAL SET which is shown to be equivalent to a BILINEAR PROGRAM : minimizing a BILINEAR FUNCTION on a POLYHE-DRAL SET . a FAST NITE K-MEDIAN ALGORITHM consisting of solving few LINEAR PROGRAMS in CLOSED FORM leads to a stationary point of the BILINEAR PROGRAM . computational testing on a number of REAL-WORLD DATABASES was carried out . on the WISCONSIN DIAGNOSTIC BREAST CANCER DATABASE , K-MEDIAN TRAINING SET CORRECT-NESS was comparable to that of the K-MEDIAN ALGORITHM , however its testing set correctness was better . additionally , on the WISCONSIN PROGNOSTIC BREAST CANCER DATABASE , distinct and clinically important SURVIVAL CURVES were extracted by the K-MEDIAN ALGORITHM , whereas the K-MEDIAN ALGORITHM failed to obtain such distinct SURVIVAL CURVES for the same database . \n",
            "in this paper , we propose a novel FAST NITE K-MEDIAN ALGORITHM to the problem of LINEAR PROGRAMS . the proposed FAST NITE K-MEDIAN ALGORITHM is based on the use of a BILINEAR FUNCTION , a PIECEWISE-LINEAR CONCAVE FUNCTION , and a PIECEWISE-LINEAR CONCAVE FUNCTION . the proposed FAST NITE K-MEDIAN ALGORITHM is based on a FAST NITE K-MEDIAN ALGORITHM , which is based on a FAST NITE K-MEDIAN ALGORITHM . the proposed FAST NITE K-MEDIAN ALGORITHM is based on a PIECEWISE-LINEAR CONCAVE FUNCTION , which is a PIECEWISE-LINEAR CONCAVE FUNCTION . the proposed FAST NITE K-MEDIAN ALGORITHM is evaluated on the WISCONSIN DIAGNOSTIC BREAST CANCER DATABASE , and the results show that the proposed FAST NITE K-MEDIAN ALGORITHM is more effective than the conventional K-MEDIAN ALGORITHM . in addition , the proposed FAST NITE K-MEDIAN ALGORITHM can be easily extended to other REAL-WORLD DATABASES . the experimental results on the WISCONSIN DIAGNOSTIC BREAST CANCER DATABASE show that the proposed FAST NITE K-MEDIAN ALGORITHM is able to achieve a better performance than the standard K-MEDIAN ALGORITHM .\n",
            "\n",
            "267 1000\n",
            "we investigate the use of TIME-FREQUENCY METHODS to query BIOLOGICAL SEQUENCES in search of regions of similarity or critical relationships among the sequences . existing QUERYING APPROACHES are insensitive to repeats , especially in LOW-COMPLEXITY REGIONS , and do not provide much support for <unk> querying sub-sequences with inserts and <unk> -lrb- or gaps -rrb- . our TIME-FREQUENCY METHODS uses HIGHLY-LOCALIZED BASIS FUNCTIONS and multiple transformations in the TF PLANE to map characters in a sequence as well as different properties of a SUB-SEQUENCE , such as its position in the sequence or number of gaps between sub-sequences . we analyze GAPPED QUERY-BASED ALIGNMENT METHODS using transformations in the TF PLANE while demonstrating the TIME-FREQUENCY METHODS 's possible operation in real-time without PRE-PROCESSING . the TIME-FREQUENCY METHODS 's performance is compared to the WIDELY-ACCEPTED BLAST ALIGNMENT APPROACH , and a SIGNICANCE IMPROVEMENT is observed for queries with REPETITIVE SEGMENTS . \n",
            "this paper presents a novel approach to GAPPED QUERY-BASED ALIGNMENT METHODS based on HIGHLY-LOCALIZED BASIS FUNCTIONS . the proposed approach is based on the use of HIGHLY-LOCALIZED BASIS FUNCTIONS extracted from the TF PLANE . the proposed approach is based on the use of HIGHLY-LOCALIZED BASIS FUNCTIONS to estimate the TF PLANE . the proposed method is based on the use of HIGHLY-LOCALIZED BASIS FUNCTIONS derived from the TF PLANE of the TF PLANE . the experimental results show that the proposed method is effective in improving the SIGNICANCE IMPROVEMENT in terms of SIGNICANCE IMPROVEMENT .\n",
            "\n",
            "268 1000\n",
            "a recent trend in SALIENCY ALGORITHM DEVELOPMENT is LARGE-SCALE BENCHMARKING and ALGORITHM RANKING with ground truth provided by DATASETS OF HUMAN FIXATIONS . in order to accommodate the strong BIAS humans have toward CENTRAL FIXATIONS , SHUFFLED ROC METRIC is common to replace traditional ROC METRICS with a SHUFFLED ROC METRIC which uses RANDOMLY SAMPLED FIXATIONS from other images in the database as the negative set . however , the <unk> roc introduces a number of problematic elements , including a fundamental assumption that SHUFFLED ROC METRIC is possible to separate VISUAL SALIENCE and image spatial arrangement . we argue that SHUFFLED ROC METRIC is more informative to directly measure the effect of SPATIAL BIAS on algorithm performance rather than try to correct for SHUFFLED ROC METRIC . to capture and quantify these known sources of BIAS , we propose a novel metric for measuring SALIENCY ALGORITHM performance : the spatially <unk> roc -lrb- <unk> -rrb- . this metric provides direct insight into the SPATIAL BIASES of a SALIENCY ALGORITHM without sacrificing the intuitive raw performance evaluation of traditional ROC METRICS . by quantitatively measuring the BIAS in SALIENCY ALGORITHMS , researchers will be better equipped to select and optimize the most appropriate algorithm for a given task . we use a baseline measure of inherent algorithm BIAS to show that ADAPTIVE WHITENING SALIENCY -lsb- 14 -rsb- , attention by INFORMATION MAXIMIZA-TION -lsb- 8 -rsb- , and DYNAMIC VISUAL ATTENTION -lsb- 20 -rsb- provide the least spatially biased results , <unk> them for tasks in which there is no information about the underlying SPATIAL BIAS of the stimuli , whereas algorithms such as GRAPH BASED VISUAL SALIENCY -lsb- 18 -rsb- and CONTEXT-AWARE SALIENCY -lsb- 15 -rsb- have a significant inherent central BIAS . \n",
            "this paper addresses the problem of ADAPTIVE WHITENING SALIENCY in the context of LARGE-SCALE BENCHMARKING . in particular , we propose a novel SALIENCY ALGORITHM , called ADAPTIVE WHITENING SALIENCY , for ADAPTIVE WHITENING SALIENCY . the proposed SHUFFLED ROC METRIC is based on a SHUFFLED ROC METRIC of the SHUFFLED ROC METRIC . the proposed SHUFFLED ROC METRIC is based on a SHUFFLED ROC METRIC , which is based on the SHUFFLED ROC METRIC . the proposed SHUFFLED ROC METRIC is based on the SHUFFLED ROC METRIC , which is based on the SHUFFLED ROC METRIC . the proposed SALIENCY ALGORITHM is evaluated on a variety of ROC METRICS , including LARGE-SCALE BENCHMARKING and LARGE-SCALE BENCHMARKING . the experimental results show that the proposed SHUFFLED ROC METRIC is effective in improving the performance of the proposed SALIENCY ALGORITHM .\n",
            "\n",
            "269 1000\n",
            "this paper presents a GEOMETRIC BASED APPROACH for multiple MOBILE ROBOT MOTION COORDINATION . all the ROBOT PATHS being computed independently , we address the problem of coordinating the motion of the robots along their own path in such a way they do not <unk> each other . the proposed GEOMETRIC BASED APPROACH is based on a BOUNDING BOX REPRESENTATION of the obstacles in the SO-CALLED COORDINATION DIAGRAM . the GEOMETRIC BASED APPROACH is <unk> . its efficiency is illustrated by examples involving more than 100 robots . \n",
            "this paper addresses the problem of MOBILE ROBOT MOTION COORDINATION from a single BOUNDING BOX REPRESENTATION . we propose a novel GEOMETRIC BASED APPROACH based on a BOUNDING BOX REPRESENTATION . the proposed GEOMETRIC BASED APPROACH is based on a BOUNDING BOX REPRESENTATION , which is based on a BOUNDING BOX REPRESENTATION . the proposed GEOMETRIC BASED APPROACH is based on a BOUNDING BOX REPRESENTATION . the proposed GEOMETRIC BASED APPROACH is applied to the problem of MOBILE ROBOT MOTION COORDINATION . the experimental results show that the proposed GEOMETRIC BASED APPROACH is effective in MOBILE ROBOT MOTION COORDINATION .\n",
            "\n",
            "270 1000\n",
            "the ONE-SHOT SIMILARITY MEASURE has recently been introduced in the context of FACE RECOGNITION where ONE-SHOT SIMILARITY MEASURE was used to produce state-of-the-art results . given two vectors , their ONE-SHOT SIMILARITY SCORE reflects the likelihood of each vector belonging in the same class as the other vector and not in a class defined by a fixed set of '' negative '' examples . the potential of this approach has thus far been largely <unk> . in this paper we analyze the ONE-SHOT SCORE and show that : -lrb- 1 -rrb- when using a version of LDA as the underlying CLASSIFIER , this score is a CONDITIONALLY POSITIVE DEFINITE KERNEL and may be used within <unk> -lrb- e.g. , svm -rrb- , -lrb- 2 -rrb- ONE-SHOT SIMILARITY MEASURE can be efficiently computed , and -lrb- 3 -rrb- that ONE-SHOT SIMILARITY MEASURE is effective as an underlying mechanism for IMAGE REPRESENTATION . we further demonstrate the effectiveness of the ONE-SHOT SIMILARITY SCORE in a number of applications including MULTI-CLASS IDENTIFICATION and DESCRIPTOR GENERATION . \n",
            "in this paper , we propose a novel method for FACE RECOGNITION and MULTI-CLASS IDENTIFICATION . the proposed ONE-SHOT SIMILARITY MEASURE is based on the use of a CONDITIONALLY POSITIVE DEFINITE KERNEL and a CONDITIONALLY POSITIVE DEFINITE KERNEL for MULTI-CLASS IDENTIFICATION . the proposed method is based on the use of a CONDITIONALLY POSITIVE DEFINITE KERNEL , a CONDITIONALLY POSITIVE DEFINITE KERNEL , and a CONDITIONALLY POSITIVE DEFINITE KERNEL . the proposed method is applied to the problem of MULTI-CLASS IDENTIFICATION and MULTI-CLASS IDENTIFICATION . the experimental results show that the proposed method outperforms the state-of-the-art methods .\n",
            "\n",
            "271 1000\n",
            "multi-task learning attempts to simultaneously leverage data from multiple domains in order to estimate related functions on each domain . for example , a special case of MULTI-TASK LEARNING , TRANSFER LEARNING , is often employed when one has a good estimate of a function on a SOURCE DOMAIN , but is unable to estimate a related function well on a target domain using only target data . MULTI-TASK/TRANSFER LEARNING PROBLEMS are usually solved by imposing some kind of '' smooth '' relationship <unk> tasks . in this paper , we study how different SMOOTHNESS ASSUMPTIONS on TASK RELATIONS affect the upper bounds of algorithms proposed for these MULTI-TASK/TRANSFER LEARNING PROBLEMS under different settings . for general MULTI-TASK LEARNING , we study a family of algorithms which utilize a REWEIGHTING MATRIX on task weights to capture the smooth relationship among tasks , which has many instantiations in existing literature . furthermore , for MULTI-TASK LEARNING in a TRANSFER LEARNING FRAMEWORK , we study the recently proposed algorithms for the '' model shift '' , where the conditional distribution p -lrb- y | x -rrb- is allowed to change across tasks but the change is assumed to be smooth . in addition , we illustrate our results with experiments on both SIMULATED AND REAL DATA . \n",
            "this paper presents a novel TRANSFER LEARNING FRAMEWORK for MULTI-TASK LEARNING . the proposed TRANSFER LEARNING FRAMEWORK is based on a TRANSFER LEARNING FRAMEWORK for MULTI-TASK LEARNING . the proposed TRANSFER LEARNING FRAMEWORK is based on a TRANSFER LEARNING FRAMEWORK , which is a generalization of the TRANSFER LEARNING FRAMEWORK to the SOURCE DOMAIN . the proposed TRANSFER LEARNING FRAMEWORK is based on a TRANSFER LEARNING FRAMEWORK , which is a generalization of the TRANSFER LEARNING FRAMEWORK . the proposed TRANSFER LEARNING FRAMEWORK is applied to the problem of MULTI-TASK LEARNING , and the results show that the proposed TRANSFER LEARNING FRAMEWORK is effective in MULTI-TASK LEARNING .\n",
            "\n",
            "272 1000\n",
            "symmetry reduction has significantly contributed to the success of CLASSICAL PLANNING as HEURISTIC SEARCH . however , it is an open question if SYMMETRY REDUCTION TECHNIQUES can be lifted to fully observable NONDETERMINISTIC PLANNING . we generalize the concepts of STRUCTURAL SYMMETRIES and SYMMETRY REDUCTION to FOND PLANNING and specifically to the LAO ⇤ ALGORITHM . our base implementation of LAO ⇤ ALGORITHM in the FAST DOWNWARD PLANNER is competitive with the LAO ⇤ ALGORITHM - based <unk> planner <unk> . our experiments further show that SYMMETRY REDUCTION can yield strong performance gains compared to our base implementation of LAO ⇤ ALGORITHM . \n",
            "this paper addresses the problem of NONDETERMINISTIC PLANNING and NONDETERMINISTIC PLANNING . in particular , we consider the problem of NONDETERMINISTIC PLANNING and NONDETERMINISTIC PLANNING . in particular , we consider the problem of NONDETERMINISTIC PLANNING and NONDETERMINISTIC PLANNING . in particular , we consider the problem of NONDETERMINISTIC PLANNING and NONDETERMINISTIC PLANNING . we propose a new LAO ⇤ ALGORITHM , which is able to solve the problem of SYMMETRY REDUCTION and NONDETERMINISTIC PLANNING . we demonstrate the effectiveness of our LAO ⇤ ALGORITHM in comparison to the existing LAO ⇤ ALGORITHM .\n",
            "\n",
            "273 1000\n",
            "audio coding at LOW BITRATES suffers from artifacts due to SPECTRUM TRUNCATION . typical AUDIO CODECS CODE MULTI-CHANNEL SOURCES using transforms across the channels to remove REDUNDANCY such as middle -lrb- mid -rrb- - side -lrb- <unk> -rrb- coding . at LOW BITRATES , the spectrum of the CODED CHANNELS is truncated and the spectrum of the channels with lower energy , such as the SIDE CHANNEL , is truncated severely , sometimes entirely . this results in a MUFFLED SOUND due to truncation of all CODED CHANNELS beyond a certain frequency . it also results in a LOSS OF SPATIAL IMAGE even at LOW FREQUENCIES due to SEVERE TRUNCATION OF THE SIDE CHANNEL . previously we have developed a LOW BITRATE CODING METHOD to combat the LOSS OF HIGHER FREQUENCIES caused by SPECTRUM TRUNCATION . in this paper , we present a novel LOW BITRATE AUDIO CODING SCHEME to mitigate the LOSS OF SPATIAL IMAGE . listening tests show that the combination of the two LOW BITRATE CODING METHODS results in a AUDIO CODEC that can get good quality even at bitrates as low as <unk> for STEREO CONTENT with LOW DECODER COMPLEXITY . \n",
            "this paper addresses the problem of SPECTRUM TRUNCATION in the presence of LOW BITRATES in the presence of LOW BITRATES . we propose a novel method to estimate the LOSS OF HIGHER FREQUENCIES of the STEREO CONTENT using a LOW BITRATE AUDIO CODING SCHEME . the proposed LOW BITRATE AUDIO CODING SCHEME is based on a LOW BITRATE AUDIO CODING SCHEME . the proposed LOW BITRATE AUDIO CODING SCHEME is based on a LOW BITRATE AUDIO CODING SCHEME . the proposed LOW BITRATE AUDIO CODING SCHEME is based on a LOW BITRATE AUDIO CODING SCHEME . the proposed LOW BITRATE AUDIO CODING SCHEME is applied to the AUDIO CODECS CODE MULTI-CHANNEL SOURCES in the presence of LOW BITRATES in the presence of LOW BITRATES . the experimental results show that the proposed LOW BITRATE AUDIO CODING SCHEME is able to accurately estimate the REDUNDANCY of a scene from a SEVERE TRUNCATION OF THE SIDE CHANNEL .\n",
            "\n",
            "274 1000\n",
            "in this paper , we propose a novel RADIUS CONTROL STRATEGY for SPHERE DECODING referred to as INTER SEARCH RADIUS CONTROL that provides further improvement of the COMPUTATIONAL COMPLEXITY with minimal extra cost and NEGLIGIBLE PERFORMANCE PENALTY . the proposed RADIUS CONTROL STRATEGY focuses on the SPHERE RADIUS CONTROL STRATEGY when a CANDIDATE LATTICE POINT is found . for this purpose , the DYNAMIC RADIUS UPDATE STRATEGY as well as the LATTICE INDEPENDENT RADIUS SELECTION SCHEME are jointly exploited . from simulations in MULTIPLE-INPUT AND MULTIPLE-OUTPUT CHANNELS , it is shown that the proposed RADIUS CONTROL STRATEGY provides a substantial improvement in COMPLEXITY with <unk> performance . \n",
            "this paper presents a novel DYNAMIC RADIUS UPDATE STRATEGY for MULTIPLE-INPUT AND MULTIPLE-OUTPUT CHANNELS . the proposed DYNAMIC RADIUS UPDATE STRATEGY is based on a SPHERE RADIUS CONTROL STRATEGY of the MULTIPLE-INPUT AND MULTIPLE-OUTPUT CHANNELS . the proposed DYNAMIC RADIUS UPDATE STRATEGY is based on a SPHERE RADIUS CONTROL STRATEGY , which is a generalization of the SPHERE RADIUS CONTROL STRATEGY . the proposed DYNAMIC RADIUS UPDATE STRATEGY is applied to the problem of INTER SEARCH RADIUS CONTROL . the proposed DYNAMIC RADIUS UPDATE STRATEGY is evaluated on both MULTIPLE-INPUT AND MULTIPLE-OUTPUT CHANNELS . the experimental results show that the proposed DYNAMIC RADIUS UPDATE STRATEGY is able to significantly reduce the COMPLEXITY of the RADIUS CONTROL STRATEGY .\n",
            "\n",
            "275 1000\n",
            "in SPEAKER RECOGNITION , it is a problem that variation of SPEECH FEATURES is caused by sentences and TIME DIFFERENCE . SPEECH DATA includes a PHONETIC INFORMATION and a SPEAKER INFORMATION . if they are separated each other , ROBUST SPEAKER VERIFICATION will be realized by using only the SPEAKER INFORMATION . however , it is difficult to separate the SPEAKER INFORMATION from the PHONETIC INFORMATION included in SPEECH DATA at present . from this viewpoint , we propose a SPEAKER VERIFICATION METHOD using a SUBSPACE METHOD based on PRINCIPAL COMPONENT ANALYSIS in order to extract only the SPEAKER INFORMATION included in SPEECH DATA . we also propose DYNAMIC AND STATIC FEATURES of each speaker presented in the SPEAKER EIGENSPACE as well as their integration for ROBUST NORMALIZATION OF SPEECH FEATURE VARIATIONS . we carried out comparative experiments between the proposed SPEAKER VERIFICATION METHOD and conventional GMM to show an effectiveness of our proposed SPEAKER VERIFICATION METHOD . as a result , integrated DYNAMIC AND STATIC FEATURES in SPEAKER EIGENSPACE were shown to be effective for SPEAKER VERIFICATION . \n",
            "in this paper , we propose a novel SPEAKER VERIFICATION METHOD based on PRINCIPAL COMPONENT ANALYSIS . the proposed SPEAKER VERIFICATION METHOD is based on a SUBSPACE METHOD and a GMM for ROBUST SPEAKER VERIFICATION . the proposed SPEAKER VERIFICATION METHOD is based on PRINCIPAL COMPONENT ANALYSIS and GMM . the proposed SUBSPACE METHOD is based on a SUBSPACE METHOD and a GMM . the proposed method is evaluated on the SPEECH DATA and the GMM is shown to be effective in improving the ROBUST NORMALIZATION OF SPEECH FEATURE VARIATIONS performance . the proposed SPEAKER VERIFICATION METHOD is evaluated on the SPEECH DATA and the results demonstrate the effectiveness of the proposed SPEAKER VERIFICATION METHOD .\n",
            "\n",
            "276 1000\n",
            "previous RESOLUTION-BASED APPROACHES to THEORY-GUIDED INDUCTION OF LOGIC PROGRAMS produce hypotheses in the form of a set of <unk> of a theory , where the RESOLVENTS represent allowed sequences of RESOLUTION STEPS for the initial theory . there are , however , many characterizations of allowed sequences of RESOLUTION STEPS that can not be expressed by a set of RESOLVENTS . one approach to this THEORY-GUIDED INDUCTION OF LOGIC PROGRAMS is presented , the system <unk> , which is based on an earlier technique for learning NITE-STATE AUTOMATA that represent allowed sequences of RESOLUTION STEPS . <unk> extends the previous technique in three ways : i -rrb- negative examples are considered in addition to positive examples , ii -rrb- a new strategy for performing GENERALIZATION is used , and iii -rrb- a technique for converting the learned automaton to a LOGIC PROGRAM is included . results from experiments are presented in which <unk> outperforms both a system using the old strategy for performing GENERALIZATION , and a traditional COVERING TECHNIQUE . the latter result can be explained by the limited expressiveness of hypotheses produced by covering and also by the fact that covering needs to produce the correct BASE CLAUSES for a RECURSIVE DENITION before producing the RECURSIVE CLAUSES . <unk> on the other hand does not require that particular examples of the base cases are given , since both BASE CLAUSES and RECURSIVE CLAUSES can be inferred from a single example . \n",
            "in this paper , we propose a novel approach to THEORY-GUIDED INDUCTION OF LOGIC PROGRAMS . the proposed approach is based on the use of RECURSIVE CLAUSES and RECURSIVE CLAUSES . the proposed approach is based on the use of RECURSIVE CLAUSES and RECURSIVE CLAUSES . the proposed approach is based on the use of RECURSIVE CLAUSES , RECURSIVE CLAUSES , and RECURSIVE CLAUSES . we show that the proposed method can be applied to THEORY-GUIDED INDUCTION OF LOGIC PROGRAMS , and can be applied to any LOGIC PROGRAM . the proposed method is evaluated on a number of BASE CLAUSES , and the results show that the proposed method can achieve better performance than the state-of-the-art methods .\n",
            "\n",
            "277 1000\n",
            "many COMPUTER VISION PROBLEMS can be formulated as BINARY QUADRATIC PROGRAMS . two classic RELAXATION METHODS are widely used for solving BINARY QUADRATIC PROGRAMS , namely , SPECTRAL METHODS and SEMIDEFINITE PROGRAMMING , each with their own advantages and disadvantages . BINARY QUADRATIC PROGRAMS is simple and easy to implement , but its bound is loose . BINARY QUADRATIC PROGRAMS has a tighter bound , but its COMPUTATIONAL COMPLEXITY is high for LARGE SCALE PROBLEMS . we present a new SDP FORMULATION for BINARY QUADRATIC PROGRAMS , with two desirable properties . first , SDP FORMULATION has a similar relaxation bound to conventional SDP FORMULATIONS . second , compared with conventional SDP FORMULATIONS , the new SDP FORMULATION leads to a significantly more efficient and scalable DUAL OPTIMIZATION APPROACH , which has the same degree of COMPLEXITY as SPECTRAL METHODS . extensive experiments on various applications including CLUSTERING , IMAGE SEGMEN-TATION , CO-SEGMENTATION and REGISTRATION demonstrate the usefulness of our SDP FORMULATION for solving LARGE-SCALE BQPS . \n",
            "this paper addresses the problem of IMAGE SEGMEN-TATION , IMAGE SEGMEN-TATION , and CLUSTERING . in this paper , we propose a novel DUAL OPTIMIZATION APPROACH to IMAGE SEGMEN-TATION , IMAGE SEGMEN-TATION , and CO-SEGMENTATION . the proposed DUAL OPTIMIZATION APPROACH is based on the idea of SEMIDEFINITE PROGRAMMING , which is a generalization of the DUAL OPTIMIZATION APPROACH to the SDP FORMULATION . the proposed DUAL OPTIMIZATION APPROACH is based on the use of BINARY QUADRATIC PROGRAMS , which is able to deal with LARGE SCALE PROBLEMS , CO-SEGMENTATION , and CLUSTERING . the proposed DUAL OPTIMIZATION APPROACH is applied to LARGE SCALE PROBLEMS , CO-SEGMENTATION , and CLUSTERING . the experimental results show that the proposed DUAL OPTIMIZATION APPROACH is effective in improving the COMPUTATIONAL COMPLEXITY of the proposed DUAL OPTIMIZATION APPROACH in terms of COMPUTATIONAL COMPLEXITY and COMPUTATIONAL COMPLEXITY .\n",
            "\n",
            "278 1000\n",
            "we propose a compact , low power VLSI NETWORK of spiking neurons which can learn to classify COMPLEX PATTERNS OF MEAN FIRING RATES on -- line and in real -- time . the NETWORK OF INTEGRATE-AND-FIRE NEURONS is connected by BISTABLE SYNAPSES that can change their weight using a LOCAL SPIKE -- based <unk> mechanism . LEARNING is supervised by a teacher which provides an extra input to the output neurons during training . the SYNAPTIC WEIGHTS are updated only if the current generated by the PLASTIC SYNAPSES does not match the output desired by the teacher -lrb- as in the PERCEPTRON LEARNING RULE -rrb- . we present experimental results that demonstrate how this VLSI NETWORK is able to robustly classify uncorrelated linearly separable spatial patterns of mean firing rates . \n",
            "this paper presents a novel approach to LEARNING in a VLSI NETWORK . the approach is based on the use of a set of PLASTIC SYNAPSES that are extracted from the LOCAL SPIKE of the LOCAL SPIKE . the proposed approach is based on the use of a PERCEPTRON LEARNING RULE to estimate the SYNAPTIC WEIGHTS . the proposed method is based on the use of PLASTIC SYNAPSES in the VLSI NETWORK . the proposed method is applied to the NETWORK OF INTEGRATE-AND-FIRE NEURONS of the VLSI NETWORK . the experimental results show that the proposed method outperforms the existing methods in terms of the COMPLEX PATTERNS OF MEAN FIRING RATES .\n",
            "\n",
            "279 1000\n",
            "it is known that the deformations of the APPARENT CONTOURS of a surface under PERSPECTIVE PROJECTION and VIEWER MOTION enable the recovery of the GEOMETRY OF THE SURFACE , for example by utilising the EPIPOLAR PARAMETRIZATION . these methods break down with APPARENT CONTOURS that are singular i.e. with <unk> . in this paper we study this situation in detail and show how , nevertheless , the SURFACE GEOMETRY -lrb- including the GAUSS CURVATURE and mean curvature of the surface -rrb- can be recovered by following the <unk> . indeed the FOR-MULAE are much simpler in this case and require lower SPATIO-TEMPORAL DERIVATIVES than in the general case of NONSINGULAR APPARENT CONTOURS . we give a simulated example , and also show that following <unk> does not by itself provide us with information on EGO-MOTION . \n",
            "this paper presents a method for estimating the GEOMETRY OF THE SURFACE of a scene from a single image . the method is based on a PERSPECTIVE PROJECTION that is invariant to VIEWER MOTION , such as GAUSS CURVATURE and GAUSS CURVATURE . the proposed method is based on the EPIPOLAR PARAMETRIZATION of the SURFACE GEOMETRY of the signal . the proposed method is based on the EPIPOLAR PARAMETRIZATION of the signal to the VIEWER MOTION . the proposed method is based on the estimation of the EPIPOLAR PARAMETRIZATION of the signal and the VIEWER MOTION of the signal . the proposed method is evaluated on a number of APPARENT CONTOURS , and the results show that the proposed method outperforms the existing methods in terms of the EPIPOLAR PARAMETRIZATION .\n",
            "\n",
            "280 1000\n",
            "we present a VARIATIONAL BAYESIAN FRAMEWORK for performing INFERENCE , DENSITY ESTIMATION and MODEL SELECTION in a special class of GRAPHICAL MODELS -- HIDDEN MARKOV RANDOM FIELDS . HIDDEN MARKOV RANDOM FIELDS are particularly well suited to IMAGE MODELLING and in this paper , we apply HIDDEN MARKOV RANDOM FIELDS to the problem of IMAGE SEGMENTATION . unfortunately , HIDDEN MARKOV RANDOM FIELDS are notoriously hard to train and use because the exact INFERENCE PROBLEMS they create are intractable . our main contribution is to introduce an efficient VARIATIONAL APPROACH for performing approximate INFERENCE of the VARIATIONAL BAYESIAN FRAMEWORK of HIDDEN MARKOV RANDOM FIELDS , which we can then apply to the DENSITY ESTIMATION and MODEL SELECTION PROBLEMS that arise when learning IMAGE MODELS from data . with this VARIATIONAL APPROACH , we can conveniently tackle the problem of IMAGE SEGMENTATION . we present experimental results which show that our VARIATIONAL BAYESIAN FRAMEWORK outperforms recent HMRF-BASED SEGMENTATION METHODS on REAL WORLD IMAGES . \n",
            "this paper presents a novel VARIATIONAL BAYESIAN FRAMEWORK for IMAGE SEGMENTATION and MODEL SELECTION . the proposed VARIATIONAL BAYESIAN FRAMEWORK is based on the use of HIDDEN MARKOV RANDOM FIELDS , a VARIATIONAL APPROACH for IMAGE SEGMENTATION and MODEL SELECTION . the proposed VARIATIONAL BAYESIAN FRAMEWORK is based on the use of HIDDEN MARKOV RANDOM FIELDS and MODEL SELECTION . the proposed VARIATIONAL BAYESIAN FRAMEWORK is applied to IMAGE SEGMENTATION , DENSITY ESTIMATION and MODEL SELECTION . the experimental results show that the proposed VARIATIONAL BAYESIAN FRAMEWORK is effective in MODEL SELECTION and MODEL SELECTION .\n",
            "\n",
            "281 1000\n",
            "we consider the problem of ESTIMATING 3-D STRUCTURE from a single still image of an outdoor urban scene . our goal is to efficiently create 3-D MODEL STRUCTURE which are visually pleasant . we <unk> an appropriate 3-D MODEL STRUCTURE and formulate the task of 3-D RECONSTRUCTION as MODEL FITTING PROBLEM . our 3-D MODEL STRUCTURE are composed of a number of VERTICAL WALLS and a GROUND PLANE , where GROUND-VERTICAL BOUNDARY is a CONTINUOUS POLYLINE . we achieve COMPUTATIONAL EFFICIENCY by special preprocessing together with STEPWISE SEARCH OF 3-D MODEL PARAMETERS dividing the problem into two smaller sub-problems on CHAIN GRAPHS . the use of CONDITIONAL RANDOM FIELD MODELS for both problems allows to various cues . we infer orientation of VERTICAL WALLS of 3-d model vanishing points . \n",
            "this paper addresses the problem of ESTIMATING 3-D STRUCTURE from a single image . in particular , we consider the problem of ESTIMATING 3-D STRUCTURE in the presence of VERTICAL WALLS and VERTICAL WALLS . in particular , we propose a novel approach to the problem of ESTIMATING 3-D STRUCTURE , which is a MODEL FITTING PROBLEM . the proposed approach is based on a MODEL FITTING PROBLEM that exploits the 3-D MODEL STRUCTURE of the CHAIN GRAPHS and the GROUND PLANE . the proposed approach is based on the use of a GROUND-VERTICAL BOUNDARY and a GROUND-VERTICAL BOUNDARY . the proposed approach is evaluated on a variety of datasets and compared to the state of the art methods .\n",
            "\n",
            "282 1000\n",
            "it has been shown that the combination of MULTI-MODAL MRI IMAGES can improve the DISCRIMINATION OF DISEASED TISSUE . the fusion of DISSIMILAR IMAGING DATA for CLASSIFICATION AND SEG-MENTATION PURPOSES however , is not a trivial task , as there is an inherent difference in INFORMATION DOMAINS , DIMENSION-ALITY and scales . this work proposes a MULTI-VIEW CONSENSUS CLUSTERING METHODOLOGY for the integration of MULTI-MODAL MRI IMAGES into a UNIFIED SEGMENTATION OF TUMORAL LESIONS for HETEROGENEITY ASSESSMENT . using a variety of metrics and distance functions this MULTI-VIEW CONSENSUS CLUSTERING METHODOLOGY calculates multiple VECTORIAL DISSIMILARITY-SPACES for each MRI MODALITY and makes use of CLUSTER ENSEMBLES to combine a set of UN-SUPERVISED BASE SEGMENTATIONS into an unified partition of the VOXEL-BASED DATA . the MULTI-VIEW CONSENSUS CLUSTERING METHODOLOGY is demonstrated in application to DCE-MRI and DTI-MR , for which a MANIFOLD LEARNING STEP is implemented in order to account for the GEOMETRIC CONSTRAINS of the HIGH DIMENSIONAL DIFFUSION INFORMATION . \n",
            "this paper presents a novel MULTI-VIEW CONSENSUS CLUSTERING METHODOLOGY for CLASSIFICATION AND SEG-MENTATION PURPOSES in MULTI-MODAL MRI IMAGES . the proposed MULTI-VIEW CONSENSUS CLUSTERING METHODOLOGY is based on the use of a UNIFIED SEGMENTATION OF TUMORAL LESIONS and a UNIFIED SEGMENTATION OF TUMORAL LESIONS for CLUSTER ENSEMBLES . the proposed MULTI-VIEW CONSENSUS CLUSTERING METHODOLOGY is based on the use of GEOMETRIC CONSTRAINS and GEOMETRIC CONSTRAINS . the proposed MULTI-VIEW CONSENSUS CLUSTERING METHODOLOGY is based on the use of GEOMETRIC CONSTRAINS and GEOMETRIC CONSTRAINS . the proposed MULTI-VIEW CONSENSUS CLUSTERING METHODOLOGY is applied to MULTI-MODAL MRI IMAGES , and the experimental results show that the proposed MULTI-VIEW CONSENSUS CLUSTERING METHODOLOGY outperforms the conventional DTI-MR in terms of both CLASSIFICATION AND SEG-MENTATION PURPOSES and DCE-MRI .\n",
            "\n",
            "283 1000\n",
            "we introduce PARMA , a PARMA for CROSS-DOCUMENT , SEMANTIC PREDICATE and argument alignment . our PARMA combines a number of LINGUISTIC RESOURCES familiar to researchers in areas such as recognizing TEXTUAL ENTAILMENT and QUESTION ANSWERING , integrating LINGUISTIC RESOURCES into a simple DISCRIMINA-TIVE MODEL . PARMA achieves state of the art results on an existing and a new dataset . we suggest that previous efforts have focussed on data that is biased and too easy , and we provide a more difficult dataset based on TRANSLATION DATA with a low base-line which we beat by 17 % F1 . \n",
            "this paper addresses the problem of QUESTION ANSWERING in the presence of TEXTUAL ENTAILMENT . we propose a novel method to incorporate LINGUISTIC RESOURCES into a DISCRIMINA-TIVE MODEL for QUESTION ANSWERING . the proposed method consists of two steps : -lrb- 1 -rrb- a DISCRIMINA-TIVE MODEL which consists of a set of TRANSLATION DATA and a set of TRANSLATION DATA ; and -lrb- 2 -rrb- a DISCRIMINA-TIVE MODEL to incorporate LINGUISTIC RESOURCES into a DISCRIMINA-TIVE MODEL . experimental results show that the proposed method outperforms the state-of-the-art methods in terms of F1 and F1 .\n",
            "\n",
            "284 1000\n",
            "we propose a TRANSITION-BASED MODEL for JOINT WORD SEGMENTATION , POS TAGGING and TEXT NORMALIZATION . different from previous methods , the TRANSITION-BASED MODEL can be trained on standard TEXT CORPORA , overcoming the lack of ANNOTATED MICROBLOG CORPORA . to evaluate our TRANSITION-BASED MODEL , we develop an ANNOTATED CORPUS based on MICROBLOGS . experimental results show that our TRANSITION-BASED MODEL can help improve the performance of WORD SEGMENTATION on MICROBLOGS , giving an ERROR REDUCTION in SEGMENTATION ACCURACY of <unk> % , compared to the traditional approach . \n",
            "this paper presents a novel TRANSITION-BASED MODEL for JOINT WORD SEGMENTATION and JOINT WORD SEGMENTATION . the proposed TRANSITION-BASED MODEL is based on a TRANSITION-BASED MODEL for JOINT WORD SEGMENTATION and JOINT WORD SEGMENTATION . the proposed TRANSITION-BASED MODEL is applied to the task of JOINT WORD SEGMENTATION and WORD SEGMENTATION . the proposed TRANSITION-BASED MODEL is evaluated on two TEXT CORPORA . the experimental results show that the proposed TRANSITION-BASED MODEL is effective in improving the SEGMENTATION ACCURACY and SEGMENTATION ACCURACY of the proposed TRANSITION-BASED MODEL .\n",
            "\n",
            "285 1000\n",
            "efficient decoding for SYNTACTIC PARSING has become a necessary research area as STATISTICAL GRAMMARS grow in ACCURACY and size and as more NLP APPLICATIONS leverage SYNTACTIC ANALYSES . we review prior methods for PRUNING and then present a new framework that unifies their strengths into a single approach . using a LOG LINEAR MODEL , we learn the optimal BEAM-SEARCH PRUNING PARAMETERS for each CYK CHART CELL , effectively predicting the most promising areas of the MODEL SPACE to explore . we demonstrate that our method is faster than COARSE-TO-FINE PRUNING , exemplified in both the CHARNIAK AND BERKELEY PARSERS , by empirically comparing our PARSER to the BERKELEY PARSER using the same GRAMMAR and under identical operating conditions . \n",
            "in this paper , we propose a novel approach to SYNTACTIC PARSING based on STATISTICAL GRAMMARS . the proposed approach is based on the use of STATISTICAL GRAMMARS , which is based on a GRAMMAR . the proposed PARSER is based on the use of a GRAMMAR , which is based on a GRAMMAR . the proposed PARSER is based on the use of a GRAMMAR , which is a GRAMMAR of the GRAMMAR . the proposed PARSER is applied to the CYK CHART CELL of the BERKELEY PARSER . the performance of the proposed PARSER is evaluated in terms of ACCURACY and ACCURACY . the performance of the proposed PARSER is evaluated in terms of ACCURACY and ACCURACY . the performance of the proposed PARSER is evaluated on a CYK CHART CELL , and the results show that the proposed PARSER is effective in reducing the ACCURACY .\n",
            "\n",
            "286 1000\n",
            "the goal of BANDWIDTH EXTENSION OF SPEECH is to extrapolate the MISSING LOW OR HIGH FREQUENCY COMPONENTS of the wide-band speech -lrb- <unk> hz -rrb- based entirely on information contained in a NARROW-BAND SIGNAL -lrb- <unk> hz -rrb- . in this paper we propose a new method for HIGH-FREQUENCY REGENERATION OF THE EXCITATION SIGNAL , using the correlation between the SHAPE of the GLOTTAL FLOW WAVEFORM and the spectrum of the voice source . the HIGH-BAND EXCITATION is generated by performing a PITCH-SYNCHRONOUS TIMESCALE TRANSFORMATION on the LINEAR PREDICTION NARROW-BAND RESIDUAL to generate an HIGH-PASS SIGNAL that retains the PERIODIC CHARACTERISTICS of the original signal but with a larger open quotient . this method is easy to implement and does not introduce discontinuities in the spectrum of the REGENERATED EXCITATION . it can be used in applications for BANDWIDTH EXTENSION OF SPEECH where no SIDE INFORMATION is transmitted or for LOW BIT CODING OF WIDE-BAND SPEECH . \n",
            "this paper presents a novel method to recover the GLOTTAL FLOW WAVEFORM of a scene from a NARROW-BAND SIGNAL . the proposed PITCH-SYNCHRONOUS TIMESCALE TRANSFORMATION is based on a PITCH-SYNCHRONOUS TIMESCALE TRANSFORMATION of the GLOTTAL FLOW WAVEFORM of the GLOTTAL FLOW WAVEFORM . the proposed PITCH-SYNCHRONOUS TIMESCALE TRANSFORMATION is based on a PITCH-SYNCHRONOUS TIMESCALE TRANSFORMATION of the GLOTTAL FLOW WAVEFORM of the GLOTTAL FLOW WAVEFORM . the proposed PITCH-SYNCHRONOUS TIMESCALE TRANSFORMATION is based on a PITCH-SYNCHRONOUS TIMESCALE TRANSFORMATION of the GLOTTAL FLOW WAVEFORM of the GLOTTAL FLOW WAVEFORM of the GLOTTAL FLOW WAVEFORM . the proposed PITCH-SYNCHRONOUS TIMESCALE TRANSFORMATION is applied to the GLOTTAL FLOW WAVEFORM of the GLOTTAL FLOW WAVEFORM and the PERIODIC CHARACTERISTICS of the GLOTTAL FLOW WAVEFORM is estimated . the experimental results demonstrate the effectiveness of the proposed PITCH-SYNCHRONOUS TIMESCALE TRANSFORMATION .\n",
            "\n",
            "287 1000\n",
            "execution speed of programs on modern COMPUTER ARCHITECTURES is sensitive , by a factor of two or more , to the order in which instructions are presented to the processor . to realize potential EXECUTION EFFICIENCY , it is now <unk> for an optimizing COMPILER to employ a HEURISTIC ALGORITHM for INSTRUCTION SCHEDULING . these HEURISTIC ALGORITHM are <unk> hand-crafted , which is <unk> and time-consuming . we show how to cast the INSTRUCTION SCHEDULING PROBLEM as a LEARNING TASK , so that one obtains the HEURISTIC SCHEDULING ALGORITHM automatically . our focus is the narrower problem of SCHEDULING STRAIGHT-LINE CODE , also known as a basic block of instructions . our empirical results show that just a few FEATURES are adequate for quite good performance at this task for a real modern processor , and that any of several SUPERVISED LEARNING METHODS perform nearly optimally with respect to the FEATURES used . \n",
            "this paper addresses the problem of INSTRUCTION SCHEDULING in a INSTRUCTION SCHEDULING PROBLEM . the goal of this work is to design a HEURISTIC SCHEDULING ALGORITHM to the INSTRUCTION SCHEDULING PROBLEM . the proposed HEURISTIC SCHEDULING ALGORITHM is based on a HEURISTIC SCHEDULING ALGORITHM , which is a generalization of the HEURISTIC ALGORITHM to the INSTRUCTION SCHEDULING PROBLEM . the proposed HEURISTIC SCHEDULING ALGORITHM is based on a HEURISTIC ALGORITHM , which is based on a HEURISTIC ALGORITHM . the proposed HEURISTIC SCHEDULING ALGORITHM is applied to the problem of INSTRUCTION SCHEDULING in a LEARNING TASK . the experimental results show that the proposed algorithm is able to reduce the EXECUTION SPEED by up to 50 % .\n",
            "\n",
            "288 1000\n",
            "we propose a novel , LARGE-SCALE , STRUCTURE-FROM-MOTION FRAMEWORK that advances the state of the art in DATA SCAL-ABILITY from CITY-SCALE MODELING -lrb- millions of IMAGES -rrb- to WORLD-SCALE MODELING -lrb- several tens of millions of IMAGES -rrb- using just a single computer . the main enabling technology is the use of a STREAMING-BASED FRAMEWORK for CONNECTED COMPONENT DISCOVERY . moreover , our LARGE-SCALE , STRUCTURE-FROM-MOTION FRAMEWORK employs an ADAPTIVE , ONLINE , ICONIC IMAGE CLUSTERING APPROACH based on an AUGMENTED BAG-OF-WORDS REPRESENTATION , in order to balance the goals of REGISTRATION , <unk> , and DATA COMPACTNESS . we demonstrate our proposal by operating on a recent publicly available 100 million IMAGE CROWD-SOURCED PHOTO COLLECTION containing IMAGES <unk> distributed throughout the entire world . results illustrate that our LARGE-SCALE , STRUCTURE-FROM-MOTION FRAMEWORK does not compromise MODEL COMPLETENESS , but achieves unprecedented levels of efficiency and SCALABILITY . \n",
            "this paper addresses the problem of CONNECTED COMPONENT DISCOVERY in IMAGES . we propose a novel approach to CONNECTED COMPONENT DISCOVERY for CONNECTED COMPONENT DISCOVERY . the proposed approach is based on a STREAMING-BASED FRAMEWORK , which is able to deal with IMAGES in the presence of DATA COMPACTNESS . the proposed approach is based on a STREAMING-BASED FRAMEWORK , which is based on a STREAMING-BASED FRAMEWORK . the proposed ADAPTIVE , ONLINE , ICONIC IMAGE CLUSTERING APPROACH is based on a STREAMING-BASED FRAMEWORK , which is based on a STREAMING-BASED FRAMEWORK . the experimental results show that the proposed method outperforms the state-of-the-art methods in terms of SCALABILITY and SCALABILITY .\n",
            "\n",
            "289 1000\n",
            "<unk> '' knows '' that NEURAL NETWORKS need more than a single layer of NONLINEAR UNITS to compute interesting functions . we show that this is false if one employs WINNER-TAKE-ALL as NONLINEAR UNIT : any BOOLEAN FUNCTION can be computed by a single ¡ - WINNER-TAKE-ALL unit applied to WEIGHTED SUMS OF THE INPUT VARIABLES . any CONTINUOUS FUNCTION can be approximated arbitrarily well by a single SOFT WINNER-TAKE-ALL UNIT applied to WEIGHTED SUMS OF THE INPUT VARIABLES . only POSITIVE WEIGHTS are needed in these -LRB- LINEAR -RRB- WEIGHTED SUMS . this may be of interest from the point of view of NEUROPHYSIOLOGY , since only 15 % of the SYNAPSES in the cortex are INHIBITORY . in addition it is widely believed that there are special MICROCIRCUITS in the cortex that compute WINNER-TAKE-ALL . our results support the view that WINNER-TAKE-ALL is a very useful basic COMPUTATIONAL UNIT in NEURAL VLSI : cents it is <unk> that WINNER-TAKE-ALL of # input variables can be computed very efficiently with $ ¥ # <unk> -lrb- and a total <unk> length and area that is linear in # -rrb- in analog vlsi -lsb- <unk> et al. , 1989 -rsb- cents we show that WINNER-TAKE-ALL is not just useful for special purpose computations , but may serve as the only NONLINEAR UNIT for NEURAL CIRCUITS with UNIVERSAL COMPUTATIONAL POWER CENTS we show that any MULTI-LAYER PERCEPTRON needs quadratically in # many gates to compute WINNER-TAKE-ALL for # input variables , hence WINNER-TAKE-ALL provides a substantially more powerful COMPUTATIONAL UNIT than a PERCEPTRON -lrb- at about the same cost of implementation in analog vlsi -rrb- . complete proofs and further details to these results can be found in -lsb- <unk> , 2000 -rsb- . \n",
            "this paper presents a novel approach to NEURAL VLSI . the proposed approach is based on the use of a MULTI-LAYER PERCEPTRON , a MULTI-LAYER PERCEPTRON , and a WINNER-TAKE-ALL . the proposed method is based on the use of a MULTI-LAYER PERCEPTRON , which is a WINNER-TAKE-ALL of the PERCEPTRON . the proposed approach is based on the use of a MULTI-LAYER PERCEPTRON , a MULTI-LAYER PERCEPTRON , and a WINNER-TAKE-ALL . the proposed method is based on the use of a MULTI-LAYER PERCEPTRON , which is a WINNER-TAKE-ALL of the PERCEPTRON . the proposed approach is evaluated on a WINNER-TAKE-ALL and compared to the conventional PERCEPTRON . the proposed method is compared to the PERCEPTRON , and the results show that the proposed method is effective in reducing the number of SYNAPSES in the PERCEPTRON .\n",
            "\n",
            "290 1000\n",
            "a DESCRIPTION CLASSIFIER organizes concepts and relations into a taxonomy based on the results of SUBSUMPTION COMPUTATIONS applied to pairs of relation definitions . until now , DESCRIPTION CLASSIFIER have only been designed to operate over definitions <unk> in highly restricted subsets of the PREDICATE CALCULUS . this paper describes a CLASSIFIER able to reason with definitions <unk> in the FULL FIRST ORDER PREDICATE CALCULUS , extended with sets , CARDINALITY , EQUALITY , SCALAR INEQUALITIES , and PREDICATE VARIABLES . the performance of the new CLASSIFIER is comparable to that of existing DESCRIPTION CLASSIFIER . our CLASSIFIER introduces two new techniques , DUAL REPRESENTATIONS and AUTO-SOCRATIC ELABORATION , that may be expected to improve the performance of existing DESCRIPTION CLASSIFIER . \n",
            "in this paper , we propose a novel DESCRIPTION CLASSIFIER for the DESCRIPTION CLASSIFIER . the proposed CLASSIFIER is based on the use of the FULL FIRST ORDER PREDICATE CALCULUS , EQUALITY , EQUALITY , and PREDICATE VARIABLES . the proposed CLASSIFIER is based on the use of the FULL FIRST ORDER PREDICATE CALCULUS , EQUALITY , SCALAR INEQUALITIES , and PREDICATE VARIABLES . the proposed CLASSIFIER is evaluated on a number of SCALAR INEQUALITIES , and the results show that the proposed CLASSIFIER is more effective than the conventional DESCRIPTION CLASSIFIER .\n",
            "\n",
            "291 1000\n",
            "information extraction -lrb- INFORMATION EXTRACTION -rrb- is the problem of <unk> out PRE-DEENED STRUCTURED SUMMARIES from TEXT DOCUMENTS . we are interested in performing INFORMATION EXTRACTION in NON-TRADITIONAL DOMAINS , where much of the text is often <unk> , such as ELECTRONIC BULLETIN BOARD POSTS and WEB PAGES . we suggest that the best approach is one that takes into account many diierent kinds of information , and argue for the suitability of a MULTISTRAT-EGY APPROACH . we describe LEARNERS for INFORMATION EXTRACTION drawn from three separate MACHINE LEARNING PARADIGMS : ROTE MEMORIZATION , TERM-SPACE TEXT CLASSIICATION , and RELATIONAL RULE INDUCTION . by building REGRESSION MODELS mapping from LEARNER CONNDENCE to PROBABILITY OF COR-RECTNESS and combining probabilities appropriately , it is possible to improve EXTRACTION ACCURACY over that achieved by any individual learner . we describe three diierent MUL-TISTRATEGY APPROACHES . experiments on two IE DOMAINS , a collection of ELECTRONIC SEMINAR ANNOUNCEMENTS from a university computer science department and a set of NEWSWIRE ARTICLES describing <unk> <unk> from the REUTERS COLLECTION , demonstrate the <unk> of all three MUL-TISTRATEGY APPROACHES . \n",
            "this paper addresses the problem of INFORMATION EXTRACTION from NEWSWIRE ARTICLES . we propose a novel approach to RELATIONAL RULE INDUCTION and RELATIONAL RULE INDUCTION . the key idea is to use a MULTISTRAT-EGY APPROACH , which is able to deal with WEB PAGES , WEB PAGES , and WEB PAGES . the proposed approach is based on the use of LEARNERS , which is able to deal with WEB PAGES and TEXT DOCUMENTS . the proposed approach is based on the use of LEARNERS , which is able to deal with WEB PAGES and WEB PAGES . the proposed approach is evaluated on a variety of NON-TRADITIONAL DOMAINS , including TERM-SPACE TEXT CLASSIICATION , TERM-SPACE TEXT CLASSIICATION , and RELATIONAL RULE INDUCTION . the experimental results show that the proposed approach is effective in improving the EXTRACTION ACCURACY of the proposed MULTISTRAT-EGY APPROACH .\n",
            "\n",
            "292 1000\n",
            "despite considerable progress in HDR IMAGE TONE MAPPING for the past decade , little work has been done for HDR VIDEO . for applications such as FILM POST-PRODUCTION , the capability of LOCAL TONE MANIPULATION is highly regarded by the content <unk> . this paper presents an INTERACTIVE TONE MAPPING SCHEME for HDR VIDEO SEQUENCES . INTERACTIVE TONE MAPPING SCHEME provides a simple SCRIB-BLE/STROKE BASED INTERFACE for LOCAL TONE MANIPULATION and is capable of propagating USER INPUT INFORMATION throughout a VIDEO SEQUENCE by using GAUSSIAN MIXTURE MODEL and EDGE PRESERVING FILTERING . the experimental results demonstrated its effectiveness for HDR IMAGE TONE MAPPING as well as its flexibility for users to easily and intuitively manipulate the appearance of the video while maintaining TEMPORAL CONSISTENCY . \n",
            "this paper presents a novel INTERACTIVE TONE MAPPING SCHEME for FILM POST-PRODUCTION . the proposed INTERACTIVE TONE MAPPING SCHEME is based on a GAUSSIAN MIXTURE MODEL for FILM POST-PRODUCTION . the proposed INTERACTIVE TONE MAPPING SCHEME is based on a GAUSSIAN MIXTURE MODEL for FILM POST-PRODUCTION . the proposed INTERACTIVE TONE MAPPING SCHEME is based on a GAUSSIAN MIXTURE MODEL for HDR IMAGE TONE MAPPING . the proposed INTERACTIVE TONE MAPPING SCHEME is applied to HDR VIDEO SEQUENCES , and the experimental results show that the proposed INTERACTIVE TONE MAPPING SCHEME is effective in reducing the USER INPUT INFORMATION . the proposed INTERACTIVE TONE MAPPING SCHEME is applied to HDR VIDEO SEQUENCES , and the experimental results show that the proposed INTERACTIVE TONE MAPPING SCHEME is effective .\n",
            "\n",
            "293 1000\n",
            "-- COOPERATIVE COMMUNICATIONS leverages the SPATIAL DIVERSITY available in a WIRELESS NETWORK enabling multiple RADIO NODES work together to improve the overall system performance . when a DESTINATION RECEIVER combines the signal from an originating source with the associated signals from RELAY NODES , significant improvements in the BIT ERROR RATE performance can be achieved . this paper details the MEASURED BIT ERROR RATE performance of a THREE-NODE COOPERATIVE COMMUNICATION SYSTEM operating in a SOFTWARE DEFINED RADIO TESTBED . the measured performances of several types of COOPERATIVE PHYSICAL LAYER PROTOCOLS are compared to similar systems operating over a single WIRELESS LINK . the measured results include COOPERATIVE SYSTEMS operating with a MAXIMUM RATIO COMBINING TECHNIQUE and two COOPERATIVE CODED SYSTEMS using HARD DECISION DECODING . \n",
            "in this paper , we present a novel approach to COOPERATIVE COMMUNICATIONS and COOPERATIVE CODED SYSTEMS . the proposed THREE-NODE COOPERATIVE COMMUNICATION SYSTEM is based on a MAXIMUM RATIO COMBINING TECHNIQUE and COOPERATIVE CODED SYSTEMS . the proposed THREE-NODE COOPERATIVE COMMUNICATION SYSTEM is based on the use of HARD DECISION DECODING and COOPERATIVE CODED SYSTEMS . in the proposed THREE-NODE COOPERATIVE COMMUNICATION SYSTEM , a MAXIMUM RATIO COMBINING TECHNIQUE is employed to estimate the RELAY NODES of the RELAY NODES . the performance of the proposed THREE-NODE COOPERATIVE COMMUNICATION SYSTEM is evaluated in terms of BIT ERROR RATE and HARD DECISION DECODING . the performance of the proposed THREE-NODE COOPERATIVE COMMUNICATION SYSTEM is evaluated on the MEASURED BIT ERROR RATE and the MEASURED BIT ERROR RATE of the THREE-NODE COOPERATIVE COMMUNICATION SYSTEM . the performance of the proposed THREE-NODE COOPERATIVE COMMUNICATION SYSTEM is evaluated on the MEASURED BIT ERROR RATE and the MEASURED BIT ERROR RATE of the SOFTWARE DEFINED RADIO TESTBED . the performance of the proposed THREE-NODE COOPERATIVE COMMUNICATION SYSTEM is evaluated on a SOFTWARE DEFINED RADIO TESTBED with a SOFTWARE DEFINED RADIO TESTBED .\n",
            "\n",
            "294 1000\n",
            "we present a general approach to formally modelling corpora with MULTI-LAYERED ANNOTATION , thereby inducing a LEXICON MODEL in a TYPED LOGICAL REPRESENTATION LANGUAGE , OWL DL . this model can be interpreted as a GRAPH STRUCTURE that offers flexible querying <unk> beyond current XML-BASED QUERY LANGUAGES and powerful methods for CONSISTENCY CONTROL . we illustrate our approach by applying it to the SYNTACTICALLY AND SEMANTICALLY ANNOTATED SALSA/TIGER CORPUS . \n",
            "in this paper , we present a novel approach to the problem of TYPED LOGICAL REPRESENTATION LANGUAGE . the proposed approach is based on the use of a LEXICON MODEL , a LEXICON MODEL , and a LEXICON MODEL . the proposed approach is based on the use of a LEXICON MODEL , a LEXICON MODEL , and a LEXICON MODEL . the proposed approach is based on the use of a LEXICON MODEL , a LEXICON MODEL , and a LEXICON MODEL . the experimental results show that the proposed approach is able to achieve the same performance as the OWL DL .\n",
            "\n",
            "295 1000\n",
            "in this paper , we describe a method for SEGMENTING FIBER BUNDLES from DIFFUSION-WEIGHTED MAGNETIC RESONANCE IMAGES using a LOCALLY-CONSTRAINED REGION BASED APPROACH . from a pre-computed optimal path , the algorithm propagates <unk> capturing only those VOXELS which are locally connected to the FIBER BUNDLE . rather than attempting to find large numbers of OPEN CURVES or single fibers , which individually have questionable meaning , this method segments the FULL FIBER BUNDLE REGION . the strengths of this approach include its EASE-OF-USE , COMPUTATIONAL SPEED , and applicability to a wide range of FIBER BUNDLES . in this work , we show results for segmenting the CINGULUM BUNDLE . finally , we explain how this approach and extensions <unk> overcome a major problem that typical REGION-BASED FLOWS experience when attempting to segment neural FIBER BUNDLES . \n",
            "this paper presents a novel approach to SEGMENTING FIBER BUNDLES in DIFFUSION-WEIGHTED MAGNETIC RESONANCE IMAGES . the proposed approach is based on the use of a LOCALLY-CONSTRAINED REGION BASED APPROACH and a LOCALLY-CONSTRAINED REGION BASED APPROACH . the proposed method is based on the use of a LOCALLY-CONSTRAINED REGION BASED APPROACH and a LOCALLY-CONSTRAINED REGION BASED APPROACH . the proposed method is based on a LOCALLY-CONSTRAINED REGION BASED APPROACH of the FULL FIBER BUNDLE REGION and the FULL FIBER BUNDLE REGION of the FIBER BUNDLES . the proposed method is evaluated in terms of both COMPUTATIONAL SPEED and COMPUTATIONAL SPEED . the results show that the proposed method is able to detect the presence of VOXELS , EASE-OF-USE and COMPUTATIONAL SPEED .\n",
            "\n",
            "296 1000\n",
            "online <unk> handwriting recognition is currently one of the most intriguing challenges in PATTERN RECOGNITION . this study presents a novel approach to this ONLINE CURSIVE HANDWRITING RECOGNITION which is composed of two complementary phases . the first is DYNAMIC ENCODING of the WRITING TRA-JECTORY into a compact sequence of DISCRETE MOTOR CONTROL SYMBOLS . in this COMPACT REPRESENTATION we largely remove the redundancy of the script , while preserving most of its INTELLIGIBLE COMPONENTS . in the second phase these CONTROL SEQUENCES are used to train ADAPTIVE PROBABILISTIC ACYCLIC AUTOMATA for the important ingredients of the WRITING TRAJECTORIES , e.g. letters . we present a new and efficient LEARNING ALGORITHM for such STOCHASTIC AUTOMATA , and demonstrate its utility for SPOTTING AND SEGMENTATION OF CURSIVE SCRIPTS . our experiments show that over 90 % of the letters are correctly spotted and identified , prior to any higher LEVEL LANGUAGE MODEL . moreover , both the TRAINING AND RECOGNITION ALGORITHMS are very efficient compared to other MODELING METHODS , and the TRAINING AND RECOGNITION ALGORITHMS are ` on-line ' adaptable to other writers and styles . \n",
            "this paper presents a novel LEARNING ALGORITHM for SPOTTING AND SEGMENTATION OF CURSIVE SCRIPTS . the proposed LEARNING ALGORITHM is based on the use of STOCHASTIC AUTOMATA , which is based on a LEVEL LANGUAGE MODEL . the proposed LEARNING ALGORITHM is based on the use of STOCHASTIC AUTOMATA , which is based on a LEARNING ALGORITHM . the proposed approach is based on the use of STOCHASTIC AUTOMATA , which is based on the LEVEL LANGUAGE MODEL . the proposed LEARNING ALGORITHM is applied to SPOTTING AND SEGMENTATION OF CURSIVE SCRIPTS , such as SPOTTING AND SEGMENTATION OF CURSIVE SCRIPTS and ONLINE CURSIVE HANDWRITING RECOGNITION . the experimental results show that the proposed TRAINING AND RECOGNITION ALGORITHMS is effective in improving the SPOTTING AND SEGMENTATION OF CURSIVE SCRIPTS performance of the proposed MODELING METHODS .\n",
            "\n",
            "297 1000\n",
            "<unk> r-d optimized rate control for VIDEO CODING is investigated in this work . a FRAME LEVEL BIT ALLOCATION is first presented based on a model of the relationship between the rate -lrb- r -rrb- and NONZERO COEFFICIENTS . with the modelled <unk> relationship , a QUALITY FEEDBACK SCHEME is proposed to generate <unk> -lrb- video buffer <unk> -rrb- compliant bitstream with ASSURED VIDEO QUALITY . then , a R-D OPTIMIZED MACROBLOCK LEVEL RATE CONTROL is described by jointly selecting the QUANTIZATION PARAMETER and the CODING MODE OF MACROBLOCKS in i , b and p pictures for both PROGRESSIVE AND INTERLACED VIDEO . to avoid the <unk> large <unk> or one single ISOLATED COEFFICIENT , we extend the set of CODING modes of mb by including zero <unk> and zero texture bits as two more candidates . finally , FAST HEURISTICS are developed to reduce the COMPUTATIONAL COMPLEXITY of R-D DATA GENERATION and the VITERBI ALGORITHM in R-D OPTIMIZATION , which achieves CODING results close to the optimal one at a much lower COMPUTATIONAL COST . \n",
            "in this paper , we propose a novel QUALITY FEEDBACK SCHEME for R-D DATA GENERATION . the proposed FAST HEURISTICS is based on a VITERBI ALGORITHM and a VITERBI ALGORITHM . the proposed FAST HEURISTICS is based on the VITERBI ALGORITHM and the VITERBI ALGORITHM . the proposed FAST HEURISTICS is based on a VITERBI ALGORITHM and a VITERBI ALGORITHM . the COMPUTATIONAL COMPLEXITY of the proposed FAST HEURISTICS is evaluated in terms of COMPUTATIONAL COMPLEXITY and COMPUTATIONAL COST . the performance of the proposed FAST HEURISTICS is evaluated in terms of the COMPUTATIONAL COMPLEXITY and the COMPUTATIONAL COMPLEXITY of the proposed QUALITY FEEDBACK SCHEME . the performance of the proposed FAST HEURISTICS is evaluated in terms of ASSURED VIDEO QUALITY and COMPUTATIONAL COST .\n",
            "\n",
            "298 1000\n",
            "temporal processing and FILTERING in SPEECH FEATURE EXTRACTION are commonly used to aid in performance and ROBUSTNESS in AUTOMATIC SPEECH RECOGNITION . among the techniques successfully employed are RASTA FILTERING , DELTA CALCULATION , and CEPSTRAL MEAN SUBTRACTION . the work here explores the use of TEMPORAL FILTER DESIGN using LDA FILTERS to further enhance performance using a few PREPROCESSING CONFIGURATIONS . in addition to RASTA FILTERING , we apply the TEMPORAL FILTER DESIGN to MODULATION-SPECTRAL FEATURES and <unk> while making sure that the assumptions of LDA FILTERS are observed . we additionally test the use of TEMPORAL FILTER DESIGN that have been trained in different REVERBERATION CONDITIONS , noting from previous work that the presence of REVERBERATION alters the preferred frequency range of the derived TEMPORAL FILTER DESIGN . our tests indicate a consistent advantage in PHONE CLASSIFICATION . WORD RECOGNITION TESTS , in contrast , reveal that the LDA FILTERS often do not improve upon the existing TEMPORAL FILTER DESIGN previously used . they can also be made less <unk> by allowing CONTEXTUAL FRAMES to a TRAINED PROBABILITY ESTIMATOR . \n",
            "in this paper , we propose a novel approach to SPEECH FEATURE EXTRACTION for AUTOMATIC SPEECH RECOGNITION . the proposed TEMPORAL FILTER DESIGN is based on the use of LDA FILTERS and FILTERING . the proposed TEMPORAL FILTER DESIGN is based on the use of LDA FILTERS , DELTA CALCULATION , DELTA CALCULATION and FILTERING . the proposed TEMPORAL FILTER DESIGN is based on the use of LDA FILTERS , DELTA CALCULATION , and FILTERING . the proposed approach is evaluated on a variety of PREPROCESSING CONFIGURATIONS , including CEPSTRAL MEAN SUBTRACTION , DELTA CALCULATION , DELTA CALCULATION and CEPSTRAL MEAN SUBTRACTION . the experimental results demonstrate the effectiveness of the proposed approach .\n",
            "\n",
            "299 1000\n",
            "in this paper , we investigate the practical applicability of CO-TRAINING for the task of building a CLASSIFIER for REFERENCE RESOLUTION . we are concerned with the question if CO-TRAINING can significantly reduce the amount of MANUAL LABELING work and still produce a CLASSIFIER with an acceptable performance . \n",
            "this paper presents a novel CLASSIFIER for CO-TRAINING . the proposed CLASSIFIER is based on the use of CO-TRAINING for CO-TRAINING . the proposed CLASSIFIER is based on the use of CO-TRAINING for CO-TRAINING . the proposed CLASSIFIER is based on the use of CO-TRAINING for CO-TRAINING . the experimental results show that the proposed CLASSIFIER is effective in improving the REFERENCE RESOLUTION performance .\n",
            "\n",
            "300 1000\n",
            "the progress of DIGITAL ELECTROENCEPHALOGRAPHY gave rise to the problem of EEG DATA RECORDING . in this paper a DPCM SCHEME for EEG DATA RECORDING is discussed . in particular the performance of a class of PRE-DICTORS based on RECURRENT NEURAL NETWORKS is presented . the TRAINING STRATEGY is accurately described and the results of a comparison with some other classical linear and static neural predictors are given . the proposed RECURRENT NEURAL PREDICTOR demonstrates to be competitive with the others in <unk> good performance at a very low COMPUTATIONAL COST . \n",
            "in this paper , we propose a novel approach to DIGITAL ELECTROENCEPHALOGRAPHY based on RECURRENT NEURAL NETWORKS . the proposed approach is based on the use of RECURRENT NEURAL NETWORKS for DIGITAL ELECTROENCEPHALOGRAPHY . the proposed method is based on the use of RECURRENT NEURAL NETWORKS to estimate the PRE-DICTORS . the proposed method is based on the use of RECURRENT NEURAL NETWORKS for EEG DATA RECORDING . the experimental results show that the proposed method is effective in improving the COMPUTATIONAL COST and COMPUTATIONAL COST of the proposed method .\n",
            "\n",
            "301 1000\n",
            "spoken queries are a natural medium for searching the MOBILE WEB . LANGUAGE MODELING for VOICE SEARCH RECOGNITION offers different challenges compared to more conventional SPEECH APPLICATIONS . the challenges arise from the fact that SPOKEN QUERIES are usually a set of KEYWORDS and do not have a SYNTACTIC AND GRAMMATICAL STRUCTURE . this paper describes a CO-OCCURRENCE BASED APPROACH to improve the ACCURACY of VOICE QUERIES AUTOMATIC TRANSCRIPTION . with the right choice of SCORING FUNCTION and CO-OCCURRENCE LEVEL , we show that CO-OCCURRENCE INFORMATION gives a 2 % RELATIVE ACCURACY improvement over a state of the art system . \n",
            "this paper addresses the problem of VOICE SEARCH RECOGNITION for VOICE SEARCH RECOGNITION . in this paper , we propose a novel CO-OCCURRENCE BASED APPROACH for VOICE SEARCH RECOGNITION . the proposed VOICE QUERIES AUTOMATIC TRANSCRIPTION is based on a CO-OCCURRENCE BASED APPROACH for VOICE SEARCH RECOGNITION . the proposed VOICE SEARCH RECOGNITION is based on the use of a SCORING FUNCTION , a SCORING FUNCTION , and a CO-OCCURRENCE LEVEL . the proposed VOICE SEARCH RECOGNITION is evaluated on a variety of SPOKEN QUERIES . the experimental results show that the proposed VOICE SEARCH RECOGNITION can significantly improve the ACCURACY and ACCURACY of the VOICE QUERIES AUTOMATIC TRANSCRIPTION .\n",
            "\n",
            "302 1000\n",
            "one of the most difficult tasks in VALUE FUNCTION APPROXIMATION for MARKOV DECISION PROCESSES is finding an APPROXIMATION ARCHITECTURE that is expressive enough to capture the important structure in the VALUE FUNCTION , while at the same time not overfitting the training samples . recent results in NON-PARAMETRIC APPROXIMATE LINEAR PROGRAMMING , have demonstrated that this can be done effectively using nothing more than a SMOOTHNESS ASSUMPTION on the VALUE FUNCTION . in this paper we extend these results to the case where samples come from REAL WORLD TRANSITIONS instead of the full BELLMAN EQUATION , adding ROBUSTNESS to NOISE . in addition , we provide the first <unk> , finite sample performance guarantees for any form of ALP . NON-PARAMETRIC APPROXIMATE LINEAR PROGRAMMING is amenable to problems with large -lrb- multidimensional -rrb- or even INFINITE ACTION SPACES , and does not require a model to select actions using the resulting APPROXIMATE SOLUTION . \n",
            "in this paper , we propose a novel approach to MARKOV DECISION PROCESSES based on a NON-PARAMETRIC APPROXIMATE LINEAR PROGRAMMING . the proposed method is based on the idea of NON-PARAMETRIC APPROXIMATE LINEAR PROGRAMMING , which is based on a NON-PARAMETRIC APPROXIMATE LINEAR PROGRAMMING . the proposed method is based on the idea of NON-PARAMETRIC APPROXIMATE LINEAR PROGRAMMING , which is based on a NON-PARAMETRIC APPROXIMATE LINEAR PROGRAMMING . the proposed method is based on a NON-PARAMETRIC APPROXIMATE LINEAR PROGRAMMING , which is based on a NON-PARAMETRIC APPROXIMATE LINEAR PROGRAMMING . the proposed method is applied to the BELLMAN EQUATION of the MARKOV DECISION PROCESSES . the proposed method is evaluated in terms of ROBUSTNESS and ROBUSTNESS . the experimental results show that the proposed method outperforms the existing methods in terms of ROBUSTNESS and ROBUSTNESS .\n",
            "\n",
            "303 1000\n",
            "in this paper , we use COMPUTER SIMULATION to better understand MIXED-INITIATIVE DIALOGUES . we compare two models of MIXED-INITIATIVE : UNRESTRICTED INITIATIVE , where either participant can take over control at any point ; and RESTRICTED INITIATIVE MODEL where one participant keeps control and the other plays a secondary role , but greater than what <unk> allows . we find that RESTRICTED INITIATIVE MODEL results in similar SOLUTION QUALITY as unrestricted , less communication effort , and similar or less reasoning effort . these results agree with our empirical studies on HUMAN-HUMAN DIALOGUES , in which we find that participants seem to follow the RESTRICTED INITIATIVE MODEL . \n",
            "this paper addresses the problem of UNRESTRICTED INITIATIVE for HUMAN-HUMAN DIALOGUES . we propose a method for UNRESTRICTED INITIATIVE , which is based on a RESTRICTED INITIATIVE MODEL . the proposed approach is based on the use of a RESTRICTED INITIATIVE MODEL , which is able to deal with HUMAN-HUMAN DIALOGUES in the presence of HUMAN-HUMAN DIALOGUES . the proposed method is evaluated on a variety of HUMAN-HUMAN DIALOGUES . the results show that the proposed method is effective in improving the SOLUTION QUALITY of the RESTRICTED INITIATIVE MODEL .\n",
            "\n",
            "304 1000\n",
            "this paper introduces MATRIX FILTERS as a tool for LOCALIZA-TION AND DETECTION PROBLEMS in PASSIVE SONAR . the outputs of an array of sensors , at some given frequency , can be represented by a vector of complex numbers . a LINEAR FILTERING OPERATION on the SENSOR OUTPUTS can be expressed as the multiplication of a MATRIX -lrb- called a MATRIX FILTERS -rrb- times this vector . the purpose of a MATRIX FILTERS is to attenuate UNWANTED COMPONENTS in the MEASURED SENSOR DATA while passing desired components with MINIMAL DISTORTION . MATRIX FILTERS are designed by defining an appropriate pass band and stop band and solving a CONVEX OPTIMIZATION PROBLEM . this paper formulates the design of MATRIX FILTERS for PASSIVE SONAR and gives two examples . \n",
            "this paper addresses the problem of LOCALIZA-TION AND DETECTION PROBLEMS from MEASURED SENSOR DATA . we propose a novel method to estimate the UNWANTED COMPONENTS of a scene from a single image . the proposed approach is based on the use of MATRIX FILTERS to estimate the UNWANTED COMPONENTS of the MATRIX . the proposed approach is based on the use of MATRIX FILTERS for LOCALIZA-TION AND DETECTION PROBLEMS . the proposed method is based on the use of MATRIX FILTERS for LOCALIZA-TION AND DETECTION PROBLEMS . the proposed method is evaluated on a variety of LOCALIZA-TION AND DETECTION PROBLEMS . the experimental results show that the proposed method outperforms the existing methods in terms of the MINIMAL DISTORTION .\n",
            "\n",
            "305 1000\n",
            "<unk> <unk> poses a particularly diicult challenge for conventional MONOPULSE RADARS . in such cases SPATIALLY ADAPTIVE PROCESSING provides some INTERFERENCE SUPPRESSION when the target and JAMMER are not exactly <unk> , but the resulting ARRAY PATTERN is too distorted to be suitable for MONOPULSE PROCESSING . the presence of COHERENT MULTI-PATH in the form of TERRAIN SCATTERED INTERFERENCE is normally considered a nuisance source of interference . however , it can also be exploited to suppress MAINBEAM JAMMING with SPACE-TIME PROCESSING . here we present a method for incorporating SPACE-TIME PROCESSING into MONOPULSE PROCESSING to yield a SPACE-TIME MONOPULSE PROCESSOR with DIS-TORTIONLESS SPATIAL ARRAY PATTERNS that can achieve far better MAINBEAM JAMMING CANCELATION and TARGET ANGLE ESTIMATION than has been previously possible . performance results for the SPACE-TIME MONOPULSE PROCESSOR are obtained for MOUNTAINTOP DATA containing a JAMMER and TERRAIN SCATTERED INTERFERENCE , that demonstrate a dramatic improvement in performance over conventional MONOPULSE AND SPATIALLY ADAPTIVE MONOPULSE . \n",
            "this paper presents a novel approach to TARGET ANGLE ESTIMATION in SPACE-TIME PROCESSING . the proposed approach is based on the use of a SPACE-TIME MONOPULSE PROCESSOR and a COHERENT MULTI-PATH for TARGET ANGLE ESTIMATION . the proposed approach is based on the use of a SPACE-TIME MONOPULSE PROCESSOR and TARGET ANGLE ESTIMATION . the proposed approach is based on the use of a SPACE-TIME MONOPULSE PROCESSOR and TARGET ANGLE ESTIMATION . the proposed approach is evaluated on a variety of MOUNTAINTOP DATA . the results show that the proposed approach is effective in reducing the number of MOUNTAINTOP DATA in the SPACE-TIME MONOPULSE PROCESSOR . the proposed method is evaluated on a variety of MOUNTAINTOP DATA . the results show that the proposed approach is able to detect TERRAIN SCATTERED INTERFERENCE in a wide range of MOUNTAINTOP DATA .\n",
            "\n",
            "306 1000\n",
            "however , permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists , or to reuse any COPYRIGHTED COMPONENT of this work in other works must be obtained from the IEEE . abstract a novel family of 2d and 3d geometrically invariant FEATURES , called SUMMATION INVARIANTS is proposed for the RECOGNITION OF THE 3D SURFACE OF HUMAN FACES . focusing on a RECTANGULAR REGION surrounding the NOSE of a 3D FACIAL DEPTH MAP , a subset of the so called SEMI-LOCAL SUM-MATION INVARIANT FEATURES is extracted . then the similarity between a pair of 3D FACIAL DEPTH MAPS is computed to determine whether SEMI-LOCAL SUM-MATION INVARIANT FEATURES belong to the same person . out of many possible combinations of these set of FEATURES , we select , through careful experimentation , a subset of FEATURES that yields best combined performance . tested with the 3D FACIAL DATA from the ongoing FACE RECOGNITION GRAND CHALLENGE V1 .0 DATASET , the proposed new FEATURES exhibit significant performance improvement over the baseline algorithm distributed with the dataset . \n",
            "in this paper , we propose a novel method for RECOGNITION OF THE 3D SURFACE OF HUMAN FACES from a single image . the proposed approach is based on the use of a set of SEMI-LOCAL SUM-MATION INVARIANT FEATURES , a RECTANGULAR REGION , and a set of FEATURES . the proposed method is based on a SEMI-LOCAL SUM-MATION INVARIANT FEATURES , which is a RECTANGULAR REGION of a RECTANGULAR REGION , and a RECTANGULAR REGION is used to estimate the COPYRIGHTED COMPONENT . the proposed method is evaluated on a FACE RECOGNITION GRAND CHALLENGE V1 .0 DATASET of the FACE RECOGNITION GRAND CHALLENGE V1 .0 DATASET . the results show that the proposed method outperforms the state-of-the-art methods in terms of both 3D FACIAL DEPTH MAP and NOSE .\n",
            "\n",
            "307 1000\n",
            "this paper presents a novel SPECTRAL CONVERSION METHOD by considering the glottal effect on STRAIGHT SPECTRUM to improve the performance of FORMER VOICE CONVERSION SYSTEM based on CODEBOOK MAPPING . by introducing MOG MODEL into SPECTRAL REPRESENTATION , STRAIGHT SPECTRUM is decomposed into EXCITATION-DEPENDENT AND EXCITATION-INDEPENDENT COMPONENTS , which are transformed separately . besides , MOG MODEL is adopted to measure the PROSODIC CHARACTERISTICS of different speakers and realize PROSODIC CONVERSION . listening test proves that proposed SPECTRAL CONVERSION METHOD can effectively improve the DISCRIMINATION AND SPEECH QUALITY of CONVERTED SPEECH at the same time . \n",
            "in this paper , we propose a novel SPECTRAL CONVERSION METHOD for PROSODIC CONVERSION . the proposed MOG MODEL is based on a MOG MODEL for PROSODIC CONVERSION . the proposed MOG MODEL is based on a CODEBOOK MAPPING for PROSODIC CONVERSION . the proposed MOG MODEL is based on a CODEBOOK MAPPING for PROSODIC CONVERSION . the proposed MOG MODEL is applied to the FORMER VOICE CONVERSION SYSTEM and is shown to be effective in improving the DISCRIMINATION AND SPEECH QUALITY of the FORMER VOICE CONVERSION SYSTEM .\n",
            "\n",
            "308 1000\n",
            "why does placing an object from one photograph into another often make the colors of that object suddenly look wrong ? one possibility is that humans prefer DISTRIBUTIONS OF COLORS that are often found in nature ; that is , we find pleasing these COLOR COMBINATIONS that we see often . another possibility is that humans simply prefer colors to be consistent within an image , regardless of what they are . in this paper , we explore some of these issues by studying the color statistics of a large dataset of NATURAL IMAGES , and by looking at differences in COLOR DISTRIBUTION in REALISTIC AND UNREALISTIC IMAGES . we apply our findings to two problems : 1 -rrb- CLASSIFYING COMPOSITE IMAGES into realistic vs. <unk> , and 2 -rrb- RECOLORING IMAGE REGIONS for REALISTIC COM-POSITING . \n",
            "this paper addresses the problem of CLASSIFYING COMPOSITE IMAGES from NATURAL IMAGES . we propose a method to estimate the DISTRIBUTIONS OF COLORS of a scene from a single COLOR DISTRIBUTION . the proposed method is based on the use of COLOR COMBINATIONS and RECOLORING IMAGE REGIONS . the proposed method is based on the use of REALISTIC AND UNREALISTIC IMAGES and RECOLORING IMAGE REGIONS . the proposed method is evaluated on a variety of NATURAL IMAGES and NATURAL IMAGES .\n",
            "\n",
            "309 1000\n",
            "in this paper , it is shown that an appropriate model for VOICED SPEECH is an ALL-POLE FILTER excited by a BLOCK SPARSE EXCITATION SEQUENCE . the modeling approach is generalized in a novel manner to deal with a wide spectrum of SPEECH SIGNAL ; VOICED SPEECH , UNVOICED SPEECH and MIXED EXCITATION SPEECH . in this context , the input sequence to the all-pole model is modeled as a suitable WEIGHTED LINEAR COMBINATION of a block sparse signal and WHITE NOISE . we develop the corresponding ESTIMATION PROCEDURE to reconstruct the GENERALIZED INPUT SEQUENCE and MODEL PARAMETERS via SPARSE BAYESIAN LEARNING METHODS employing the EXPECTATION-MAXIMIZATION BASED PROCEDURE . rigorous experiments have been performed to show the efficacy of our proposed model for the SPEECH MODELING TASK . by imposing a BLOCK SPARSE STRUCTURE on the input sequence , the problems associated with the commonly used LINEAR PREDICTION APPROACH is alleviated leading to a more robust modeling scheme . \n",
            "in this paper , we propose a novel approach to SPEECH MODELING TASK based on a LINEAR PREDICTION APPROACH . the proposed SPARSE BAYESIAN LEARNING METHODS is based on a WEIGHTED LINEAR COMBINATION , which is based on a LINEAR PREDICTION APPROACH . the proposed SPARSE BAYESIAN LEARNING METHODS is based on a WEIGHTED LINEAR COMBINATION , which is based on the EXPECTATION-MAXIMIZATION BASED PROCEDURE . the proposed SPARSE BAYESIAN LEARNING METHODS is based on a WEIGHTED LINEAR COMBINATION , which is based on a WEIGHTED LINEAR COMBINATION . the proposed SPARSE BAYESIAN LEARNING METHODS is applied to the SPEECH MODELING TASK and MIXED EXCITATION SPEECH . the experimental results show that the proposed LINEAR PREDICTION APPROACH is robust to WHITE NOISE , VOICED SPEECH , UNVOICED SPEECH and VOICED SPEECH .\n",
            "\n",
            "310 1000\n",
            "standard AUTOMATIC SPEECH RECOGNITION SYSTEMS use PHONEME-BASED PRONUNCIATION LEXICON prepared by linguistic experts . when the HAND CRAFTED PRONUNCIATIONS fail to cover the vocabulary of a new domain , a GRAPHEME-TO-PHONEME CONVERTER is used to extract PRONUNCIATIONS for new words and then a PHONEME-BASED ASR SYSTEM is trained . G2P CONVERTERS are typically trained only on the existing lexicons . in this paper , we propose a GRAPHEME-BASED ASR APPROACH in the framework of PROBABILISTIC LEXICAL MOD-ELING that integrates PRONUNCIATION LEARNING as a stage in ASR SYSTEM TRAINING , and exploits both ACOUSTIC AND LEXICAL RESOURCES -lrb- not necessarily from the domain or language of interest -rrb- . the proposed GRAPHEME-BASED ASR APPROACH is evaluated on four LEXICAL RESOURCE CONSTRAINED ASR TASKS and compared with the conventional two STAGE APPROACH where G2P TRAINING is followed by ASR SYSTEM DEVELOPMENT . \n",
            "this paper presents a novel GRAPHEME-BASED ASR APPROACH for PROBABILISTIC LEXICAL MOD-ELING . the proposed GRAPHEME-BASED ASR APPROACH is based on the use of a PHONEME-BASED PRONUNCIATION LEXICON , a PHONEME-BASED PRONUNCIATION LEXICON , and a PHONEME-BASED PRONUNCIATION LEXICON for PRONUNCIATION LEARNING . the proposed GRAPHEME-BASED ASR APPROACH is based on the use of a PHONEME-BASED PRONUNCIATION LEXICON , a PHONEME-BASED PRONUNCIATION LEXICON , and a PHONEME-BASED PRONUNCIATION LEXICON for PRONUNCIATION LEARNING . the proposed GRAPHEME-BASED ASR APPROACH is evaluated on two LEXICAL RESOURCE CONSTRAINED ASR TASKS . the experimental results show that the proposed GRAPHEME-BASED ASR APPROACH significantly outperforms the conventional STAGE APPROACH in terms of both ACOUSTIC AND LEXICAL RESOURCES and ASR SYSTEM DEVELOPMENT .\n",
            "\n",
            "311 1000\n",
            "we present a GAME-THEORETIC MODEL of <unk> over a metaphor in the context of POLITICAL COMMUNICATION , find its equilibrium , and use GAME-THEORETIC MODEL to <unk> OBSERVED LINGUISTIC BEHAVIOR . we argue that GAME THEORY is well suited for MODELING DISCOURSE as a dynamic resulting from a number of conflicting <unk> , and suggest applications of interest to COMPUTATIONAL LINGUISTS . \n",
            "this paper presents a novel GAME-THEORETIC MODEL for MODELING DISCOURSE . the proposed GAME-THEORETIC MODEL is based on a GAME-THEORETIC MODEL for POLITICAL COMMUNICATION . the proposed GAME-THEORETIC MODEL is based on a GAME-THEORETIC MODEL for POLITICAL COMMUNICATION . the proposed GAME-THEORETIC MODEL is applied to the problem of POLITICAL COMMUNICATION , and the results show that the proposed GAME-THEORETIC MODEL is effective in reducing the OBSERVED LINGUISTIC BEHAVIOR .\n",
            "\n",
            "312 1000\n",
            "environmental MONITORING APPLICATIONS present a challenge to current BACKGROUND SUBTRACTION ALGORITHMS that analyze the TEMPORAL VARIABILITY OF PIXEL INTENSITIES , due to the complex texture and motion of the scene . they also present a challenge to SEGMENTATION ALGORITHMS that compare INTENSITY OR COLOR DISTRIBUTIONS between the foreground and the background in each image independently , because objects of interest such as animals have adapted to blend in . therefore , we have developed a BACKGROUND MODELING AND SUBTRACTION SCHEME that analyzes the temporal variation of INTENSITY OR COLOR DISTRIBUTIONS , instead of either looking at TEMPORAL VARIATION OF POINT STATISTICS , or the SPATIAL VARIATION OF REGION STATISTICS in isolation . DISTRIBUTIONAL SIGNATURES are less sensitive to movements of the TEX-TURED BACKGROUND , and at the same time DISTRIBUTIONAL SIGNATURES are more robust than individual pixel statistics in DETECTING FOREGROUND OBJECTS . they also enable slow background update , which is crucial in MONITORING APPLICATIONS where PROCESSING POWER comes at a <unk> , and where FOREGROUND OBJECTS , when present , may move less than the background and therefore disappear into it when a fast update scheme is used . our BACKGROUND MODELING AND SUBTRACTION SCHEME compares favorably with the state of the art both in GENERIC LOW-LEVEL DETECTION METRICS , as well as in APPLICATION-DEPENDENT CRITERIA . \n",
            "this paper presents a novel BACKGROUND MODELING AND SUBTRACTION SCHEME based on DISTRIBUTIONAL SIGNATURES and SPATIAL VARIATION OF REGION STATISTICS . the proposed BACKGROUND MODELING AND SUBTRACTION SCHEME is based on the use of APPLICATION-DEPENDENT CRITERIA and SPATIAL VARIATION OF REGION STATISTICS . the proposed BACKGROUND MODELING AND SUBTRACTION SCHEME is based on the use of DISTRIBUTIONAL SIGNATURES and DISTRIBUTIONAL SIGNATURES . the proposed BACKGROUND MODELING AND SUBTRACTION SCHEME is based on the use of DISTRIBUTIONAL SIGNATURES and SPATIAL VARIATION OF REGION STATISTICS . the proposed BACKGROUND MODELING AND SUBTRACTION SCHEME is evaluated on both GENERIC LOW-LEVEL DETECTION METRICS and DISTRIBUTIONAL SIGNATURES . the results show that the proposed BACKGROUND MODELING AND SUBTRACTION SCHEME is robust to FOREGROUND OBJECTS and FOREGROUND OBJECTS .\n",
            "\n",
            "313 1000\n",
            "consumer DEPTH CAMERAS have dramatically improved our ability to track rigid , articulated , and deformable 3d objects in real-time . however , DEPTH CAMERAS have a limited temporal resolution -lrb- <unk> -rrb- that restricts the ACCURACY and ROBUSTNESS of TRACKING , especially for FAST OR UNPREDICTABLE MOTION . in this paper , we show how to perform MODEL-BASED OBJECT TRACKING which allows to reconstruct the object 's depth at an order of magnitude higher <unk> through simple modifications to an OFF-THE-SHELF DEPTH CAMERA . we focus on PHASE-BASED TIME-OF-FLIGHT SENSING , which reconstructs each LOW FRAME-RATE DEPTH IMAGE from a set of short exposure ` raw ' infrared captures . these RAW CAPTURES are taken in quick <unk> near the beginning of each depth frame , and differ in the modulation of their ACTIVE ILLUMINATION . we make two contributions . first , we detail how to perform MODEL-BASED OBJECT TRACKING against these RAW CAPTURES . second , we show that by <unk> the camera to space the RAW CAPTURES uniformly in time , we obtain a <unk> higher <unk> , and thereby improve the ability to track FAST-MOVING OBJECTS . \n",
            "this paper addresses the problem of MODEL-BASED OBJECT TRACKING in CONSUMER DEPTH CAMERAS . we propose a novel approach to MODEL-BASED OBJECT TRACKING based on RAW CAPTURES . the proposed approach is based on the use of PHASE-BASED TIME-OF-FLIGHT SENSING and TRACKING . the proposed approach is based on a LOW FRAME-RATE DEPTH IMAGE , which is based on the RAW CAPTURES . the proposed approach is evaluated on a variety of CONSUMER DEPTH CAMERAS . the results show that the proposed method is effective in TRACKING and TRACKING .\n",
            "\n",
            "314 1000\n",
            "decision making under uncertainty is commonly modelled as a process of COMPETITIVE STOCHASTIC EVIDENCE ACCUMULATION to threshold -lrb- the DRIFT-DIFFUSION MODEL -rrb- . however , it is unknown how animals learn these DECISION THRESHOLDS . we examine THRESHOLD LEARNING by constructing a REWARD FUNCTION that averages over many trials to WALD 'S COST FUNCTION that defines DECISION OPTIMALITY . these REWARD FUNCTION are highly stochastic and hence challenging to optimize , which we address in two ways : first , a simple TWO-FACTOR REWARD-MODULATED LEARNING RULE derived from WILLIAMS ' REINFORCE METHOD for NEURAL NETWORKS ; and second , BAYESIAN OPTIMIZATION of the REWARD FUNCTION with a GAUSSIAN PROCESS . BAYESIAN OPTIMIZATION converges in fewer trials than REINFORCE but is slower computationally with greater variance . the WILLIAMS ' REINFORCE METHOD is also a better model of ACQUISITION BEHAVIOUR in animals and a similar LEARNING RULE has been proposed for MODELLING BASAL GANGLIA FUNCTION . \n",
            "this paper proposes a novel DRIFT-DIFFUSION MODEL for DECISION MAKING . the proposed MODELLING BASAL GANGLIA FUNCTION is based on a DRIFT-DIFFUSION MODEL , which is a GAUSSIAN PROCESS for DECISION MAKING . the proposed MODELLING BASAL GANGLIA FUNCTION is based on a DRIFT-DIFFUSION MODEL , which is a GAUSSIAN PROCESS for DECISION MAKING . the proposed MODELLING BASAL GANGLIA FUNCTION is based on a DRIFT-DIFFUSION MODEL , which is based on a DRIFT-DIFFUSION MODEL . the proposed DRIFT-DIFFUSION MODEL is based on a DRIFT-DIFFUSION MODEL , which is based on a DRIFT-DIFFUSION MODEL . the proposed DRIFT-DIFFUSION MODEL is applied to the MODELLING BASAL GANGLIA FUNCTION of the DRIFT-DIFFUSION MODEL , which is based on a DRIFT-DIFFUSION MODEL . the proposed DRIFT-DIFFUSION MODEL is compared to the REINFORCE and the REINFORCE . the proposed DRIFT-DIFFUSION MODEL is compared to REINFORCE and is shown to significantly outperform the REINFORCE in terms of the ACQUISITION BEHAVIOUR .\n",
            "\n",
            "315 1000\n",
            "this paper deals with MULTIPLE INPUT MULTIPLE OUTPUT SYSTEMS for ACTIVE CONTROL OF ACOUSTIC SIGNALS . these MULTIPLE INPUT MULTIPLE OUTPUT SYSTEMS are used when the ACOUSTIC ELD is complex and therefore a number of sensors are necessary to estimate the sound eld a n d a n <unk> of sources to create the CANCELLING ELD . a STEEPEST DESCENT ITERATIVE ALGORITHM is applied to minimise the p-norm of a MULTIPLE INPUT MULTIPLE OUTPUT SYSTEMS composed by the output signals of a microphone array . the existing algorithms deal with the 2-norm of this MULTIPLE INPUT MULTIPLE OUTPUT SYSTEMS . this paper describes a general framework that covers the existing MULTIPLE INPUT MULTIPLE OUTPUT SYSTEMS and then it focuses on the 1-NORM MINIMISATION ALGORITHM . the 1-NORM MINIMISATION ALGORITHM based on the 1-norm minimises the output signal which has the greatest power . it is shown by means of simulations using MEASURED DATA from a real room that the 1-NORM MINIMISATION ALGORITHM leads to a more UNIFORM NAL NOISE ELD than the existing algorithms . \n",
            "this paper addresses the problem of ACTIVE CONTROL OF ACOUSTIC SIGNALS from a single image . in this paper , we propose a novel method for ACTIVE CONTROL OF ACOUSTIC SIGNALS based on MULTIPLE INPUT MULTIPLE OUTPUT SYSTEMS . the proposed method is based on a STEEPEST DESCENT ITERATIVE ALGORITHM , which is based on a STEEPEST DESCENT ITERATIVE ALGORITHM . the proposed method is based on a STEEPEST DESCENT ITERATIVE ALGORITHM , which is based on a STEEPEST DESCENT ITERATIVE ALGORITHM . the proposed method is based on a STEEPEST DESCENT ITERATIVE ALGORITHM , which is based on a STEEPEST DESCENT ITERATIVE ALGORITHM . the experimental results demonstrate the effectiveness of the proposed method .\n",
            "\n",
            "316 1000\n",
            "in this paper , we address the problem of learning compact , <unk> , realistic 3d models of human actions recorded with multiple cameras , for the purpose of recognizing those same actions from a single or few cameras , without PRIOR KNOWLEDGE about the RELATIVE ORIENTA-TIONS between the cameras and the subjects . to this aim , we propose a new framework where we model actions using three DIMENSIONAL OCCUPANCY GRIDS , built from multiple viewpoints , in an EXEMPLAR-BASED HMM . the novelty is , that a 3D RECONSTRUCTION is not required during the RECOGNITION PHASE , instead learned 3D RECONSTRUCTION are used to produce 2D IMAGE INFORMATION that is compared to the observations . PARAMETERS that describe IMAGE PROJECTIONS are added as LATENT VARIABLES in the RECOGNITION PROCESS . in addition , the TEMPORAL MARKOV DEPENDENCY applied to VIEW PARAMETERS allows them to evolve during RECOGNITION as with a SMOOTHLY MOVING CAMERA . the effectiveness of the framework is demonstrated with experiments on REAL DATASETS and with challenging RECOGNITION SCENARIOS . \n",
            "this paper addresses the problem of 3D RECONSTRUCTION from a single image . we propose a novel method to learn a TEMPORAL MARKOV DEPENDENCY from a set of IMAGE PROJECTIONS . the proposed approach is based on the use of LATENT VARIABLES extracted from the IMAGE PROJECTIONS to the LATENT VARIABLES . the proposed approach is based on the use of LATENT VARIABLES extracted from the IMAGE PROJECTIONS . the proposed approach is based on the use of LATENT VARIABLES extracted from the IMAGE PROJECTIONS . the proposed approach is evaluated on REAL DATASETS , and the results show that the proposed method is effective in improving the RECOGNITION performance .\n",
            "\n",
            "317 1000\n",
            "a main issue in SOURCE SEPARATION is to deal with the INDE-TERMINACIES . well known are the ORDERING AND SCALE AMBIGUITIES , but other types of INDETERMINACIES may also occur . in this paper we address these INDETERMINACIES in the case of NON-NEGATIVE SOURCES and NON-NEGATIVE MIXING COEFFICIENTS . on the one hand , we fully develop the case of two sources . on the other hand , in the general case we formulate necessary conditions for the uniqueness of the solution -lrb- up to ORDERING AND SCALE AMBIGUITIES -rrb- . \n",
            "in this paper , we present a novel approach to SOURCE SEPARATION in SOURCE SEPARATION . the method is based on the use of a set of NON-NEGATIVE SOURCES , each of which is a combination of NON-NEGATIVE SOURCES and INDETERMINACIES . the method is based on INDETERMINACIES and NON-NEGATIVE MIXING COEFFICIENTS . the proposed method is evaluated on a variety of NON-NEGATIVE SOURCES . the results show that the proposed method outperforms the existing methods in terms of both ORDERING AND SCALE AMBIGUITIES and NON-NEGATIVE MIXING COEFFICIENTS .\n",
            "\n",
            "318 1000\n",
            "this paper proposes a COMPUTATIONAL SYSTEM OF OBJECT CATEGORIZATION based on decomposition and ADAPTIVE FUSION OF VISUAL INFORMATION . a coupled CONDITIONAL RANDOM FIELD is developed to model the interaction between LOW LEVEL CUES OF CONTOUR and texture , and to decompose contour and texture in NATURAL IMAGES . the advantages of using coupled rather than SINGLE-LAYER RANDOM FIELDS are demonstrated with MODEL LEARNING and evaluation . MULTIPLE DECOMPOSED VISUAL CUES are adaptively combined for OBJECT CATEGORIZATION to fully leverage different DISCRIMINA-TIVE CUES for different classes . experimental results show that the proposed COMPUTATIONAL SYSTEM OF OBJECT CATEGORIZATION of '' <unk> '' achieves better performance than most of the state-of-the-art methods , especially when only a limited number of training samples are available . \n",
            "this paper addresses the problem of COMPUTATIONAL SYSTEM OF OBJECT CATEGORIZATION for OBJECT CATEGORIZATION . we propose a novel approach to the COMPUTATIONAL SYSTEM OF OBJECT CATEGORIZATION , which is based on MULTIPLE DECOMPOSED VISUAL CUES . the proposed approach is based on a CONDITIONAL RANDOM FIELD , which is able to capture the LOW LEVEL CUES OF CONTOUR of the NATURAL IMAGES . the proposed approach is based on the use of a CONDITIONAL RANDOM FIELD , which is able to deal with the LOW LEVEL CUES OF CONTOUR . the proposed approach is evaluated on a variety of NATURAL IMAGES . the results show that the proposed approach is effective in improving the performance of OBJECT CATEGORIZATION in NATURAL IMAGES .\n",
            "\n",
            "319 1000\n",
            "cortical AMPLIFICATION has been proposed as a mechanism for enhancing the selectivity of neurons in the primary visual cortex . less <unk> is the fact that the same form of AMPLIFICATION can also be used to <unk> or broaden selectivity . using a NETWORK MODEL with RECURRENT CORTICAL CIRCUITRY , we propose that the spatial phase invariance of COMPLEX CELL RESPONSES arises through RECURRENT AMPLIFICATION OF FEEDFORWARD INPUT . neurons in the NETWORK MODEL respond like simple cells at low gain and complex cells at high gain . similar RECURRENT MECHANISMS may play a role in generating invariant representations of feedforward input elsewhere in the VISUAL PROCESSING PATHWAY . \n",
            "this paper presents a novel approach to RECURRENT AMPLIFICATION OF FEEDFORWARD INPUT based on RECURRENT AMPLIFICATION OF FEEDFORWARD INPUT . the proposed approach is based on a RECURRENT AMPLIFICATION OF FEEDFORWARD INPUT , which is based on a RECURRENT AMPLIFICATION OF FEEDFORWARD INPUT . the proposed method is based on a RECURRENT AMPLIFICATION OF FEEDFORWARD INPUT , which is based on a RECURRENT AMPLIFICATION OF FEEDFORWARD INPUT . the proposed approach is based on the use of a NETWORK MODEL . the proposed approach is evaluated on a variety of COMPLEX CELL RESPONSES . the results show that the proposed approach is able to detect COMPLEX CELL RESPONSES in a VISUAL PROCESSING PATHWAY .\n",
            "\n",
            "320 1000\n",
            "<unk> pure exploration in MULTI-ARMED BANDITS is a key component of MONTE-CARLO TREE SEARCH ALGORITHMS for SEQUENTIAL DECISION PROBLEMS . we introduce DISCRIMINATIVE BUCKETING , a novel family of strategies for pure exploration in MULTI-ARMED BANDITS , which allows for adapting recent advances in NON-INTERRUPTIBLE STRATEGIES to the INTERRUPTIBLE SETTING , while guaranteeing <unk> performance improvement over time . our experimental evaluation demonstrates that the corresponding instances of DISCRIMINATIVE BUCKETING favorably compete both with the currently popular strategies <unk> and Ε-GREEDY , as well as with the CONSERVATIVE UNIFORM SAMPLING . \n",
            "this paper addresses the problem of MULTI-ARMED BANDITS for SEQUENTIAL DECISION PROBLEMS . in particular , we consider the problem of MULTI-ARMED BANDITS for SEQUENTIAL DECISION PROBLEMS . in particular , we show that CONSERVATIVE UNIFORM SAMPLING can be interpreted as a special case of MULTI-ARMED BANDITS . we also show that CONSERVATIVE UNIFORM SAMPLING can be interpreted as a special case of the standard Ε-GREEDY . we also show that the Ε-GREEDY is equivalent to a CONSERVATIVE UNIFORM SAMPLING . we also show that the Ε-GREEDY can be extended to the case of SEQUENTIAL DECISION PROBLEMS .\n",
            "\n",
            "321 1000\n",
            "we study the problem of identifying individuals based on their CHARACTERISTIC GAZE PATTERNS during reading of arbitrary text . the motivation for this problem is an <unk> <unk> setting in which a user is observed during access to a document , but no specific challenge protocol requiring the user 's time and attention is carried out . existing models of individual differences in GAZE CONTROL during reading are either based on simple AGGREGATE FEATURES OF EYE MOVEMENTS , or rely on PARAMET-RIC DENSITY MODELS to describe , for instance , SACCADE AMPLITUDES or WORD FIXATION DURATIONS . we develop FLEXIBLE SEMIPARAMETRIC MODELS of eye movements during reading in which densities are inferred under a GAUSSIAN PROCESS prior centered at a PARAMETRIC DISTRIBUTION FAMILY that is expected to approximate the true distribution well . an empirical study on reading data from <unk> individuals shows significant improvements over the state of the art . \n",
            "this paper addresses the problem of GAZE CONTROL in the presence of WORD FIXATION DURATIONS and WORD FIXATION DURATIONS . we propose a novel approach to the problem of GAZE CONTROL . the proposed approach is based on the use of a PARAMETRIC DISTRIBUTION FAMILY , which is able to estimate the CHARACTERISTIC GAZE PATTERNS and the CHARACTERISTIC GAZE PATTERNS . the proposed approach is based on the use of a PARAMETRIC DISTRIBUTION FAMILY , which is able to deal with CHARACTERISTIC GAZE PATTERNS and WORD FIXATION DURATIONS . the proposed approach is evaluated on a variety of WORD FIXATION DURATIONS and WORD FIXATION DURATIONS .\n",
            "\n",
            "322 1000\n",
            "in this paper , we propose a BILINGUAL LEXICAL COHESION TRIGGER MODEL to capture LEXICAL COHESION for DOCUMENT-LEVEL MACHINE TRANSLATION . we integrate the BILINGUAL LEXICAL COHESION TRIGGER MODEL into HIERARCHICAL PHRASE-BASED MACHINE TRANSLATION and achieve an absolute improvement of 0.85 bleu points on average over the baseline on NIST CHINESE-ENGLISH TEST SETS . \n",
            "this paper presents a novel BILINGUAL LEXICAL COHESION TRIGGER MODEL for DOCUMENT-LEVEL MACHINE TRANSLATION . the proposed BILINGUAL LEXICAL COHESION TRIGGER MODEL is based on a BILINGUAL LEXICAL COHESION TRIGGER MODEL for DOCUMENT-LEVEL MACHINE TRANSLATION . the proposed BILINGUAL LEXICAL COHESION TRIGGER MODEL is applied to the task of DOCUMENT-LEVEL MACHINE TRANSLATION . the experimental results on the NIST CHINESE-ENGLISH TEST SETS show that the proposed BILINGUAL LEXICAL COHESION TRIGGER MODEL significantly improves the performance of HIERARCHICAL PHRASE-BASED MACHINE TRANSLATION .\n",
            "\n",
            "323 1000\n",
            "model-based methods for SEQUENTIAL ORGANIZATION in COCHANNEL SPEECH require PRETRAINED SPEAKER MODELS and often prior knowledge of participating speakers . we propose an UNSUPERVISED APPROACH to SEQUENTIAL ORGANIZATION OF COCHANNEL SPEECH . based on CEPSTRAL FEATURES , we first cluster voiced speech into two speaker groups by maximizing the ratio of <unk> <unk> distances penalized by WITHIN-GROUP CONCURRENT PITCHES . to group UNVOICED SPEECH , we employ an ONSET/OFFSET BASED ANALYSIS to generate TIME-FREQUENCY SEGMENTS . UNVOICED SEGMENTS are then labeled by the complementary portions of SEGREGATED VOICED SPEECH . our UNSUPERVISED APPROACH does not require any PRETRAINED MODEL and is computationally simple . evaluations and comparisons show that the proposed UNSUPERVISED APPROACH outperforms a MODEL-BASED METHOD in terms of SPEECH SEGREGATION . \n",
            "this paper presents a novel UNSUPERVISED APPROACH for SEQUENTIAL ORGANIZATION OF COCHANNEL SPEECH . the proposed UNSUPERVISED APPROACH is based on a PRETRAINED MODEL to the SEQUENTIAL ORGANIZATION OF COCHANNEL SPEECH . the proposed UNSUPERVISED APPROACH is based on the use of a PRETRAINED MODEL for UNVOICED SPEECH . the proposed UNSUPERVISED APPROACH is based on the use of a PRETRAINED MODEL for UNVOICED SPEECH . the proposed UNSUPERVISED APPROACH is evaluated on SPEECH SEGREGATION and compared with the conventional UNSUPERVISED APPROACH . the proposed UNSUPERVISED APPROACH is evaluated in terms of SPEECH SEGREGATION performance on a variety of COCHANNEL SPEECH . the experimental results show that the proposed UNSUPERVISED APPROACH significantly outperforms the conventional MODEL-BASED METHOD in terms of SPEECH SEGREGATION performance .\n",
            "\n",
            "324 1000\n",
            "context decision trees are widely used in the SPEECH RECOGNITION COMMUNITY . besides questions about <unk> classes of a phone 's context , questions about their position within a word <unk> -rsb- and questions about the gender of the current speaker <unk> -rsb- have been used so far . in this paper we additionally incorporate questions about current modalities of the SPOKEN UTTERANCE like the SPEAKER 'S DIALECT , the SPEAKING RATE , the signal to noise ratio , the latter two of which may change while speaking one utterance . we present a framework that treats all these modalities in a uniform way . experiments with the JANUS SPEECH RECOGNIZER have produced ERROR RATE REDUCTIONS of up to 10 % when compared to systems that do not use MODALITY QUESTIONS . \n",
            "in this paper , we propose a novel approach to MODALITY QUESTIONS based on CONTEXT DECISION TREES . the proposed approach is based on the CONTEXT DECISION TREES of the SPOKEN UTTERANCE . the proposed method is based on the use of CONTEXT DECISION TREES in the SPEECH RECOGNITION COMMUNITY . the proposed method is evaluated in a SPEECH RECOGNITION COMMUNITY using a SPEAKER 'S DIALECT . the experimental results show that the proposed JANUS SPEECH RECOGNIZER achieves higher ERROR RATE REDUCTIONS than the conventional JANUS SPEECH RECOGNIZER .\n",
            "\n",
            "325 1000\n",
            "we introduce a new CLASSIFICATION ALGORITHM based on the concept of SYMMETRIC MAXIMIZED MINIMAL DISTANCE in subspace -lrb- SMMS -rrb- . given the training data of authentic samples and IMPOSTER SAMPLES in the FEATURE SPACE , SMMS tries to identify a SUBSPACE in which all the authentic samples are close to each other and all the IMPOSTER SAMPLES are far away from the authentic samples . the OPTIMALITY of the SUBSPACE is determined by maximizing the minimal distance between the authentic samples and the IMPOSTER SAMPLES in the SUBSPACE . we present a procedure to achieve such OPTIMALITY and to identify the DECISION BOUNDARY . the verification procedure is simple since we only need to project the test sample to the SUBSPACE and compare it against the DECISION BOUNDARY . using FACE AUTHENTICATION as an example , we show that the proposed algorithm outperforms several other algorithms based on SUPPORT VECTOR MACHINES . \n",
            "in this paper , we propose a novel approach to FACE AUTHENTICATION in the context of FACE AUTHENTICATION . the proposed method is based on the use of a SYMMETRIC MAXIMIZED MINIMAL DISTANCE , which is based on the SYMMETRIC MAXIMIZED MINIMAL DISTANCE . the proposed method is based on the SYMMETRIC MAXIMIZED MINIMAL DISTANCE of the DECISION BOUNDARY of the SUBSPACE . the proposed method is based on the SYMMETRIC MAXIMIZED MINIMAL DISTANCE of the SUBSPACE of the source and target speaker . the proposed method is based on the use of the SYMMETRIC MAXIMIZED MINIMAL DISTANCE . the proposed method is applied to the problem of FACE AUTHENTICATION . the experimental results show that the proposed method outperforms the existing methods in terms of OPTIMALITY and OPTIMALITY .\n",
            "\n",
            "326 1000\n",
            "speech can be divided into DISCOURSE GENRES based on the CONTEXTUAL ENVIRONMENT it occurs in -lrb- e.g. POLITICAL SPEECH , SPORT COMMENTARY SPEECH , etc. -rrb- . the present study investigated whether listeners can distinguish between speech from different DISCOURSE GENRES on the basis of ACOUSTIC PROSODIC CUES only 1 . in a perception experiment with <unk> speech 70 listeners with varying experience in FRENCH -LRB- NATIVE SPEAKERS , NON-NATIVE SPEAKERS , and <unk> -rrb- were asked to identify four different types of DISCOURSE GENRES -lrb- <unk> service , political , journal , and SPORT COMMENTARY -rrb- . results revealed a fair IDENTIFICATION ABILITY with a significant increase in performance with increasing experience in FRENCH . IDENTIFICATION ABILITY was used to cluster DISCOURSE GENRES according to their PERCEPTUAL SIMILARITY . the possible application of the results for the evaluation of SPEAKING STYLE SPEECH SYNTHESIS will be discussed . \n",
            "in this paper , we present a novel approach to SPEAKING STYLE SPEECH SYNTHESIS in the presence of POLITICAL SPEECH . the proposed approach is based on the use of ACOUSTIC PROSODIC CUES and ACOUSTIC PROSODIC CUES . the proposed approach is based on the use of ACOUSTIC PROSODIC CUES and ACOUSTIC PROSODIC CUES . the proposed approach is based on the use of ACOUSTIC PROSODIC CUES and ACOUSTIC PROSODIC CUES . the proposed method is evaluated on a variety of POLITICAL SPEECH including POLITICAL SPEECH and SPORT COMMENTARY SPEECH . the results show that the proposed method is effective in improving the IDENTIFICATION ABILITY performance .\n",
            "\n",
            "327 1000\n",
            "this paper presents a VOICE CONVERSION TECHNIQUE using DEEP BELIEF NETS to build HIGH-ORDER EIGEN SPACES of the <unk> speakers , where VOICE CONVERSION TECHNIQUE is easier to convert the source speech to the target speech than in the traditional CEPSTRUM SPACE . DEEP BELIEF NETS have a DEEP ARCHITECTURE that automatically discovers ABSTRACTIONS to maximally express the original input features . if we train the DEEP BELIEF NETS using only the speech of an individual speaker , VOICE CONVERSION TECHNIQUE can be considered that there is less PHONOLOGI-CAL INFORMATION and relatively more SPEAKER INDIVIDUALITY in the output features at the highest layer . training the DEEP BELIEF NETS for a source speaker and a target speaker , we can then connect and convert the SPEAKER INDIVIDUALITY ABSTRACTIONS using NEURAL NETWORKS . the converted abstraction of the source speaker is then brought back to the CEPSTRUM SPACE using an INVERSE PROCESS of the DEEP BELIEF NETS of the target speaker . we conducted <unk> conversion experiments and confirmed the efficacy of our VOICE CONVERSION TECHNIQUE with respect to SUBJECTIVE AND OBJECTIVE CRITERIA , comparing VOICE CONVERSION TECHNIQUE with the conventional GAUSSIAN MIXTURE MODEL-BASED METHOD . \n",
            "this paper presents a novel VOICE CONVERSION TECHNIQUE based on DEEP BELIEF NETS . the proposed VOICE CONVERSION TECHNIQUE is based on a GAUSSIAN MIXTURE MODEL-BASED METHOD of the INVERSE PROCESS . the proposed VOICE CONVERSION TECHNIQUE is based on a GAUSSIAN MIXTURE MODEL-BASED METHOD of the INVERSE PROCESS . the proposed VOICE CONVERSION TECHNIQUE is based on the use of DEEP BELIEF NETS . the proposed VOICE CONVERSION TECHNIQUE is compared to a GAUSSIAN MIXTURE MODEL-BASED METHOD . the proposed VOICE CONVERSION TECHNIQUE is compared to a standard GAUSSIAN MIXTURE MODEL-BASED METHOD , and the results show that the proposed VOICE CONVERSION TECHNIQUE is more effective than the conventional GAUSSIAN MIXTURE MODEL-BASED METHOD .\n",
            "\n",
            "328 1000\n",
            "this paper proposes a PRIOR DISTRIBUTION DETERMINATION TECHNIQUE using CROSS VALIDATION for SPEECH RECOGNITION based on the BAYESIAN APPROACH . the DISTRIBUTION DETERMINATION TECHNIQUE is a STATISTICAL TECHNIQUE for estimating reliable PREDICTIVE DISTRIBUTIONS by marginalizing MODEL PARAMETERS and its APPROXIMATE VERSION , the VARIATIONAL BAYESIAN METHOD has been applied to HMM-BASED SPEECH RECOGNITION . since PRIOR DISTRIBUTIONS representing PRIOR INFORMATION about MODEL PARAMETERS affect the POSTERIOR DISTRIBUTIONS and MODEL SELECTION , the determination of PRIOR DISTRIBUTIONS is an important problem . however , it has not been thoroughly investigate in SPEECH RECOGNITION . the proposed DISTRIBUTION DETERMINATION TECHNIQUE can determine reliable PRIOR DISTRIBUTIONS without TUNING PARAMETERS and select an appropriate MODEL STRUCTURE <unk> on the amount of training data . continuous phoneme recognition experiments show that the proposed DISTRIBUTION DETERMINATION TECHNIQUE achieved a higher performance than the conventional methods . \n",
            "in this paper , we propose a novel BAYESIAN APPROACH for HMM-BASED SPEECH RECOGNITION and MODEL SELECTION . the proposed BAYESIAN APPROACH is based on a BAYESIAN APPROACH , which is a STATISTICAL TECHNIQUE for SPEECH RECOGNITION and MODEL SELECTION . the proposed PRIOR DISTRIBUTION DETERMINATION TECHNIQUE is based on a BAYESIAN APPROACH , which is based on a BAYESIAN APPROACH . the proposed DISTRIBUTION DETERMINATION TECHNIQUE is based on the idea of MODEL SELECTION , which is a PRIOR DISTRIBUTION DETERMINATION TECHNIQUE for SPEECH RECOGNITION and MODEL SELECTION . the proposed DISTRIBUTION DETERMINATION TECHNIQUE is based on the use of PRIOR DISTRIBUTIONS , which is a PRIOR DISTRIBUTION DETERMINATION TECHNIQUE for SPEECH RECOGNITION and MODEL SELECTION . the experimental results show that the proposed BAYESIAN APPROACH is effective in improving SPEECH RECOGNITION and MODEL SELECTION .\n",
            "\n",
            "329 1000\n",
            "most HMM-BASED SPEECH RECOGNITION SYSTEMS use GAUSSIAN MIXTURES as OBSERVATION PROBABILITY DENSITY FUNCTIONS . an important goal in all such HMM-BASED SPEECH RECOGNITION SYSTEMS is to improve PARSIMONY . one method is to adjust the type of COVARIANCE MATRICES used . in this work , FAC-TORED SPARSE INVERSE COVARIANCE MATRICES are introduced . based on Í 1/4 Í FACTORIZATION , the INVERSE COVARIANCE MATRIX can be represented using LINEAR REGRESSIVE COEFFICIENTS which 1 -rrb- correspond to SPARSE PATTERNS in the INVERSE COVARIANCE MATRIX -lrb- and therefore represent CONDITIONAL INDEPENDENCE PROPERTIES of the gaussian -rrb- , and 2 -rrb- , result in a method of partial tying of the COVARIANCE MATRICES without requiring NON-LINEAR EM UPDATE EQUATIONS . results show that the performance of FULL-COVARIANCE GAUSSIANS can be matched by FACTORED SPARSE INVERSE COVARIANCE GAUSSIANS having significantly fewer parameters . \n",
            "in this paper , we propose a novel method for HMM-BASED SPEECH RECOGNITION SYSTEMS based on FACTORED SPARSE INVERSE COVARIANCE GAUSSIANS . the proposed approach is based on the use of FACTORED SPARSE INVERSE COVARIANCE GAUSSIANS to estimate the INVERSE COVARIANCE MATRIX of the INVERSE COVARIANCE MATRIX . the proposed method is based on the use of FACTORED SPARSE INVERSE COVARIANCE GAUSSIANS to estimate the INVERSE COVARIANCE MATRIX . the proposed method is based on the use of FAC-TORED SPARSE INVERSE COVARIANCE MATRICES derived from the INVERSE COVARIANCE MATRIX of the INVERSE COVARIANCE MATRIX . the proposed method is applied to HMM-BASED SPEECH RECOGNITION SYSTEMS , and the results show that the proposed method is effective in improving the CONDITIONAL INDEPENDENCE PROPERTIES of the HMM-BASED SPEECH RECOGNITION SYSTEMS . the proposed method is applied to HMM-BASED SPEECH RECOGNITION SYSTEMS , and the results show that the proposed method can be applied to HMM-BASED SPEECH RECOGNITION SYSTEMS .\n",
            "\n",
            "330 1000\n",
            "this paper addresses the problem of WALL CLUTTER MITIGATION in COMPRESSED SENSING THROUGH-THE-WALL RADAR IMAGING , where a different set of frequencies is sensed at different antenna locations . a JOINT BAYESIAN SPARSE APPROXIMATION FRAMEWORK is first employed to reconstruct all the signals simultaneously by exploiting SIGNAL SPARSITY and correlations between antenna signals . this is in contrast to previous approaches where the signal at each ANTENNA LOCATION is reconstructed independently . furthermore , to promote SPARSITY and improve RECONSTRUCTION ACCURACY , a SPARSIFYING WAVELET DICTIONARY is employed in the SPARSE SIGNAL RECOVERY . following WALL CLUTTER MITIGATION , a SUBSPACE PROJECTION TECHNIQUE is applied to remove WALL CLUTTER , prior to IMAGE FORMATION . experimental results on real data show that the proposed approach produces significantly higher RECONSTRUCTION ACCURACY and requires far fewer measurements for forming HIGH-QUALITY IMAGES , compared to the SINGLE-SIGNAL COMPRESSED SENSING MODEL , where each ANTENNA SIGNAL is reconstructed independently . \n",
            "this paper proposes a novel SINGLE-SIGNAL COMPRESSED SENSING MODEL for COMPRESSED SENSING THROUGH-THE-WALL RADAR IMAGING in COMPRESSED SENSING THROUGH-THE-WALL RADAR IMAGING . the proposed SUBSPACE PROJECTION TECHNIQUE is based on a JOINT BAYESIAN SPARSE APPROXIMATION FRAMEWORK for SPARSE SIGNAL RECOVERY . the proposed SINGLE-SIGNAL COMPRESSED SENSING MODEL is based on a JOINT BAYESIAN SPARSE APPROXIMATION FRAMEWORK for SPARSE SIGNAL RECOVERY . the proposed JOINT BAYESIAN SPARSE APPROXIMATION FRAMEWORK is based on a JOINT BAYESIAN SPARSE APPROXIMATION FRAMEWORK for SPARSE SIGNAL RECOVERY . the proposed JOINT BAYESIAN SPARSE APPROXIMATION FRAMEWORK is applied to the IMAGE FORMATION and the RECONSTRUCTION ACCURACY of the SINGLE-SIGNAL COMPRESSED SENSING MODEL is analyzed . the proposed method is evaluated in terms of RECONSTRUCTION ACCURACY and RECONSTRUCTION ACCURACY . the results show that the proposed method is effective in reducing the RECONSTRUCTION ACCURACY and RECONSTRUCTION ACCURACY of the SINGLE-SIGNAL COMPRESSED SENSING MODEL .\n",
            "\n",
            "331 1000\n",
            "we have recently proposed a new PLDA-BASED ACOUSTIC MODEL based on PROB-ABILISTIC LINEAR DISCRIMINANT ANALYSIS which enjoys the flexibility of using higher DIMENSIONAL ACOUSTIC FEATURES , and is more capable to capture the INTRA-FRAME FEATURE CORRELATIONS . in this paper , we investigate the use of BOTTLENECK FEATURES obtained from a DEEP NEURAL NETWORK for the PLDA-BASED ACOUSTIC MODEL . experiments were performed on the SWITCHBOARD DATASET -- a LARGE VOCABULARY CONVERSATIONAL TELEPHONE SPEECH CORPUS . we observe significant WORD ERROR REDUCTION by using the BOTTLENECK FEATURES . in addition , we have also compared the PLDA-BASED ACOUSTIC MODEL to three others using GAUSSIAN MIXTURE MODELS , SUBSPACE GMMS and HYBRID DEEP NEURAL NETWORKS , and PLDA can achieve comparable or slightly higher RECOGNITION ACCURACY from our experiments . \n",
            "this paper proposes a novel PLDA-BASED ACOUSTIC MODEL based on HYBRID DEEP NEURAL NETWORKS and HYBRID DEEP NEURAL NETWORKS . the proposed PLDA-BASED ACOUSTIC MODEL is based on a DEEP NEURAL NETWORK , which is based on a DEEP NEURAL NETWORK . the proposed PLDA-BASED ACOUSTIC MODEL is based on a DEEP NEURAL NETWORK , which is based on a DEEP NEURAL NETWORK . the proposed PLDA-BASED ACOUSTIC MODEL is applied to the PLDA-BASED ACOUSTIC MODEL and PLDA . the experimental results on the SWITCHBOARD DATASET show that the proposed PLDA-BASED ACOUSTIC MODEL is effective in improving RECOGNITION ACCURACY and RECOGNITION ACCURACY .\n",
            "\n",
            "332 1000\n",
            "we address an ANOMALY DETECTION SETTING in which TRAINING SEQUENCES are unavailable and ANOMALIES are scored independently of TEMPORAL ORDERING . current algorithms in ANOMALY DETECTION SETTING are based on the CLASSICAL DENSITY ESTIMATION APPROACH of learning HIGH-DIMENSIONAL MODELS and finding LOW-PROBABILITY EVENTS . these algorithms are sensitive to the order in which ANOMALIES appear and require either TRAINING DATA or EARLY CONTEXT ASSUMPTIONS that do not hold for longer , more COMPLEX VIDEOS . by defining ANOMALIES as examples that can be distinguished from other examples in the same video , our definition inspires a shift in approaches from CLASSICAL DENSITY ESTIMATION to simple DISCRIMINATIVE LEARNING . our contributions include a novel framework for ANOMALY DETECTION SETTING that is -lrb- 1 -rrb- independent of TEMPORAL ORDERING OF ANOMALIES , and -lrb- 2 -rrb- unsupervised , requiring no separate TRAINING SEQUENCES . we show that our algorithm can achieve state-of-the-art results even when we adjust the setting by removing TRAINING SEQUENCES from standard datasets . \n",
            "this paper addresses the problem of DISCRIMINATIVE LEARNING in COMPLEX VIDEOS . we propose a novel approach to DISCRIMINATIVE LEARNING based on CLASSICAL DENSITY ESTIMATION and EARLY CONTEXT ASSUMPTIONS . the proposed approach is based on a CLASSICAL DENSITY ESTIMATION APPROACH , which is based on CLASSICAL DENSITY ESTIMATION . the proposed method is based on a CLASSICAL DENSITY ESTIMATION APPROACH , which is based on CLASSICAL DENSITY ESTIMATION and EARLY CONTEXT ASSUMPTIONS . the proposed method is based on a CLASSICAL DENSITY ESTIMATION APPROACH , which is able to deal with COMPLEX VIDEOS in the presence of ANOMALIES and EARLY CONTEXT ASSUMPTIONS . experimental results demonstrate the effectiveness of the proposed approach .\n",
            "\n",
            "333 1000\n",
            "we address the problem of answering new questions in COMMUNITY FORUMS , by selecting suitable answers to already asked questions . we approach the task as an ANSWER RANKING PROBLEM , adopting a PAIRWISE NEURAL NETWORK ARCHITECTURE that selects which of two competing answers is better . we focus on the utility of the three types of similarities occurring in the triangle formed by the original question , the related question , and an answer to the related comment , which we call relevance , RELATEDNESS , and appropriateness . our proposed PAIRWISE NEURAL NETWORK ARCHITECTURE models the interactions among all INPUT COMPONENTS using SYNTACTIC AND SEMANTIC EMBEDDINGS , LEXICAL MATCHING , and DOMAIN-SPECIFIC FEATURES . it achieves state-of-the-art results , showing that the three similarities are important and need to be mod-eled together . our experiments demonstrate that all FEATURE TYPES are relevant , but the most important ones are the LEXICAL SIMILARITY FEATURES , the DOMAIN-SPECIFIC FEATURES , and the SYNTACTIC AND SEMANTIC EMBEDDINGS . \n",
            "this paper presents a novel PAIRWISE NEURAL NETWORK ARCHITECTURE based on LEXICAL MATCHING , LEXICAL MATCHING , and LEXICAL SIMILARITY FEATURES for ANSWER RANKING PROBLEM . the proposed PAIRWISE NEURAL NETWORK ARCHITECTURE is based on the use of LEXICAL SIMILARITY FEATURES , LEXICAL SIMILARITY FEATURES , LEXICAL SIMILARITY FEATURES , and LEXICAL SIMILARITY FEATURES . the proposed PAIRWISE NEURAL NETWORK ARCHITECTURE is based on the use of LEXICAL SIMILARITY FEATURES , LEXICAL SIMILARITY FEATURES , LEXICAL SIMILARITY FEATURES , and LEXICAL SIMILARITY FEATURES . the proposed PAIRWISE NEURAL NETWORK ARCHITECTURE is applied to the problem of ANSWER RANKING PROBLEM from COMMUNITY FORUMS .\n",
            "\n",
            "334 1000\n",
            "language modeling for AUTOMATIC SPEECH RECOGNITION SYSTEMS has been traditionally in the VERBAL DOMAIN . in this paper , we present FINITE-STATE MODELING TECHNIQUES that we developed for LANGUAGE MODELING in the WRITTEN DOMAIN . the first FINITE-STATE MODELING TECHNIQUES we describe is for the VERBALIZATION OF WRITTEN-DOMAIN VOCABULARY ITEMS , which include LEXICAL AND NON-LEXICAL ENTITIES . the second FINITE-STATE MODELING TECHNIQUES is the DECOMPOSITION -- RECOMPOSITION APPROACH to address the OUT-OF-VOCABULARY and the DATA SPARSITY PROBLEMS with NON-LEXICAL ENTITIES such as URLS , E-MAIL ADDRESSES , PHONE NUMBERS , and DOLLAR AMOUNTS . we evaluate the proposed WRITTEN-DOMAIN LANGUAGE MODELING APPROACHES on a very large vocabulary SPEECH RECOGNITION system for EN-GLISH . we show that the WRITTEN-DOMAIN LANGUAGE MODELING APPROACHES improves the SPEECH RECOGNITION and the ASR TRANSCRIPT RENDERING ACCURACY in the WRITTEN DOMAIN over a baseline system using a VERBAL-DOMAIN LANGUAGE MODEL . in addition , the WRITTEN-DOMAIN LANGUAGE MODELING APPROACHES is much simpler since WRITTEN-DOMAIN LANGUAGE MODELING APPROACHES does not require complex and error-prone text normalization and DENORMALIZATION RULES , which are generally required for VERBAL-DOMAIN LANGUAGE MODELING . \n",
            "in this paper , we propose a novel DECOMPOSITION -- RECOMPOSITION APPROACH for AUTOMATIC SPEECH RECOGNITION SYSTEMS in WRITTEN DOMAIN . the proposed WRITTEN-DOMAIN LANGUAGE MODELING APPROACHES is based on a novel DECOMPOSITION -- RECOMPOSITION APPROACH , which is able to capture LEXICAL AND NON-LEXICAL ENTITIES such as URLS , PHONE NUMBERS , PHONE NUMBERS , and PHONE NUMBERS . the proposed DECOMPOSITION -- RECOMPOSITION APPROACH is based on a DECOMPOSITION -- RECOMPOSITION APPROACH that combines FINITE-STATE MODELING TECHNIQUES with FINITE-STATE MODELING TECHNIQUES . the proposed DECOMPOSITION -- RECOMPOSITION APPROACH is based on a DECOMPOSITION -- RECOMPOSITION APPROACH , which is able to deal with NON-LEXICAL ENTITIES such as URLS , PHONE NUMBERS , PHONE NUMBERS , and PHONE NUMBERS . experimental results demonstrate the effectiveness of the proposed DECOMPOSITION -- RECOMPOSITION APPROACH in comparison to the state-of-the-art methods .\n",
            "\n",
            "335 1000\n",
            "this paper gives a practical and accurate algorithm for the computation of the QUADRIFOCAL TENSOR and EXTRACTION OF CAMERA MATRICES from it . previous methods for using the QUADRIFOCAL TENSOR in PROJECTIVE SCENE RECONSTRUCTION have not emphasized ACCURACY of the algorithm in conditions of noise . methods given in this paper minimize ALGEBRAIC ERROR either through a NON-ITERATIVE LINEAR ALGORITHM , or two alternative ITERATIVE ALGORITHMS . it is shown by experiments with SYNTHETIC DATA that the ITERATIVE METHODS , though minimizing ALGEBRAIC , rather than more correctly geometric error measured in the image , give almost optimal results . \n",
            "this paper addresses the problem of PROJECTIVE SCENE RECONSTRUCTION in the presence of PROJECTIVE SCENE RECONSTRUCTION . in particular , we consider the problem of PROJECTIVE SCENE RECONSTRUCTION in the presence of PROJECTIVE SCENE RECONSTRUCTION . in particular , we consider the problem of PROJECTIVE SCENE RECONSTRUCTION in the presence of ALGEBRAIC . in particular , we propose a novel NON-ITERATIVE LINEAR ALGORITHM to the problem of PROJECTIVE SCENE RECONSTRUCTION . the proposed NON-ITERATIVE LINEAR ALGORITHM is based on the NON-ITERATIVE LINEAR ALGORITHM and the ITERATIVE METHODS . the proposed NON-ITERATIVE LINEAR ALGORITHM is evaluated on both SYNTHETIC DATA and SYNTHETIC DATA .\n",
            "\n",
            "336 1000\n",
            "in this paper , we propose CO-PRIME ARRAYS for effective DIRECTION-OF-ARRIVAL ESTIMATION . to fully utilize the VIRTUAL APERTURE achieved in the difference <unk> constructed from a CO-PRIME ARRAY STRUCTURE , SPARSITY-BASED SPATIAL SPECTRUM ESTIMATION TECHNIQUE is exploited . compared to existing techniques , the proposed technique achieves better utilization of the CO-ARRAY APERTURE and thus results in increased DEGREES-OF-FREEDOM as well as improved DOA ESTIMATION PERFORMANCE . \n",
            "in this paper , we present a novel approach to DIRECTION-OF-ARRIVAL ESTIMATION based on CO-PRIME ARRAYS . the proposed approach is based on the use of CO-PRIME ARRAYS to estimate the VIRTUAL APERTURE of the VIRTUAL APERTURE . the proposed method is based on the use of CO-PRIME ARRAYS . the proposed method is based on the use of CO-PRIME ARRAYS to estimate the VIRTUAL APERTURE . the proposed method is evaluated in terms of DOA ESTIMATION PERFORMANCE and DOA ESTIMATION PERFORMANCE . the experimental results show that the proposed method is effective in improving the DOA ESTIMATION PERFORMANCE of the DIRECTION-OF-ARRIVAL ESTIMATION .\n",
            "\n",
            "337 1000\n",
            "naive bayes -lrb- nb -rrb- is well-known to be a simple but effective CLASSIFIER , especially when combined with FEATURE SELECTION . unfortunately , FEATURE SELECTION METHODS are often greedy and thus can not guarantee an optimal feature set is selected . an alternative to FEATURE SELECTION is to use BAYESIAN MODEL averaging -lrb- BMA -rrb- , which computes a weighted average over multiple predictors ; when the different PREDICTOR MODELS correspond to different feature sets , BMA has the advantage over FEATURE SELECTION that its predictions tend to have lower variance on average in comparison to any single model . in this paper , we show for the first time that BMA-NB CLASSIFIER is possible to exactly evaluate BMA over the <unk> <unk> of NB FEATURE MODELS in linear-time in the number of FEATURES ; this yields an algorithm about as expensive to train as a single NB MODEL with all FEATURES , but yet provably converges to the GLOBALLY OPTIMAL FEATURE SUBSET in the ASYMPTOTIC LIMIT OF DATA . we evaluate this novel BMA-NB CLASSIFIER on a range of datasets showing that BMA-NB CLASSIFIER never <unk> nb -lrb- as expected -rrb- and sometimes offers performance competitive -lrb- or superior -rrb- to CLASSIFIERS such as SVMS and logistic regression while taking a fraction of the time to train . \n",
            "in this paper , we propose a novel BAYESIAN MODEL for FEATURE SELECTION . the proposed BAYESIAN MODEL is based on the idea that the BMA of the BMA is a GLOBALLY OPTIMAL FEATURE SUBSET of the FEATURES . the proposed BAYESIAN MODEL is based on the use of the NB MODEL , which is a BAYESIAN MODEL of the NB MODEL . the proposed BAYESIAN MODEL is based on the use of the NB MODEL , which is a generalization of the NB MODEL . the proposed NB MODEL is evaluated on a number of FEATURES , including SVMS , and SVMS . the experimental results show that the proposed NB MODEL is more effective than the conventional FEATURE SELECTION and the other FEATURE SELECTION METHODS .\n",
            "\n",
            "338 1000\n",
            "voice morphing is a technique for modifying a source speaker 's speech to sound as if VOICE MORPHING was spoken by some designated target speaker . most of the recent approaches to VOICE MORPHING apply a LINEAR TRANSFORMATION to the SPECTRAL ENVELOPE and PITCH SCALING to modify the PROSODY . whilst these methods are effective , they also introduce artifacts arising from the effects of GLOTTAL COUPLING , PHASE INCOHERENCE , UNNATURAL PHASE DISPERSION and the HIGH SPECTRAL VARIANCE OF UNVOICED SOUNDS . a practical VOICE MORPHING SYSTEM must account for these if high AUDIO QUALITY is to be preserved . this paper describes a complete VOICE MORPHING SYSTEM and the enhancements needed for dealing with the various artifacts , including a novel method for synthesising NATURAL PHASE DISPERSION . each technique is assessed individually and the overall performance of the VOICE MORPHING SYSTEM evaluated using LISTENING TESTS . overall VOICE MORPHING is found that the enhancements significantly improve SPEAKER IDENTIFICATION SCORES and PERCEIVED AUDIO QUALITY . \n",
            "in this paper , we present a novel approach to VOICE MORPHING . the proposed approach is based on the use of a LINEAR TRANSFORMATION and a LINEAR TRANSFORMATION to estimate the SPECTRAL ENVELOPE and PHASE INCOHERENCE . the proposed method is based on a LINEAR TRANSFORMATION , which is based on a LINEAR TRANSFORMATION . the proposed method is based on the use of LINEAR TRANSFORMATION and PHASE INCOHERENCE . the proposed method is evaluated in terms of AUDIO QUALITY and PERCEIVED AUDIO QUALITY . the experimental results show that the proposed method is effective in improving the PERCEIVED AUDIO QUALITY and PERCEIVED AUDIO QUALITY .\n",
            "\n",
            "339 1000\n",
            "occlusions provide critical cues about the 3D STRUCTURE OF MAN-MADE AND NATURAL SCENES . we present a MATHEMATICAL FRAMEWORK and algorithm to detect and localize OCCLUSIONS in image sequences of scenes that include DEFORMING OBJECTS . our OCCLUSION DETECTOR works under far weaker assumptions than other detectors . we prove that OCCLU-SIONS in DEFORMING SCENES occur when certain WELL-DEFINED LOCAL TOPOLOGI-CAL INVARIANTS are not preserved . our framework employs these invariants to detect OCCLUSIONS with a ZERO FALSE POSITIVE RATE under assumptions of BOUNDED DEFORMATIONS and COLOR VARIATION . the novelty and strength of this methodology is that it does not rely on SPATIO-TEMPORAL DERIVATIVES or MATCHING , which can be problematic in scenes including DEFORMING OBJECTS , but is instead based on a MATHEMATICAL REPRESENTATION of the underlying cause of OCCLUSIONS in a DEFORMING 3D SCENE . we demonstrate the effectiveness of the OCCLUSION DETECTOR using IMAGE SEQUENCES OF NATURAL SCENES , including DEFORMING CLOTH AND HAND MOTIONS . \n",
            "this paper presents a novel approach to 3D STRUCTURE OF MAN-MADE AND NATURAL SCENES from IMAGE SEQUENCES OF NATURAL SCENES . the proposed method is based on a MATHEMATICAL FRAMEWORK that exploits the 3D STRUCTURE OF MAN-MADE AND NATURAL SCENES and MATCHING . the proposed method consists of two steps : -lrb- 1 -rrb- a WELL-DEFINED LOCAL TOPOLOGI-CAL INVARIANTS , and -lrb- 2 -rrb- a MATHEMATICAL REPRESENTATION to estimate the ZERO FALSE POSITIVE RATE of the DEFORMING 3D SCENE . the proposed method is based on a MATHEMATICAL FRAMEWORK that exploits the 3D STRUCTURE OF MAN-MADE AND NATURAL SCENES and MATCHING to estimate the SPATIO-TEMPORAL DERIVATIVES . the proposed method is evaluated on a variety of IMAGE SEQUENCES OF NATURAL SCENES , and the results show that the proposed method is robust to OCCLUSIONS and OCCLUSIONS .\n",
            "\n",
            "340 1000\n",
            "in our paper , we divide the corpus into 8 domains through TEXT CLASSIFICATION using K-MEANS ALGORITHM , and calculate the TRIGRAM LMS for each one . but the experiment shows the performance in some ones becomes worse . in order to solve this problem , we try to do the LM ADAPTATION based on the DOMAIN LMS . the adaptation is done by mixing the DOMAIN LMS with the BACKGROUND LM by a LINEAR INTERPOLATION . RELATIVE WORD ERROR RATE REDUCTIONS of between 5 and 10 % over the PRUNED BACKGROUND LM are achieved . \n",
            "in this paper , we propose a novel approach to TEXT CLASSIFICATION based on DOMAIN LMS and BACKGROUND LM . the proposed method is based on the use of DOMAIN LMS and BACKGROUND LM . the proposed method is based on the use of DOMAIN LMS and BACKGROUND LM . the experimental results show that the proposed PRUNED BACKGROUND LM is effective in improving the RELATIVE WORD ERROR RATE REDUCTIONS of the PRUNED BACKGROUND LM .\n",
            "\n",
            "341 1000\n",
            "we consider the problem of LEARNING CONTROL POLICIES via TRAJECTORY PREFERENCE QUERIES to an expert . in particular , the LEARNING CONTROL POLICIES presents an expert with short runs of a pair of POLICIES originating from the same state and the expert indicates which trajectory is preferred . the LEARNING CONTROL POLICIES 's goal is to elicit a LATENT TARGET POLICY from the expert with as few queries as possible . to tackle this problem we propose a novel BAYESIAN MODEL of the QUERYING PROCESS and introduce two methods that exploit this BAYESIAN MODEL to actively select expert queries . experimental results on four benchmark problems indicate that our BAYESIAN MODEL can effectively learn POLICIES from TRAJECTORY PREFERENCE QUERIES and that ACTIVE QUERY SELECTION can be substantially more efficient than RANDOM SELECTION . \n",
            "in this paper , we propose a novel approach to ACTIVE QUERY SELECTION based on TRAJECTORY PREFERENCE QUERIES . the proposed BAYESIAN MODEL is based on the use of TRAJECTORY PREFERENCE QUERIES , which is a generalization of the standard BAYESIAN MODEL . the proposed BAYESIAN MODEL is based on the use of TRAJECTORY PREFERENCE QUERIES , which is a generalization of the BAYESIAN MODEL . the experimental results show that the proposed ACTIVE QUERY SELECTION is more effective than the conventional RANDOM SELECTION . moreover , the proposed BAYESIAN MODEL is much more efficient than RANDOM SELECTION .\n",
            "\n",
            "342 1000\n",
            "we derive a recursive <unk> pruned <unk> fast fourier transform -lrb- FFT -rrb- algorithm in KRONECKER PRODUCT NOTATION . the algorithm is compatible with VECTORIZATION and parallelization required on state-of-the-art MULTICORE CPUS . we include the PRUNED FFT ALGORITHM into the PROGRAM GENERATION SYSTEM SPIRAL , and automatically generate optimized implementations of the pruned FFT for the INTEL CORE2DUO MULTICORE PROCESSOR . experimental results show that using the pruned FFT can indeed speed up the fastest available FFT IMPLEMENTATIONS by up to 30 % when the PROBLEM SIZE and the pattern of UNUSED INPUTS and outputs are known in advance . \n",
            "in this paper , we propose a novel PRUNED FFT ALGORITHM for PROGRAM GENERATION SYSTEM SPIRAL . the proposed PRUNED FFT ALGORITHM is based on a PRUNED FFT ALGORITHM , which is based on a KRONECKER PRODUCT NOTATION . the proposed PRUNED FFT ALGORITHM is based on a KRONECKER PRODUCT NOTATION . the proposed PRUNED FFT ALGORITHM is based on a PRUNED FFT ALGORITHM , which is based on the KRONECKER PRODUCT NOTATION . the proposed PRUNED FFT ALGORITHM is applied to the INTEL CORE2DUO MULTICORE PROCESSOR and the experimental results show that the proposed PRUNED FFT ALGORITHM is effective in improving the PROBLEM SIZE of the PROGRAM GENERATION SYSTEM SPIRAL .\n",
            "\n",
            "343 1000\n",
            "we investigate the utility of SUPERTAG INFORMATION for guiding an existing DEPENDENCY PARSER OF GERMAN . using WEIGHTED CONSTRAINTS to integrate the additionally available information , the DECISION PROCESS of the DEPENDENCY PARSER OF GERMAN is influenced by changing its preferences , without excluding alternative structural interpretations from being considered . the paper reports on a series of experiments using varying MODELS OF SU-PERTAGS that significantly increase the PARSING ACCURACY . in addition , an upper bound on the ACCURACY that can be achieved with perfect SUPERTAGS is estimated . \n",
            "in this paper , we propose a novel approach to DEPENDENCY PARSER OF GERMAN based on the MODELS OF SU-PERTAGS . the proposed approach is based on the use of WEIGHTED CONSTRAINTS in the DECISION PROCESS . the proposed method is based on the use of WEIGHTED CONSTRAINTS , which is based on a DECISION PROCESS . the proposed method is evaluated in terms of ACCURACY and ACCURACY . the experimental results show that the proposed method can reduce the ACCURACY by up to 50 % .\n",
            "\n",
            "344 1000\n",
            "we investigate the incorporation of context into the spoken language understanding -lrb- slu -rrb- sub-tasks of INTENT PREDICTION and SLOT DETECTION . using a corpus that contains information about whole sessions rather than just single utterances , we experiment with the incorporation of information from previous INTRA-SESSION UTTERANCES into the SLU TASKS on a given utterance . for SLOT DETECTION , we find no significant increase using CRF FEATURES indicating slots in previous utterances . for INTENT PREDICTION , we achieve ERROR RATE REDUCTIONS of upto 8.7 % by incorporating the intent of the previous utterance as an SVM FEATURE , and similar gains when treating INTENT PREDICTION as a SEQUENTIAL TAGGING PROBLEM with SVM-HMMS . u 1 get clip show me the -lsb- <unk> -rsb- content − name -lsb- <unk> -rsb- type show me the -lsb- <unk> -rsb- content − name -lsb- <unk> -rsb- type u 2 find <unk> who directed -lsb- it -rsb- content − name − <unk> who directed -lsb- it -rsb- content − name − <unk> u 3 find content what else has -lsb- he -rsb- director − <unk> done what else has -lsb- he -rsb- director − <unk> done u 4 play content play -lsb- the <unk> -rsb- content − name plane -lsb- <unk> -rsb- content − name traditionally , both intents and -lsb- slots -rsb- are PREDICTED PER-UTTERANCE , while ignoring previous utterances within the session . however , the data is gathered not one utterance at a time but one session at a time ; each utterance occurs in the context of a larger DISCOURSE . we examine the effect of incorporating information from previous INTRA-SESSION UTTERANCES -lrb- <unk> <unk> , context -rrb- . CONTEXT can serve as an additional source of information and help get around other errors such as those introduced during the ASR PROCESS . \n",
            "this paper addresses the problem of SLOT DETECTION and SLOT DETECTION . we propose a novel approach to the problem of INTENT PREDICTION and SLOT DETECTION . the proposed approach is based on a SEQUENTIAL TAGGING PROBLEM that is based on a SVM-HMMS . the proposed approach is based on the use of CRF FEATURES , which is a SEQUENTIAL TAGGING PROBLEM . the proposed approach is based on the use of CRF FEATURES , which is a SEQUENTIAL TAGGING PROBLEM . the proposed approach is evaluated on a variety of SLU TASKS and SLOT DETECTION . the experimental results show that the proposed method achieves better performance than the state-of-the-art methods .\n",
            "\n",
            "345 1000\n",
            "we present a new approach to INTRINSIC SUMMARY EVALUATION , based on initial experiments in van <unk> and <unk> -lrb- 2003 -rrb- , which combines two novel aspects : comparison of information content -lrb- rather than STRING SIMILARITY -rrb- in gold standard and SYSTEM SUMMARY , measured in SHARED ATOMIC INFORMATION UNITS which we call <unk> , and comparison to more than one gold standard summary -lrb- in our data : 20 and 50 summaries respectively -rrb- . in this paper , we show that FACTOID ANNOTATION is highly reproducible , introduce a WEIGHTED FACTOID SCORE , estimate how many summaries are required for STABLE SYSTEM RANKINGS , and show that the FAC-TOID SCORES can not be sufficiently approximated by UNIGRAMS and the DUC INFORMATION OVERLAP MEASURE . \n",
            "this paper addresses the problem of FACTOID ANNOTATION in the presence of UNIGRAMS . we propose a method to estimate the STABLE SYSTEM RANKINGS of a set of SHARED ATOMIC INFORMATION UNITS . the proposed approach is based on the use of a WEIGHTED FACTOID SCORE and a WEIGHTED FACTOID SCORE . the proposed method is based on the use of a WEIGHTED FACTOID SCORE and a WEIGHTED FACTOID SCORE . the proposed method is based on a WEIGHTED FACTOID SCORE and a WEIGHTED FACTOID SCORE . experimental results show that the proposed method outperforms the state-of-the-art methods .\n",
            "\n",
            "346 1000\n",
            "this paper describes an empirical study of the '' INFORMATION SYNTHESIS '' TASK , defined as the process of -lrb- given a complex information need -rrb- extracting , organizing and <unk> the pieces of information contained in a set of relevant documents , in order to obtain a comprehensive , non redundant report that satisfies the information need . two main results are presented : a -rrb- the creation of an INFORMATION SYNTHESIS TESTBED with 72 reports manually generated by nine subjects for eight complex topics with 100 relevant documents each ; and b -rrb- an empirical comparison of SIMILARITY METRICS between reports , under the hypothesis that the best metric is the one that best distinguishes between manual and automatically generated reports . a metric based on key concepts overlap gives better results than metrics based on N-GRAM OVERLAP -lrb- such as rouge -rrb- or SENTENCE OVERLAP . \n",
            "in this paper , we propose a novel approach to the problem of SENTENCE OVERLAP in the context of INFORMATION SYNTHESIS TESTBED . the proposed approach is based on the use of a SENTENCE OVERLAP , such as N-GRAM OVERLAP , and N-GRAM OVERLAP . the proposed approach is based on the use of a set of SIMILARITY METRICS . the proposed method is evaluated on a INFORMATION SYNTHESIS '' TASK . the experimental results show that the proposed method is effective in improving the performance of the proposed method .\n",
            "\n",
            "347 1000\n",
            "we introduce a new approach for recognizing and reconstructing 3d objects in images . our approach is based on an analysis by SYNTHESIS STRATEGY . a FORWARD SYNTHESIS MODEL constructs possible GEOMETRIC INTERPRETATIONS OF THE WORLD , and then selects the interpretation that best agrees with the MEASURED VISUAL EVIDENCE . the FORWARD SYNTHESIS MODEL synthesizes VISUAL TEMPLATES defined on INVARIANT FEATURES . these VISUAL TEMPLATES are discriminatively trained to be accurate for INVERSE ESTIMATION . we introduce an efficient '' BRUTE-FORCE '' APPROACH to INFERENCE that searches through a large number of CANDIDATE RECONSTRUCTIONS , returning the optimal one . one benefit of such an approach is that RECOGNITION is inherently -lrb- re -rrb- constructive . we show state of the art performance for DETECTION and reconstruction on two challenging 3D OBJECT RECOGNITION DATASETS of cars and <unk> . \n",
            "this paper presents a novel approach to INVERSE ESTIMATION based on the FORWARD SYNTHESIS MODEL . the proposed FORWARD SYNTHESIS MODEL is based on the use of INVARIANT FEATURES extracted from the FORWARD SYNTHESIS MODEL . the proposed FORWARD SYNTHESIS MODEL is based on the use of INVARIANT FEATURES extracted from the FORWARD SYNTHESIS MODEL . the proposed FORWARD SYNTHESIS MODEL is based on the use of INVARIANT FEATURES derived from the FORWARD SYNTHESIS MODEL . the proposed FORWARD SYNTHESIS MODEL is applied to 3D OBJECT RECOGNITION DATASETS , and the experimental results show that the proposed method is effective in improving the RECOGNITION performance . the proposed approach is evaluated on a variety of 3D OBJECT RECOGNITION DATASETS . the experimental results show that the proposed method is effective in improving the RECOGNITION performance .\n",
            "\n",
            "348 1000\n",
            "this work addresses the problem of developing a DOMAIN-INDEPENDENT BINARY CLASSIFIER for a TEST DOMAIN given labeled data from several training domains where the TEST DOMAIN is not necessarily present in TRAINING DATA . the DOMAIN-INDEPENDENT BINARY CLASSIFIER accepts or rejects the ASR HYPOTHESIS based on the confidence generated by the ASR HYPOTHESIS . in the proposed approach , TRAINING DATA is grouped into ACROSS-DOMAIN CLUSTERS and separate CLUSTER-SPECIFIC CLASSIFIERS are trained . one of the main findings is that the CLUSTER PURITY and the NORMALIZED MUTUAL INFORMATION of the clusters are not very high which suggests that the domains might not necessarily be NATURAL CLUSTERS . the performance of these CLUSTER-SPECIFIC CLASSIFIERS is better than that of : -lrb- a -rrb- a single DOMAIN-INDEPENDENT BINARY CLASSIFIER trained on data from all the domains , and -lrb- b -rrb- a set of CLASSIFIERS trained separately for each of the training domains . at an operating point corresponding to low false accept , the correct accept of the proposed technique is on an average 2.3 % higher than that obtained by the SINGLE-CLASSIFIER or the individual <unk> CLASSIFIERS . \n",
            "in this paper , we propose a novel DOMAIN-INDEPENDENT BINARY CLASSIFIER for TRAINING DATA . the proposed CLUSTER-SPECIFIC CLASSIFIERS is based on the use of TRAINING DATA and NORMALIZED MUTUAL INFORMATION . the proposed CLUSTER-SPECIFIC CLASSIFIERS is based on the use of a DOMAIN-INDEPENDENT BINARY CLASSIFIER and the NORMALIZED MUTUAL INFORMATION . the proposed CLUSTER-SPECIFIC CLASSIFIERS is based on the use of a DOMAIN-INDEPENDENT BINARY CLASSIFIER and the NORMALIZED MUTUAL INFORMATION . the proposed CLUSTER-SPECIFIC CLASSIFIERS is applied to the problem of CLUSTER PURITY in the TEST DOMAIN . the experimental results show that the proposed CLUSTER-SPECIFIC CLASSIFIERS can significantly improve the performance of CLUSTER-SPECIFIC CLASSIFIERS and CLUSTER-SPECIFIC CLASSIFIERS .\n",
            "\n",
            "349 1000\n",
            "a common approach in VISUAL SPEECH SYNTHESIS is the use of VISEMES as ATOMIC UNITS OF SPEECH . in this paper , PHONEME-BASED AND VISEME-BASED AUDIOVISUAL SPEECH SYNTHESIS TECHNIQUES are compared in order to explore the balancing between data availability and an improved AUDIOVISUAL COHERENCE for SYNTHESIS OPTIMIZATION . a technique for AUTOMATIC VISEME CLUSTERING is described and it is compared to the STANDARDIZED VISEME SET described in MPEG-4 . both objective and subjective testing indicated that a <unk> approach leads to better synthesis results . in addition , the test results improve when more different VISEMES are defined . this raises some questions on the widely applied <unk> approach . it appears that a MANY-TO-ONE PHONEME-TO-VISEME MAPPING is not capable of describing all subtle details of the VISUAL SPEECH INFORMATION . in addition , with VISEME-BASED SYNTHESIS the PERCEIVED SYNTHESIS QUALITY is affected by the loss of AUDIOVISUAL COHERENCE in the SYNTHETIC SPEECH . \n",
            "this paper addresses the problem of AUTOMATIC VISEME CLUSTERING in VISUAL SPEECH SYNTHESIS . in particular , we propose a novel approach to AUTOMATIC VISEME CLUSTERING based on MANY-TO-ONE PHONEME-TO-VISEME MAPPING . the proposed approach is based on the use of MANY-TO-ONE PHONEME-TO-VISEME MAPPING , which is able to deal with ATOMIC UNITS OF SPEECH with ATOMIC UNITS OF SPEECH in the presence of AUDIOVISUAL COHERENCE . the proposed approach is based on the use of MANY-TO-ONE PHONEME-TO-VISEME MAPPING , which is able to deal with ATOMIC UNITS OF SPEECH in the presence of VISEMES . the proposed approach is evaluated on a variety of SYNTHETIC SPEECH . experimental results show that the proposed approach is effective in improving the PERCEIVED SYNTHESIS QUALITY of SYNTHETIC SPEECH in SYNTHETIC SPEECH with VISEMES .\n",
            "\n",
            "350 1000\n",
            "stress and <unk> are essential attributes for several components in TEXT-TO SPEECH SYSTEMS . STRESS are responsible for improving GRAPHEME-TO-PHONEME CONVERSION RULES and for enhancing the SYNTHETIC INTELLIGIBILITY , since STRESS and SYLLABLE are key units in PROSODY PREDICTION . this paper presents three LINGUISTICALLY RULE-BASED AUTOMATIC ALGORITHMS for CATALAN TEXT-TO-SPEECH CONVERSION : a WORD STRESS MARKER , an ORTHOGRAPHIC SYLLABIFICATION ALGORITHM and a PHONOLOGICAL SYLLABIFICATION ALGORITHM . the LINGUISTICALLY RULE-BASED AUTOMATIC ALGORITHMS were implemented and tested . the results gave rise to the following WORD ACCURACY RATES : 100 % for the STRESS MARKER ALGORITHM , <unk> % for the ORTHOGRAPHIC SYLLABIFICATION ALGORITHM and <unk> % for the PHONOLOGICAL SYLLABIFICATION ALGORITHM . \n",
            "in this paper , we propose a novel approach to CATALAN TEXT-TO-SPEECH CONVERSION in TEXT-TO SPEECH SYSTEMS . the proposed method is based on a PHONOLOGICAL SYLLABIFICATION ALGORITHM , a PHONOLOGICAL SYLLABIFICATION ALGORITHM , and a PHONOLOGICAL SYLLABIFICATION ALGORITHM . the proposed method is based on a PHONOLOGICAL SYLLABIFICATION ALGORITHM , a PHONOLOGICAL SYLLABIFICATION ALGORITHM , and a PHONOLOGICAL SYLLABIFICATION ALGORITHM . the proposed method is based on a PHONOLOGICAL SYLLABIFICATION ALGORITHM and a PHONOLOGICAL SYLLABIFICATION ALGORITHM . the proposed method is evaluated in terms of WORD ACCURACY RATES and WORD ACCURACY RATES . experimental results show that the proposed method is effective in improving the WORD ACCURACY RATES and WORD ACCURACY RATES of the proposed method .\n",
            "\n",
            "351 1000\n",
            "in this research we investigated USER 'S BEHAVIOR while facing a system coping with COMMON KNOWLEDGE about KEYWORDS and compared it with not only classic WORD-SPOTTING METHOD but also with RANDOM TEXT-MINING . we show how even a simple implementation of our idea can enrich the conversation and increase the naturalness of computer 's utterances . our results show that even very COM-MONSENSICAL UTTERANCES are more natural than classic approaches and also methods we developed to make a conversation more interesting . for <unk> opinion exchange during the session , we will also briefly introduce our idea of combining latest NLP ACHIEVEMENTS into one HOLISTIC SYSTEM where the main engine we want to base on COMMONSENSE PROCESSING and AFFECTIVE COMPUTING . \n",
            "this paper addresses the problem of AFFECTIVE COMPUTING and AFFECTIVE COMPUTING in a HOLISTIC SYSTEM . in particular , we focus on the problem of AFFECTIVE COMPUTING and AFFECTIVE COMPUTING . in particular , we propose a novel approach to AFFECTIVE COMPUTING and AFFECTIVE COMPUTING . we show that our approach is able to detect KEYWORDS in a HOLISTIC SYSTEM . we show that our approach is able to detect KEYWORDS in a HOLISTIC SYSTEM .\n",
            "\n",
            "352 1000\n",
            "we propose a combined LINE SEGMENT and ELLIPTICAL ARC DETECTOR , which formally guarantees the control of the number of false positives and requires no PARAMETER TUNING . the ACCURACY of the DETECTED ELLIPTICAL FEATURES is improved by using a novel NON-ITERATIVE ELLIPSE FITTING TECHNIQUE , which merges the ALGEBRAIC DISTANCE with the GRADIENT ORIENTATION . the performance of the ELLIPTICAL ARC DETECTOR is evaluated on COMPUTER-GENERATED IMAGES and on NATURAL IMAGES . \n",
            "in this paper , we propose a novel NON-ITERATIVE ELLIPSE FITTING TECHNIQUE for COMPUTER-GENERATED IMAGES and COMPUTER-GENERATED IMAGES . the proposed NON-ITERATIVE ELLIPSE FITTING TECHNIQUE is based on a NON-ITERATIVE ELLIPSE FITTING TECHNIQUE and a NON-ITERATIVE ELLIPSE FITTING TECHNIQUE . the proposed ELLIPTICAL ARC DETECTOR is based on a NON-ITERATIVE ELLIPSE FITTING TECHNIQUE and a NON-ITERATIVE ELLIPSE FITTING TECHNIQUE . the experimental results show that the proposed ELLIPTICAL ARC DETECTOR is effective in improving the ACCURACY of the ELLIPTICAL ARC DETECTOR in terms of both ACCURACY and GRADIENT ORIENTATION .\n",
            "\n",
            "353 1000\n",
            "a fundamental issue in DIFFERENTIAL MOTION ANALYSIS is the compromise between the flexibility of the MATCHING CRITERION for IMAGE REGIONS and the ability of recovering the motion . LOCALIZED MATCHING CRITERIA , e.g. , PIXEL-BASED SSD , may enable the RECOVERY OF ALL MOTION PARAMETERS , but it does not tolerate much APPEARANCE CHANGES . on the other hand , GLOBAL CRITERIA , e.g. , MATCHING HISTOGRAMS , can accommodate DRAMATIC APPEARANCE CHANGES , but may be blind to some MOTION PARAMETERS , e.g. , SCALING and ROTATION . this paper presents a novel CLOSED FORM SOLUTION that integrates the advantages of both in a principled way based on a SPATIAL-APPEARANCE MODEL that combines LOCAL APPEARANCES VARIATIONS and GLOBAL SPATIAL STRUCTURES . this CLOSED FORM SOLUTION can capture a large variety of APPEARANCE VARIATIONS that are attributed to the LOCAL NON-RIGIDITY . at the same time , this CLOSED FORM SOLUTION enables efficient RECOVERY OF ALL MOTION PARAMETERS . a MAXIMUM LIKELIHOOD MATCHING CRITERION is defined and rigorous analytical results are obtained that lead to a CLOSED FORM SOLUTION to MOTION TRACKING . very encouraging results demonstrate the effectiveness and efficiency of the proposed CLOSED FORM SOLUTION for TRACKING NON-RIGID OBJECTS that exhibit DRAMATIC APPEARANCE DEFORMATIONS , LARGE OBJECT SCALE CHANGES and PARTIAL OCCLUSIONS . \n",
            "in this paper , we propose a novel SPATIAL-APPEARANCE MODEL for TRACKING NON-RIGID OBJECTS . the proposed CLOSED FORM SOLUTION is based on a SPATIAL-APPEARANCE MODEL for TRACKING NON-RIGID OBJECTS . the proposed CLOSED FORM SOLUTION is based on a MAXIMUM LIKELIHOOD MATCHING CRITERION , which is a SPATIAL-APPEARANCE MODEL for TRACKING NON-RIGID OBJECTS . the proposed CLOSED FORM SOLUTION is based on a MAXIMUM LIKELIHOOD MATCHING CRITERION , which is a CLOSED FORM SOLUTION for TRACKING NON-RIGID OBJECTS . the proposed CLOSED FORM SOLUTION is based on a SPATIAL-APPEARANCE MODEL that exploits the LOCAL APPEARANCES VARIATIONS and the LOCAL APPEARANCES VARIATIONS . the proposed CLOSED FORM SOLUTION is applied to TRACKING NON-RIGID OBJECTS and TRACKING NON-RIGID OBJECTS . the experimental results show that the proposed CLOSED FORM SOLUTION is effective for TRACKING NON-RIGID OBJECTS and TRACKING NON-RIGID OBJECTS . the proposed CLOSED FORM SOLUTION is able to detect and track moving objects under various IMAGE REGIONS , such as ROTATION , LARGE OBJECT SCALE CHANGES , and PARTIAL OCCLUSIONS .\n",
            "\n",
            "354 1000\n",
            "the BARK COHERENCE FUNCTION -lsb- 1 -rsb- defines a COHERENCE FUNCTION with LOUDNESS SPEECH as a new COGNITION MODULE , robust to LINEAR DISTORTIONS due to the analog interface of DIGITAL MOBILE SYSTEM . preliminary experiments have shown the superiority of BARK COHERENCE FUNCTION over current measures . in this paper , a new BARK COHERENCE FUNCTION suitable for VOIP is developed . the new BARK COHERENCE FUNCTION is based on the WAVELET SERIES EXPANSION that provides good frequency resolution while keeping good time locality . the proposed WAVELET BASED BARK COHERENCE FUNCTION is robust to VARIABLE DELAY often observed in INTERNET TELEPHONY such as VOIP . we also show that the refinement of TIME SYNCHRONIZATION after SIGNAL DECOMPOSITION can improve the performance of the WAVELET BASED BARK COHERENCE FUNCTION . the TIME SYNCHRONIZATION was performed with VOIP SPEECH DATA . the CORRELATION COEFFICIENTS and the standard error of estimates computed using the WAVELET BASED BARK COHERENCE FUNCTION showed noticeable improvement over the PSQM that is recommended by WAVELET BASED BARK COHERENCE FUNCTION . \n",
            "in this paper , we propose a novel WAVELET BASED BARK COHERENCE FUNCTION based on WAVELET SERIES EXPANSION . the proposed WAVELET BASED BARK COHERENCE FUNCTION is based on a WAVELET SERIES EXPANSION , a COHERENCE FUNCTION , a COHERENCE FUNCTION , and a COHERENCE FUNCTION . the proposed WAVELET BASED BARK COHERENCE FUNCTION is based on a WAVELET SERIES EXPANSION , a COHERENCE FUNCTION , and the COHERENCE FUNCTION . the proposed WAVELET BASED BARK COHERENCE FUNCTION is based on a WAVELET SERIES EXPANSION , a COHERENCE FUNCTION , and the COHERENCE FUNCTION . the proposed WAVELET BASED BARK COHERENCE FUNCTION is compared to the PSQM and the PSQM in terms of TIME SYNCHRONIZATION .\n",
            "\n",
            "355 1000\n",
            "in this paper we examine the construction of LONG-RANGE LANGUAGE MODELS using LOG-LINEAR INTERPOLATION and how this can be achieved effectively . particular attention is paid to the efficient computation of the <unk> in the LONG-RANGE LANGUAGE MODELS . using the PENN TREEBANK for experiments we argue that the perplexity performance demonstrated recently in the literature using GRAMMAR-BASED APPROACHES can actually be achieved with an appropriately SMOOTHED 4-GRAM LANGUAGE MODEL . using such a model as the baseline , we demonstrate how further improvements can be obtained using LOG-LINEAR INTERPOLATION to combine DISTANCE WORD AND CLASS MODELS . we also examine the performance of similar MODEL COMBINATIONS for RESCORING WORD LATTICES on a MEDIUM-SIZED VOCABULARY WALL STREET JOURNAL TASK . \n",
            "this paper addresses the problem of LOG-LINEAR INTERPOLATION for LONG-RANGE LANGUAGE MODELS . we propose a novel approach to the MEDIUM-SIZED VOCABULARY WALL STREET JOURNAL TASK , which is based on a SMOOTHED 4-GRAM LANGUAGE MODEL . the proposed approach is based on the use of a SMOOTHED 4-GRAM LANGUAGE MODEL , which is based on a SMOOTHED 4-GRAM LANGUAGE MODEL . the proposed approach is based on the use of LOG-LINEAR INTERPOLATION , which is based on a SMOOTHED 4-GRAM LANGUAGE MODEL . the proposed approach is evaluated on the MEDIUM-SIZED VOCABULARY WALL STREET JOURNAL TASK , and the results show that the proposed method is effective in RESCORING WORD LATTICES .\n",
            "\n",
            "356 1000\n",
            "this paper examines the issues in extending a LARGE VOCABULARY SPEECH RECOGNITION SYSTEM designed for CLEAN AND NOISY READ SPEECH TASKS to handle BROADCAST NEWS TRANSCRIPTION . results using the 1995 darpa h 4 e v <unk> data set are presented for DIERENT FRONT-END ANALYSES and use of UNSUPERVISED MODEL ADAPTATION using MAXIMUM LIKELIHOOD LINEAR REGRESSION . the LARGE VOCABULARY SPEECH RECOGNITION SYSTEM for the 1996 H4 EVALUATION is then described . LARGE VOCABULARY SPEECH RECOGNITION SYSTEM includes a number of new FEATURES over previous HTK LARGE VOCABULARY SYSTEMS including DECODER-GUIDED SEGMENTATION , SEGMENT CLUSTERING , CACHE-BASED LANGUAGE MODELLING , and combined map and mllr adaptation . the LARGE VOCABULARY SPEECH RECOGNITION SYSTEM runs in multiple passes through the data and the detailed results of each pass are given . \n",
            "this paper presents a novel LARGE VOCABULARY SPEECH RECOGNITION SYSTEM for BROADCAST NEWS TRANSCRIPTION . the proposed LARGE VOCABULARY SPEECH RECOGNITION SYSTEM is based on the use of MAXIMUM LIKELIHOOD LINEAR REGRESSION , which is an extension of the MAXIMUM LIKELIHOOD LINEAR REGRESSION . the proposed LARGE VOCABULARY SPEECH RECOGNITION SYSTEM is based on the MAXIMUM LIKELIHOOD LINEAR REGRESSION , which is based on the MAXIMUM LIKELIHOOD LINEAR REGRESSION . the proposed LARGE VOCABULARY SPEECH RECOGNITION SYSTEM is based on the use of MAXIMUM LIKELIHOOD LINEAR REGRESSION , which is an extension of the MAXIMUM LIKELIHOOD LINEAR REGRESSION . the proposed LARGE VOCABULARY SPEECH RECOGNITION SYSTEM is evaluated on a variety of CLEAN AND NOISY READ SPEECH TASKS including BROADCAST NEWS TRANSCRIPTION , DECODER-GUIDED SEGMENTATION and BROADCAST NEWS TRANSCRIPTION . the experimental results demonstrate the effectiveness of the proposed LARGE VOCABULARY SPEECH RECOGNITION SYSTEM .\n",
            "\n",
            "357 1000\n",
            "in this work , we consider a CDMA CELL with multiple terminals transmitting video signals . we minimize the sum of SIGNAL PROCESSING and TRANSMITTER POWER while the received QUALITY at each terminal is guaranteed . the system parameters to be adjusted include VIDEO CODING BIT RATE , VIDEO COMPRESSION COMPLEXITY and TRANSMITTER POWER . instead of FULL SEARCH in the space of bit rate , COMPLEXITY , TRANSMITTER POWER ¡ for all users , we design a TWO-STEP FAST ALGORITHM to reduce the COMPUTATION BURDEN in the base station . in our TWO-STEP FAST ALGORITHM , the search in the base station is over the space of COMPLEXITY only . our results indicate that for the same class of video users , the one who is closer to the base station compresses at less COMPLEXITY . this is used to further reduce the computation required by our TWO-STEP FAST ALGORITHM . \n",
            "this paper presents a novel approach to SIGNAL PROCESSING . the proposed approach is based on the use of a TWO-STEP FAST ALGORITHM which is robust to SIGNAL PROCESSING and COMPUTATION BURDEN . the proposed approach is based on the use of a TWO-STEP FAST ALGORITHM , which is robust to SIGNAL PROCESSING , VIDEO CODING BIT RATE and COMPUTATION BURDEN . the proposed method is based on a TWO-STEP FAST ALGORITHM , which is based on a TWO-STEP FAST ALGORITHM . experimental results show that the proposed method is effective in reducing the COMPUTATION BURDEN and the COMPUTATION BURDEN .\n",
            "\n",
            "358 1000\n",
            "when multiple views of data are available for a set of subjects , CO-CLUSTERING aims to identify subject CLUSTERS that agree across the different views . we explore the problem of CO-CLUSTERING when the underlying CLUSTERS exist in different sub-spaces of each view . we propose a PROXIMAL ALTERNATING LINEARIZED MINIMIZATION ALGORITHM that simultaneously decomposes multiple data matrices into SPARSE ROW and columns vectors . this PROXIMAL ALTERNATING LINEARIZED MINIMIZATION ALGORITHM is able to group subjects consistently across the views and simultaneously identify the subset of FEATURES in each view that are associated with the CLUSTERS . the proposed PROXIMAL ALTERNATING LINEARIZED MINIMIZATION ALGORITHM can globally converge to a critical point of the problem . a simulation study validates that the proposed PROXIMAL ALTERNATING LINEARIZED MINIMIZATION ALGORITHM can identify the HYPOTHESIZED CLUSTERS and their associated FEATURES . comparison with several latest MULTI-VIEW CO-CLUSTERING METHODS on BENCHMARK DATASETS demonstrates the superior performance of the proposed PROXIMAL ALTERNATING LINEARIZED MINIMIZATION ALGORITHM . \n",
            "in this paper , we propose a novel PROXIMAL ALTERNATING LINEARIZED MINIMIZATION ALGORITHM for CO-CLUSTERING . the proposed PROXIMAL ALTERNATING LINEARIZED MINIMIZATION ALGORITHM is based on the idea of CO-CLUSTERING , which is a generalization of the standard PROXIMAL ALTERNATING LINEARIZED MINIMIZATION ALGORITHM . the proposed PROXIMAL ALTERNATING LINEARIZED MINIMIZATION ALGORITHM is based on the idea of CO-CLUSTERING , which is a generalization of the standard PROXIMAL ALTERNATING LINEARIZED MINIMIZATION ALGORITHM . the proposed PROXIMAL ALTERNATING LINEARIZED MINIMIZATION ALGORITHM is applied to the problem of CO-CLUSTERING , and the PROXIMAL ALTERNATING LINEARIZED MINIMIZATION ALGORITHM is applied to the problem of CO-CLUSTERING . the experimental results on BENCHMARK DATASETS show that the proposed PROXIMAL ALTERNATING LINEARIZED MINIMIZATION ALGORITHM is superior to the existing MULTI-VIEW CO-CLUSTERING METHODS .\n",
            "\n",
            "359 1000\n",
            "we present an approach for MODEL-FREE MARKERLESS MOTION CAPTURE OF ARTICULATED KINEMATIC STRUCTURES . this approach is centered on our method for generating underlying NONLINEAR AXES -lrb- or a SKELETON CURVE -rrb- of a volume of genus zero -lrb- i.e. , without holes -rrb- . we describe the use of SKELETON CURVES for deriving a KINEMATIC MODEL and motion -lrb- in the form of JOINT ANGLES over time -rrb- from a CAPTURED VOLUME SEQUENCE . our motion capture method uses a SKELETON CURVE , found in each frame of a VOLUME SEQUENCE , to automatically determine KINEMATIC POSTURES . these SKELETON CURVES are aligned to determine a common KINEMATIC MODEL for the VOLUME SEQUENCE . the derived KINEMATIC MODEL is then <unk> to each frame in the VOLUME SEQUENCE to find the MOTION SEQUENCE suited to this KINEMATIC MODEL . we demonstrate our method on several types of motion , from SYNTHETICALLY GENERATED VOLUME SEQUENCES with an ARBITRARY KINEMATIC TOPOLOGY , to HUMAN VOLUME SEQUENCES captured from a set of multiple calibrated cameras . \n",
            "this paper presents a novel approach to the MODEL-FREE MARKERLESS MOTION CAPTURE OF ARTICULATED KINEMATIC STRUCTURES from HUMAN VOLUME SEQUENCES . the proposed KINEMATIC MODEL is based on a KINEMATIC MODEL to the CAPTURED VOLUME SEQUENCE . the proposed KINEMATIC MODEL is based on a KINEMATIC MODEL of the MOTION SEQUENCE of the CAPTURED VOLUME SEQUENCE . the proposed KINEMATIC MODEL is based on a KINEMATIC MODEL of the CAPTURED VOLUME SEQUENCE . the proposed method is based on a KINEMATIC MODEL that uses a KINEMATIC MODEL to estimate the JOINT ANGLES . the proposed method is evaluated on the SYNTHETICALLY GENERATED VOLUME SEQUENCES and the results show that the proposed method is effective in MODEL-FREE MARKERLESS MOTION CAPTURE OF ARTICULATED KINEMATIC STRUCTURES .\n",
            "\n",
            "360 1000\n",
            "1 efficient NATURAL LANGUAGE GENERATION has been successfully demonstrated using highly compiled knowledge about SPEECH ACTS and their related social actions . a design and prototype implementation of a PARSER which utilizes this same PRAGMATIC KNOWLEDGE to efficiently guide parsing is presented . such guidance is shown to prune the SEARCH SPACE and thus avoid <unk> processing of <unk> unlikely constituent structures . INTRODUCTION the use of purely SYNTACTIC KNOWLEDGE during the PARSE phase of NATURAL LANGUAGE UNDERSTANDING yields considerable local ambiguity -lrb- consideration of impossible <unk> -rrb- as well global ambiguity -lrb- construction of syntactically valid parses not applicable to the SOCIO-PRAGMATIC CONTEXT -rrb- . this research investigates bringing SOCIO-PRAGMATIC KNOWLEDGE to bear during the PARSE , while maintaining a DOMAIN INDEPENDENT GRAMMAR and PARSER . the particular technique explored uses knowledge about the PRAGMATIC CONTEXT to order the consideration of proposed PARSE constituents , thus guiding the PARSER to consider the best -lrb- wrt the expectations -rrb- \n",
            "this paper presents a novel approach to NATURAL LANGUAGE UNDERSTANDING in NATURAL LANGUAGE GENERATION . the proposed approach is based on the use of PRAGMATIC KNOWLEDGE extracted from the DOMAIN INDEPENDENT GRAMMAR . the proposed method is based on the use of PRAGMATIC KNOWLEDGE extracted from the PARSE of the DOMAIN INDEPENDENT GRAMMAR . the proposed method is based on the use of a DOMAIN INDEPENDENT GRAMMAR and a PARSER to estimate the PARSE . the proposed approach is evaluated on a SOCIO-PRAGMATIC CONTEXT and a PARSER . the results show that the proposed method is effective in improving the INTRODUCTION in terms of INTRODUCTION and the PARSER .\n",
            "\n",
            "361 1000\n",
            "a BAYESIAN-KULLBACK LEARNING SCHEME , called YING-YANG MACHINE , is proposed based on the two complement but equivalent BAYESIAN REPRESENTATIONS for JOINT DENSITY and their KULLBACK DIVERGENCE . not only the BAYESIAN-KULLBACK LEARNING SCHEME unifies existing MAJOR SUPERVISED AND UNSU-PERVISED LEARNINGS , including the classical maximum likelihood or LEAST SQUARE LEARNING , the MAXIMUM INFORMATION PRESERVATION , the EM & EM ALGORITHM and INFORMATION GEOMETRY , the recent popular HELMHOLTZ MACHINE , as well as other LEARNING METHODS with new variants and new results ; but also the BAYESIAN-KULLBACK LEARNING SCHEME provides a number of new LEARNING METHODS . \n",
            "in this paper , we propose a novel BAYESIAN-KULLBACK LEARNING SCHEME called HELMHOLTZ MACHINE , which is based on a combination of BAYESIAN REPRESENTATIONS and HELMHOLTZ MACHINE . the proposed BAYESIAN-KULLBACK LEARNING SCHEME is based on a BAYESIAN-KULLBACK LEARNING SCHEME , called EM & EM ALGORITHM , which is a BAYESIAN-KULLBACK LEARNING SCHEME . the proposed BAYESIAN-KULLBACK LEARNING SCHEME is based on the EM & EM ALGORITHM , which is a BAYESIAN-KULLBACK LEARNING SCHEME . the proposed BAYESIAN-KULLBACK LEARNING SCHEME is based on a new BAYESIAN-KULLBACK LEARNING SCHEME called the EM & EM ALGORITHM . the proposed BAYESIAN-KULLBACK LEARNING SCHEME is applied to MAJOR SUPERVISED AND UNSU-PERVISED LEARNINGS , including MAJOR SUPERVISED AND UNSU-PERVISED LEARNINGS , LEAST SQUARE LEARNING , MAXIMUM INFORMATION PRESERVATION , and HELMHOLTZ MACHINE . experimental results show that the proposed BAYESIAN-KULLBACK LEARNING SCHEME is able to achieve better performance than the state-of-the-art methods .\n",
            "\n",
            "362 1000\n",
            "-- a novel SUBOPTIMAL HIDING ALGORITHM for BINARY DATA based on WEIGHT APPROXIMATION EMBEDDING , WAE , is proposed . given a specified EMBEDDING rate , this SUBOPTIMAL HIDING ALGORITHM exhibits an advantage of efficient BINARY EMBEDDING with reduced EMBEDDING COMPLEXITY . the SUBOPTIMAL HIDING ALGORITHM performs an EMBEDDING PROCEDURE through a PARITY CHECK MATRIX . the optimal EMBEDDING based on MAXIMAL LIKELIHOOD ALGORITHM aims to locate the COSET LEADER to minimize the EMBEDDING DISTORTION . on the contrary , the SUBOPTIMAL HIDING ALGORITHM looks for a TARGET VECTOR close to the COSET LEADER in an efficiently iterative manner . given an LINEAR EMBEDDING CODE c -lrb- n , k -rrb- , the EMBEDDING COMPLEXITY using the optimal SUBOPTIMAL HIDING ALGORITHM is o -lrb- 2 k -rrb- , while the COMPLEXITY in the SUBOPTIMAL WAE is reduced to o -lrb- <unk> -rrb- where s is the average iterations . \n",
            "in this paper , we propose a novel SUBOPTIMAL HIDING ALGORITHM based on a MAXIMAL LIKELIHOOD ALGORITHM . the proposed SUBOPTIMAL HIDING ALGORITHM is based on a MAXIMAL LIKELIHOOD ALGORITHM , which is based on a MAXIMAL LIKELIHOOD ALGORITHM . the proposed SUBOPTIMAL HIDING ALGORITHM is based on a MAXIMAL LIKELIHOOD ALGORITHM , which is based on the WEIGHT APPROXIMATION EMBEDDING . the proposed SUBOPTIMAL HIDING ALGORITHM is based on the PARITY CHECK MATRIX . the proposed SUBOPTIMAL HIDING ALGORITHM is based on a MAXIMAL LIKELIHOOD ALGORITHM , which is based on the PARITY CHECK MATRIX . the proposed SUBOPTIMAL HIDING ALGORITHM is based on a MAXIMAL LIKELIHOOD ALGORITHM , which is based on a MAXIMAL LIKELIHOOD ALGORITHM . the proposed SUBOPTIMAL HIDING ALGORITHM is applied to BINARY DATA , and the experimental results show that the proposed SUBOPTIMAL HIDING ALGORITHM is effective in improving the COMPLEXITY of the proposed SUBOPTIMAL HIDING ALGORITHM .\n",
            "\n",
            "363 1000\n",
            "this paper proposes a DATA-DRIVEN METHOD for CONCEPT-TO-TEXT GENERATION , the task of automatically producing textual output from NON-LINGUISTIC INPUT . a key insight in our DATA-DRIVEN METHOD is to reduce the tasks of CONTENT SELECTION -lrb- '' what to say '' -rrb- and surface realization -lrb- '' how to say '' -rrb- into a common PARSING PROBLEM . we define a PROBABILISTIC CONTEXT-FREE GRAMMAR that describes the structure of the input -lrb- a CORPUS OF DATABASE RECORDS and text describing some of them -rrb- and represent PROBABILISTIC CONTEXT-FREE GRAMMAR compactly as a WEIGHTED HYPERGRAPH . the HYPER-GRAPH STRUCTURE encodes exponentially many derivations , which we rerank discriminatively using LOCAL AND GLOBAL FEATURES . we propose a novel DECODING ALGORITHM for finding the best SCORING DERIVATION and generating in this setting . experimental evaluation on the ATIS DOMAIN shows that our DATA-DRIVEN METHOD outperforms a competitive DISCRIMINATIVE SYSTEM both using BLEU and in a JUDGMENT ELICITATION STUDY . \n",
            "this paper presents a novel DATA-DRIVEN METHOD for CONCEPT-TO-TEXT GENERATION . the proposed DATA-DRIVEN METHOD is based on a WEIGHTED HYPERGRAPH to the PARSING PROBLEM . the proposed DATA-DRIVEN METHOD is based on the use of a PROBABILISTIC CONTEXT-FREE GRAMMAR and a PROBABILISTIC CONTEXT-FREE GRAMMAR to estimate the LOCAL AND GLOBAL FEATURES . the proposed DATA-DRIVEN METHOD is based on the use of a WEIGHTED HYPERGRAPH and a PROBABILISTIC CONTEXT-FREE GRAMMAR to estimate the LOCAL AND GLOBAL FEATURES . the proposed DATA-DRIVEN METHOD is evaluated on a JUDGMENT ELICITATION STUDY and a JUDGMENT ELICITATION STUDY . the experimental results show that the proposed DATA-DRIVEN METHOD significantly outperforms the conventional DISCRIMINATIVE SYSTEM in terms of BLEU and BLEU .\n",
            "\n",
            "364 1000\n",
            "in this paper , an approach of continuous speech recognition based on LAYERED SELF-ADJUSTING DECODING GRAPH is described . it utilizes a SCAOLDING LAYER to support FAST NETWORK EXPANSION and RELEASING . a TWO LEVEL HASHING STRUCTURE is also described . it introduces SELF-ADJUSTING CAPABILITY i n DYNAMIC DECODING on GENERAL RE-ENTRANT DECODING NETWORK . in STACK DECODING , the SCAOLDING LAYER in the proposed approach enables the DECODER to look several layers into the future so that long span <unk> context dependency can be exactly preserved . experimental results indicate that highly ECIENT DECODING can be achieved with a signicant savings on RECOGNITION RESOURCES . \n",
            "this paper addresses the problem of FAST NETWORK EXPANSION in a LAYERED SELF-ADJUSTING DECODING GRAPH . in particular , we focus on the problem of FAST NETWORK EXPANSION in the context of a LAYERED SELF-ADJUSTING DECODING GRAPH . we show that the SCAOLDING LAYER of the DECODER can be reduced to the SCAOLDING LAYER of the LAYERED SELF-ADJUSTING DECODING GRAPH . we also show that the SELF-ADJUSTING CAPABILITY is a GENERAL RE-ENTRANT DECODING NETWORK of the DECODER . we also show that the SELF-ADJUSTING CAPABILITY is equivalent to a GENERAL RE-ENTRANT DECODING NETWORK . we also show that the SCAOLDING LAYER can be approximated by a GENERAL RE-ENTRANT DECODING NETWORK . we also show that the SELF-ADJUSTING CAPABILITY can be applied to a GENERAL RE-ENTRANT DECODING NETWORK .\n",
            "\n",
            "365 1000\n",
            "<unk> -lrb- th -rrb- circuits in the front end of HIGH-SPEED HIGH-RESOLUTION ANALOG-TO-DIGITAL CONVERTERS typically limit ADC performance at high INPUT SIGNAL frequencies . this paper develops MATHEMATICAL MODELS for ADC implemented in both BIPOLAR AND MOS TECHNOLOGIES . the MATHEMATICAL MODELS are derived by analyzing the SAMPLING INSTANT ERROR and reveal that the NONLINEAR BEHAVIOR is dependent on the INPUT SIGNAL and DIGITAL POST COMPENSATION METHOD 's derivatives . a DIGITAL POST COMPENSATION METHOD is then presented with DIGITAL POST COMPENSATION METHOD 's coefficients estimated using an ENERGY-FREE METHOD in a BACKGROUND CALIBRATION CONFIGURATION . simulation results on a NONLINEAR TH MODEL show that the proposed DIGITAL POST COMPENSATION METHOD achieves a significant improvement in the SPURIOUS FREE DYNAMIC RANGE . the DIGITAL POST COMPENSATION METHOD is also applied to a commercially available ADC to demonstrate DIGITAL POST COMPENSATION METHOD 's effectiveness . \n",
            "in this paper , we propose a novel DIGITAL POST COMPENSATION METHOD for HIGH-SPEED HIGH-RESOLUTION ANALOG-TO-DIGITAL CONVERTERS . the proposed DIGITAL POST COMPENSATION METHOD is based on a NONLINEAR TH MODEL for HIGH-SPEED HIGH-RESOLUTION ANALOG-TO-DIGITAL CONVERTERS . the proposed DIGITAL POST COMPENSATION METHOD is based on the use of a NONLINEAR TH MODEL for HIGH-SPEED HIGH-RESOLUTION ANALOG-TO-DIGITAL CONVERTERS . the proposed DIGITAL POST COMPENSATION METHOD is based on the use of a NONLINEAR TH MODEL to estimate the ADC of the ADC . the proposed DIGITAL POST COMPENSATION METHOD is evaluated on a NONLINEAR TH MODEL . the experimental results show that the proposed DIGITAL POST COMPENSATION METHOD is effective in improving the SAMPLING INSTANT ERROR performance of the NONLINEAR TH MODEL .\n",
            "\n",
            "366 1000\n",
            "belief revision is a ubiquitous process underlying many forms of INTELLIGENT BEHAVIOUR . the AGM PARADIGM is a powerful framework for modeling and implementing BELIEF REVISION SYSTEMS based on the principle of MINIMAL CHANGE ; AGM PARADIGM provides a rich and rigorous foundation for COMPUTER-BASED BELIEF REVISION ARCHITECTURES . MAXI-ADJUSTMENT is a BELIEF REVISION STRATEGY for THEORY BASES that can be implemented using a standard THEOREM PROVER , and one that has been used successfully for several applications . in this paper we provide an ANYTIME DECISION PROCEDURE for MAXI-ADJUSTMENTS , and study its COMPLEXITY . furthermore , we outline a set of GUIDELINES that serve as a PROTOMETHODOLOGY for building BELIEF REVISION SYSTEMS employing a MAXI-ADJUSTMENT . the ANYTIME DECISION PROCEDURE is under development in the BELIEF REVISION MODULE of the CIN PROJECT . \n",
            "this paper presents a novel ANYTIME DECISION PROCEDURE for BELIEF REVISION SYSTEMS . the proposed BELIEF REVISION STRATEGY is based on the use of a THEOREM PROVER , a THEOREM PROVER , a THEOREM PROVER , a BELIEF REVISION MODULE , and a BELIEF REVISION MODULE . the proposed BELIEF REVISION STRATEGY is based on the use of a THEOREM PROVER , a THEOREM PROVER , a BELIEF REVISION MODULE , and the BELIEF REVISION MODULE . the proposed BELIEF REVISION STRATEGY is based on a THEOREM PROVER and a BELIEF REVISION MODULE . the proposed BELIEF REVISION STRATEGY is applied to the CIN PROJECT , and the results show that the proposed BELIEF REVISION STRATEGY is effective in improving the COMPLEXITY of the BELIEF REVISION SYSTEMS .\n",
            "\n",
            "367 1000\n",
            "we describe a method of using a LAGRANGE MULTIPLIER to make a locally optimal trade off between RATE and DISTORTION in the MOTION SEARCH for VIDEO SEQUENCES , while maintaining a CONSTANT BIT RATE CHANNEL . simulation of this method shows that it gives up to 3.5 db psnr improvement in a high motion sequence . a locally rate-distortion -lrb- r-d -rrb- OPTIMAL MODE SELECTION mechanism is also described . this method also gives significant quality benefit over the NOMINAL METHOD . though the benefit of these techniques is significant when used separately , when the OPTIMAL MODE SELECTION is combined with the R-D OPTIMAL MOTION SEARCH , it does not perform much better than the codec does with only the R-D OPTIMAL MOTION SEARCH . \n",
            "in this paper , we propose a novel method for R-D OPTIMAL MOTION SEARCH based on MOTION SEARCH . the proposed approach is based on the use of MOTION SEARCH and DISTORTION . the proposed approach is based on the use of MOTION SEARCH and DISTORTION . the proposed method is based on the use of MOTION SEARCH and DISTORTION . the proposed method is applied to the problem of R-D OPTIMAL MOTION SEARCH . the experimental results show that the proposed method outperforms the existing methods in terms of the RATE and the RATE .\n",
            "\n",
            "368 1000\n",
            "in a broad range of NATURAL LANGUAGE PROCESSING TASKS , LARGE-SCALE KNOWLEDGE-BASE OF PARAPHRASES is anticipated to improve their performance . the key issue in creating such a resource is to establish a practical method of COMPUTING SEMANTIC EQUIVALENCE and SYNTACTIC SUBSTITUTABILITY , i.e. , PARAPHRASABILITY , between given pair of expressions . this paper addresses the issues of COMPUTING PARAPHRASABILITY , focusing on SYNTACTIC VARIANTS OF PREDICATE PHRASES . our model estimates PARAPHRASABILITY based on traditional DISTRIBUTIONAL SIMILARITY MEASURES , where the WEB SNIPPETS are used to overcome the DATA SPARSENESS PROBLEM in handling PREDICATE PHRASES . several feature sets are evaluated through empirical experiments . \n",
            "this paper addresses the problem of COMPUTING SEMANTIC EQUIVALENCE from WEB SNIPPETS . we propose a novel approach to the problem of COMPUTING SEMANTIC EQUIVALENCE from WEB SNIPPETS . the proposed approach is based on the use of DISTRIBUTIONAL SIMILARITY MEASURES and SYNTACTIC SUBSTITUTABILITY . the proposed approach is based on the use of DISTRIBUTIONAL SIMILARITY MEASURES and PARAPHRASABILITY . the proposed approach is based on the use of DISTRIBUTIONAL SIMILARITY MEASURES and PARAPHRASABILITY . the proposed approach is evaluated on a variety of NATURAL LANGUAGE PROCESSING TASKS and NATURAL LANGUAGE PROCESSING TASKS .\n",
            "\n",
            "369 1000\n",
            "an ASSOCIATIVE MEMORY is a STRUCTURE learned from a dataset m of vectors -lrb- signals -rrb- in a way such that , given a noisy version of one of the vectors as input , the nearest valid VECTOR from m -lrb- NEAREST NEIGHBOR -rrb- is provided as output , preferably via a fast ITERATIVE ALGORITHM . traditionally , BINARY -LRB- OR Q-ARY -RRB- HOPFIELD NEURAL NETWORKS are used to model the above STRUCTURE . in this paper , for the first time , we propose a model of ASSOCIATIVE MEMORY based on SPARSE RECOVERY OF SIGNALS . our basic premise is simple . for a dataset , we learn a set of LINEAR CONSTRAINTS that every VECTOR in the dataset must satisfy . provided these LINEAR CONSTRAINTS possess some special properties , it is possible to cast the task of finding NEAREST NEIGHBOR as a SPARSE RECOVERY PROBLEM . assuming GENERIC RANDOM MODELS for the dataset , we show that it is possible to store SUPER-POLYNOMIAL OR EXPONENTIAL NUMBER OF N-LENGTH VECTORS in a neural network of size o -lrb- n -rrb- . furthermore , given a noisy version of one of the STORED VECTORS corrupted in NEAR-LINEAR NUMBER OF COORDINATES , the VECTOR can be correctly <unk> using a NEURALLY FEASIBLE ALGORITHM . \n",
            "this paper addresses the problem of SPARSE RECOVERY OF SIGNALS in the presence of ASSOCIATIVE MEMORY . the problem of SPARSE RECOVERY OF SIGNALS is considered in the context of GENERIC RANDOM MODELS . in this paper , we propose a novel method to estimate the STORED VECTORS of a VECTOR based on a BINARY -LRB- OR Q-ARY -RRB- HOPFIELD NEURAL NETWORKS . the proposed method is based on the use of BINARY -LRB- OR Q-ARY -RRB- HOPFIELD NEURAL NETWORKS to estimate the STORED VECTORS of the STORED VECTORS . the proposed approach is based on the use of a BINARY -LRB- OR Q-ARY -RRB- HOPFIELD NEURAL NETWORKS to estimate the STORED VECTORS of the STORED VECTORS . the proposed NEURALLY FEASIBLE ALGORITHM is applied to the SPARSE RECOVERY PROBLEM of the STORED VECTORS , and the SPARSE RECOVERY PROBLEM is performed using a NEURALLY FEASIBLE ALGORITHM . the experimental results demonstrate the effectiveness of the proposed method in comparison to the NEURALLY FEASIBLE ALGORITHM .\n",
            "\n",
            "370 1000\n",
            "in this paper , we present a CLASS-BASED VARIABLE MEMORY LENGTH MARKOV MODEL and its LEARNING ALGORITHM . this is an extension of a VARIABLE MEMORY LENGTH MARKOV MODEL . our CLASS-BASED VARIABLE MEMORY LENGTH MARKOV MODEL is based on a CLASS-BASED PROBABILISTIC SUFFIX TREE , whose NODES have an automatically acquired WORD-CLASS RELATION . we experimentally compared our new CLASS-BASED VARIABLE MEMORY LENGTH MARKOV MODEL with a WORD-BASED BI-GRAM MODEL , a WORD-BASED TRI-GRAM MODEL , a CLASS-BASED BI-GRAM MODEL , and a WORD-BASED VARIABLE MEMORY LENGTH MARKOV MODEL . the results show that a CLASS-BASED VARIABLE MEMORY LENGTH MARKOV MODEL outperforms the other models in perplexity and MODEL SIZE . \n",
            "this paper proposes a novel CLASS-BASED VARIABLE MEMORY LENGTH MARKOV MODEL based on a WORD-BASED VARIABLE MEMORY LENGTH MARKOV MODEL . the proposed CLASS-BASED VARIABLE MEMORY LENGTH MARKOV MODEL is based on a WORD-BASED VARIABLE MEMORY LENGTH MARKOV MODEL . the proposed CLASS-BASED VARIABLE MEMORY LENGTH MARKOV MODEL is based on a WORD-BASED VARIABLE MEMORY LENGTH MARKOV MODEL . the proposed CLASS-BASED VARIABLE MEMORY LENGTH MARKOV MODEL is based on a WORD-BASED VARIABLE MEMORY LENGTH MARKOV MODEL . the proposed CLASS-BASED VARIABLE MEMORY LENGTH MARKOV MODEL is based on a WORD-BASED VARIABLE MEMORY LENGTH MARKOV MODEL . the proposed CLASS-BASED VARIABLE MEMORY LENGTH MARKOV MODEL is compared to a WORD-BASED TRI-GRAM MODEL . the experimental results show that the proposed CLASS-BASED VARIABLE MEMORY LENGTH MARKOV MODEL outperforms the conventional WORD-BASED TRI-GRAM MODEL .\n",
            "\n",
            "371 1000\n",
            "this paper argues for using AMBIGUITY PLANE FEATURES within DYNAMIC STATISTICAL MODELS for CLASSIFICATION PROBLEMS . the relative contribution of the two DYNAMIC STATISTICAL MODELS are investigated in the context of ACOUSTICALLY MONITORING CUTTER WEAR during <unk> of TITANIUM , an application where it is known that standard STATIC CLASSIFICATION TECHNIQUES work poorly . experiments show that EXPLICIT MODELING OF LONG-TERM CONTEXT via a HIDDEN MARKOV MODEL STATE improves performance , but mainly by using this to augment SPARSELY LABELED TRAINING DATA . an additional performance gain is achieved by using the SHORTER-TERM CONTEXT OF AMBIGUITY PLANE FEATURES . \n",
            "this paper addresses the problem of EXPLICIT MODELING OF LONG-TERM CONTEXT from SPARSELY LABELED TRAINING DATA . we propose a novel approach to the problem of EXPLICIT MODELING OF LONG-TERM CONTEXT from SPARSELY LABELED TRAINING DATA . the proposed approach is based on the use of DYNAMIC STATISTICAL MODELS to estimate the AMBIGUITY PLANE FEATURES . the proposed approach is based on the use of AMBIGUITY PLANE FEATURES to estimate the AMBIGUITY PLANE FEATURES . the proposed approach is evaluated on a variety of CLASSIFICATION PROBLEMS . the experimental results show that the proposed method is effective in improving the performance of the proposed DYNAMIC STATISTICAL MODELS .\n",
            "\n",
            "372 1000\n",
            "the best voices in TEXT-TO-SPEECH SYNTHESIS are currently obtained via ACOUSTIC UNITS CONCATENATION-BASED SYSTEMS . in such ACOUSTIC UNITS CONCATENATION-BASED SYSTEMS , the choice of units whose <unk> will produce an ACOUSTIC MESSAGE is a crucial stage . moreover , it can be observed that current TTS SYSTEMS use ACOUSTIC UNITS which most often correspond to VARIABLE-LENGTH PHONETIC DESCRIPTIONS . in this article , an original framework is proposed which allows the automatic determination of an optimum set of VARIABLE-LENGTH ACOUSTIC UNITS . \n",
            "in this paper , we present a novel approach to TEXT-TO-SPEECH SYNTHESIS based on the use of VARIABLE-LENGTH ACOUSTIC UNITS for TEXT-TO-SPEECH SYNTHESIS . the proposed approach is based on the use of VARIABLE-LENGTH ACOUSTIC UNITS extracted from the ACOUSTIC UNITS . the proposed approach is based on the use of VARIABLE-LENGTH ACOUSTIC UNITS extracted from the ACOUSTIC UNITS . the experimental results show that the proposed approach is effective in improving the performance of TTS SYSTEMS .\n",
            "\n",
            "373 1000\n",
            "1 this paper presents a low-power structure of DIGITAL MATCHED FILTERS , which is proposed for DIRECT SEQUENCE SPREAD SPECTRUM SYSTEMS . traditionally , LOW-POWER APPROACHES for DMFS are based on either the TRANSPOSED-FORM STRUCTURE or the <unk> one . a new HYBRID STRUCTURE that employs the DIRECT-FORM STRUCTURE for local addition and the TRANSPOSED-FORM STRUCTURE for global addition is used to take advantages of both structures . for a 128-TAP DMF , the proposed DMFS that processes 32 <unk> a cycle consumes 46 % less power at the expense of 6 % AREA OVERHEAD as compared to the state-of-the-art low-power DMFS -lsb- 7 -rsb- . \n",
            "in this paper , we propose a novel approach to DIRECT SEQUENCE SPREAD SPECTRUM SYSTEMS based on DIGITAL MATCHED FILTERS . the proposed method is based on the use of a TRANSPOSED-FORM STRUCTURE and a HYBRID STRUCTURE to estimate the DIRECT-FORM STRUCTURE . the proposed method is based on the use of a TRANSPOSED-FORM STRUCTURE and a TRANSPOSED-FORM STRUCTURE . the proposed method is evaluated on a variety of DIGITAL MATCHED FILTERS . the results show that the proposed method is effective in improving the AREA OVERHEAD and the AREA OVERHEAD .\n",
            "\n",
            "374 1000\n",
            "we present a CNF to <unk> -lrb- CNF -rrb- <unk> with COMPLEXITY at most exponential in the TREE WIDTH . we then present algorithms for interesting queries on CNF . although some of the presented query algorithms are in the worst case exponential in the TREE WIDTH , our experiments show that CNF can answer non-trivial queries like CLAUSAL ENTAILMENT in reasonable time for several realistic instances . while our <unk> compiles all the used 91 instances , D-DNNF COMPILATION failed for 12 or 8 of them based on the DECOMPOSITION HEURIS-TIC used . also , on the succeeded instances , a D-DNNF COMPILATION is up to 1000 times larger than the MATCHING TOB . the TOB COMPILATIONS are often an order of magnitude faster than the D-DNNF COMPILATION . this makes CNF a quite interesting KNOWLEDGE COMPILATION FORM . \n",
            "in this paper , we propose a novel method for D-DNNF COMPILATION . the proposed DECOMPOSITION HEURIS-TIC is based on the DECOMPOSITION HEURIS-TIC of the TREE WIDTH . the proposed DECOMPOSITION HEURIS-TIC is based on the DECOMPOSITION HEURIS-TIC of the TREE WIDTH . the proposed DECOMPOSITION HEURIS-TIC is compared to a TOB COMPILATIONS , and the results show that the proposed TOB COMPILATIONS is much faster than the conventional TOB COMPILATIONS . the performance of the proposed DECOMPOSITION HEURIS-TIC is compared to the standard D-DNNF COMPILATION . the proposed TOB COMPILATIONS also shows better performance than the conventional D-DNNF COMPILATION .\n",
            "\n",
            "375 1000\n",
            "the explosive growth of the internet and the <unk> have attracted a great deal of attention to the implementation and performance of NETWORKED MULTIMEDIA SERVICES . which involve the transport of REAL-TIME MULTIMEDIA DATA STREAMS over NON-GUARANTEED QUALITY OF SERVICE NETWORKS based on the INTERNET PROTOCOL . in this paper , i present an overview of the existing ARCHITECTURAL ELEMENTS supporting REAL-TIME DATA TRANSMISSION over the internet . effective implementations of such systems require a thorough understanding of both the NETWORK PROTOCOLS and the CODING SYSTEMS used for compressing the signals to be transmitted in real-time . the paper includes a section discussing the issues to be considered in designing SIGNAL COMPRESSION APPLICATIONS suitable for network use . \n",
            "this paper presents a novel approach to REAL-TIME DATA TRANSMISSION based on NON-GUARANTEED QUALITY OF SERVICE NETWORKS . the proposed approach is based on the use of ARCHITECTURAL ELEMENTS for REAL-TIME DATA TRANSMISSION . the proposed approach is based on the use of ARCHITECTURAL ELEMENTS extracted from the ARCHITECTURAL ELEMENTS . the proposed approach is evaluated on a variety of REAL-TIME MULTIMEDIA DATA STREAMS . the experimental results show that the proposed approach is effective in improving the performance of REAL-TIME DATA TRANSMISSION .\n",
            "\n",
            "376 1000\n",
            "the goal of this study is to investigate whether learners ' WRITTEN DATA in highly INFLECTIONAL CZECH can suggest a consistent set of clues for AUTOMATIC IDENTIFICATION OF THE LEARNERS ' L1 BACKGROUND . for our experiments , we use texts written by learners of CZECH , which have been automatically and manually annotated for errors . we define two classes of learners : SPEAKERS OF INDO-EUROPEAN LANGUAGES and speakers of <unk> languages . we use an SVM CLASSIFIER to perform the BINARY CLASSIFICATION . we show that NON-CONTENT BASED FEATURES perform well on HIGHLY INFLECTIONAL DATA . in particular , FEATURES reflecting errors in ORTHOGRAPHY are the most useful , yielding about 89 % PRECISION and the same RECALL . a detailed discussion of the best performing FEATURES is provided . \n",
            "this paper addresses the problem of AUTOMATIC IDENTIFICATION OF THE LEARNERS ' L1 BACKGROUND from WRITTEN DATA . we propose a novel method for AUTOMATIC IDENTIFICATION OF THE LEARNERS ' L1 BACKGROUND based on AUTOMATIC IDENTIFICATION OF THE LEARNERS ' L1 BACKGROUND . the proposed approach is based on the use of NON-CONTENT BASED FEATURES and NON-CONTENT BASED FEATURES . the proposed method is based on the use of NON-CONTENT BASED FEATURES and NON-CONTENT BASED FEATURES . the proposed method is evaluated on a variety of WRITTEN DATA , and the results show that the proposed method is effective in improving the PRECISION and RECALL .\n",
            "\n",
            "377 1000\n",
            "this paper proposes a new class of LIFTING WAVELET TRANSFORM which can guarantee LOSSLESSNESS OF SPECIFIC SIGNALS , e.g. WHITE BALANCE . the <unk> WAVELET TRANSFORM composed of two LIFTING STEPS can reconstruct an input signal without any loss and has been utilized for LOSSLESS CODING . the 9/7 WAVELET contains two more LIFTING STEPS and two SCALING PAIRS for effective LOSSY CODING . however the LOSSLESSNESS is not guaranteed due to ROUNDING OF SIGNAL VALUES and SCALING COEFFICIENT VALUES . this paper analyzes condition on word length -lrb- wl -rrb- and bit depth -lrb- <unk> -rrb- for the LOSSLESSNESS and proposes a new class of WAVELET TRANSFORM with '' dc lossless '' property which is a kind of specific LOSSLESSNESS . this can be utilized as a standard condition for algorithms or LSI PROCESSORS to guarantee no error from the WAVELET TRANSFORM for WHITE BALANCE SIGNALS . \n",
            "in this paper , we propose a novel approach to LOSSY CODING based on a LIFTING WAVELET TRANSFORM . the proposed approach is based on the LIFTING WAVELET TRANSFORM , which is based on the LIFTING WAVELET TRANSFORM . the proposed method is based on the LIFTING WAVELET TRANSFORM , which is based on the LIFTING WAVELET TRANSFORM . the proposed method is based on the LIFTING WAVELET TRANSFORM , which is based on the LIFTING WAVELET TRANSFORM . the proposed method is based on the use of LIFTING STEPS and SCALING COEFFICIENT VALUES . the experimental results show that the proposed method achieves better performance than the state-of-the-art methods .\n",
            "\n",
            "378 1000\n",
            "we describe an ENERGY MINIMIZATION ALGORITHM for FUNCTIONS defined on 4-CONNECTED LATTICES , of the type usually encountered in problems involving IMAGES . such FUNCTIONS are often minimized using GRAPH-CUTS/MAX-FLOW , but this ENERGY MINIMIZATION ALGORITHM is only applicable to SUBMODULAR PROBLEMS . in this paper , we describe an ENERGY MINIMIZATION ALGORITHM that will solve any BINARY PROBLEM , irrespective of whether it is SUBMODULAR or not , and for MULTILABEL PROBLEMS we use ALPHA-EXPANSION . the ENERGY MINIMIZATION ALGORITHM is based on the ELIMINATION ALGORITHM , which eliminates NODES from the GRAPH until the remaining function is SUBMODULAR . it can then be solved using MAX-FLOW . values of ELIMINATED VARIABLES are recovered using BACK-SUBSTITUTION . we compare the ENERGY MINIMIZATION ALGORITHM 's performance against alternative methods for solving SUBMODULAR PROBLEMS , with favourable results . \n",
            "this paper presents a novel ELIMINATION ALGORITHM for MULTILABEL PROBLEMS . the proposed ENERGY MINIMIZATION ALGORITHM is based on the use of MAX-FLOW , which is a BINARY PROBLEM . the proposed ENERGY MINIMIZATION ALGORITHM is based on the use of MAX-FLOW , which is a BINARY PROBLEM . the proposed ENERGY MINIMIZATION ALGORITHM is based on the use of MAX-FLOW , which is a BINARY PROBLEM . the proposed ENERGY MINIMIZATION ALGORITHM is based on the use of MAX-FLOW , which is a BINARY PROBLEM . the proposed ENERGY MINIMIZATION ALGORITHM is applied to the problem of MULTILABEL PROBLEMS in IMAGES . the proposed ENERGY MINIMIZATION ALGORITHM is applied to the problem of MULTILABEL PROBLEMS from IMAGES . the experimental results show that the proposed ELIMINATION ALGORITHM is effective for MULTILABEL PROBLEMS in IMAGES .\n",
            "\n",
            "379 1000\n",
            "in this paper , we propose a SPARSE RANDOM FEATURES ALGORITHM , which learns a SPARSE NON-LINEAR PREDICTOR by minimizing an ℓ 1-REGULARIZED OBJECTIVE FUNCTION over the HILBERT SPACE induced from a KERNEL FUNCTION . by interpreting the SPARSE RANDOM FEATURES ALGORITHM as RANDOMIZED COORDINATE DESCENT in an INFINITE-DIMENSIONAL SPACE , we show the proposed SPARSE RANDOM FEATURES ALGORITHM converges to a solution within <unk> of that using an exact KERNEL METHOD , by drawing o -lrb- 1 / <unk> -rrb- random features , in contrast to the O -LRB- 1 / Ε 2 -RRB- CONVERGENCE achieved by current monte-carlo analyses of RANDOM FEATURES . in our experiments , the SPARSE RANDOM FEATURES ALGORITHM obtains a sparse solution that requires less MEMORY AND PREDICTION TIME , while maintaining comparable performance on REGRESSION AND CLASSIFICATION TASKS . moreover , as an APPROXIMATE SOLVER for the INFINITE-DIMENSIONAL ℓ 1-REGULARIZED PROBLEM , the SPARSE RANDOM FEATURES ALGORITHM also enjoys better convergence guarantees than a BOOSTING APPROACH in the setting where the GREEDY BOOSTING STEP can not be performed exactly . \n",
            "this paper proposes a novel SPARSE RANDOM FEATURES ALGORITHM to the INFINITE-DIMENSIONAL ℓ 1-REGULARIZED PROBLEM . the proposed SPARSE RANDOM FEATURES ALGORITHM is based on a SPARSE RANDOM FEATURES ALGORITHM , which is a generalization of the SPARSE RANDOM FEATURES ALGORITHM to the INFINITE-DIMENSIONAL ℓ 1-REGULARIZED PROBLEM . the proposed SPARSE RANDOM FEATURES ALGORITHM is based on the idea of RANDOMIZED COORDINATE DESCENT to the INFINITE-DIMENSIONAL ℓ 1-REGULARIZED PROBLEM . the proposed SPARSE RANDOM FEATURES ALGORITHM is based on a SPARSE RANDOM FEATURES ALGORITHM , which is based on a SPARSE RANDOM FEATURES ALGORITHM . the proposed SPARSE RANDOM FEATURES ALGORITHM is applied to the INFINITE-DIMENSIONAL ℓ 1-REGULARIZED PROBLEM of the INFINITE-DIMENSIONAL ℓ 1-REGULARIZED PROBLEM . the proposed SPARSE RANDOM FEATURES ALGORITHM is applied to the INFINITE-DIMENSIONAL ℓ 1-REGULARIZED PROBLEM of the INFINITE-DIMENSIONAL ℓ 1-REGULARIZED PROBLEM . the experimental results show that the proposed SPARSE RANDOM FEATURES ALGORITHM outperforms the conventional BOOSTING APPROACH in terms of both MEMORY AND PREDICTION TIME and O -LRB- 1 / Ε 2 -RRB- CONVERGENCE .\n",
            "\n",
            "380 1000\n",
            "in this paper we address the issue of output instability of DEEP NEURAL NETWORKS : SMALL PERTURBATIONS in the VISUAL INPUT can significantly distort the FEATURE EMBEDDINGS and output of a NEURAL NETWORK . such instability affects many DEEP ARCHITECTURES with state-of-the-art performance on a wide range of COMPUTER VISION TASKS . we present a GENERAL STABILITY TRAINING METHOD to stabilize DEEP NEURAL NETWORKS against SMALL INPUT DISTORTIONS that result from various types of COMMON IMAGE PROCESSING , such as COMPRESSION , RESCALING , and CROPPING . we validate our method by <unk> the state-of-the-art INCEPTION ARCHITECTURE -lsb- 11 -rsb- against these types of distortions . in addition , we demonstrate that our <unk> model gives robust state-of-the-art performance on LARGE-SCALE NEAR-DUPLICATE DETECTION , SIMILAR-IMAGE RANKING , and CLASSIFICATION on NOISY DATASETS . \n",
            "this paper addresses the problem of LARGE-SCALE NEAR-DUPLICATE DETECTION and CLASSIFICATION in a NEURAL NETWORK . we propose a novel approach to the problem of LARGE-SCALE NEAR-DUPLICATE DETECTION in a NEURAL NETWORK . the proposed approach is based on a GENERAL STABILITY TRAINING METHOD that combines FEATURE EMBEDDINGS , RESCALING , RESCALING , and RESCALING . the proposed approach is based on a GENERAL STABILITY TRAINING METHOD , which is able to deal with SMALL INPUT DISTORTIONS , such as COMPRESSION , RESCALING , RESCALING , and CROPPING . the proposed approach is evaluated on a variety of NOISY DATASETS and NOISY DATASETS , and the experimental results demonstrate the effectiveness of the proposed approach .\n",
            "\n",
            "381 1000\n",
            "one of the main challenges in learning FINE-GRAINED VISUAL CATEGORIES is gathering training images . recent work in ZERO-SHOT LEARNING circumvents this challenge by describing categories via attributes or text . however , not all VISUAL CONCEPTS , e.g. , two people <unk> , are easily amenable to such descriptions . in this paper , we propose a new MODALITY for ZERO-SHOT LEARNING using VISUAL ABSTRACTION to learn DIFFICULT-TO-DESCRIBE CONCEPTS . specifically , we explore CONCEPTS related to people and their interactions with others . our proposed MODALITY allows one to provide TRAINING DATA by manipulating ABSTRACT VISUALIZATIONS , e.g. , one can illustrate interactions between two <unk> people by manipulating each person 's pose , expression , GAZE , and gender . the feasibility of our MODALITY is shown on a HUMAN POSE DATASET and a new dataset containing complex interactions between two people , where we outperform several baselines . to better match across the two domains , we learn an explicit mapping between the abstract and real worlds . \n",
            "this paper addresses the problem of ZERO-SHOT LEARNING from a single MODALITY . we propose a novel approach to ZERO-SHOT LEARNING based on VISUAL ABSTRACTION . the proposed approach is based on the use of a MODALITY , a MODALITY , and a MODALITY for ZERO-SHOT LEARNING . the proposed approach is based on the use of a MODALITY , a MODALITY , and a MODALITY . the proposed method is evaluated on the HUMAN POSE DATASET and the results show that the proposed method outperforms the state-of-the-art methods .\n",
            "\n",
            "382 1000\n",
            "we present a generalization of SIMILARITY-BASED RETRIEVAL in RECOMMENDER SYSTEMS which ensures that for any case that is acceptable to the user , the RETRIEVAL SET contains a case that is at least as good in an objective sense and so also likely to be acceptable . our approach recognizes that similarity to the target query is only one of several possible criteria according to which a given case might be considered at least as good as another . \n",
            "in this paper , we present a novel approach to SIMILARITY-BASED RETRIEVAL for RECOMMENDER SYSTEMS . the proposed approach is based on the use of SIMILARITY-BASED RETRIEVAL for RECOMMENDER SYSTEMS . the proposed approach is based on the use of SIMILARITY-BASED RETRIEVAL for RECOMMENDER SYSTEMS . the experimental results show that the proposed approach is effective in improving the performance of the RECOMMENDER SYSTEMS .\n",
            "\n",
            "383 1000\n",
            "many INVERSE PROBLEMS require to minimize a CRITERION being the sum of a NON NECESSARILY SMOOTH FUNCTION and a LIPSCHITZ DIFFEREN-TIABLE FUNCTION . such an OPTIMIZATION PROBLEM can be solved with the FORWARD-BACKWARD ALGORITHM which can be accelerated thanks to the use of VARIABLE METRICS derived from the MAJORIZE-MINIMIZE PRINCIPLE . the convergence of this approach is guaranteed provided that the CRITERION satisfies some additional technical conditions . combining this method with an ALTERNATING MINIMIZATION STRATEGY will be shown to allow us to address a broad class of OPTIMIZATION PROBLEMS involving LARGE-SIZE SIGNALS . an application example to a NONCONVEX SPECTRAL UNMIXING PROBLEM will be presented . \n",
            "this paper addresses the problem of NONCONVEX SPECTRAL UNMIXING PROBLEM in the presence of LARGE-SIZE SIGNALS . in particular , we consider the problem of INVERSE PROBLEMS in the OPTIMIZATION PROBLEMS . we propose a novel method to solve the NONCONVEX SPECTRAL UNMIXING PROBLEM , which is based on a FORWARD-BACKWARD ALGORITHM . the proposed algorithm is based on a FORWARD-BACKWARD ALGORITHM , which is a generalization of the FORWARD-BACKWARD ALGORITHM to the NONCONVEX SPECTRAL UNMIXING PROBLEM . the proposed algorithm is based on a FORWARD-BACKWARD ALGORITHM , which is based on a FORWARD-BACKWARD ALGORITHM . the proposed algorithm is applied to the problem of LARGE-SIZE SIGNALS , and the results show that the proposed algorithm is able to solve the problem of NONCONVEX SPECTRAL UNMIXING PROBLEM .\n",
            "\n",
            "384 1000\n",
            "in this paper we present a novel MODEL BASED SPARSE PRINCIPAL COMPONENT ANALYSIS METHOD based on the L 0 PENALTY . we develop an ESTIMATION METHOD based on the GENERALIZED EM ALGORITHM and ITERATIVE HARD THRESHOLDING and an ASSOCIATED MODEL SELECTION METHOD based on BAYESIAN INFORMATION CRITERION . the ESTIMATION METHOD is compared to a previous SPARSE PCA METHOD using both SIMULATED DATA and DNA MICROARRAY DATA . \n",
            "this paper proposes a novel ESTIMATION METHOD based on BAYESIAN INFORMATION CRITERION and ITERATIVE HARD THRESHOLDING . the proposed ESTIMATION METHOD is based on a GENERALIZED EM ALGORITHM , which is based on a GENERALIZED EM ALGORITHM . the proposed ESTIMATION METHOD is based on a GENERALIZED EM ALGORITHM , which is based on the BAYESIAN INFORMATION CRITERION . the proposed ESTIMATION METHOD is based on the GENERALIZED EM ALGORITHM and the GENERALIZED EM ALGORITHM . the performance of the proposed ESTIMATION METHOD is compared to the standard GENERALIZED EM ALGORITHM and the GENERALIZED EM ALGORITHM . the proposed ESTIMATION METHOD is applied to the problem of DNA MICROARRAY DATA and DNA MICROARRAY DATA . the experimental results show that the proposed ESTIMATION METHOD is effective in improving the performance of the ESTIMATION METHOD .\n",
            "\n",
            "385 1000\n",
            "in recent years , DEEP ARCHITECTURES have gained a lot of prominence for LEARNING complex AI TASKS because of their capability to incorporate complex variations in data within the model . however , these models often need to be trained for a long time in order to obtain good results . in this paper , we propose a technique , called ` <unk> ' , that allows the same models to perform considerably better with very little training . we show that LEARNING can be done tractably , even when the WEIGHT MATRIX is <unk> to infinity , for some specific models . we also study TRACTABLE ALGORITHMS for implementing STRETCHING in DEEP CONVOLUTIONAL ARCHITECTURES in an ITERATIVE MANNER and derive bounds for its convergence . our experimental results suggest that the proposed STRETCHED DEEP CONVOLUTIONAL NETWORKS are capable of achieving good performance for many OBJECT RECOGNITION TASKS . more importantly , for a FIXED NETWORK ARCHITECTURE , one can achieve much better ACCURACY using STRETCHING rather than LEARNING the weights using BACKPROPAGATION . \n",
            "this paper addresses the problem of LEARNING in AI TASKS . in particular , we focus on the problem of LEARNING in the context of STRETCHED DEEP CONVOLUTIONAL NETWORKS . in particular , we consider the problem of LEARNING in the context of STRETCHED DEEP CONVOLUTIONAL NETWORKS . in particular , we consider the problem of LEARNING in the context of STRETCHED DEEP CONVOLUTIONAL NETWORKS . in particular , we consider the problem of LEARNING in the context of STRETCHED DEEP CONVOLUTIONAL NETWORKS . in particular , we show that the WEIGHT MATRIX of the WEIGHT MATRIX can be reduced to the WEIGHT MATRIX of the WEIGHT MATRIX . we also show that the WEIGHT MATRIX of the WEIGHT MATRIX can be reduced to the WEIGHT MATRIX .\n",
            "\n",
            "386 1000\n",
            "a novel method for determining the set of parameters for a PHASE SPACE REPRESENTATION of a time series is proposed . based upon the DIFFERENTIAL ENTROPY , both the OPTIMAL EMBEDDING DIMENSION , and TIME LAG , are simultaneously determined . the choice of these parameters is closely related to the length of the optimal tap input delay line of an ADAPTIVE FILTER or TIME-DELAY NEURAL NETWORK . the method employs a single criterion -- the '' entropy ratio '' between the PHASE SPACE REPRESENTATION of a signal and an ensemble of its <unk> -- and is first systematically tested on SYNTHETIC TIME SERIES for which the optimal EMBEDDING PARAMETERS are known , after which it is verified on a number of benchmark real-world time series . the proposed entropy ratio method is shown to consistently outperform some well-established methods . \n",
            "in this paper , we propose a novel approach to the problem of PHASE SPACE REPRESENTATION . the proposed approach is based on the use of a PHASE SPACE REPRESENTATION and a TIME-DELAY NEURAL NETWORK . the proposed method is based on a PHASE SPACE REPRESENTATION and a PHASE SPACE REPRESENTATION . the proposed method is based on the use of a PHASE SPACE REPRESENTATION and a PHASE SPACE REPRESENTATION . the proposed method is evaluated on a SYNTHETIC TIME SERIES , and the results show that the proposed method outperforms the state-of-the-art methods .\n",
            "\n",
            "387 1000\n",
            "empirical mode decomposition -lrb- EMPIRICAL MODE DECOMPOSITION -rrb- is an ADAPTIVE METHOD for NONLINEAR AND NONSTATIONARY SIGNAL PROCESSING . although the ADAPTIVE METHOD is easy to implement and widely deployed , its THEORETICAL BACKGROUND and limitations remain uncertain . this paper investigates the performance of EMPIRICAL MODE DECOMPOSITION in two TONE SEPARATION PROBLEM , especially for the TRANSITION REGION between perfect separation and failure , with emphasis on the effect of the INITIAL PHASE . relationships between AMPLITUDE RATIO , FREQUENCY RATIO , INITIAL PHASE and performance are derived . \n",
            "in this paper , we propose a novel ADAPTIVE METHOD , called EMPIRICAL MODE DECOMPOSITION , to address the problem of NONLINEAR AND NONSTATIONARY SIGNAL PROCESSING . the proposed ADAPTIVE METHOD is based on the EMPIRICAL MODE DECOMPOSITION , which is based on the EMPIRICAL MODE DECOMPOSITION . the proposed ADAPTIVE METHOD , called EMPIRICAL MODE DECOMPOSITION , is proposed to solve the TONE SEPARATION PROBLEM . the proposed ADAPTIVE METHOD is based on the EMPIRICAL MODE DECOMPOSITION , which is based on the EMPIRICAL MODE DECOMPOSITION . the proposed ADAPTIVE METHOD is applied to the problem of NONLINEAR AND NONSTATIONARY SIGNAL PROCESSING . the experimental results show that the proposed ADAPTIVE METHOD is effective in reducing the number of sources and to achieve the same TONE SEPARATION PROBLEM .\n",
            "\n",
            "388 1000\n",
            "most SENTIMENT ANALYSIS approaches use as baseline a SUPPORT VECTOR MACHINES CLASSIFIER with BINARY UNIGRAM WEIGHTS . in this paper , we explore whether more sophisticated FEATURE WEIGHTING SCHEMES from INFORMATION RETRIEVAL can enhance CLASSIFICATION ACCURACY . we show that variants of the classic TF.IDF SCHEME adapted to SENTIMENT ANALYSIS provide significant increases in ACCURACY , especially when using a SUBLINEAR FUNCTION for TERM FREQUENCY WEIGHTS and DOCUMENT FREQUENCY SMOOTHING . the techniques are tested on a wide selection of DATA SETS and produce the best ACCURACY to our knowledge . \n",
            "this paper addresses the problem of SENTIMENT ANALYSIS for INFORMATION RETRIEVAL . in this paper , we propose a novel approach to SENTIMENT ANALYSIS based on INFORMATION RETRIEVAL and DOCUMENT FREQUENCY SMOOTHING . the proposed FEATURE WEIGHTING SCHEMES is based on the use of DOCUMENT FREQUENCY SMOOTHING and DOCUMENT FREQUENCY SMOOTHING . the proposed FEATURE WEIGHTING SCHEMES is based on the use of BINARY UNIGRAM WEIGHTS and TERM FREQUENCY WEIGHTS . the proposed FEATURE WEIGHTING SCHEMES is applied to the problem of INFORMATION RETRIEVAL and DOCUMENT FREQUENCY SMOOTHING . experimental results show that the proposed method is effective in improving the CLASSIFICATION ACCURACY of the proposed method in terms of CLASSIFICATION ACCURACY and DOCUMENT FREQUENCY SMOOTHING .\n",
            "\n",
            "389 1000\n",
            "while HUMAN LISTENING is robust in complex AUDITORY SCENES , current SPEECH SEGREGATION ALGORITHMS do not perform well in NOISY AND REVERBERANT ENVIRONMENTS . this paper addresses the ROBUSTNESS in BINAURAL SPEECH SEGREGATION by employing BINARY CLASSIFICATION based on DEEP NEURAL NETWORKS -lrb- dnns -rrb- . we systematically examine DNN BASED BINAURAL CLASSIFICATION to UNTRAINED CONFIGURATIONS . evaluations and comparisons show that DNN BASED BINAURAL CLASSIFICATION produces superior segregation performance in a variety of MULTISOURCE AND REVERBERANT CONDITIONS . \n",
            "this paper addresses the problem of BINAURAL SPEECH SEGREGATION in NOISY AND REVERBERANT ENVIRONMENTS . we propose a novel approach to BINAURAL SPEECH SEGREGATION in AUDITORY SCENES . the proposed approach is based on the use of DEEP NEURAL NETWORKS , which is able to deal with MULTISOURCE AND REVERBERANT CONDITIONS . the proposed approach is based on the use of DEEP NEURAL NETWORKS , which is able to deal with HUMAN LISTENING in NOISY AND REVERBERANT ENVIRONMENTS . the proposed approach is evaluated on a variety of UNTRAINED CONFIGURATIONS , and the results show that the proposed approach is effective in improving the ROBUSTNESS and ROBUSTNESS of the proposed approach .\n",
            "\n",
            "390 1000\n",
            "in this paper we provide the first , to the best of our knowledge , BAYESIAN FORMULATION of one of the most successful and well-studied STATISTICAL MODELS OF SHAPE AND TEXTURE , i.e. ACTIVE APPEARANCE MODELS . to this end , we use a simple PROBABILISTIC MODEL for TEXTURE GENERATION assuming both GAUSSIAN NOISE and a GAUSSIAN PRIOR over a LATENT TEXTURE SPACE . we retrieve the SHAPE PARAMETERS by formulating a novel COST FUNCTION obtained by marginalizing out the LATENT TEXTURE SPACE . this results in a fast implementation when compared to other SIMULTANEOUS ALGORITHMS for fitting ACTIVE APPEARANCE MODELS , mainly due to the removal of the CALCULATION OF TEXTURE PARAMETERS . we demonstrate that , contrary to what is believed regarding the performance of ACTIVE APPEARANCE MODELS in GENERIC FITTING SCENARIOS , optimization of the proposed COST FUNCTION produces results that outperform discriminatively trained state-of-the-art methods in the problem of facial alignment '' in the wild '' . \n",
            "in this paper , we propose a novel PROBABILISTIC MODEL for TEXTURE GENERATION . the proposed ACTIVE APPEARANCE MODELS is based on a PROBABILISTIC MODEL for TEXTURE GENERATION . the proposed ACTIVE APPEARANCE MODELS is based on a PROBABILISTIC MODEL and a GAUSSIAN PRIOR for TEXTURE GENERATION . the proposed PROBABILISTIC MODEL is based on a PROBABILISTIC MODEL , which is based on a PROBABILISTIC MODEL and a GAUSSIAN PRIOR . the proposed PROBABILISTIC MODEL is applied to TEXTURE GENERATION , and the results show that the proposed PROBABILISTIC MODEL is effective for TEXTURE GENERATION .\n",
            "\n",
            "391 1000\n",
            "we introduce an ERROR MINING TECHNIQUE for AUTOMATICALLY DETECTING ERRORS in resources that are used in PARSING SYSTEMS . we applied this ERROR MINING TECHNIQUE on PARSING results produced on several million words by two distinct PARSING SYSTEMS , which share the SYNTACTIC LEXICON and the PRE-PARSING PROCESSING CHAIN . we were thus able to identify MISSING AND ERRONEOUS INFORMATION in these resources . \n",
            "this paper presents a novel ERROR MINING TECHNIQUE for PARSING SYSTEMS . the proposed ERROR MINING TECHNIQUE is based on a PRE-PARSING PROCESSING CHAIN and a PRE-PARSING PROCESSING CHAIN . the proposed ERROR MINING TECHNIQUE is based on a PRE-PARSING PROCESSING CHAIN and a PRE-PARSING PROCESSING CHAIN . the proposed ERROR MINING TECHNIQUE is based on the ERROR MINING TECHNIQUE and the PRE-PARSING PROCESSING CHAIN . the experimental results show that the proposed ERROR MINING TECHNIQUE is effective in improving the PARSING performance .\n",
            "\n",
            "392 1000\n",
            "differential privacy preserving regression models guarantee protection against attempts to infer whether a subject was included in the training set used to derive a model . it is not designed to protect ATTRIBUTE PRIVACY of a target individual when MODEL INVERSION ATTACKS are <unk> . in MODEL INVERSION ATTACKS , an adversary uses the RELEASED MODEL to make predictions of sensitive attributes -lrb- used as input to the model -rrb- of a target individual when some background information about the target individual is available . previous research showed that existing DIFFERENTIAL PRIVACY MECHANISMS can not effectively prevent MODEL INVERSION ATTACKS while retaining MODEL EFFICACY . in this paper , we develop a novel approach which leverages the FUNCTIONAL MECHANISM to perturb coefficients of the POLYNOMIAL REPRESENTATION OF THE OBJECTIVE FUNCTION but effectively balances the PRIVACY BUDGET for SENSITIVE AND NON-SENSITIVE ATTRIBUTES in learning the DIFFERENTIAL PRIVACY PRESERVING REGRESSION MODEL . THEORETICAL ANALYSIS and empirical evaluations demonstrate our approach can effectively prevent MODEL INVERSION ATTACKS and retain MODEL UTILITY . \n",
            "in this paper , we propose a novel DIFFERENTIAL PRIVACY PRESERVING REGRESSION MODEL based on SENSITIVE AND NON-SENSITIVE ATTRIBUTES . the proposed DIFFERENTIAL PRIVACY PRESERVING REGRESSION MODEL is based on the SENSITIVE AND NON-SENSITIVE ATTRIBUTES of the SENSITIVE AND NON-SENSITIVE ATTRIBUTES of the SENSITIVE AND NON-SENSITIVE ATTRIBUTES . the proposed DIFFERENTIAL PRIVACY PRESERVING REGRESSION MODEL is based on a POLYNOMIAL REPRESENTATION OF THE OBJECTIVE FUNCTION , which is based on a POLYNOMIAL REPRESENTATION OF THE OBJECTIVE FUNCTION . the proposed DIFFERENTIAL PRIVACY PRESERVING REGRESSION MODEL is based on the use of SENSITIVE AND NON-SENSITIVE ATTRIBUTES to estimate the MODEL UTILITY . the proposed DIFFERENTIAL PRIVACY PRESERVING REGRESSION MODEL is applied to the problem of ATTRIBUTE PRIVACY , and the results show that the proposed DIFFERENTIAL PRIVACY PRESERVING REGRESSION MODEL is effective in reducing the PRIVACY BUDGET . the proposed DIFFERENTIAL PRIVACY PRESERVING REGRESSION MODEL is applied to the SENSITIVE AND NON-SENSITIVE ATTRIBUTES and the experimental results show that the proposed DIFFERENTIAL PRIVACY PRESERVING REGRESSION MODEL is effective in reducing the PRIVACY BUDGET .\n",
            "\n",
            "393 1000\n",
            "in this paper we propose a novel method for IMAGE SEMANTIC SEGMENTATION using multiple graphs . the MULTI-VIEW AFFINITY GRAPH is constructed by leveraging the consistency between SEMANTIC SPACE and multiple VISUAL SPACES . with BLOCK-DIAGONAL CONSTRAINTS , we enforce the AFFINITY MATRIX to be sparse such that the PAIRWISE POTENTIAL for DISSIMILAR SUPERPIXELS is close to zero . by a DIVIDE-AND-CONQUER STRATEGY , the OPTIMIZATION for LEARNING AFFINITY MATRIX is decomposed into several subproblems that can be solved in parallel . using the neighborhood relationship between superpixels and the consistency between AFFINITY MATRIX and LABEL-CONFIDENCE MATRIX , we infer the SEMANTIC LABEL for each superpixel of UNLABELED IMAGES by minimizing an objective whose CLOSED FORM SOLUTION can be easily obtained . experimental results on two REAL-WORLD IMAGE DATASETS demonstrate the effectiveness of our method . \n",
            "this paper addresses the problem of IMAGE SEMANTIC SEGMENTATION from a single image . we propose a novel method to learn a LEARNING AFFINITY MATRIX from a set of UNLABELED IMAGES . the proposed method consists of two steps : -lrb- 1 -rrb- a PAIRWISE POTENTIAL , a LABEL-CONFIDENCE MATRIX and a LABEL-CONFIDENCE MATRIX , and -lrb- 2 -rrb- a DIVIDE-AND-CONQUER STRATEGY to estimate the LEARNING AFFINITY MATRIX . the proposed approach is based on the use of a DIVIDE-AND-CONQUER STRATEGY and a DIVIDE-AND-CONQUER STRATEGY . the proposed approach is based on the use of a DIVIDE-AND-CONQUER STRATEGY and a DIVIDE-AND-CONQUER STRATEGY . the proposed method is evaluated on two REAL-WORLD IMAGE DATASETS . the experimental results show that the proposed method achieves better performance than the state-of-the-art methods .\n",
            "\n",
            "394 1000\n",
            "this paper presents an IMAGE PROCESSING SYSTEM for DETECTING MOUSE PRETERM LABOR using SECOND HARMONIC GENERATION MICROSCOPY IMAGES . two classes of SHG IMAGES are considered : NORMAL PREGNANT CERVIX and PREMATURE CERVICAL REMODELING induced by MIFEPRISTONE . among the commonly used TEXTURE FEATURES in IMAGE PROCESSING , WAVELET-BASED TEXTURE FEATURES together with previously utilized IMAGE FEATURES for SHG MICROSCOPY of ARTIFICIAL COLLAGEN GELS are identified to form an effective set of FEATURES for distinguishing the two classes of IMAGES . the results obtained indicate that correct DETECTION RATES above 98 % are achievable . \n",
            "this paper presents a novel approach to DETECTING MOUSE PRETERM LABOR based on WAVELET-BASED TEXTURE FEATURES extracted from SECOND HARMONIC GENERATION MICROSCOPY IMAGES . the proposed approach is based on the use of WAVELET-BASED TEXTURE FEATURES extracted from the IMAGE FEATURES of the IMAGE PROCESSING SYSTEM . the proposed approach is based on the use of WAVELET-BASED TEXTURE FEATURES extracted from the IMAGE FEATURES of the IMAGE FEATURES . the proposed approach is evaluated on a variety of IMAGES including NORMAL PREGNANT CERVIX , and SECOND HARMONIC GENERATION MICROSCOPY IMAGES . the experimental results show that the proposed approach is effective in improving the DETECTION RATES and DETECTION RATES of the IMAGE PROCESSING SYSTEM .\n",
            "\n",
            "395 1000\n",
            "in this paper we relate the PARTITION FUNCTION to the MAX-STATISTICS OF RANDOM VARIABLES . in particular , we provide a novel framework for approximating and bounding the PARTITION FUNCTION using MAP INFERENCE on RANDOMLY PERTURBED MODELS . as a result , we can use efficient MAP SOLVERS such as GRAPH-CUTS to evaluate the corresponding PARTITION FUNCTION . we show that our method excels in the typical '' high <unk> coupling '' regime that results in RAGGED ENERGY LANDSCAPES difficult for alternative approaches . \n",
            "in this paper , we propose a novel method for MAP INFERENCE based on RANDOMLY PERTURBED MODELS . the proposed method is based on the use of RANDOMLY PERTURBED MODELS to estimate the PARTITION FUNCTION . the proposed method is based on the use of a PARTITION FUNCTION for MAP INFERENCE . the proposed method is based on the use of RANDOMLY PERTURBED MODELS to estimate the PARTITION FUNCTION . experimental results show that the proposed method achieves better performance than the state-of-the-art methods .\n",
            "\n",
            "396 1000\n",
            "most methods for DECISION-THEORETIC ONLINE LEARNING are based on the HEDGE ALGORITHM , which takes a PARAMETER called the LEARNING RATE . in most previous analyses the LEARNING RATE was carefully tuned to obtain optimal worst-case performance , leading to suboptimal performance on easy instances , for example when there exists an action that is significantly better than all others . we propose a new way of setting the LEARNING RATE , which adapts to the difficulty of the DECISION-THEORETIC ONLINE LEARNING : in the worst case our procedure still guarantees optimal performance , but on easy instances it achieves much smaller regret . in particular , our adaptive method achieves constant regret in a PROBABILISTIC SETTING , when there exists an action that on average obtains strictly smaller loss than all other actions . we also provide a SIMULATION STUDY comparing our approach to existing methods . \n",
            "this paper presents a novel HEDGE ALGORITHM for DECISION-THEORETIC ONLINE LEARNING . the proposed HEDGE ALGORITHM is based on a HEDGE ALGORITHM , which is a generalization of the HEDGE ALGORITHM . the proposed HEDGE ALGORITHM is based on a HEDGE ALGORITHM , which is based on the HEDGE ALGORITHM . the proposed HEDGE ALGORITHM is applied to the problem of DECISION-THEORETIC ONLINE LEARNING . the experimental results show that the proposed HEDGE ALGORITHM is effective in improving the LEARNING RATE of the proposed HEDGE ALGORITHM .\n",
            "\n",
            "397 1000\n",
            "this paper presents a new approach for estimating voice source and vocal tract filter characteristics of VOICED SPEECH . when it is required to know the TRANSFER FUNCTION of a system in SIGNAL PROCESSING , the input and output of the system are experimentally observed and used to calculate the function . however , in the case of SOURCE-FILTER SEPARATION we deal with in this paper , only the output -lrb- speech -rrb- is observed and the characteristics of the system -lrb- vocal tract -rrb- and the input -lrb- voice source -rrb- must simultaneously be estimated . hence the estimate becomes extremely difficult , and it is usually solved approximately using OVERSIMPLIFIED MODELS . we demonstrate that these characteristics are separable under the assumption that they are independently controlled by different factors . the separation is realised using an ITERATIVE APPROXIMATION along with the MULTI-FRAME ANALYSIS METHOD , which we have proposed to find spectral envelopes of VOICED SPEECH with minimum interference of the HARMONIC STRUCTURE . \n",
            "this paper presents a novel approach to SIGNAL PROCESSING in SIGNAL PROCESSING . the proposed approach is based on the use of a MULTI-FRAME ANALYSIS METHOD and a MULTI-FRAME ANALYSIS METHOD . the proposed method consists of two steps : -lrb- 1 -rrb- a HARMONIC STRUCTURE , and -lrb- 2 -rrb- a MULTI-FRAME ANALYSIS METHOD . the proposed method consists of two steps : -lrb- 1 -rrb- a TRANSFER FUNCTION , and -lrb- 2 -rrb- a MULTI-FRAME ANALYSIS METHOD . the proposed method consists of two steps : -lrb- 1 -rrb- a TRANSFER FUNCTION , and -lrb- 2 -rrb- a MULTI-FRAME ANALYSIS METHOD based on a MULTI-FRAME ANALYSIS METHOD . the proposed method can be applied to a wide range of VOICED SPEECH .\n",
            "\n",
            "398 1000\n",
            "target TARGET CLASSIFICATION using DISTRIBUTED SENSOR ARRAYS remains a challenging problem due to the NON-STATIONARITY OF TARGET SIGNATURES , large geographical area coverage of sensor arrays , and the requirements of <unk> and reliable information delivery . in this paper , we develop an algorithm to derive effective and stable features from both the frequency and the time-frequency domains of the acoustic signals . a modified DATA FUSION ALGORITHM for DISTRIBUTED SENSOR ARRAYS is also developed in order to integrate the TARGET CLASSIFICATION results from different sensors and provide <unk> . by using DATA FUSION , the ACCURACY of the TARGET CLASSIFICATION can be increased by as many as 50 % . \n",
            "this paper presents a novel DATA FUSION ALGORITHM for TARGET CLASSIFICATION in DISTRIBUTED SENSOR ARRAYS . the proposed DATA FUSION ALGORITHM is based on a DATA FUSION ALGORITHM for TARGET CLASSIFICATION . the proposed DATA FUSION ALGORITHM is based on the use of DISTRIBUTED SENSOR ARRAYS and NON-STATIONARITY OF TARGET SIGNATURES . the experimental results show that the ACCURACY of the proposed DATA FUSION is significantly improved by the proposed DATA FUSION ALGORITHM .\n",
            "\n",
            "399 1000\n",
            "the recent availability of large corpora for training n-gram language models has shown the utility of models of higher order than just <unk> . in this paper , we investigate methods to control the increase in MODEL SIZE resulting from applying standard methods at higher orders . we introduce SIGNIFICANCE-BASED N-GRAM SELECTION , which not only reduces MODEL SIZE , but also improves PERPLEXITY for several SMOOTHING METHODS , including KATZ BACK-OFF and ABSOLUTE DISCOUNTING . we also show that , when combined with a new SMOOTHING METHOD and a novel variant of WEIGHTED-DIFFERENCE PRUNING , our SELECTION METHOD performs better in the trade-off between MODEL SIZE and PERPLEXITY than the best PRUNING METHOD we found for MODIFIED KNESER-NEY SMOOTHING . \n",
            "in this paper , we propose a novel SMOOTHING METHOD for SIGNIFICANCE-BASED N-GRAM SELECTION . the proposed SELECTION METHOD is based on the idea of MODIFIED KNESER-NEY SMOOTHING and WEIGHTED-DIFFERENCE PRUNING . the proposed SELECTION METHOD is based on the idea that the MODEL SIZE of the signal is unknown . the proposed SELECTION METHOD is based on a PRUNING METHOD , which is a generalization of the SELECTION METHOD . the proposed SELECTION METHOD is evaluated in terms of both PERPLEXITY and PERPLEXITY . the performance of the proposed SELECTION METHOD is evaluated on the PERPLEXITY and PERPLEXITY . the experimental results show that the proposed SELECTION METHOD is very effective in terms of PERPLEXITY and ABSOLUTE DISCOUNTING .\n",
            "\n",
            "400 1000\n",
            "<unk> is the most common GAME TREE SEARCH ALGORITHM , due to its high-performance and straightforward implementation . in practice one must find the best trade-off between HEURISTIC EVALUATION TIME and bringing the subset of NODES explored closer to a MINIMUM PROOF GRAPH . in this paper we present a series of structural properties of MINIMUM PROOF GRAPHS that help us to prove that finding such GRAPHS is np-hard for ARBITRARY DAG INPUTS , but can be done in LINEAR TIME for trees . we then introduce the class of FASTEST-CUT-FIRST SEARCH HEURISTICS that aim to approximate MINIMUM PROOF GRAPHS by sorting moves based on APPROXIMATIONS OF SUB-DAG VALUES and sizes . to explore how various aspects of the GAME TREE -lrb- such as BRANCHING FACTOR and distribution of move values -rrb- affect the performance of ALPHA-BETA we introduce the class of '' prefix value game trees '' that allows us to label INTERIOR NODES with true MINIMAX VALUES on the fly without search . using these trees we show that by explicitly attempting to approximate a MINIMUM GAME TREE we are able to achieve performance gains over ALPHA-BETA with common extensions . \n",
            "in this paper , we propose a novel GAME TREE SEARCH ALGORITHM called ALPHA-BETA , called ALPHA-BETA , for MINIMUM PROOF GRAPHS . the proposed GAME TREE SEARCH ALGORITHM is based on a GAME TREE SEARCH ALGORITHM , called ALPHA-BETA , which is a MINIMUM PROOF GRAPH of a set of ARBITRARY DAG INPUTS . the proposed GAME TREE SEARCH ALGORITHM , called ALPHA-BETA , is based on a GAME TREE SEARCH ALGORITHM , called ALPHA-BETA , which is a generalization of the GAME TREE SEARCH ALGORITHM to the GAME TREE . the proposed GAME TREE SEARCH ALGORITHM is based on a GAME TREE SEARCH ALGORITHM , called ALPHA-BETA , which is a MINIMUM PROOF GRAPH . the proposed GAME TREE SEARCH ALGORITHM is a MINIMUM PROOF GRAPH , which is a MINIMUM PROOF GRAPH of the GAME TREE . the proposed GAME TREE SEARCH ALGORITHM is applied to the problem of ARBITRARY DAG INPUTS in a MINIMUM PROOF GRAPH , and is shown to be superior to the state of the art .\n",
            "\n",
            "401 1000\n",
            "in the WRAPPER APPROACH for FEATURE SELECTION , a popular criterion used is the LEAVE-ONE-OUT ESTIMATE of the CLASSIFICATION ERROR . while being relatively unbiased , the LEAVE-ONE-OUT ERROR ESTIMATE is nonetheless known to exhibit a large variance , which can be detrimental especially for small samples . we propose reducing its variance -lrb- i.e. smoothing -rrb- at two levels . at the first level , we smooth the ERROR COUNT using ESTIMATES OF POSTERIOR PROBABILITIES ; while at the second level , we smooth the POSTERIOR PROBABILITY ESTIMATES themselves using BAYESIAN ESTIMATION with CONJUGATE PRIORS . furthermore , we propose using the JACKKNIFE to reduce the BIAS inherent in BAYESIAN ESTIMATORS . we then show empirically that smoothing the ERROR ESTIMATE gives improved performance in FEATURE SELECTION . \n",
            "this paper addresses the problem of BAYESIAN ESTIMATION in BAYESIAN ESTIMATION with CONJUGATE PRIORS . in particular , we consider the problem of BAYESIAN ESTIMATION with CONJUGATE PRIORS . in particular , we consider the problem of FEATURE SELECTION in BAYESIAN ESTIMATION with CONJUGATE PRIORS . in particular , we consider the problem of BAYESIAN ESTIMATION with CONJUGATE PRIORS . we show that the ESTIMATES OF POSTERIOR PROBABILITIES can be approximated by a LEAVE-ONE-OUT ESTIMATE , which is a LEAVE-ONE-OUT ESTIMATE of the LEAVE-ONE-OUT ERROR ESTIMATE . we show that the ESTIMATES OF POSTERIOR PROBABILITIES can be approximated by a LEAVE-ONE-OUT ESTIMATE . we also show that the proposed WRAPPER APPROACH can be applied to the problem of BAYESIAN ESTIMATION with CONJUGATE PRIORS .\n",
            "\n",
            "402 1000\n",
            "in this paper , an efficient GLOBAL ALGORITHM for VECTORIZING LINE DRAWINGS is presented . GLOBAL ALGORITHM first extracts a SEED SEGMENT of a GRAPHIC ENTITY from a RASTER IMAGE to obtain its direction and width , then tracks the pixels under the guidance of the direction so that the TRACKING can track through junctions and is not affected by NOISE and DEGRADATION OF IMAGE QUALITY . thus , an ENTITY will be <unk> in one step without POSTPROCESSING . the relations among lines are also used to realize the CONTINUOUS VECTORIZATION OF A LINE NET . the speed and quality of VECTORIZATION are greatly improved with this GLOBAL ALGORITHM . the performance evaluation is carried out both by THEORETICAL ANALYSIS and by experiments . comparisons with other VECTORIZATION ALGORITHMS are also made . \n",
            "this paper addresses the problem of TRACKING from a CONTINUOUS VECTORIZATION OF A LINE NET . we propose a novel approach to the problem of TRACKING in VECTORIZING LINE DRAWINGS . the proposed GLOBAL ALGORITHM is based on a CONTINUOUS VECTORIZATION OF A LINE NET , a CONTINUOUS VECTORIZATION OF A LINE NET , and a VECTORIZATION . the VECTORIZATION is formulated as a CONTINUOUS VECTORIZATION OF A LINE NET , and the VECTORIZATION is solved using a GLOBAL ALGORITHM . the proposed GLOBAL ALGORITHM is based on a CONTINUOUS VECTORIZATION OF A LINE NET , and the VECTORIZATION is solved using a GLOBAL ALGORITHM . the proposed GLOBAL ALGORITHM is applied to VECTORIZING LINE DRAWINGS and TRACKING . the experimental results show that the proposed method outperforms the state-of-the-art methods in terms of TRACKING and TRACKING .\n",
            "\n",
            "403 1000\n",
            "gaussian mixture models -lrb- gmms -rrb- and ERGODIC HIDDEN MARKOV MODELS have been successfully applied to model SHORT-TERM ACOUSTIC VECTORS for SPEAKER RECOGNITION SYSTEMS . PROSODIC FEATURES are known to carry information concerning the SPEAKER 'S IDENTITY and PROSODIC FEATURES can be combined with the SHORT-TERM ACOUSTIC VECTORS in order to increase the performance of the SPEAKER RECOGNITION SYSTEMS . in this paper , a STATISTICAL APPROACH using PITCH-DEPENDENT GMMS for modeling speakers is presented . this new STATISTICAL APPROACH is capable of simultaneously modeling the statistical distributions of the SHORT-TERM ACOUSTIC VECTORS and LONG-TERM PROSODIC FEATURES . \n",
            "in this paper , we propose a novel approach to SPEAKER RECOGNITION SYSTEMS based on GAUSSIAN MIXTURE MODELS and ERGODIC HIDDEN MARKOV MODELS . the proposed approach is based on the use of PITCH-DEPENDENT GMMS and ERGODIC HIDDEN MARKOV MODELS . the proposed method is based on the use of PITCH-DEPENDENT GMMS and ERGODIC HIDDEN MARKOV MODELS . the proposed method is based on the use of PITCH-DEPENDENT GMMS and ERGODIC HIDDEN MARKOV MODELS . the proposed approach is evaluated on a variety of SPEAKER RECOGNITION SYSTEMS . the experimental results show that the proposed method outperforms the state-of-the-art methods in terms of both SPEAKER 'S IDENTITY and SPEAKER 'S IDENTITY .\n",
            "\n",
            "404 1000\n",
            "in MACHINE LEARNING PROBLEMS with tens of thousands of FEATURES and only dozens or hundreds of independent training examples , DIMENSIONALITY REDUCTION is essential for good learning performance . in previous work , many researchers have treated the MACHINE LEARNING PROBLEMS in two separate phases : first use an algorithm such as SINGULAR VALUE DECOMPOSITION to reduce the dimensionality of the data set , and then use a CLASSIFICATION ALGORITHM such as na <unk> ve bayes or support vector machines to learn a CLASSIFIER . we demonstrate that it is possible to combine the two goals of DIMENSIONALITY REDUCTION and classification into a single learning objective , and present a novel and efficient algorithm which optimizes this objective directly . we present experimental results in FMRI ANALYSIS which show that we can achieve better learning performance and LOWER-DIMENSIONAL REPRESENTATIONS than TWO-PHASE APPROACHES can . \n",
            "this paper addresses the problem of DIMENSIONALITY REDUCTION in MACHINE LEARNING PROBLEMS with LOWER-DIMENSIONAL REPRESENTATIONS . in particular , we focus on the problem of DIMENSIONALITY REDUCTION in the context of FMRI ANALYSIS . in particular , we propose a novel CLASSIFICATION ALGORITHM to the problem of DIMENSIONALITY REDUCTION . the proposed CLASSIFICATION ALGORITHM is based on the use of SINGULAR VALUE DECOMPOSITION , which is a generalization of the standard CLASSIFICATION ALGORITHM . the proposed CLASSIFICATION ALGORITHM is evaluated on a variety of MACHINE LEARNING PROBLEMS with LOWER-DIMENSIONAL REPRESENTATIONS . the experimental results show that the proposed TWO-PHASE APPROACHES can significantly improve the performance of the TWO-PHASE APPROACHES .\n",
            "\n",
            "405 1000\n",
            "this paper presents a novel 3D FACE RECOGNITION METHOD by means of the EVOLUTION OF ISO-GEODESIC DISTANCE CURVES . specifically , the proposed 3D FACE RECOGNITION METHOD compares two neighboring ISO-GEODESIC DISTANCE CURVES , and formalizes the evolution between them as a ONE-DIMENSIONAL FUNCTION , named EVOLUTION ANGLE FUNCTION , which is EUCLIDEAN INVARIANT . the novelty of this paper consists in formalizing 3D FACE by an EVOLUTION ANGLE FUNCTIONS , and in computing the distance between two faces by that of two functions . experiments on face recognition grand challenge -lrb- <unk> -rrb- <unk> .0 shows that our 3D FACE RECOGNITION METHOD works very well on both NEUTRAL FACES and NON-NEUTRAL FACES . by introducing a WEIGHT FUNCTION , we also show a very promising result on NON-NEUTRAL FACE DATABASE . \n",
            "in this paper , we propose a novel 3D FACE RECOGNITION METHOD based on a EVOLUTION OF ISO-GEODESIC DISTANCE CURVES . the proposed 3D FACE RECOGNITION METHOD is based on the EVOLUTION OF ISO-GEODESIC DISTANCE CURVES of a 3D FACE , such as the EVOLUTION ANGLE FUNCTION , the EVOLUTION ANGLE FUNCTION , and the EVOLUTION ANGLE FUNCTIONS . the proposed 3D FACE RECOGNITION METHOD is based on a EVOLUTION OF ISO-GEODESIC DISTANCE CURVES , which is based on the EVOLUTION OF ISO-GEODESIC DISTANCE CURVES . the proposed 3D FACE RECOGNITION METHOD is based on a EVOLUTION OF ISO-GEODESIC DISTANCE CURVES , which is based on the EVOLUTION OF ISO-GEODESIC DISTANCE CURVES . the experimental results show that the proposed 3D FACE RECOGNITION METHOD is able to accurately estimate the 3D FACE of the 3D FACE .\n",
            "\n",
            "406 1000\n",
            "in this paper , we introduce a new approach to TWO-DIMENSIONAL PROCESSING of the ONE-DIMENSIONAL SPEECH SIGNAL in the TIME-FREQUENCY PLANE . specifically , we obtain the SHORT-SPACE 2-D FOURIER TRANSFORM MAGNITUDE of a NARROWBAND SPECTROGRAM of the signal and show that this 2-D TRANSFORMATION MAPS HARMONICALLY-RELATED SIGNAL COMPONENTS to a concentrated entity in the new 2-D PLANE . we refer to this series of operations as the '' GRATING COMPRESSION TRANSFORM '' , consistent with SINE-WAVE GRATING PATTERNS in the SPECTROGRAM reduced to SMEARED IMPULSES . the GRATING COMPRESSION TRANSFORM '' forms the basis of a SPEECH PITCH ESTIMATOR that uses the RADIAL DISTANCE to the largest peak in the GCT PLANE . using an average magnitude difference between <unk> estimates , the GCT-BASED PITCH ESTIMATOR is shown to compare favorably to a SINE-WAVE-BASED PITCH ESTIMATOR for ALL-VOICED SPEECH in ADDITIVE WHITE NOISE . an extension to a basis for TWO-SPEAKER PITCH ESTIMATION is also proposed . \n",
            "in this paper , we propose a novel method for TWO-SPEAKER PITCH ESTIMATION in ALL-VOICED SPEECH . the proposed GCT-BASED PITCH ESTIMATOR is based on a SINE-WAVE-BASED PITCH ESTIMATOR , which is a SINE-WAVE-BASED PITCH ESTIMATOR for TWO-SPEAKER PITCH ESTIMATION . the proposed SINE-WAVE-BASED PITCH ESTIMATOR is based on a GCT-BASED PITCH ESTIMATOR , which is able to estimate the 2-D TRANSFORMATION MAPS HARMONICALLY-RELATED SIGNAL COMPONENTS of a ONE-DIMENSIONAL SPEECH SIGNAL . the proposed SINE-WAVE-BASED PITCH ESTIMATOR is based on the SHORT-SPACE 2-D FOURIER TRANSFORM MAGNITUDE of the NARROWBAND SPECTROGRAM , which is a ONE-DIMENSIONAL SPEECH SIGNAL . the proposed SINE-WAVE-BASED PITCH ESTIMATOR is applied to the NARROWBAND SPECTROGRAM of the NARROWBAND SPECTROGRAM in the NARROWBAND SPECTROGRAM . the experimental results show that the proposed GCT-BASED PITCH ESTIMATOR is able to accurately estimate the 2-D TRANSFORMATION MAPS HARMONICALLY-RELATED SIGNAL COMPONENTS of the ALL-VOICED SPEECH . the proposed SINE-WAVE-BASED PITCH ESTIMATOR is compared with the conventional SINE-WAVE-BASED PITCH ESTIMATOR and the GCT-BASED PITCH ESTIMATOR is better than the conventional GCT-BASED PITCH ESTIMATOR .\n",
            "\n",
            "407 1000\n",
            "for MULTIUSER MISO SYSTEMS with BOUNDED UNCERTAINTIES in the CHANNEL STATE INFORMATION , we consider two CLASSICAL ROBUST DESIGN PROBLEMS : maximizing the minimum rate subject to a TRANSMIT POWER CONSTRAINT , and POWER MINIMIZATION under a RATE CONSTRAINT . contrary to conventional strategies , we propose a RATE-SPLITTING STRATEGY where each message is divided into two parts , a common part and a private part . all common parts are packed into one super common message encoded using a shared codebook and decoded by all users , while private parts are independently encoded and retrieved by their corresponding users . we prove that RS-BASED DESIGNS achieve higher MAX-MIN DEGREES OF FREEDOM compared to conventional designs -lrb- MAX-MIN DEGREES OF FREEDOM -rrb- for UNCERTAINTY REGIONS that scale with SNR . for the special case of NON-SCALING UNCERTAINTY REGIONS , MAX-MIN DEGREES OF FREEDOM contrasts with MAX-MIN DEGREES OF FREEDOM and achieves a NON-SATURATING MAX-MIN RATE . in the POWER MINIMIZATION PROBLEM , MAX-MIN DEGREES OF FREEDOM is shown to combat the FEASIBILITY PROBLEM arising from MULTIUSER INTERFERENCE in MAX-MIN DEGREES OF FREEDOM . a robust design of PRECODERS for MAX-MIN DEGREES OF FREEDOM is proposed , and performance gains over MAX-MIN DEGREES OF FREEDOM are demonstrated through simulations . \n",
            "this paper addresses the problem of MULTIUSER MISO SYSTEMS in MULTIUSER MISO SYSTEMS with BOUNDED UNCERTAINTIES . the proposed MAX-MIN DEGREES OF FREEDOM is based on a RATE-SPLITTING STRATEGY of the TRANSMIT POWER CONSTRAINT , which is based on a RATE-SPLITTING STRATEGY of the TRANSMIT POWER CONSTRAINT . the proposed MAX-MIN DEGREES OF FREEDOM is based on a RATE-SPLITTING STRATEGY of the TRANSMIT POWER CONSTRAINT , which is based on a RATE-SPLITTING STRATEGY of the TRANSMIT POWER CONSTRAINT . the NON-SATURATING MAX-MIN RATE of the proposed MAX-MIN DEGREES OF FREEDOM is compared to the MAX-MIN DEGREES OF FREEDOM and the TRANSMIT POWER CONSTRAINT . the NON-SATURATING MAX-MIN RATE of the proposed MAX-MIN DEGREES OF FREEDOM is compared to the MAX-MIN DEGREES OF FREEDOM , and the NON-SATURATING MAX-MIN RATE is comparable to that of the conventional MAX-MIN DEGREES OF FREEDOM .\n",
            "\n",
            "408 1000\n",
            "current diagnostic methods for MENTAL PATHOLOGIES , including POST-TRAUMATIC STRESS DISORDER , involve a CLINICIAN-CODED INTERVIEW , which can be subjective . HEART RATE and skin <unk> , as well as other PERIPHERAL PHYSIOLOGY MEASURES , have previously shown utility in PREDICTING BINARY DIAGNOSTIC DECISIONS . the BINARY DIAGNOSTIC DECISIONS is easier , but misses important information on the severity of the patients condition . this work utilizes a novel experimental setup that exploits VIRTUAL REALITY VIDEOS and PERIPHERAL PHYSIOLOGY for PTSD DIAGNOSIS . in pursuit of an AUTOMATED PHYSIOLOGY-BASED OBJECTIVE DIAGNOSTIC METHOD , we propose a LEARNING FORMULATION that integrates the description of the experimental data and expert knowledge on desirable properties of a PHYSIOLOGICAL DIAGNOSTIC SCORE . from a list of desired criteria , we derive a new COST FUNCTION that combines regression and CLASSIFICATION while learning the salient features for PREDICTING PHYSIOLOGICAL SCORE . the PHYSIOLOGICAL SCORE produced by SPARSE COMBINED REGRESSION-CLASSIFICATION is assessed with respect to three sets of criteria chosen to reflect design goals for an objective , PHYSIOLOGICAL PTSD SCORE : <unk> and context of selected features , DIAGNOSTIC SCORE VALIDITY , and LEARNING GENERALIZABILITY . for these criteria , we demonstrate that SPARSE COMBINED REGRESSION-CLASSIFICATION performs better than more GENERIC LEARNING APPROACHES . \n",
            "this paper presents a novel approach to PREDICTING BINARY DIAGNOSTIC DECISIONS in VIRTUAL REALITY VIDEOS . the proposed approach is based on the use of PERIPHERAL PHYSIOLOGY MEASURES , which is based on a LEARNING FORMULATION . the proposed AUTOMATED PHYSIOLOGY-BASED OBJECTIVE DIAGNOSTIC METHOD is based on the use of PERIPHERAL PHYSIOLOGY MEASURES and PERIPHERAL PHYSIOLOGY . the proposed AUTOMATED PHYSIOLOGY-BASED OBJECTIVE DIAGNOSTIC METHOD is based on the use of PERIPHERAL PHYSIOLOGY MEASURES and SPARSE COMBINED REGRESSION-CLASSIFICATION . the proposed AUTOMATED PHYSIOLOGY-BASED OBJECTIVE DIAGNOSTIC METHOD is applied to the problem of PREDICTING BINARY DIAGNOSTIC DECISIONS and CLASSIFICATION . experimental results show that the proposed SPARSE COMBINED REGRESSION-CLASSIFICATION is effective in improving the HEART RATE and HEART RATE .\n",
            "\n",
            "409 1000\n",
            "this paper addresses the problem of VIDEO ANOMALY RECOVERY from a sequence of SPECTRALLY COMPRESSED VIDEO FRAMES . ANALYSIS OF ANOMALIES occurring in both time and spectrum is important in VIDEO SURVEILLANCE APPLICATIONS . we present a methodology for the RECOVERY OF ANOMALIES such as moving objects and their SPECTRAL SIGNATURES from SPECTRALLY COMPRESSED VIDEO . the SPECTRALLY COMPRESSED VIDEO FRAMES are obtained by using a coded aperture snapshot spectral imaging -lrb- <unk> -rrb- system . the CASSI SYSTEM encodes a 3-D DATA CUBE containing both 2-D SPATIAL INFORMATION and SPECTRAL INFORMATION in a single 2-D MEASUREMENT . in the proposed methodology , we use the SPECTRALLY COMPRESSED VIDEO as columns of a large data matrix g g g. PRINCIPAL COMPONENT PURSUIT is then used to decompose G G G into the STATIONARY BACKGROUND and a SPARSE MATRIX capturing the anomalies in the foreground . the SPARSE MATRIX is then used jointly with G G G to recover the SPECTRAL INFORMATION of the objects of interest . an example for the RECOVERY OF VIDEO ANOMALIES in a 3-CHANNEL SPECTRAL VIDEO SYSTEM -lrb- rgb -rrb- is presented . \n",
            "in this paper , we propose a novel approach to VIDEO ANOMALY RECOVERY in SPECTRALLY COMPRESSED VIDEO . the proposed approach is based on a CASSI SYSTEM that exploits the 2-D SPATIAL INFORMATION and the SPECTRAL INFORMATION in the SPECTRALLY COMPRESSED VIDEO . the proposed approach is based on the G G G , which is based on the G G G . the proposed approach is based on the G G G , which is based on the G G G . the proposed method is based on the G G G , which is based on the G G G . the proposed approach is evaluated on a 3-D DATA CUBE with STATIONARY BACKGROUND . the results show that the proposed method is effective in improving the RECOVERY OF VIDEO ANOMALIES performance in VIDEO SURVEILLANCE APPLICATIONS .\n",
            "\n",
            "410 1000\n",
            "we consider the problem of IMAGE SEGMENTATION using ACTIVE CONTOURS through the minimization of an ENERGY CRITERION involving both REGION AND BOUNDARY FUNCTIONALS . these IMAGE SEGMENTATION are derived through a SHAPE DERIVATIVE APPROACH instead of CLASSICAL CALCULUS OF VARIATION . the IMAGE SEGMENTATION can be elegantly derived without converting the REGION INTEGRALS into boundary integrals . from the DERIVATIVE , we deduce the EVOLUTION EQUATION of an ACTIVE CONTOUR that makes it evolve towards a minimum of the criterion . we focus more particularly on STATISTICAL FEATURES globally attached to the region and especially to the probability density functions of IMAGE FEATURES such as the COLOR HISTOGRAM of a region . a THEORETICAL FRAMEWORK is set for the MINIMIZATION OF THE DISTANCE between two HISTOGRAMS for matching or tracking purposes . an application of this THEORETICAL FRAMEWORK to the SEGMENTATION OF COLOR HISTOGRAMS in VIDEO SEQUENCES is then proposed . we briefly describe our NUMERICAL SCHEME and show some experimental results . \n",
            "in this paper , we propose a novel THEORETICAL FRAMEWORK for IMAGE SEGMENTATION . the proposed THEORETICAL FRAMEWORK is based on a THEORETICAL FRAMEWORK to the SEGMENTATION OF COLOR HISTOGRAMS . the proposed THEORETICAL FRAMEWORK is based on the use of HISTOGRAMS to estimate the MINIMIZATION OF THE DISTANCE . the proposed THEORETICAL FRAMEWORK is based on a THEORETICAL FRAMEWORK that uses HISTOGRAMS to estimate the MINIMIZATION OF THE DISTANCE and the MINIMIZATION OF THE DISTANCE . the proposed THEORETICAL FRAMEWORK is based on a THEORETICAL FRAMEWORK that exploits the CLASSICAL CALCULUS OF VARIATION of the IMAGE FEATURES to estimate the MINIMIZATION OF THE DISTANCE . the proposed THEORETICAL FRAMEWORK is applied to the IMAGE SEGMENTATION of the IMAGE SEGMENTATION . the experimental results show that the proposed THEORETICAL FRAMEWORK is able to recover the IMAGE FEATURES of the IMAGE FEATURES , and to estimate the MINIMIZATION OF THE DISTANCE .\n",
            "\n",
            "411 1000\n",
            "despite its success , UNIT SELECTION BASED TEXT-TO-SPEECH SYNTHESIS has has some disadvantages such as sudden discontinuities in SPEECH that <unk> the listeners . the HMM-BASED TTS APPROACH has been increasingly getting more attention from the tts research community . one of the advantage is the lack of spurious errors that are observed in the UNIT SELECTION SCHEME . another advantage of the HMM-BASED TTS APPROACH is the small MEMORY FOOTPRINT requirement which makes HMM-BASED TTS APPROACH attractive for EMBEDDED DEVICES . here , we propose a novel HYBRID STATISTICAL UNIT SELECTION TTS SYSTEM for AGGLUTINATIVE LANGUAGES that aims at improving the quality of the BASELINE HTS SYSTEM while keeping the MEMORY FOOTPRINT small . the INTELLIGI-BILITY AND QUALITY SCORES of the BASELINE HTS SYSTEM are comparable to the mos scores of english reported in the BLIZZARD CHALLENGE TESTS . listeners preferred the BASELINE HTS SYSTEM over the BASELINE HTS SYSTEM in the A/B PREFERENCE TESTS . \n",
            "this paper presents a HYBRID STATISTICAL UNIT SELECTION TTS SYSTEM for AGGLUTINATIVE LANGUAGES in AGGLUTINATIVE LANGUAGES . the proposed HYBRID STATISTICAL UNIT SELECTION TTS SYSTEM is based on a HYBRID STATISTICAL UNIT SELECTION TTS SYSTEM for SPEECH . the proposed HYBRID STATISTICAL UNIT SELECTION TTS SYSTEM is based on a HYBRID STATISTICAL UNIT SELECTION TTS SYSTEM , which is based on a HYBRID STATISTICAL UNIT SELECTION TTS SYSTEM . the proposed HYBRID STATISTICAL UNIT SELECTION TTS SYSTEM is evaluated on the BLIZZARD CHALLENGE TESTS . the experimental results show that the proposed HYBRID STATISTICAL UNIT SELECTION TTS SYSTEM is effective in improving the INTELLIGI-BILITY AND QUALITY SCORES of the BASELINE HTS SYSTEM in terms of INTELLIGI-BILITY AND QUALITY SCORES and MEMORY FOOTPRINT . the performance of the proposed HMM-BASED TTS APPROACH is evaluated on BLIZZARD CHALLENGE TESTS . the performance of the proposed HMM-BASED TTS APPROACH is evaluated on BLIZZARD CHALLENGE TESTS .\n",
            "\n",
            "412 1000\n",
            "we propose an ACTIVE LEARNING APPROACH to training a SEGMENTATION CLASSIFIER that exploits GEOMETRIC PRIORS to <unk> the ANNOTATION PROCESS in 3D IMAGE VOLUMES . to this end , we use these priors not only to select voxels most in need of ANNOTATION but to guarantee that they lie on 2D PLANAR PATCH , which makes it much easier to annotate than if they were randomly distributed in the volume . a simplified version of this ACTIVE LEARNING APPROACH is effective in NATURAL 2D IMAGES . we evaluated our ACTIVE LEARNING APPROACH on ELECTRON MICROSCOPY AND MAGNETIC RESONANCE IMAGE VOLUMES , as well as on NATURAL 2D IMAGES . comparing our ACTIVE LEARNING APPROACH against several accepted baselines demonstrates a marked performance increase . \n",
            "this paper presents a novel ACTIVE LEARNING APPROACH for SEGMENTATION CLASSIFIER based on GEOMETRIC PRIORS . the proposed ACTIVE LEARNING APPROACH is based on the use of GEOMETRIC PRIORS extracted from the 3D IMAGE VOLUMES . the proposed ACTIVE LEARNING APPROACH is based on the use of GEOMETRIC PRIORS in the ANNOTATION PROCESS . the proposed ACTIVE LEARNING APPROACH is based on the use of GEOMETRIC PRIORS in the ANNOTATION PROCESS . the proposed ACTIVE LEARNING APPROACH is evaluated on the ELECTRON MICROSCOPY AND MAGNETIC RESONANCE IMAGE VOLUMES . the experimental results show that the proposed ACTIVE LEARNING APPROACH significantly improves the performance of the proposed ACTIVE LEARNING APPROACH .\n",
            "\n",
            "413 1000\n",
            "template protection is an issue of paramount importance in the design of BIOMETRIC RECOGNITION SYSTEMS . in this paper we present a BIOMETRIC CRYPTOSYSTEM applied to IRIS BIOMETRICS , where TEMPLATE SECURITY is guaranteed by means of a framework inspired by the DIGITAL MODULATION PARADIGM . specifically , the properties of MODULATION CONSTELLATIONS and TURBO CODES with SOFT-DECODING are exploited to design a BIOMETRIC CRYPTOSYSTEM with high performance in terms of both VERIFICATION RATES and SECURITY , even while dealing with a IRIS BIOMETRICS characterized by a high INTRA-CLASS VARIABILITY such as the IRIS . the effectiveness of the proposed BIOMETRIC CRYPTOSYSTEM is evaluated by performing tests on the interval subset of the CASIA-IRISV4 DATABASE . \n",
            "this paper presents a novel approach to BIOMETRIC RECOGNITION SYSTEMS in BIOMETRIC RECOGNITION SYSTEMS . the proposed approach is based on the DIGITAL MODULATION PARADIGM and TURBO CODES . the proposed approach is based on the use of TURBO CODES and MODULATION CONSTELLATIONS in the IRIS . the proposed approach is based on a DIGITAL MODULATION PARADIGM , which is able to deal with INTRA-CLASS VARIABILITY such as IRIS , IRIS , and SECURITY . the experimental results on the CASIA-IRISV4 DATABASE show that the proposed approach achieves better performance than the state-of-the-art methods .\n",
            "\n",
            "414 1000\n",
            "in this work we empirically study the MULTI-SCALE BOUNDARY DETECTION PROBLEM in NATURAL IMAGES . we utilize LOCAL BOUNDARY CUES including CONTRAST , LOCALIZATION and RELATIVE CONTRAST , and train a CLASSIFIER to integrate them across scales . our approach successfully combines strengths from both LARGE-SCALE DETECTION -lrb- robust but POOR LOCALIZATION -rrb- and SMALL-SCALE DETECTION -lrb- <unk> but sensitive to CLUTTER -rrb- . we carry out quantitative evaluations on a variety of BOUNDARY AND OBJECT DATASETS with HUMAN-MARKED GROUNDTRUTH . we show that MULTI-SCALE BOUNDARY DETECTION PROBLEM offers large improvements , ranging from 20 % to 50 % , over SINGLE-SCALE APPROACHES . this is the first time that multi-scale is demonstrated to improve BOUNDARY DETECTION on large datasets of NATURAL IMAGES . \n",
            "this paper addresses the problem of LARGE-SCALE DETECTION and LOCALIZATION in the context of NATURAL IMAGES . we propose a novel method to address the problem of LARGE-SCALE DETECTION and LOCALIZATION . the proposed MULTI-SCALE BOUNDARY DETECTION PROBLEM is based on the use of LOCAL BOUNDARY CUES , which is robust to CLUTTER , LOCALIZATION , and RELATIVE CONTRAST . the MULTI-SCALE BOUNDARY DETECTION PROBLEM is formulated as a MULTI-SCALE BOUNDARY DETECTION PROBLEM and solved using a CLASSIFIER . the proposed approach is evaluated on a variety of BOUNDARY AND OBJECT DATASETS with NATURAL IMAGES , LOCALIZATION , and POOR LOCALIZATION . the experimental results show that the proposed method outperforms the state-of-the-art SINGLE-SCALE APPROACHES .\n",
            "\n",
            "415 1000\n",
            "a key challenge in MULTIAGENT ENVIRONMENTS is the CONSTRUCTION OF AGENTS that are able to learn while acting in the presence of other agents that are simultaneously learning and adapting . these domains require ON-LINE LEARNING METHODS without the benefit of repeated training examples , as well as the ability to adapt to the evolving behavior of other agents in the environment . the difficulty is further exacerbated when the agents are in an ADVERSARIAL RELATIONSHIP , demanding that a robust -lrb- i.e. winning -rrb- NON-STATIONARY POLICY be rapidly learned and adapted . we propose an ON-LINE SEQUENCE LEARNING ALGORITHM , ELPH , based on a straightforward ENTROPY PRUNING TECHNIQUE that is able to rapidly learn and adapt to NON-STATIONARY POLICIES . we demonstrate the performance of this ON-LINE SEQUENCE LEARNING ALGORITHM in a non-stationary learning environment of ADVERSARIAL ZERO-SUM MATRIX GAMES . \n",
            "this paper proposes a novel ON-LINE SEQUENCE LEARNING ALGORITHM called ELPH , which is a generalization of the well-known ON-LINE SEQUENCE LEARNING ALGORITHM . the proposed algorithm is based on a ENTROPY PRUNING TECHNIQUE , called ELPH , which is based on the ENTROPY PRUNING TECHNIQUE . the proposed algorithm is based on a ENTROPY PRUNING TECHNIQUE , called ELPH , which is able to deal with NON-STATIONARY POLICIES . the proposed algorithm is based on a ENTROPY PRUNING TECHNIQUE , called ELPH , which is able to deal with NON-STATIONARY POLICIES . the proposed algorithm is based on a ENTROPY PRUNING TECHNIQUE , called ELPH , which is able to deal with NON-STATIONARY POLICIES . we show that the proposed algorithm is able to achieve better performance in comparison with the state-of-the-art methods .\n",
            "\n",
            "416 1000\n",
            "linear discriminant or <unk> eve transforms are established techniques for MAPPING FEATURES into a LOWER DIMENSIONAL SUBSPACE . this paper introduces a UNIFORM STATISTICAL FRAMEWORK , where the computation of the OPTIMAL FEATURE REDUCTION is formalized as a MAXIMUM-LIKELIHOOD ESTIMATION PROBLEM . the experimental evaluation of this suggested extension of LINEAR SELECTION METHODS shows a slight improvement of the RECOGNITION ACCURACY . \n",
            "this paper addresses the problem of OPTIMAL FEATURE REDUCTION for OPTIMAL FEATURE REDUCTION . in particular , we consider the problem of OPTIMAL FEATURE REDUCTION in the context of OPTIMAL FEATURE REDUCTION . in particular , we consider the problem of OPTIMAL FEATURE REDUCTION , where the goal is to minimize the RECOGNITION ACCURACY of the MAPPING FEATURES . we show that this problem can be solved efficiently using a UNIFORM STATISTICAL FRAMEWORK . we show that the proposed LINEAR SELECTION METHODS can be applied to the problem of OPTIMAL FEATURE REDUCTION . we also show that the proposed LINEAR SELECTION METHODS can be applied to the problem of OPTIMAL FEATURE REDUCTION .\n",
            "\n",
            "417 1000\n",
            "a scheme for BINAURAL PRE-PROCESSING OF SPEECH SIGNALS for input to a standard LINEAR HEARING AID has been investigated . the system is based on that of <unk> & <unk> -lsb- l -rsb- who applied the LEAST MEAN SQUARES ALGORITHM in SUB-BANDS to SPEECH SIGNALS from various ACOUSTIC ENVIRONMENTS and signal to NOISE RATIOS . the processing scheme attempts to take advantage of the multiple inputs to perform NOISE CANCELLATION . the use of SUB-BANDS enables a diverse PROCESSING MECHANISM to be employed , where the WIDE-BAND SIGNAL is split into smaller frequency limited SUB-BANDS , which can subsequently he processed according to their SIGNAL CHARACTERISTICS . the results of a large scale series of intelligibility tests are presented from experiments in which ACOUSTIC SPEECH AND NOISE DATA , generated using SIMULATED AND REAL-ROOM ACOUSTICS was tested on HEARING IMPAIRED VOLUNTEERS . \n",
            "this paper presents a novel method for BINAURAL PRE-PROCESSING OF SPEECH SIGNALS from SPEECH SIGNALS . the proposed method is based on a LEAST MEAN SQUARES ALGORITHM for BINAURAL PRE-PROCESSING OF SPEECH SIGNALS . the proposed method uses a LEAST MEAN SQUARES ALGORITHM to estimate the SIGNAL CHARACTERISTICS of the SUB-BANDS . the proposed method uses a LEAST MEAN SQUARES ALGORITHM to estimate the SIGNAL CHARACTERISTICS of the SUB-BANDS and the NOISE RATIOS of the SUB-BANDS . the proposed method is evaluated in a LINEAR HEARING AID using a LINEAR HEARING AID . the experimental results show that the proposed method outperforms the existing methods in terms of both SIMULATED AND REAL-ROOM ACOUSTICS and NOISE RATIOS .\n",
            "\n",
            "418 1000\n",
            "the ON-CHIP LINE BUFFER dominates the total area and power of LINE-BASED 2-D DWT . therefore , the LINE BUFFER WORDLENGTH has to be carefully designed to maintain the quality level due to the DYNAMIC RANGE growing and the ROUND-OFF ERRORS . in this paper , a complete ANALYSIS METHODOLOGY is proposed to derive the required WORDLENGTH OF LINE BUFFER given the desired quality level of RECONSTRUCTED IMAGE . the proposed ANALYSIS METHODOLOGY can guarantee to avoid OVERFLOW OF COEFFICIENTS , and the difference between predicted and experimental quality level is <unk> 0.06 db in terms of PSNR . \n",
            "this paper presents a novel ANALYSIS METHODOLOGY for the WORDLENGTH OF LINE BUFFER . the proposed ANALYSIS METHODOLOGY is based on a WORDLENGTH OF LINE BUFFER , which is based on the WORDLENGTH OF LINE BUFFER of the RECONSTRUCTED IMAGE . the proposed ANALYSIS METHODOLOGY is based on the use of a LINE-BASED 2-D DWT to estimate the OVERFLOW OF COEFFICIENTS of the RECONSTRUCTED IMAGE . the performance of the proposed ANALYSIS METHODOLOGY is evaluated in terms of both PSNR and PSNR . the results show that the proposed method is robust to ROUND-OFF ERRORS , and is robust to ROUND-OFF ERRORS .\n",
            "\n",
            "419 1000\n",
            "our understanding of the INPUT-OUTPUT FUNCTION OF SINGLE CELLS has been substantially advanced by BIOPHYSICALLY ACCURATE MULTI-COMPARTMENTAL MODELS . the large number of parameters needing HAND TUNING in these BIOPHYSICALLY ACCURATE MULTI-COMPARTMENTAL MODELS has , however , somewhat hampered their applicability and <unk> . here we propose a simple and well-founded method for AUTOMATIC ESTIMATION of many of these key parameters : 1 -rrb- the SPATIAL DISTRIBUTION OF CHANNEL DENSITIES on the cell 's <unk> ; 2 -rrb- the SPATIOTEMPORAL PATTERN OF SYNAPTIC INPUT ; 3 -rrb- the channels ' REVERSAL POTENTIALS ; 4 -rrb- the IN-TERCOMPARTMENTAL CONDUCTANCES ; and 5 -rrb- the NOISE LEVEL in each <unk> . we assume experimental access to : a -rrb- the SPATIOTEMPORAL VOLTAGE SIGNAL in the <unk> -lrb- or some contiguous <unk> thereof , e.g. via VOLTAGE SENSITIVE IMAGING TECHNIQUES -rrb- , b -rrb- an approximate <unk> description of the channels and synapses present in each <unk> , and c -rrb- the morphology of the part of the neuron under investigation . the key observation is that , given data a -rrb- - c -rrb- , all of the parameters 1 -rrb- -4 -rrb- may be simultaneously inferred by a version of CONSTRAINED LINEAR REGRESSION ; this CONSTRAINED LINEAR REGRESSION , in turn , is efficiently solved using standard algorithms , without any '' local minima '' problems despite the large number of parameters and complex dynamics . the NOISE LEVEL 5 -RRB- may also be estimated by standard techniques . we demonstrate the method 's ACCURACY on several MODEL DATASETS , and describe techniques for quantifying the uncertainty in our estimates . \n",
            "this paper addresses the problem of AUTOMATIC ESTIMATION in the presence of NOISE LEVEL and NOISE LEVEL . in particular , we propose a novel approach to the problem of AUTOMATIC ESTIMATION . the proposed approach is based on the use of a SPATIOTEMPORAL PATTERN OF SYNAPTIC INPUT and a SPATIOTEMPORAL PATTERN OF SYNAPTIC INPUT . the proposed approach is based on the use of REVERSAL POTENTIALS and SPATIOTEMPORAL PATTERN OF SYNAPTIC INPUT . the proposed approach is based on the use of a SPATIOTEMPORAL PATTERN OF SYNAPTIC INPUT and a SPATIOTEMPORAL PATTERN OF SYNAPTIC INPUT . the proposed approach is evaluated on a variety of MODEL DATASETS and MODEL DATASETS . the results show that the proposed method is effective in improving the ACCURACY and ACCURACY of the proposed method .\n",
            "\n",
            "420 1000\n",
            "we present a novel DIRECTIONALLY ADAPTIVE IMAGE INTERPOLATION based on a MULTIPLE-DIRECTION WAVELET TRANSFORM , called DIRECTIONLETS . the DIRECTIONALLY ADAPTIVE IMAGE INTERPOLATION uses DIRECTIONLETS to efficiently capture DIRECTIONAL FEATURES and to extract EDGE INFORMATION along different directions from the LOW-RESOLUTION IMAGE . then , the HIGH-RESOLUTION IMAGE is generated using this information to preserve SHARPNESS OF DETAILS . our DIRECTIONALLY ADAPTIVE IMAGE INTERPOLATION outperforms the state-of-the-art methods in terms of both NUMERIC AND VISUAL QUALITY of the INTERPOLATED IMAGE . \n",
            "in this paper , we propose a novel DIRECTIONALLY ADAPTIVE IMAGE INTERPOLATION , called DIRECTIONLETS , for DIRECTIONALLY ADAPTIVE IMAGE INTERPOLATION based on MULTIPLE-DIRECTION WAVELET TRANSFORM . the DIRECTIONALLY ADAPTIVE IMAGE INTERPOLATION is based on a MULTIPLE-DIRECTION WAVELET TRANSFORM , a MULTIPLE-DIRECTION WAVELET TRANSFORM , and a HIGH-RESOLUTION IMAGE . the MULTIPLE-DIRECTION WAVELET TRANSFORM is represented by a MULTIPLE-DIRECTION WAVELET TRANSFORM , which is a HIGH-RESOLUTION IMAGE . the proposed DIRECTIONALLY ADAPTIVE IMAGE INTERPOLATION is based on a MULTIPLE-DIRECTION WAVELET TRANSFORM , which is based on a MULTIPLE-DIRECTION WAVELET TRANSFORM . the proposed DIRECTIONALLY ADAPTIVE IMAGE INTERPOLATION is applied to the NUMERIC AND VISUAL QUALITY of the MULTIPLE-DIRECTION WAVELET TRANSFORM , and the results show that the proposed DIRECTIONALLY ADAPTIVE IMAGE INTERPOLATION outperforms the state-of-the-art methods in terms of both NUMERIC AND VISUAL QUALITY and SHARPNESS OF DETAILS .\n",
            "\n",
            "421 1000\n",
            "we propose two approaches for improving the OBJECTIVE FUNCTION for the DEEP NEURAL NETWORK FRAME-LEVEL TRAINING in LARGE VOCABULARY CONTINUOUS SPEECH RECOGNITION . the LARGE VOCABULARY CONTINUOUS SPEECH RECOGNITION used in LARGE VOCABULARY CONTINUOUS SPEECH RECOGNITION are often constructed with an OUTPUT LAYER with SOFTMAX ACTIVATION and the CROSS-ENTROPY OBJECTIVE FUNCTION is always employed in the <unk> training of LARGE VOCABULARY CONTINUOUS SPEECH RECOGNITION . the pairing of SOFTMAX ACTIVATION and CROSS-ENTROPY OBJECTIVE FUNCTION contributes much in the success of LARGE VOCABULARY CONTINUOUS SPEECH RECOGNITION . the first approach developed in this paper improves the CROSS-ENTROPY OBJECTIVE FUNCTION by boosting the importance of the frames for which the LARGE VOCABULARY CONTINUOUS SPEECH RECOGNITION has low target predictions -lrb- low target posterior probabilities -rrb- and the second one considers jointly minimizing the <unk> and maximizing the LOG POSTERIOR RATIO between the target <unk> -lrb- <unk> states -rrb- and the most competing one . experiments on SWITCHBOARD TASK demonstrate that the two proposed methods can provide 3.1 % and 1.5 % RELATIVE WORD ERROR RATE REDUCTION , respectively , against the already very strong conventional CROSS-ENTROPY TRAINED DNN SYSTEM . \n",
            "this paper addresses the problem of DEEP NEURAL NETWORK FRAME-LEVEL TRAINING for LARGE VOCABULARY CONTINUOUS SPEECH RECOGNITION in LARGE VOCABULARY CONTINUOUS SPEECH RECOGNITION . in this paper , we propose a novel approach to DEEP NEURAL NETWORK FRAME-LEVEL TRAINING based on a CROSS-ENTROPY OBJECTIVE FUNCTION and a CROSS-ENTROPY OBJECTIVE FUNCTION . the proposed approach is based on the use of a CROSS-ENTROPY OBJECTIVE FUNCTION and a CROSS-ENTROPY OBJECTIVE FUNCTION . the proposed method is evaluated on a SWITCHBOARD TASK using a CROSS-ENTROPY TRAINED DNN SYSTEM and a CROSS-ENTROPY OBJECTIVE FUNCTION . the experimental results show that the proposed CROSS-ENTROPY TRAINED DNN SYSTEM is effective in improving the RELATIVE WORD ERROR RATE REDUCTION of the CROSS-ENTROPY TRAINED DNN SYSTEM .\n",
            "\n",
            "422 1000\n",
            "in this paper , we compare and validate different PROBABILISTIC MODELS of HUMAN HEART BEAT INTERVALS for assessment of the ELECTROCARDIOGRAM DATA recorded with varying conditions in <unk> and <unk> <unk> <unk> . the models are validated using the ADAPTIVE POINT PROCESS FILTERING PARADIGM and KOLMOGOROV-SMIRNOV TEST . the INVERSE GAUSSIAN MODEL was found to achieve the overall best performance in the ANALYSIS OF AUTONOMIC CONTROL . we further improve the INVERSE GAUSSIAN MODEL by incorporating the RESPIRATORY COVARIATE MEASUREMENTS and present DYNAMIC RESPIRATORY SINUS ARRHYTHMIA ANALYSIS . our results suggest the INSTANTANEOUS RSA GAIN computed from our proposed INVERSE GAUSSIAN MODEL as a potential INDEX OF VAGAL CONTROL DYNAMICS . \n",
            "this paper addresses the problem of ANALYSIS OF AUTONOMIC CONTROL and ANALYSIS OF AUTONOMIC CONTROL . we propose a novel approach to ANALYSIS OF AUTONOMIC CONTROL and ANALYSIS OF AUTONOMIC CONTROL . the proposed approach is based on the use of a INVERSE GAUSSIAN MODEL and a KOLMOGOROV-SMIRNOV TEST . the proposed approach is based on the use of a INVERSE GAUSSIAN MODEL and a KOLMOGOROV-SMIRNOV TEST . the proposed approach is based on the use of a INVERSE GAUSSIAN MODEL and a KOLMOGOROV-SMIRNOV TEST . the experimental results demonstrate the effectiveness of the proposed method .\n",
            "\n",
            "423 1000\n",
            "we examine the use of HIDDEN MARKOV AND HIDDEN SEMI-MARKOV MODELS for automatically segmenting an ELECTROCARDIOGRAM WAVEFORM into its CONSTITUENT WAVEFORM FEATURES . an UNDECIMATED WAVELET TRANSFORM is used to generate an OVERCOMPLETE REPRESENTATION OF THE SIGNAL that is more appropriate for subsequent modelling . we show that the STATE DURATIONS implicit in a standard HIDDEN MARKOV MODEL are ill-suited to those of REAL ECG FEATURES , and we investigate the use of HIDDEN SEMI-MARKOV MODELS for improved STATE DURATION MODELLING . \n",
            "this paper proposes a novel approach to STATE DURATION MODELLING based on a HIDDEN MARKOV MODEL . the proposed method is based on a HIDDEN MARKOV MODEL , which is based on a HIDDEN MARKOV MODEL . the proposed method is based on a HIDDEN MARKOV MODEL , which is based on a HIDDEN MARKOV MODEL . the proposed method is based on a HIDDEN MARKOV MODEL , which is based on a HIDDEN MARKOV MODEL . the experimental results show that the proposed method outperforms the state-of-the-art methods in terms of both STATE DURATIONS and STATE DURATIONS .\n",
            "\n",
            "424 1000\n",
            "in the present study we explore the implication of HIGH AND LOW LEVEL MECHANISMS in DEGRADED SPEECH COMPREHENSION in normal hearing subjects . in experiment 1 we compared the loss of intelligibility due to the increasing size of REVERSION WINDOWS in both words and PSEUDOWORDS . results showed that words are generally reconstructed better than PSEUDOWORDS , suggesting the existence of a LEXICAL BENEFIT in DEGRADED SPEECH RESTORATION . moreover , there was greater variability between individuals when reconstructing PSEUDOWORDS than words . in experiment 2 , we demonstrated that this INTERINDIVIDUAL VARIABILITY correlated with the subjects ' MEDIAL OLIVOCOCHLEAR BUNDLE FUNCTIONALITY , as measured by CONTRALATERAL SUPPRESSION OF OTOACOUSTIC EMISSIONS . together these experiments highlight the importance of LOW-LEVEL AUDITORY MECHANISMS in DEGRADED SPEECH RESTORATION . moreover they put forward the existence of major INTERINDIVIDUAL VARIABILITY in the capacity to reconstruct DEGRADED SPEECH , which correlates with the PHYSIOLOGICAL PROPERTIES of the AUDITORY SYSTEM -lrb- low-level property -rrb- . in addition , our results also suggest the existence of multiple HIGHER-LEVEL STRATEGIES that can compensate on-line for the lack of information caused by SPEECH DEGRADATION . \n",
            "this paper presents a novel approach to DEGRADED SPEECH COMPREHENSION based on LOW-LEVEL AUDITORY MECHANISMS . the proposed approach is based on the use of LOW-LEVEL AUDITORY MECHANISMS for DEGRADED SPEECH RESTORATION . the proposed approach is based on the use of LOW-LEVEL AUDITORY MECHANISMS to estimate the INTERINDIVIDUAL VARIABILITY of the speech signal . the proposed method is based on the use of REVERSION WINDOWS in the AUDITORY SYSTEM . the proposed approach is based on the use of LOW-LEVEL AUDITORY MECHANISMS to estimate the INTERINDIVIDUAL VARIABILITY and the PHYSIOLOGICAL PROPERTIES . the proposed approach is evaluated on a variety of DEGRADED SPEECH . the results show that the proposed method is effective in improving the performance of DEGRADED SPEECH RESTORATION .\n",
            "\n",
            "425 1000\n",
            "in this paper , an accurate and robust positioning system based on STREET VIEW RECOGNITION is introduced . VISION-BASED TECHNIQUE is employed for DYNAMICALLY RECOGNIZING SHOP OR BUILDING SIGNS on the GPS MAP . two mechanisms including VIEW-ANGLE INVARIANT DISTANCE ESTIMATION and PATH REFINEMENT are proposed for ROBUST AND ACCURATE POSITION ESTIMATION . through the combination of VISUAL RECOGNITION TECHNIQUE and GPS SCALE DATA , the REAL USER LOCATION can be accurately inferred . experimental results demonstrate that the proposed system is reliable and feasible . compared with <unk> error of position estimation provided by the gps , our system only has 0.97 M ERROR ESTIMATION . \n",
            "this paper presents a novel VISION-BASED TECHNIQUE for DYNAMICALLY RECOGNIZING SHOP OR BUILDING SIGNS . the proposed VISION-BASED TECHNIQUE is based on the use of GPS SCALE DATA and GPS SCALE DATA . the proposed method is based on a VISION-BASED TECHNIQUE for ROBUST AND ACCURATE POSITION ESTIMATION . the proposed method is based on the use of a VISION-BASED TECHNIQUE and VIEW-ANGLE INVARIANT DISTANCE ESTIMATION . the performance of the proposed method is evaluated on a variety of GPS SCALE DATA and GPS SCALE DATA . the results show that the proposed method achieves better performance than the state-of-the-art methods .\n",
            "\n",
            "426 1000\n",
            "we present a new approach to QUASI TEXT-INDEPENDENT SPEAKER VERIFICATION based on PATTERN MATCHING . our method first seeks PHONETICALLY MATCHED SEGMENTS in two SPEECH SIGNALS . for all aligned frame pairs of these segments we compute the probability that they were uttered by the same speaker . based on these FRAME-LEVEL PROBABILITIES we take the decision whether the two signals were spoken by the same speaker or not . our method to find PHONETICALLY MATCHED SEGMENTS does not depend on a SPEECH RECOGNIZER . we show that our system performs better than a baseline speaker verification system based on GAUSSIAN MIXTURE MODELS when the signals are long enough . especially interesting is the fact that a combination of the devised system with the baseline system performs much better than either of the systems alone . \n",
            "in this paper , we propose a novel approach to QUASI TEXT-INDEPENDENT SPEAKER VERIFICATION in SPEECH SIGNALS . the proposed approach is based on the use of GAUSSIAN MIXTURE MODELS , which is able to deal with SPEECH SIGNALS in the presence of SPEECH SIGNALS . the proposed approach is based on the use of GAUSSIAN MIXTURE MODELS , which is able to deal with SPEECH SIGNALS in the presence of SPEECH SIGNALS . the proposed approach is evaluated on a variety of SPEECH SIGNALS . the experimental results show that the proposed approach is effective in improving the performance of QUASI TEXT-INDEPENDENT SPEAKER VERIFICATION .\n",
            "\n",
            "427 1000\n",
            "understanding common sense reasoning about the PHYSICAL WORLD is one of the goals of QUALITATIVE REASONING RESEARCH . this paper describes how we combine QUALITATIVE MECHANICS and ANALOGY to solve EVERYDAY PHYSICAL REASONING PROBLEMS posed as sketches . the problems are drawn from the BENNETT MECHANICAL COMPREHENSION TEST , which is used to evaluate <unk> candidates . we discuss SKETCH ANNOTATIONS , which define CONCEPTUAL QUANTITIES in terms of visual measurements , how MODELING DECISIONS are made by ANALOGY , and how ANALOGY can be used to frame COMPARATIVE ANALYSIS PROBLEMS . experimental results support the plausibility of this approach . \n",
            "this paper addresses the problem of UNDERSTANDING COMMON SENSE REASONING in EVERYDAY PHYSICAL REASONING PROBLEMS . in particular , we focus on the problem of UNDERSTANDING COMMON SENSE REASONING in QUALITATIVE MECHANICS . we propose a novel approach to UNDERSTANDING COMMON SENSE REASONING in QUALITATIVE MECHANICS , which is based on ANALOGY and ANALOGY . we show that the proposed algorithm can be applied to a wide range of EVERYDAY PHYSICAL REASONING PROBLEMS , such as UNDERSTANDING COMMON SENSE REASONING , and ANALOGY . we show that the proposed algorithm is able to perform well on a variety of EVERYDAY PHYSICAL REASONING PROBLEMS including ANALOGY , ANALOGY , and QUALITATIVE MECHANICS .\n",
            "\n",
            "428 1000\n",
            "in this paper we present new research in TRANSLATION ASSISTANCE . we describe a system capable of translating NATIVE LANGUAGE FRAGMENTS to FOREIGN LANGUAGE FRAGMENTS in an L2 CONTEXT . practical applications of this research can be framed in the context of SECOND LANGUAGE LEARNING . the type of TRANSLATION ASSISTANCE SYSTEM under investigation here encourages LANGUAGE LEARNERS to write in their target language while allowing them to fall back to their native language in case the correct word or expression is not known . these CODE SWITCHES are subsequently translated to L2 given the L2 CONTEXT . we study the feasibility of exploiting CROSS-LINGUAL CONTEXT to obtain HIGH-QUALITY TRANSLATION SUGGESTIONS that improve over STATISTICAL LANGUAGE MODELLING and WORD-SENSE DIS-AMBIGUATION BASELINES . a CLASSIFICATION-BASED APPROACH is presented that is indeed found to improve significantly over these baselines by making use of a CONTEX-TUAL WINDOW spanning a small number of neighbouring words . \n",
            "this paper presents a novel approach to STATISTICAL LANGUAGE MODELLING based on a CLASSIFICATION-BASED APPROACH . the proposed approach is based on the use of SECOND LANGUAGE LEARNING and HIGH-QUALITY TRANSLATION SUGGESTIONS . the proposed approach is based on the use of a CLASSIFICATION-BASED APPROACH and a CLASSIFICATION-BASED APPROACH . the proposed approach is based on the use of a CLASSIFICATION-BASED APPROACH and a CLASSIFICATION-BASED APPROACH . the proposed approach is based on the use of a CLASSIFICATION-BASED APPROACH and a CLASSIFICATION-BASED APPROACH . the proposed approach is evaluated on a variety of LANGUAGE LEARNERS and WORD-SENSE DIS-AMBIGUATION BASELINES . the results show that the proposed approach is effective in improving the L2 and the WORD-SENSE DIS-AMBIGUATION BASELINES .\n",
            "\n",
            "429 1000\n",
            "this paper describes the realization of a CORPUS-BASED CHINESE SPEECH SYNTHESIS SYSTEM , including the CORPUS DESIGN and UNIT SELECTION PROCEDURE . the CORPUS-BASED CHINESE SPEECH SYNTHESIS SYSTEM selects the SYNTHESIS UNIT according to CONTEXT SIMILARITY between target unit and candidate unit . neither PROSODY PARAMETER PREDICTION nor PROSODY FEATURE MODIFICATION is needed . the informal test shows that the SYNTHESIZED SPEECH is quite natural , and the speaking style of original speaker is preserved because units are all from the SPEAKER 'S UTTERANCES . \n",
            "this paper describes a CORPUS-BASED CHINESE SPEECH SYNTHESIS SYSTEM called the CORPUS-BASED CHINESE SPEECH SYNTHESIS SYSTEM , a CORPUS-BASED CHINESE SPEECH SYNTHESIS SYSTEM , a CORPUS-BASED CHINESE SPEECH SYNTHESIS SYSTEM , a UNIT SELECTION PROCEDURE and a UNIT SELECTION PROCEDURE . the CORPUS-BASED CHINESE SPEECH SYNTHESIS SYSTEM is based on a CORPUS-BASED CHINESE SPEECH SYNTHESIS SYSTEM , a UNIT SELECTION PROCEDURE and a UNIT SELECTION PROCEDURE . the proposed CORPUS-BASED CHINESE SPEECH SYNTHESIS SYSTEM is based on a CORPUS-BASED CHINESE SPEECH SYNTHESIS SYSTEM and a UNIT SELECTION PROCEDURE , a UNIT SELECTION PROCEDURE and a UNIT SELECTION PROCEDURE . the performance of the proposed CORPUS-BASED CHINESE SPEECH SYNTHESIS SYSTEM is evaluated on a variety of SYNTHESIZED SPEECH .\n",
            "\n",
            "430 1000\n",
            "the success of an IMAGE CLASSIFICATION ALGORITHM largely depends on how it incorporates LOCAL INFORMATION in the GLOBAL DECISION . popular approaches such as AVERAGE-POOLING and MAX-POOLING are suboptimal in many situations . in this paper we propose region ranking svm -lrb- <unk> -rrb- , a novel method for POOLING LOCAL INFORMATION from multiple regions . <unk> exploits the correlation of LOCAL REGIONS in an IMAGE , and it jointly learns a REGION EVALUATION FUNCTION and a scheme for integrating multiple regions . experiments on pascal voc 2007 , voc 2012 , and ILSVRC2014 DATASETS show that <unk> outperforms the methods that use the same FEATURE TYPE and extract features from the same set of LOCAL REGIONS . <unk> achieves similar to or better than the state-of-the-art performance on all datasets . \n",
            "in this paper , we propose a novel method to incorporate POOLING LOCAL INFORMATION into a GLOBAL DECISION . the proposed method consists of two steps : -lrb- 1 -rrb- a GLOBAL DECISION that consists of a set of LOCAL REGIONS , and -lrb- 2 -rrb- a GLOBAL DECISION that captures the LOCAL INFORMATION of the IMAGE . the proposed method consists of two steps : -lrb- 1 -rrb- a GLOBAL DECISION , a GLOBAL DECISION , and -lrb- 2 -rrb- a GLOBAL DECISION . the proposed method consists of two steps : -lrb- 1 -rrb- a GLOBAL DECISION , and -lrb- 2 -rrb- an IMAGE CLASSIFICATION ALGORITHM . the proposed method consists of two steps : -lrb- 1 -rrb- a GLOBAL DECISION , and -lrb- 2 -rrb- an IMAGE CLASSIFICATION ALGORITHM . the experimental results show that the proposed method outperforms the state-of-the-art methods in terms of both MAX-POOLING and MAX-POOLING .\n",
            "\n",
            "431 1000\n",
            "ordinal regression is an important research topic in MACHINE LEARNING . it aims to automatically determine the IMPLIED RATING of a DATA ITEM on a FIXED , DISCRETE RATING SCALE . in this paper , we present a novel ORDINAL REGRESSION APPROACH via MANIFOLD LEARNING , which is capable of uncovering the EMBEDDED NONLINEAR STRUCTURE of the data set according to the observations in the HIGH-DIMENSIONAL FEATURE SPACE . by optimizing the order information of the observations and preserving the INTRINSIC GEOMETRY of the data set simultaneously , the proposed ORDINAL REGRESSION APPROACH provides the faithful ORDINAL REGRESSION to the new coming data points . to offer more general solution to the data with NATURAL TENSOR STRUCTURE , we further introduce the multilinear extension of the proposed ORDINAL REGRESSION APPROACH , which can support the ORDINAL REGRESSION OF HIGH ORDER DATA like IMAGES . experiments on various DATA SETS validate the effectiveness of the proposed ORDINAL REGRESSION APPROACH as well as its extension . \n",
            "this paper proposes a novel ORDINAL REGRESSION APPROACH for ORDINAL REGRESSION OF HIGH ORDER DATA . the proposed ORDINAL REGRESSION APPROACH is based on the use of ORDINAL REGRESSION to estimate the INTRINSIC GEOMETRY of the DATA ITEM . the proposed ORDINAL REGRESSION APPROACH is based on the use of MANIFOLD LEARNING to estimate the INTRINSIC GEOMETRY of the DATA ITEM . the proposed ORDINAL REGRESSION APPROACH is based on the use of ORDINAL REGRESSION to estimate the INTRINSIC GEOMETRY of the DATA ITEM . the proposed ORDINAL REGRESSION APPROACH is evaluated on two DATA SETS . the proposed ORDINAL REGRESSION APPROACH is evaluated on two DATA SETS . the experimental results show that the proposed ORDINAL REGRESSION APPROACH is effective in ORDINAL REGRESSION OF HIGH ORDER DATA .\n",
            "\n",
            "432 1000\n",
            "causal structure learning from TIME SERIES DATA is a major scientific challenge . EXTANT ALGORITHMS assume that measurements occur sufficiently quickly ; more precisely , they assume approximately equal system and measurement timescales . in many domains , however , measurements occur at a significantly slower rate than the underlying system changes , but the size of the TIMESCALE MISMATCH is often unknown . this paper develops three CAUSAL STRUCTURE LEARNING ALGORITHMS , each of which discovers all DYNAMIC CAUSAL GRAPHS that explain the OBSERVED MEASUREMENT DATA , perhaps given UNDERSAMPLING . that is , these CAUSAL STRUCTURE LEARNING ALGORITHMS all learn CAUSAL STRUCTURE in a `` <unk> '' manner : they do not assume any particular relation between the measurement and system timescales . we apply these CAUSAL STRUCTURE LEARNING ALGORITHMS to data from simulations to gain insight into the challenge of UNDERSAMPLING . \n",
            "this paper addresses the problem of CAUSAL STRUCTURE LEARNING for TIME SERIES DATA . we propose a novel approach to CAUSAL STRUCTURE LEARNING based on CAUSAL STRUCTURE LEARNING for CAUSAL STRUCTURE LEARNING . the proposed approach is based on the use of TIME SERIES DATA to estimate the CAUSAL STRUCTURE . the proposed approach is based on the use of TIME SERIES DATA to estimate the CAUSAL STRUCTURE . the proposed method is applied to the problem of TIMESCALE MISMATCH from TIME SERIES DATA . the experimental results show that the proposed method is effective in improving the performance of the proposed CAUSAL STRUCTURE LEARNING ALGORITHMS .\n",
            "\n",
            "433 1000\n",
            "drawing a sample from a DISCRETE DISTRIBUTION is one of the building components for MONTE CARLO METHODS . like other SAMPLING ALGORITHMS , DISCRETE DISTRIBUTION also suffers from high computational burden in LARGE-SCALE INFERENCE PROBLEMS . we study the problem of sampling a DISCRETE RANDOM VARIABLE with a high degree of dependency that is typical in LARGE-SCALE BAYESIAN INFERENCE and GRAPHICAL MODELS , and propose an efficient APPROXIMATE SOLUTION with a SUBSAMPLING APPROACH . we make a novel connection between the DISCRETE DISTRIBUTION and MULTI-ARMED BANDITS PROBLEMS with a FINITE REWARD POPULATION and provide three algorithms with THEORETICAL GUARANTEES . empirical evaluations show the ROBUSTNESS and efficiency of the APPROXIMATE ALGORITHMS in both SYNTHETIC AND REAL-WORLD LARGE-SCALE PROBLEMS . \n",
            "this paper addresses the problem of LARGE-SCALE BAYESIAN INFERENCE in the presence of LARGE-SCALE INFERENCE PROBLEMS . we propose a novel SUBSAMPLING APPROACH for LARGE-SCALE BAYESIAN INFERENCE . the proposed APPROXIMATE ALGORITHMS is based on a DISCRETE RANDOM VARIABLE , which is a DISCRETE RANDOM VARIABLE of the DISCRETE DISTRIBUTION . the proposed APPROXIMATE ALGORITHMS is based on a DISCRETE RANDOM VARIABLE , which is a FINITE REWARD POPULATION of the DISCRETE DISTRIBUTION . the proposed APPROXIMATE ALGORITHMS is evaluated on both SYNTHETIC AND REAL-WORLD LARGE-SCALE PROBLEMS and MULTI-ARMED BANDITS PROBLEMS . the experimental results show that the proposed THEORETICAL GUARANTEES can significantly improve the ROBUSTNESS and ROBUSTNESS of the proposed APPROXIMATE ALGORITHMS .\n",
            "\n",
            "434 1000\n",
            "in this paper , a system for OVERLAPPING ACOUSTIC EVENT DETECTION is proposed , which models the TEMPORAL EVOLUTION OF SOUND EVENTS . the system is based on PROBABILISTIC LATENT COMPONENT ANALYSIS , supporting the use of a SOUND EVENT DICTIONARY where each exemplar consists of a SUCCESSION OF SPECTRAL TEMPLATES . the TEMPORAL SUCCESSION OF THE TEMPLATES is controlled through EVENT CLASS-WISE HIDDEN MARKOV MODELS . as INPUT TIME/FREQUENCY REPRESENTATION , the EQUIVALENT RECTANGULAR BANDWIDTH SPECTROGRAM is used . experiments are carried out on POLYPHONIC DATASETS OF OFFICE SOUNDS generated using an ACOUSTIC SCENE SIMULATOR , as well as REAL AND SYNTHESIZED MONOPHONIC DATASETS for comparative purposes . results show that the proposed system outperforms several state-of-the-art methods for OVERLAPPING ACOUSTIC EVENT DETECTION on the same task , using both FRAME-BASED AND EVENT-BASED METRICS , and is robust to varying event density and noise levels . \n",
            "in this paper , we propose a novel approach to OVERLAPPING ACOUSTIC EVENT DETECTION from POLYPHONIC DATASETS OF OFFICE SOUNDS . the proposed approach is based on the use of a SOUND EVENT DICTIONARY and a SOUND EVENT DICTIONARY for OVERLAPPING ACOUSTIC EVENT DETECTION . the proposed method is based on the use of a SOUND EVENT DICTIONARY and a SOUND EVENT DICTIONARY for OVERLAPPING ACOUSTIC EVENT DETECTION . the proposed method is based on the use of a SOUND EVENT DICTIONARY and a SOUND EVENT DICTIONARY . the experimental results show that the proposed method outperforms the state-of-the-art methods in terms of OVERLAPPING ACOUSTIC EVENT DETECTION .\n",
            "\n",
            "435 1000\n",
            "here we show that reproducing the FUNCTIONAL PROPERTIES OF MT CELLS with various center -- surround interactions enriches MOTION REPRESENTATION and improves the ACTION RECOGNITION performance . to do so , we propose a simplified <unk> -- inspired model of the MOTION PATHWAY in primates : it is a FEEDFORWARD MODEL restricted to V1-MT CORTICAL LAYERS , CORTICAL CELLS cover the VISUAL SPACE with a FOVEATED STRUCTURE and , more importantly , we reproduce some of the richness of CENTER-SURROUND INTERACTIONS OF MT CELLS . interestingly , as observed in NEUROPHYSIOLOGY , our MT CELLS not only behave like simple VELOCITY DETECTORS , but also respond to several kinds of MOTION CONTRASTS . results show that this diversity of MOTION REPRESENTATION at the MT LEVEL is a major advantage for an ACTION RECOGNITION TASK . DEFINING MOTION MAPS as our FEATURE VECTORS , we used a standard CLASSIFICATION METHOD on the WEIZMANN DATABASE : we obtained an AVERAGE RECOGNITION RATE of <unk> % , which is superior to the recent results by <unk> et al. -lrb- 2007 -rrb- . these promising results encourage us to further develop BIO -- INSPIRED MODELS incorporating other BRAIN MECHANISMS and cortical layers in order to deal with more COMPLEX VIDEOS . \n",
            "in this paper , we propose a novel CLASSIFICATION METHOD for ACTION RECOGNITION from COMPLEX VIDEOS . the proposed CLASSIFICATION METHOD is based on the use of MT CELLS extracted from the MOTION PATHWAY . the proposed CLASSIFICATION METHOD is based on the use of a FEEDFORWARD MODEL to estimate the FUNCTIONAL PROPERTIES OF MT CELLS . the proposed CLASSIFICATION METHOD is based on the use of a FEEDFORWARD MODEL to estimate the FOVEATED STRUCTURE of the MT CELLS . the proposed CLASSIFICATION METHOD is evaluated on a WEIZMANN DATABASE and a WEIZMANN DATABASE of the WEIZMANN DATABASE . the results show that the proposed CLASSIFICATION METHOD is effective in ACTION RECOGNITION and ACTION RECOGNITION . the proposed CLASSIFICATION METHOD is evaluated on the WEIZMANN DATABASE and the results show that the proposed CLASSIFICATION METHOD is effective in improving the AVERAGE RECOGNITION RATE of the WEIZMANN DATABASE .\n",
            "\n",
            "436 1000\n",
            "in GAME-PLAYING PROGRAMS relying on the MINIMAX PRINCIPLE , DEEPER SEARCHES generally produce better evaluations . theoretical analyses , however , suggest that in many cases <unk> amplifies the NOISE introduced by the HEURISTIC FUNCTION used to evaluate the leaves of the GAME TREE , leading to what is known as PATHOLOGICAL BEHAVIOR , where DEEPER SEARCHES produce worse evaluations . in most of the previous research , positions were evaluated as losses or wins . dependence between the values of positions close to each other was identified as the property of realistic GAME TREES that eliminates the PATHOLOGY and explains why MINIMAX is successful in practice . in this paper we present an alternative explanation that does not rely on VALUE DEPENDENCE . we show that if real numbers are used for POSITION VALUES , POSITION VALUES tend to be further apart at lower levels of the GAME TREE , which leads to a larger proportion of more extreme positions , where error is less probable . decreased probability of error in searches to greater depths is sufficient to eliminate the PATHOLOGY and no additional properties of GAME TREES are required . \n",
            "in this paper , we propose a novel method for GAME-PLAYING PROGRAMS . the proposed method is based on the MINIMAX PRINCIPLE of the GAME TREE of the GAME TREE of the GAME TREE . the proposed method is based on the use of a MINIMAX PRINCIPLE to estimate the VALUE DEPENDENCE and the VALUE DEPENDENCE of the GAME TREE . the proposed method is based on the MINIMAX PRINCIPLE of the GAME TREE of the GAME TREE and the VALUE DEPENDENCE of the GAME TREE . the proposed method is evaluated on a number of GAME-PLAYING PROGRAMS . the results show that the proposed method is effective in reducing the number of NOISE in the presence of NOISE .\n",
            "\n",
            "437 1000\n",
            "we address the problem of <unk> i.e. deciding whether there exists a CONSISTENT ESTIMATOR of a given relation q , when data are missing not at random . we employ a FORMAL REPRESENTATION called ` MISSINGNESS GRAPHS ' to explicitly <unk> the CAUSAL MECHANISMS responsible for MISSINGNESS and to encode dependencies between these CAUSAL MECHANISMS and the variables being measured . using this FORMAL REPRESENTATION , we derive conditions that the GRAPH should satisfy to ensure <unk> and devise algorithms to detect the presence of these conditions in the GRAPH . \n",
            "in this paper , we propose a novel FORMAL REPRESENTATION , called ` MISSINGNESS GRAPHS , for ` MISSINGNESS GRAPHS . the proposed FORMAL REPRESENTATION is based on a FORMAL REPRESENTATION , called ` MISSINGNESS GRAPHS , which is a generalization of the GRAPH to the GRAPH . the algorithm is based on the use of a FORMAL REPRESENTATION , called ` MISSINGNESS GRAPHS . the algorithm is based on a FORMAL REPRESENTATION , called ` MISSINGNESS GRAPHS . the algorithm is based on a FORMAL REPRESENTATION , which is a generalization of the GRAPH to the GRAPH . the performance of the proposed algorithm is demonstrated on a number of examples .\n",
            "\n",
            "438 1000\n",
            "in this paper we propose a class of efficient GENERALIZED METHOD-OF-MOMENTS ALGORITHMS for computing parameters of the PLACKETT-LUCE MODEL , where the data consists of FULL RANKINGS over alternatives . our GENERALIZED METHOD-OF-MOMENTS ALGORITHMS is based on breaking the FULL RANKINGS into PAIRWISE COMPARISONS , and then computing parameters that satisfy a set of GENERALIZED MOMENT CONDITIONS . we identify conditions for the output of GENERALIZED METHOD-OF-MOMENTS ALGORITHMS to be unique , and identify a general class of CONSISTENT AND INCONSISTENT BREAKINGS . we then show by theory and experiments that our GENERALIZED METHOD-OF-MOMENTS ALGORITHMS run significantly faster than the CLASSICAL MINORIZE-MAXIMIZATION ALGORITHM , while achieving competitive STATISTICAL EFFICIENCY . \n",
            "this paper presents a novel GENERALIZED METHOD-OF-MOMENTS ALGORITHMS based on PAIRWISE COMPARISONS . the proposed GENERALIZED METHOD-OF-MOMENTS ALGORITHMS are based on the use of PAIRWISE COMPARISONS for CONSISTENT AND INCONSISTENT BREAKINGS . the proposed GENERALIZED METHOD-OF-MOMENTS ALGORITHMS are based on the use of PAIRWISE COMPARISONS derived from PAIRWISE COMPARISONS . the proposed GENERALIZED METHOD-OF-MOMENTS ALGORITHMS are compared to a CLASSICAL MINORIZE-MAXIMIZATION ALGORITHM . the experimental results show that the proposed GENERALIZED METHOD-OF-MOMENTS ALGORITHMS are more effective than the conventional CLASSICAL MINORIZE-MAXIMIZATION ALGORITHM .\n",
            "\n",
            "439 1000\n",
            "<unk> measures for the HETEROGENEITY OF CLUSTERS have been used for a long time . this paper studies the ENTROPY-BASED CRITERION in CLUSTERING CATEGORICAL DATA . it first shows that the ENTROPY-BASED CRITERION can be derived in the formal framework of PROBABILISTIC CLUSTERING MODELS and establishes the connection between the criterion and the approach based on DISSIMILARITY CO-EFFICIENTS . an iterative monte-carlo procedure is then presented to search for the PARTITIONS minimizing the criterion . experiments are conducted to show the effectiveness of the proposed procedure . \n",
            "in this paper , we propose a novel approach to CLUSTERING CATEGORICAL DATA in CLUSTERING CATEGORICAL DATA . the proposed approach is based on the use of ENTROPY-TYPE MEASURES for CLUSTERING CATEGORICAL DATA . the proposed approach is based on the idea of DISSIMILARITY CO-EFFICIENTS , which is based on the concept of DISSIMILARITY CO-EFFICIENTS . the proposed approach is based on the idea of DISSIMILARITY CO-EFFICIENTS , which is a ENTROPY-BASED CRITERION . the proposed method is applied to the problem of CLUSTERING CATEGORICAL DATA . the experimental results show that the proposed method can effectively capture the HETEROGENEITY OF CLUSTERS , and the proposed method can be applied to other PROBABILISTIC CLUSTERING MODELS .\n",
            "\n",
            "440 1000\n",
            "the majority of the existing algorithms for LEARNING DECISION TREES are greedy -- a TREE is induced top-down , making locally optimal decisions at each node . in most cases , however , the CONSTRUCTED TREE is not globally optimal . furthermore , the GREEDY ALGORITHMS require a fixed amount of time and are not able to generate a better TREE if additional time is available . to overcome this problem , we present two LOOKAHEAD-BASED ALGORITHMS for ANYTIME INDUCTION OF DECISION TREES , thus allowing tradeoff between TREE QUALITY and LEARNING TIME . the first one is DEPTH-K LOOKAHEAD , where a larger TIME ALLOCATION permits larger k . the second algorithm uses a novel strategy for evaluating candidate splits ; a STOCHASTIC VERSION of ID3 is repeatedly invoked to estimate the size of the TREE in which each split results , and the one that minimizes the expected size is preferred . experimental results indicate that for several hard concepts , our proposed approach exhibits good anytime behavior and yields significantly better DECISION TREES when more time is available . \n",
            "in this paper , we propose a novel approach to ANYTIME INDUCTION OF DECISION TREES in the context of ANYTIME INDUCTION OF DECISION TREES . the proposed approach is based on the use of GREEDY ALGORITHMS for ANYTIME INDUCTION OF DECISION TREES . the proposed approach is based on the use of DECISION TREES , which is a STOCHASTIC VERSION of the CONSTRUCTED TREE . the proposed approach is based on a STOCHASTIC VERSION , which is based on a STOCHASTIC VERSION . the proposed method can be applied to a wide range of DECISION TREES , including ID3 and LEARNING TIME . the experimental results show that the proposed method is effective in reducing the LEARNING TIME and LEARNING TIME .\n",
            "\n",
            "441 1000\n",
            "the <unk> <unk> -lrb- 0 < p < 1 -rrb- is usually used to replace the standard NUCLEAR NORM in order to approximate the RANK FUNCTION more accurately . however , existing SCHATTEN-P QUASI-NORM MINIMIZATION ALGORITHMS involve SINGULAR VALUE DECOMPOSITION or EIGENVALUE DECOMPOSITION in each iteration , and thus may become very slow and impractical for LARGE-SCALE PROBLEMS . in this paper , we first define two TRACTABLE SCHATTEN QUASI-NORMS , i.e. , the FROBENIUS/NUCLEAR HYBRID AND BI-NUCLEAR QUASI-NORMS , and then prove that TRACTABLE SCHATTEN QUASI-NORMS are in essence the <unk> / 3 and 1/2 <unk> , respectively , which lead to the design of very efficient algorithms that only need to update two much smaller FACTOR MATRICES . we also design two efficient proximal alternating linearized minimization algorithms for solving REPRESENTATIVE MATRIX COMPLETION PROBLEMS . finally , we provide the GLOBAL CONVERGENCE and performance guarantees for our algorithms , which have better convergence properties than existing algorithms . experimental results on SYNTHETIC AND REAL-WORLD DATA show that our algorithms are more accurate than the state-of-the-art methods , and are orders of magnitude faster . \n",
            "in this paper , we propose a novel approach to LARGE-SCALE PROBLEMS based on SINGULAR VALUE DECOMPOSITION and SINGULAR VALUE DECOMPOSITION . the proposed SCHATTEN-P QUASI-NORM MINIMIZATION ALGORITHMS is based on the EIGENVALUE DECOMPOSITION , which is a RANK FUNCTION . the proposed SCHATTEN-P QUASI-NORM MINIMIZATION ALGORITHMS is based on SINGULAR VALUE DECOMPOSITION and EIGENVALUE DECOMPOSITION . the proposed SCHATTEN-P QUASI-NORM MINIMIZATION ALGORITHMS is based on the EIGENVALUE DECOMPOSITION and the EIGENVALUE DECOMPOSITION . the proposed method is evaluated on both SYNTHETIC AND REAL-WORLD DATA . the experimental results show that the proposed method outperforms the state-of-the-art methods in terms of GLOBAL CONVERGENCE and GLOBAL CONVERGENCE .\n",
            "\n",
            "442 1000\n",
            "data-driven grapheme-to-phoneme conversion involves either -LRB- TOP-DOWN -RRB- INDUCTIVE LEARNING or -lrb- bottom-up -rrb- pronunciation by analogy . as both approaches rely on LOCAL CONTEXT INFORMATION , they typically require some EXTERNAL LINGUISTIC KNOWLEDGE , e.g. , individual <unk> correspondences . to avoid such SUPERVISION , this paper proposes an alternative solution , dubbed pronunciation by LATENT ANALOGY , which adopts a more GLOBAL DEFINITION OF ANALOGOUS EVENTS . for each OUT-OF-VOCABULARY WORD , a neighborhood of GLOBALLY RELEVANT PRONUNCIATIONS is constructed through an appropriate DATA-DRIVEN MAPPING of its GRAPHEMIC FORM . PHONEME TRANSCRIPTION then proceeds via LOCALLY OPTIMAL SEQUENCE ALIGNMENT and MAXIMUM LIKELIHOOD POSITION SCORING . this method was successfully applied to the SYNTHESIS OF PROPER NAMES with a large diversity of origin . \n",
            "this paper presents a novel approach to DATA-DRIVEN GRAPHEME-TO-PHONEME CONVERSION in PHONEME TRANSCRIPTION . the proposed approach is based on the use of a DATA-DRIVEN MAPPING and MAXIMUM LIKELIHOOD POSITION SCORING . the proposed approach is based on the use of LOCALLY OPTIMAL SEQUENCE ALIGNMENT and MAXIMUM LIKELIHOOD POSITION SCORING . the proposed method is based on the use of LOCAL CONTEXT INFORMATION and MAXIMUM LIKELIHOOD POSITION SCORING . the proposed method is based on the use of LOCALLY OPTIMAL SEQUENCE ALIGNMENT and MAXIMUM LIKELIHOOD POSITION SCORING . the proposed method is applied to PHONEME TRANSCRIPTION , and the experimental results show that the proposed method outperforms the state-of-the-art methods .\n",
            "\n",
            "443 1000\n",
            "we propose a feature space maximum a POSTERIORI LINEAR REGRESSION FRAMEWORK to adapt PARAMETERS for context dependent deep neural network hidden markov models -lrb- <unk> -rrb- . due to the huge amount of PARAMETERS used in DNN ACOUSTIC MODELS in LARGE VOCABULARY CONTINUOUS SPEECH RECOGNITION , the problem of OVER-FITTING can be severe in DNN ADAPTATION , thus often impair the ROBUSTNESS of the adapted DNN ADAPTATION . LINEAR INPUT NETWORK as a STRAIGHTFORWARD FEATURE SPACE ADAPTATION METHOD for DNN ADAPTATION , similar to feature space maximum likelihood linear regression -lrb- fmllr -rrb- , can potentially suffer from the same ROBUSTNESS SITUATION . the proposed STRAIGHTFORWARD FEATURE SPACE ADAPTATION METHOD is built based on map estimation of the LIN PARAMETERS by incorporating PRIOR KNOWLEDGE into the ADAPTATION PROCESS . experimental results on the SWITCHBOARD TASK show that against the SPEAKER INDEPENDENT CD-DNN-HMM SYSTEMS , LINEAR INPUT NETWORK provides <unk> % RELATIVE WORD ERROR RATE REDUCTION and the proposed FMAPLIN METHOD is able to provide further <unk> % -lrb- totally <unk> % -rrb- RELATIVE WORD ERROR RATE REDUCTION on top of LINEAR INPUT NETWORK . \n",
            "this paper presents a novel approach to LARGE VOCABULARY CONTINUOUS SPEECH RECOGNITION . the proposed STRAIGHTFORWARD FEATURE SPACE ADAPTATION METHOD is based on the use of PARAMETERS extracted from the LINEAR INPUT NETWORK . the proposed STRAIGHTFORWARD FEATURE SPACE ADAPTATION METHOD is based on the use of PARAMETERS extracted from the LINEAR INPUT NETWORK . the proposed STRAIGHTFORWARD FEATURE SPACE ADAPTATION METHOD is based on the use of PARAMETERS extracted from the LINEAR INPUT NETWORK . the proposed STRAIGHTFORWARD FEATURE SPACE ADAPTATION METHOD is evaluated on the SWITCHBOARD TASK and compared with the FMAPLIN METHOD . the proposed STRAIGHTFORWARD FEATURE SPACE ADAPTATION METHOD is evaluated on the SWITCHBOARD TASK and the SWITCHBOARD TASK results show that the proposed STRAIGHTFORWARD FEATURE SPACE ADAPTATION METHOD is effective in improving the ROBUSTNESS of SPEAKER INDEPENDENT CD-DNN-HMM SYSTEMS . the proposed STRAIGHTFORWARD FEATURE SPACE ADAPTATION METHOD is evaluated on the SWITCHBOARD TASK and the SWITCHBOARD TASK results show that the proposed STRAIGHTFORWARD FEATURE SPACE ADAPTATION METHOD achieves better performance in terms of RELATIVE WORD ERROR RATE REDUCTION and ROBUSTNESS .\n",
            "\n",
            "444 1000\n",
            "this paper presents several novel CODEC STRUCTURES for NOISE FEEDBACK CODING incorporating both LONG-TERM AND SHORT-TERM NOISE SPECTRAL SHAPING , as well as LONG-TERM AND SHORT-TERM PREDICTION . in addition , the paper generalizes the conventional SCALAR-QUANTIZATION-BASED NFC to VECTOR-QUANTIZATION-BASED NFC , and it lays the foundation for the associated efficient VQ CODEBOOK SEARCH and CLOSED-LOOP VQ CODEBOOK DESIGN . BROADVOICE ® 16 , a PACKETCABLE 1.5 MANDATORY NARROWBAND SPEECH CODEC standardized by <unk> ® for voice over <unk> in north america , is based on one of such novel NFC CODEC STRUCTURES . \n",
            "this paper presents a novel PACKETCABLE 1.5 MANDATORY NARROWBAND SPEECH CODEC called BROADVOICE ® 16 . the proposed PACKETCABLE 1.5 MANDATORY NARROWBAND SPEECH CODEC is based on a PACKETCABLE 1.5 MANDATORY NARROWBAND SPEECH CODEC , called BROADVOICE ® 16 , which is based on a SCALAR-QUANTIZATION-BASED NFC . the proposed PACKETCABLE 1.5 MANDATORY NARROWBAND SPEECH CODEC is based on a SCALAR-QUANTIZATION-BASED NFC , called BROADVOICE ® 16 . the proposed PACKETCABLE 1.5 MANDATORY NARROWBAND SPEECH CODEC is based on a SCALAR-QUANTIZATION-BASED NFC , called BROADVOICE ® 16 . the proposed PACKETCABLE 1.5 MANDATORY NARROWBAND SPEECH CODEC is applied to BROADVOICE ® 16 , and the experimental results demonstrate the effectiveness of the proposed PACKETCABLE 1.5 MANDATORY NARROWBAND SPEECH CODEC .\n",
            "\n",
            "445 1000\n",
            "previous work in SOCIAL NETWORK ANALYSIS has modeled the existence of links from one entity to another , but not the LANGUAGE CONTENT or topics on those links . we present the AUTHOR-RECIPIENT-TOPIC MODEL for SOCIAL NETWORK ANALYSIS , which learns TOPIC DISTRIBUTIONS based on the DIRECTION-SENSITIVE MESSAGES sent between entities . the AUTHOR-RECIPIENT-TOPIC MODEL builds on LATENT DIRICHLET ALLOCATION and the AUTHOR-TOPIC MODEL , adding the key attribute that distribution over topics is conditioned distinctly on both the <unk> and <unk> -- steering the discovery of topics according to the relationships between people . we give results on both the ENRON EMAIL CORPUS and a RE-SEARCHER 'S EMAIL ARCHIVE , providing evidence not only that clearly relevant topics are discovered , but that the AUTHOR-RECIPIENT-TOPIC MODEL better predicts people 's roles . \n",
            "this paper presents a novel approach to SOCIAL NETWORK ANALYSIS based on LATENT DIRICHLET ALLOCATION . the proposed AUTHOR-TOPIC MODEL is based on a AUTHOR-RECIPIENT-TOPIC MODEL , which is based on a AUTHOR-RECIPIENT-TOPIC MODEL . the proposed AUTHOR-RECIPIENT-TOPIC MODEL is based on a AUTHOR-RECIPIENT-TOPIC MODEL . the proposed AUTHOR-RECIPIENT-TOPIC MODEL is based on the use of a AUTHOR-RECIPIENT-TOPIC MODEL and the RE-SEARCHER 'S EMAIL ARCHIVE . the proposed AUTHOR-RECIPIENT-TOPIC MODEL is applied to the ENRON EMAIL CORPUS and the RE-SEARCHER 'S EMAIL ARCHIVE . the proposed AUTHOR-RECIPIENT-TOPIC MODEL is evaluated on a ENRON EMAIL CORPUS and a RE-SEARCHER 'S EMAIL ARCHIVE .\n",
            "\n",
            "446 1000\n",
            "we present an approach to PARALLEL VARIATIONAL OPTICAL FLOW COMPUTATION on standard hardware by DOMAIN DECOMPOSITION . using an arbitrary partition of the IMAGE PLANE into RECTANGULAR SUBDOMAINS , the GLOBAL SOLUTION to the VARIATIONAL APPROACH is obtained by iteratively combining LOCAL SOLUTIONS which can be efficiently computed in parallel by separate MULTI-GRID ITERATIONS for each <unk> . the approach is particularly suited for implementations on PC-CLUSTERS because INTER-PROCESS COMMUNICATION between subdomains -lrb- i.e. processors -rrb- is minimized by restricting the exchange of data to a LOWER-DIMENSIONAL INTERFACE . by applying a DEDICATED INTERFACE PRECONDITIONER , the necessary number of ITERATIONS between subdomains to achieve a fixed error is bounded independently of the number of subdomains . our approach provides a major step towards REAL-TIME 2D IMAGE PROCESSING using off-the-shelf PC-HARDWARE and facilitates the efficient application of VARIATIONAL APPROACHES to LARGE-SCALE IMAGE PROCESSING PROBLEMS . \n",
            "this paper addresses the problem of REAL-TIME 2D IMAGE PROCESSING in the presence of LARGE-SCALE IMAGE PROCESSING PROBLEMS . we propose a novel VARIATIONAL APPROACH to the problem of REAL-TIME 2D IMAGE PROCESSING . the proposed VARIATIONAL APPROACH is based on a VARIATIONAL APPROACH of the IMAGE PLANE of the IMAGE PLANE to the IMAGE PLANE . the proposed VARIATIONAL APPROACH is based on a VARIATIONAL APPROACH , which is based on a VARIATIONAL APPROACH . the proposed VARIATIONAL APPROACH is based on a VARIATIONAL APPROACH , which is based on a VARIATIONAL APPROACH . the proposed VARIATIONAL APPROACH is applied to LARGE-SCALE IMAGE PROCESSING PROBLEMS . the experimental results show that the proposed VARIATIONAL APPROACH is effective in reducing the number of ITERATIONS in the IMAGE PLANE .\n",
            "\n",
            "447 1000\n",
            "in this paper , we describe systems that were developed for the open performance sub-challenge of the interspeech 2009 emotion challenge . we participate in both TWO-CLASS AND FIVE-CLASS EMOTION DETECTION . for the TWO-CLASS AND FIVE-CLASS EMOTION DETECTION , the best performance is obtained by LOGISTIC REGRESSION FUSION of three systems . these systems use SHORT-AND LONG-TERM SPEECH FEATURES . FUSION allowed to an absolute improvement of 2.6 % on the UNWEIGHTED RECALL VALUE compared with -lsb- 1 -rsb- . for the TWO-CLASS AND FIVE-CLASS EMOTION DETECTION , we submitted two individual systems : CEPSTRAL GMM vs. long-term gmm-ubm . the best result comes from a CEPSTRAL GMM and produces an absolute improvement of 3.5 % compared to -lsb- 6 -rsb- . \n",
            "this paper proposes a novel approach to LOGISTIC REGRESSION FUSION for TWO-CLASS AND FIVE-CLASS EMOTION DETECTION . the proposed method is based on the use of a CEPSTRAL GMM . the proposed method is based on the idea of LOGISTIC REGRESSION FUSION . the proposed method is evaluated in terms of both TWO-CLASS AND FIVE-CLASS EMOTION DETECTION and FUSION . the experimental results show that the proposed method is effective in improving the UNWEIGHTED RECALL VALUE of FUSION in terms of the UNWEIGHTED RECALL VALUE .\n",
            "\n",
            "448 1000\n",
            "this paper applies a DYNAMIC SINUSOIDAL SYNTHESIS MODEL to STATISTICAL PARAMETRIC SPEECH SYNTHESIS . for this , we utilise REGULARISED CEPSTRAL COEFFICIENTS to represent both the STATIC AMPLITUDE and dynamic slope of selected sinusoids for STATISTICAL MODELLING . during synthesis , a DYNAMIC SINUSOIDAL SYNTHESIS MODEL is used to reconstruct SPEECH . a PREFERENCE TEST is conducted to compare the selection of different sinusoids for CEPSTRAL REPRESENTATION . our results show that when integrated with STATISTICAL PARAMETRIC SPEECH SYNTHESIS , a relatively small number of sinusoids selected according to a PERCEPTUAL CRITERION can produce quality comparable to using all harmonics . a MEAN OPINION SCORE TEST shows that our proposed DYNAMIC SINUSOIDAL SYNTHESIS MODEL is preferred to one using MEL-CEPSTRA from PITCH SYNCHRONOUS SPECTRAL ANALYSIS . \n",
            "in this paper , we propose a novel DYNAMIC SINUSOIDAL SYNTHESIS MODEL for STATISTICAL PARAMETRIC SPEECH SYNTHESIS . the proposed DYNAMIC SINUSOIDAL SYNTHESIS MODEL is based on the use of MEL-CEPSTRA for STATISTICAL MODELLING . the proposed DYNAMIC SINUSOIDAL SYNTHESIS MODEL is based on the use of MEL-CEPSTRA to estimate the STATIC AMPLITUDE of the SPEECH . the proposed DYNAMIC SINUSOIDAL SYNTHESIS MODEL is based on the use of MEL-CEPSTRA to estimate the STATIC AMPLITUDE of the SPEECH . the proposed DYNAMIC SINUSOIDAL SYNTHESIS MODEL is evaluated on the MEAN OPINION SCORE TEST . the experimental results show that the proposed DYNAMIC SINUSOIDAL SYNTHESIS MODEL is effective in improving the STATISTICAL MODELLING performance of the DYNAMIC SINUSOIDAL SYNTHESIS MODEL .\n",
            "\n",
            "449 1000\n",
            "we present a new approach to model and classify BREAST PARENCHYMAL TISSUE . given a MAMMOGRAM , first , we will discover the distribution of the different TISSUE DENSITIES in an UNSUPERVISED MANNER , and second , we will use this TISSUE DISTRIBUTION to perform the CLASSIFICATION . we achieve this using a CLASSIFIER based on LOCAL DESCRIPTORS and PROBABILIS-TIC LATENT SEMANTIC ANALYSIS , a GENERATIVE MODEL from the STATISTICAL TEXT LITERATURE . we studied the influence of different DESCRIPTORS like TEXTURE and SIFT FEATURES at the CLASSIFICATION STAGE showing that textons outperform PROBABILIS-TIC LATENT SEMANTIC ANALYSIS in all cases . moreover we demonstrate that PROBABILIS-TIC LATENT SEMANTIC ANALYSIS automatically extracts meaningful latent aspects generating a COMPACT TISSUE REPRESENTATION based on their densities , useful for discriminating on MAM-MOGRAM CLASSIFICATION . we show the results of CLASSIFICATION STAGE over the MIAS AND DDSM DATASETS . we compare our method with approaches that classified these same PROBABILIS-TIC LATENT SEMANTIC ANALYSIS showing a better performance of our proposal . \n",
            "this paper presents a novel approach to PROBABILIS-TIC LATENT SEMANTIC ANALYSIS based on PROBABILIS-TIC LATENT SEMANTIC ANALYSIS and PROBABILIS-TIC LATENT SEMANTIC ANALYSIS . the proposed CLASSIFIER is based on the use of LOCAL DESCRIPTORS and PROBABILIS-TIC LATENT SEMANTIC ANALYSIS . the proposed GENERATIVE MODEL is based on the use of LOCAL DESCRIPTORS and PROBABILIS-TIC LATENT SEMANTIC ANALYSIS . the proposed GENERATIVE MODEL is based on the use of LOCAL DESCRIPTORS and PROBABILIS-TIC LATENT SEMANTIC ANALYSIS . the proposed CLASSIFIER is evaluated on a variety of MIAS AND DDSM DATASETS , and the results show that the proposed CLASSIFIER is effective in improving the CLASSIFICATION performance .\n",
            "\n",
            "450 1000\n",
            "with the growing popularity of SHORT-FORM VIDEO SHARING PLATFORMS such as INSTAGRAM and <unk> , there has been an increasing need for techniques that automatically extract highlights from video . whereas prior works have approached this problem with HEURISTIC RULES or SUPERVISED LEARNING , we present an UNSUPERVISED LEARNING APPROACH that takes advantage of the abundance of USER-EDITED VIDEOS on SOCIAL MEDIA WEBSITES such as YOUTUBE . based on the idea that the most significant sub-events within a VIDEO CLASS are commonly present among EDITED VIDEOS while less interesting ones appear less frequently , we identify the significant sub-events via a ROBUST RECURRENT AUTO-ENCODER trained on a collection of USER-EDITED VIDEOS queried for each particular class of interest . the ROBUST RECURRENT AUTO-ENCODER is trained using a proposed SHRINKING EXPONENTIAL LOSS FUNCTION that makes ROBUST RECURRENT AUTO-ENCODER robust to NOISE in the WEB-CRAWLED TRAINING DATA , and is configured with bidirectional long short term memory -lrb- lstm -rrb- -lsb- 5 -rsb- cells to better model the TEMPORAL STRUCTURE OF HIGHLIGHT SEGMENTS . different from SUPERVISED TECHNIQUES , our UNSUPERVISED LEARNING APPROACH can infer highlights using only a set of DOWN-LOADED EDITED VIDEOS , without also needing their <unk> counterparts which are rarely available online . extensive experiments indicate the promise of our proposed UNSUPERVISED LEARNING APPROACH in this challenging UNSUPERVISED SETTING . \n",
            "in this paper , we propose a novel UNSUPERVISED LEARNING APPROACH based on a ROBUST RECURRENT AUTO-ENCODER . the proposed UNSUPERVISED LEARNING APPROACH is based on a ROBUST RECURRENT AUTO-ENCODER , which is based on a SHRINKING EXPONENTIAL LOSS FUNCTION . the proposed UNSUPERVISED LEARNING APPROACH is based on a SHRINKING EXPONENTIAL LOSS FUNCTION , which is based on a SHRINKING EXPONENTIAL LOSS FUNCTION . the proposed SHRINKING EXPONENTIAL LOSS FUNCTION is based on a SHRINKING EXPONENTIAL LOSS FUNCTION , which is based on a SHRINKING EXPONENTIAL LOSS FUNCTION . the proposed ROBUST RECURRENT AUTO-ENCODER is compared to a ROBUST RECURRENT AUTO-ENCODER , and the results show that the proposed UNSUPERVISED LEARNING APPROACH is able to recover the VIDEO CLASS of the VIDEO CLASS . the proposed UNSUPERVISED LEARNING APPROACH is able to recover the VIDEO CLASS of the VIDEO CLASS , and can be applied to other SOCIAL MEDIA WEBSITES such as YOUTUBE and YOUTUBE .\n",
            "\n",
            "451 1000\n",
            "given a set of LOW RESOLUTION CAMERA IMAGES , it is possible to reconstruct HIGH RESOLUTION LUMINANCE AND DEPTH INFORMATION , specially i f the RELATIVE DISPLACEMENTS of the IMAGE FRAMES are known . we have proposed ITERATIVE ALGORITHMS for RECOVERING HIGH RESOLUTION ALBEDO AND DEPTH MAPS that require no a PRIORI KNOWLEDGE OF THE SCENE , and therefore do not depend on other methods , as regards boundary and initial conditions . the problem of SURFACE RECONSTRUCTION has been formulated as one of EXPECTATION MAXIMIZATION and has been tackled in a PROBABILISTIC FRAMEWORK <unk> MARKOV RANDOM FIELDS -lsb- 1 -rsb- -lsb- 3 -rsb- . as for the DEPTH MAP , our ITERATIVE ALGORITHMS is directly recovering surface heights without <unk> to SURFACE ORIENTATIONS , <unk> increasing the resolution by camera <unk> -lsb- 2 -rsb- . conventional STATISTICAL MODELS have been coupled with GEOMETRICAL TECHNIQUES to construct a general model of <unk> world and the IMAGING PROCESS . \n",
            "this paper addresses the problem of RECOVERING HIGH RESOLUTION ALBEDO AND DEPTH MAPS from LOW RESOLUTION CAMERA IMAGES . we propose a novel approach to RECOVERING HIGH RESOLUTION ALBEDO AND DEPTH MAPS based on EXPECTATION MAXIMIZATION . the proposed method is based on the EXPECTATION MAXIMIZATION , which is based on the EXPECTATION MAXIMIZATION . the proposed method is based on the EXPECTATION MAXIMIZATION , which is based on EXPECTATION MAXIMIZATION . the proposed method is based on the use of EXPECTATION MAXIMIZATION to estimate the RELATIVE DISPLACEMENTS and the RELATIVE DISPLACEMENTS of the IMAGE FRAMES . the proposed method is evaluated on a variety of IMAGE FRAMES , and the results show that the proposed method is effective in RECOVERING HIGH RESOLUTION ALBEDO AND DEPTH MAPS .\n",
            "\n",
            "452 1000\n",
            "we study the EVENT DETECTION PROBLEM using CONVOLUTIONAL NEURAL NETWORKS that overcome the two fundamental limitations of the traditional FEATURE-BASED APPROACHES to this EVENT DETECTION PROBLEM : complicated FEATURE ENGINEERING for RICH FEATURE SETS and ERROR PROPAGATION from the preceding stages which generate these FEATURES . the experimental results show that the CONVOLUTIONAL NEURAL NETWORKS outper-form the best reported FEATURE-BASED SYSTEMS in the general setting as well as the DOMAIN ADAPTATION SETTING without resorting to extensive EXTERNAL RESOURCES . \n",
            "this paper addresses the problem of CONVOLUTIONAL NEURAL NETWORKS in the context of FEATURE-BASED SYSTEMS . in particular , we focus on the problem of EVENT DETECTION PROBLEM and ERROR PROPAGATION . we propose a novel approach to the problem of CONVOLUTIONAL NEURAL NETWORKS , which is based on the use of CONVOLUTIONAL NEURAL NETWORKS and CONVOLUTIONAL NEURAL NETWORKS . we show that CONVOLUTIONAL NEURAL NETWORKS can be learned by CONVOLUTIONAL NEURAL NETWORKS , which can be efficiently solved by CONVOLUTIONAL NEURAL NETWORKS . we demonstrate the effectiveness of our CONVOLUTIONAL NEURAL NETWORKS on a variety of RICH FEATURE SETS .\n",
            "\n",
            "453 1000\n",
            "prediction , estimation , and SMOOTHING are fundamental to SIGNAL PROCESSING . to perform these interrelated tasks given NOISY DATA , we form a TIME SERIES MODEL of the process that generates the data . taking noise in the system explicitly into account , maximum-likelihood and KALMAN FRAMEWORKS are discussed which involve the dual process of estimating both the MODEL PARAMETERS and the underlying state of the system . we review several established methods in the LINEAR CASE , and propose SEVERA ! extensions utilizing dual kalman filters -lrb- <unk> -rrb- and FORWARD-BACKWARD FILTERS that are applicable to NEURAL NETWORKS . methods are compared on several SIMULATIONS OF NOISY TIME SERIES . we also include an example of NONLINEAR NOISE REDUCTION in SPEECH . \n",
            "this paper presents a novel approach to SIGNAL PROCESSING in SPEECH . the proposed approach is based on the use of FORWARD-BACKWARD FILTERS for SIGNAL PROCESSING . the proposed approach is based on the use of FORWARD-BACKWARD FILTERS , which is a generalization of the TIME SERIES MODEL to the LINEAR CASE . the proposed approach is based on the use of FORWARD-BACKWARD FILTERS to estimate the MODEL PARAMETERS of the FORWARD-BACKWARD FILTERS . the proposed method is evaluated on a SIMULATIONS OF NOISY TIME SERIES and compared to the conventional KALMAN FRAMEWORKS . the results show that the proposed method is effective in improving the PREDICTION performance in comparison to the conventional KALMAN FRAMEWORKS .\n",
            "\n",
            "454 1000\n",
            "the fixed point implementation of IIR DIGITAL FILTERS usually leads to the appearance of ZERO-INPUT LIMIT CYCLES , which degrade the performance of the system . in this paper , we develop an efficient MONTE CARLO ALGORITHM to detect and characterize LIMIT CYCLES in FIXED-POINT IIR DIGITAL FILTERS . the proposed MONTE CARLO ALGORITHM considers FILTERS formulated in the STATE SPACE and is valid for any FIXED POINT REPRESENTATION and QUANTIZA-TION FUNCTION . NUMERICAL SIMULATIONS on several HIGH-ORDER FILTERS , where an exhaustive search is unfeasible , show the effectiveness of the proposed MONTE CARLO ALGORITHM . \n",
            "this paper presents a novel MONTE CARLO ALGORITHM for IIR DIGITAL FILTERS . the proposed MONTE CARLO ALGORITHM is based on the use of a FIXED POINT REPRESENTATION and a QUANTIZA-TION FUNCTION for the STATE SPACE . the proposed MONTE CARLO ALGORITHM is based on a MONTE CARLO ALGORITHM and a MONTE CARLO ALGORITHM . the proposed MONTE CARLO ALGORITHM is applied to the problem of IIR DIGITAL FILTERS , and the results show that the proposed MONTE CARLO ALGORITHM is effective in reducing the number of FILTERS . the proposed MONTE CARLO ALGORITHM is applied to the problem of IIR DIGITAL FILTERS , and the experimental results show that the proposed MONTE CARLO ALGORITHM is effective in reducing the number of FILTERS .\n",
            "\n",
            "455 1000\n",
            "this paper relates experiences of ALGORITHMIC TRANSFORMATIONS in HIGH LEVEL SYNTHESIS , in the area of ACOUSTIC ECHO CANCELLATION . the PROCESSING AND MEMORY UNITS are automatically designed for various equivalent LMS ALGORITHMS , in the FIR CASE , with important COMPUTATIONAL LOAD . the results obtained with DIERENT LTER LENGTHS , give an accurate prototyping of new fast versions of the LMS ALGORITHMS . it also show that a THEORETICAL ARITHMETIC REDUCTION must be correlated to the associated increase of MEMORY REQUIREMENTS . \n",
            "in this paper , we present a novel approach to ACOUSTIC ECHO CANCELLATION based on the use of ALGORITHMIC TRANSFORMATIONS for HIGH LEVEL SYNTHESIS . the proposed approach is based on the use of ALGORITHMIC TRANSFORMATIONS for HIGH LEVEL SYNTHESIS . the proposed approach is based on the use of ALGORITHMIC TRANSFORMATIONS for HIGH LEVEL SYNTHESIS . the proposed method is based on the use of ALGORITHMIC TRANSFORMATIONS in order to reduce the COMPUTATIONAL LOAD . the proposed method is evaluated in terms of THEORETICAL ARITHMETIC REDUCTION , and the results show that the proposed method is effective in improving the COMPUTATIONAL LOAD .\n",
            "\n",
            "456 1000\n",
            "in this paper , we propose a novel general framework for TEN-SOR BASED NULL SPACE AFFINE INVARIANTS , namely , TENSOR NULL SPACE INVARIANTS with a LINEAR CLASSIFIER for HIGH ORDER DATA CLASSIFICATION and RETRIEVAL . we first derive TENSOR NULL SPACE INVARIANTS , which is perfectly invariant to MULTIDIMENSIONAL AFFINE TRANSFORMATIONS due to CAMERA MOTIONS for multiple motion trajectories in CONSECUTIVE MOTION EVENTS . we subsequently propose an efficient CLASSIFICATION AND RETRIEVAL SYSTEM relying on TENSOR NULL SPACE INVARIANTS for ARCHIV-ING AND SEARCHING MOTION EVENTS consisting of multiple motion trajectories . the simulation results demonstrate superior performance of the proposed CLASSIFICATION AND RETRIEVAL SYSTEM . \n",
            "in this paper , we propose a novel approach to HIGH ORDER DATA CLASSIFICATION based on TENSOR NULL SPACE INVARIANTS . the proposed method is based on the use of TENSOR NULL SPACE INVARIANTS for RETRIEVAL . the proposed method is based on the TEN-SOR BASED NULL SPACE AFFINE INVARIANTS , which is a LINEAR CLASSIFIER . the proposed method is based on the use of TENSOR NULL SPACE INVARIANTS for HIGH ORDER DATA CLASSIFICATION . the proposed method is evaluated on HIGH ORDER DATA CLASSIFICATION and HIGH ORDER DATA CLASSIFICATION . the experimental results show that the proposed method outperforms the existing methods in terms of RETRIEVAL and RETRIEVAL .\n",
            "\n",
            "457 1000\n",
            "noise <unk> present serious complications to accurate DATA ANALYSIS in FUNCTIONAL MAGNETIC RESONANCE IMAGING . simply relying on CONTEXTUAL IMAGE INFORMATION often results in unsatisfactory segmentation of ACTIVE BRAIN REGIONS . to remedy this , we propose a novel GROUP MARKOV RANDOM FIELD -LRB- GROUP MRF -RRB- that extends the NEIGHBORHOOD SYSTEM to other subjects to incorporate GROUP INFORMATION in modeling each subject 's BRAIN ACTIVATION . our GROUP MARKOV RANDOM FIELD -LRB- GROUP MRF -RRB- has the distinct advantage of being able to regularize the states of both INTRA-AND INTER-SUBJECT NEIGHBORS without having to create a STRINGENT ONE-TO-ONE VOXEL CORRESPONDENCE as in standard FMRI GROUP ANALYSIS . also , our GROUP MARKOV RANDOM FIELD -LRB- GROUP MRF -RRB- can be efficiently implemented as a single MRF , hence enabling ACTIVATION MAPS of a group of subjects to be simultaneously and collaboratively segmented . we validate on both SYNTHETIC AND REAL FMRI DATA and demonstrate superior performance over standard ANALYSIS TECHNIQUES . \n",
            "this paper addresses the problem of FUNCTIONAL MAGNETIC RESONANCE IMAGING for FUNCTIONAL MAGNETIC RESONANCE IMAGING . in particular , we propose a novel approach to DATA ANALYSIS for FUNCTIONAL MAGNETIC RESONANCE IMAGING . the proposed approach is based on the use of a GROUP MARKOV RANDOM FIELD -LRB- GROUP MRF -RRB- , which is able to capture the GROUP INFORMATION of the ACTIVATION MAPS . the proposed approach is based on the use of a GROUP MARKOV RANDOM FIELD -LRB- GROUP MRF -RRB- , which is able to capture the CONTEXTUAL IMAGE INFORMATION of the ACTIVATION MAPS . the proposed approach is evaluated on both SYNTHETIC AND REAL FMRI DATA . the experimental results on SYNTHETIC AND REAL FMRI DATA show that the proposed method is effective in improving the performance of FMRI GROUP ANALYSIS on SYNTHETIC AND REAL FMRI DATA .\n",
            "\n",
            "458 1000\n",
            "we show that a recently developed UNIVERSAL DEMOSAICKER by the present authors greatly outperforms existing DEMOSAICKERS when tested with a realistic OPTICAL PIPELINE . we present SPEED AND QUALITY OPTIMIZATIONS of this DEMOSAICKERS for the case of regular pattern color filter arrays . we implement and extensively test optimized versions for several common CFAS including BAYER , CMY and several RGBW PATTERNS . these tests show that the proposed algorithms outperform other DEMOSAICKERS by a substantial margin while being faster than most of them . HIGH SENSITIVITY RGBW CFAS are shown to have better performance than BAYER <unk> with previous algorithms . the proposed UNIVERSAL DEMOSAICKER is a set of FINITE IMPULSE RESPONSE FILTERS , which allows a single , efficient , IMAGE SIGNAL PROCESSOR DESIGN to support different CFAS by changing its FILTER WEIGHTS . being linear , the DEMOSAICKERS is free of NOISE INDUCED ARTI-FACTS and outputs IMAGES with NEAR POISSONIAN NOISE which is NOISE REDUCTION FRIENDLY . \n",
            "this paper presents a novel approach to IMAGE SIGNAL PROCESSOR DESIGN based on a IMAGE SIGNAL PROCESSOR DESIGN . the proposed approach is based on the use of FINITE IMPULSE RESPONSE FILTERS , which is based on a UNIVERSAL DEMOSAICKER . the proposed method is based on the use of FINITE IMPULSE RESPONSE FILTERS , which is based on the UNIVERSAL DEMOSAICKER . the proposed method is based on the use of FINITE IMPULSE RESPONSE FILTERS , which is based on the UNIVERSAL DEMOSAICKER . the proposed method is evaluated on a variety of IMAGES . the results show that the proposed method is effective in reducing the number of IMAGES in the OPTICAL PIPELINE . the performance of the proposed method is demonstrated on a wide range of IMAGES . the results show that the proposed method is able to detect IMAGES in real-time , and is robust to NOISE INDUCED ARTI-FACTS , CMY , and NOISE INDUCED ARTI-FACTS .\n",
            "\n",
            "459 1000\n",
            "long short-term memory -lrb- lstm -rrb- is a specific RECURRENT NEURAL NETWORK ARCHITECTURE that is designed to model TEMPORAL SEQUENCES and their LONG-RANGE DEPENDENCIES more accurately than conventional RNNS . in this paper , we propose to use DEEP BIDIREC-TIONAL LSTM for AUDIO/VISUAL MODELING in our PHOTO-REAL TALKING HEAD SYSTEM . an AUDIO/VISUAL DATABASE of a subject 's talking is firstly recorded as our training data . the AUDIO/VISUAL STEREO DATA are converted into two PARALLEL TEMPORAL SEQUENCES , i.e. , CON-TEXTUAL LABEL SEQUENCES obtained by forced aligning audio against text , and VISUAL FEATURE SEQUENCES by applying ACTIVE-APPEARANCE-MODEL on the LOWER FACE REGION among all the training image samples . the deep BLSTM is then trained to learn the REGRESSION MODEL by minimizing the sum of SQUARE ERROR of PREDICTING VISUAL SEQUENCE from LABEL SEQUENCE . after testing different NETWORK TOPOLOGIES , we interestingly found the best network is two BLSTM LAYERS sitting on top of one FEED-FORWARD LAYER on our datasets . compared with our previous HMM-BASED SYSTEM , the newly proposed DEEP BIDIREC-TIONAL LSTM is better on both OBJECTIVE MEASUREMENT and SUBJECTIVE A/B TEST . \n",
            "in this paper , we propose a novel approach to AUDIO/VISUAL MODELING for AUDIO/VISUAL STEREO DATA . the proposed RECURRENT NEURAL NETWORK ARCHITECTURE is based on a DEEP BIDIREC-TIONAL LSTM that uses a DEEP BIDIREC-TIONAL LSTM to estimate the LONG-RANGE DEPENDENCIES and the LONG-RANGE DEPENDENCIES . the proposed RECURRENT NEURAL NETWORK ARCHITECTURE is based on a DEEP BIDIREC-TIONAL LSTM that uses a REGRESSION MODEL to estimate the LONG-RANGE DEPENDENCIES and the LONG-RANGE DEPENDENCIES . the proposed RECURRENT NEURAL NETWORK ARCHITECTURE is based on a DEEP BIDIREC-TIONAL LSTM , which is based on a DEEP BIDIREC-TIONAL LSTM . the proposed RECURRENT NEURAL NETWORK ARCHITECTURE is evaluated on a AUDIO/VISUAL DATABASE and on a AUDIO/VISUAL DATABASE . the results show that the proposed PHOTO-REAL TALKING HEAD SYSTEM is effective in improving the SQUARE ERROR of the PHOTO-REAL TALKING HEAD SYSTEM .\n",
            "\n",
            "460 1000\n",
            "recently , ESPRIT-BASED PARAMETER ESTIMATION ALGORITHMS have been developed to exploit the structure of signals from strictly second-order -lrb- so -rrb- non-circular -lrb- nc -rrb- sources . they achieve a higher ESTIMATION ACCURACY and can resolve up to twice as many sources . however , these NC METHODS assume that all the received signals are strictly non-circular . in this paper , we present the <unk> standard <unk> and the C-NC UNITARY ESPRIT ALGORITHMS designed for the more practical scenario of a received mixture of CIRCULAR AND STRICTLY NON-CIRCULAR SIGNALS . assuming that the number of CIRCULAR AND STRICTLY NON-CIRCULAR SIGNALS is known , the two proposed ESPRIT-BASED PARAMETER ESTIMATION ALGORITHMS yield CLOSED-FORM ESTIMATES and C-NC UNITARY ESPRIT ALGORITHMS also enables an entirely real-valued implementation . as a main result , it is shown that the ESTIMATION ACCURACY of the presented ESPRIT-BASED PARAMETER ESTIMATION ALGORITHMS improves with an increasing number of strictly non-circular signals among a fixed number of sources . thereby , not only the ESTIMATION ACCURACY of the strictly non-circular signals themselves is improved , but also the ESTIMATION ACCURACY of the circular signals . these results are validated by simulations . \n",
            "this paper addresses the problem of CIRCULAR AND STRICTLY NON-CIRCULAR SIGNALS in the presence of CIRCULAR AND STRICTLY NON-CIRCULAR SIGNALS . in particular , we consider the problem of CIRCULAR AND STRICTLY NON-CIRCULAR SIGNALS in the presence of CIRCULAR AND STRICTLY NON-CIRCULAR SIGNALS . in particular , we consider the problem of CIRCULAR AND STRICTLY NON-CIRCULAR SIGNALS in the context of CIRCULAR AND STRICTLY NON-CIRCULAR SIGNALS . in particular , we show that this problem can be solved efficiently using C-NC UNITARY ESPRIT ALGORITHMS and C-NC UNITARY ESPRIT ALGORITHMS . we demonstrate the effectiveness of our ESPRIT-BASED PARAMETER ESTIMATION ALGORITHMS on the task of CIRCULAR AND STRICTLY NON-CIRCULAR SIGNALS .\n",
            "\n",
            "461 1000\n",
            "one of the main challenges in ZERO-SHOT LEARNING OF VISUAL CATEGORIES is gathering SEMANTIC ATTRIBUTES to accompany images . recent work has shown that learning from TEXTUAL DESCRIPTIONS , such as WIKIPEDIA ARTICLES , avoids the problem of having to explicitly define these attributes . we present a new model that can classify UNSEEN CATEGORIES from their TEXTUAL DESCRIPTION . specifically , we use TEXT FEATURES to predict the output weights of both the convolutional and the fully connected layers in a DEEP CONVOLUTIONAL NEU-RAL NETWORK . we take advantage of the architecture of CNNS and learn FEATURES at different layers , rather than just learning an EMBEDDING SPACE for both MODALITIES , as is common with existing approaches . the proposed model also allows us to automatically generate a list of <unk> for each VISUAL CATEGORY consisting of words from WIKIPEDIA ARTICLES . we train our models end-to-end using the CALTECH-UCSD BIRD AND FLOWER DATASETS and evaluate both ROC AND PRECISION-RECALL CURVES . our empirical results show that the proposed model significantly outper-forms previous methods . \n",
            "this paper addresses the problem of ZERO-SHOT LEARNING OF VISUAL CATEGORIES from WIKIPEDIA ARTICLES . we propose a novel approach to the ZERO-SHOT LEARNING OF VISUAL CATEGORIES from WIKIPEDIA ARTICLES . the proposed approach is based on the use of a DEEP CONVOLUTIONAL NEU-RAL NETWORK , which is able to capture the SEMANTIC ATTRIBUTES , such as WIKIPEDIA ARTICLES . the proposed approach is based on the use of a DEEP CONVOLUTIONAL NEU-RAL NETWORK , which is able to capture the SEMANTIC ATTRIBUTES in the EMBEDDING SPACE . the proposed approach is evaluated on a variety of CALTECH-UCSD BIRD AND FLOWER DATASETS . the results show that the proposed approach is able to detect and track objects with UNSEEN CATEGORIES , and is robust to UNSEEN CATEGORIES and UNSEEN CATEGORIES .\n",
            "\n",
            "462 1000\n",
            "in this paper , we investigate the performance of ACOUSTIC EQUALIZATION in REVERBERANT ENVIRONMENTS . we first highlight an efficient general ACOUSTIC EQUALIZATION of a SOUND FIELD using SPHERICAL HARMONICS . we then use this ACOUSTIC EQUALIZATION to develop a CONCISE CLOSED-FORM EXPRESSION for robustness of equalization to SENSOR MOVEMENT . this CONCISE CLOSED-FORM EXPRESSION is used -lrb- i -rrb- to characterize equalization performance for a general class of NON-ISOTROPIC SOUND FIELDS and -lrb- ii -rrb- to quantify the improvements to EQUALIZER ROBUSTNESS that can be obtained by using a DIRECTIONAL MICROPHONE . the CONCISE CLOSED-FORM EXPRESSION used here does not use any of the assumptions of STATISTICAL ACOUSTICS , but instead exploits the inherent properties of a SOUND FIELD as described by the WAVE EQUATION . \n",
            "this paper presents a novel approach to ACOUSTIC EQUALIZATION in REVERBERANT ENVIRONMENTS . the proposed approach is based on a CONCISE CLOSED-FORM EXPRESSION , which is based on a DIRECTIONAL MICROPHONE . the proposed method is based on a CONCISE CLOSED-FORM EXPRESSION , which is based on a CONCISE CLOSED-FORM EXPRESSION . the proposed method is based on a CONCISE CLOSED-FORM EXPRESSION , which is based on a DIRECTIONAL MICROPHONE . the proposed method is evaluated on a variety of REVERBERANT ENVIRONMENTS . the results show that the proposed method is effective in improving the EQUALIZER ROBUSTNESS and EQUALIZER ROBUSTNESS of the proposed method .\n",
            "\n",
            "463 1000\n",
            "the EIGENVALUE PROBLEM is solved on the FINITE ELEMENT MODEL of the EXTERNAL OUTER EAR CANAL . the <unk> of the CANAL WALLS and the interaction between EXTERNAL EAR CAVITY SUBSYSTEM and the ELASTIC TYMPANIC MEMBRANE is considered . the results of the FINITE ELEMENT MODEL are compared with experimental measurements on HUMAN DISSECTIONS . the calculations support hypothesis of possible influence of EXTERNAL EAR CANAL on the ENHANCEMENT OF HEARING SENSITIVITY in <unk> khz frequency range . \n",
            "this paper addresses the problem of ENHANCEMENT OF HEARING SENSITIVITY from a single image . we propose a method to estimate the ELASTIC TYMPANIC MEMBRANE of a scene from a FINITE ELEMENT MODEL . the method is based on a FINITE ELEMENT MODEL , which is based on a FINITE ELEMENT MODEL of the ELASTIC TYMPANIC MEMBRANE and the ELASTIC TYMPANIC MEMBRANE . a FINITE ELEMENT MODEL is used to estimate the ELASTIC TYMPANIC MEMBRANE and the ELASTIC TYMPANIC MEMBRANE . the performance of the proposed method is demonstrated on several benchmark data sets .\n",
            "\n",
            "464 1000\n",
            "in this paper , we present a new LEARNING FRAMEWORK for IMAGE STYLE TRANSFORMS . considering that the images in different style representations constitute different VECTOR SPACES , we propose a novel framework called COUPLED GAUSSIAN MIXTURE MODEL to learn the relations between different spaces and use COUPLED GAUSSIAN MIXTURE MODEL to infer the images from one style to another style . observing that for each style , only the components correlated to the space of the target style are useful for INFERENCE , we first develop the COUPLED GAUSSIAN MIXTURE MODEL to pursue the EMBEDDED HIDDEN SUBSPACES that best preserve the INTER-SPACE CORRELATION INFORMATION . then we develop the coupled bidirectional transform algorithm to estimate the transforms between the two EMBEDDED SPACES , where the COUPLING between the FORWARD TRANSFORM and the BACKWARD TRANSFORM is explicitly taken into account . to enhance the capability of modelling complex data , we further develop the COUPLED GAUSSIAN MIXTURE MODEL to generalize our framework to a MIXTURE-MODEL ARCHITECTURE . the effectiveness of the framework is demonstrated in the applications including FACE SUPER-RESOLUTION and BIDIRECTIONAL PORTRAIT STYLE TRANSFORMS . \n",
            "in this paper , we propose a novel LEARNING FRAMEWORK for FACE SUPER-RESOLUTION . the proposed COUPLED GAUSSIAN MIXTURE MODEL is based on a COUPLED GAUSSIAN MIXTURE MODEL for FACE SUPER-RESOLUTION . the proposed COUPLED GAUSSIAN MIXTURE MODEL is based on a COUPLED GAUSSIAN MIXTURE MODEL that exploits the INTER-SPACE CORRELATION INFORMATION between the EMBEDDED HIDDEN SUBSPACES and the EMBEDDED HIDDEN SUBSPACES . the proposed COUPLED GAUSSIAN MIXTURE MODEL is based on a COUPLED GAUSSIAN MIXTURE MODEL that exploits the INTER-SPACE CORRELATION INFORMATION of the EMBEDDED HIDDEN SUBSPACES and the BIDIRECTIONAL PORTRAIT STYLE TRANSFORMS . the proposed COUPLED GAUSSIAN MIXTURE MODEL is applied to the EMBEDDED HIDDEN SUBSPACES and the BIDIRECTIONAL PORTRAIT STYLE TRANSFORMS . the experimental results show that the proposed COUPLED GAUSSIAN MIXTURE MODEL is able to accurately recover the EMBEDDED HIDDEN SUBSPACES of the EMBEDDED HIDDEN SUBSPACES .\n",
            "\n",
            "465 1000\n",
            "<unk> analysis <unk> in applications of BINARY CLASSIFICATION where true negatives do not add value and hence should not affect assessment of the CLASSIFIER 's performance . perhaps inspired by the many advantages of RECEIVER OPERATING CHARACTERISTIC CURVES and the area under such curves for ACCURACY-BASED PERFORMANCE ASSESSMENT , many researchers have taken to report PRECISION-RECALL CURVES and associated areas as PERFORMANCE METRIC . we demonstrate in this paper that this practice is <unk> with difficulties , mainly because of INCOHERENT SCALE ASSUMPTIONS -- e.g. , the area under a PR CURVE takes the ARITHMETIC MEAN OF PRECISION VALUES whereas the F Β SCORE applies the harmonic mean . we show how to fix this by plotting PR CURVES in a different COORDINATE SYSTEM , and demonstrate that the new PRECISION-RECALL-GAIN CURVES inherit all key advantages of ROC CURVES . in particular , the area under PRECISION-RECALL-GAIN CURVES conveys an expected f 1 score on a HARMONIC SCALE , and the CONVEX HULL of a PRECISION-RECALL-GAIN CURVE allows us to calibrate the CLASSIFIER 's scores so as to determine , for each operating point on the CONVEX HULL , the INTERVAL OF Β VALUES for which the point optimises F Β . we demonstrate experimentally that the area under traditional PR CURVES can easily favour models with lower expected f 1 score than others , and so the use of PRECISION-RECALL-GAIN CURVES will result in better MODEL SELECTION . \n",
            "in this paper , we propose a novel approach to MODEL SELECTION based on PR CURVES . the proposed approach is based on the use of PR CURVES , which is a CONVEX HULL of the PR CURVE . the proposed method is based on the use of PR CURVES , which is a CONVEX HULL . the proposed method is based on the use of PRECISION-RECALL-GAIN CURVES , which is a CONVEX HULL . the proposed method is based on the use of PRECISION-RECALL-GAIN CURVES , which is a CONVEX HULL . the proposed method can be applied to BINARY CLASSIFICATION . the proposed method is evaluated in terms of the F Β SCORE and the quality of the resulting CLASSIFIER . the results show that the proposed method is effective in improving the F Β SCORE of the COORDINATE SYSTEM .\n",
            "\n",
            "466 1000\n",
            "sparse FEATURE SELECTION has been demonstrated to be effective in handling HIGH-DIMENSIONAL DATA . while promising , most of the existing works use CONVEX METHODS , which may be suboptimal in terms of the ACCURACY of FEATURE SELECTION and PARAMETER ESTIMATION . in this paper , we expand a NONCONVEX PARADIGM to SPARSE GROUP FEATURE SELECTION , which is motivated by applications that require identifying the underlying group structure and performing FEATURE SELECTION simultaneously . the main contributions of this article are twofold : -lrb- 1 -rrb- statistically , we introduce a NONCONVEX SPARSE GROUP FEATURE SELECTION MODEL which can reconstruct the ORACLE ESTIMATOR . therefore , consistent FEATURE SELECTION and PARAMETER ESTIMATION can be achieved ; -lrb- 2 -rrb- computationally , we propose an efficient algorithm that is applicable to LARGE-SCALE PROBLEMS . numerical results suggest that the proposed NONCONVEX SPARSE GROUP FEATURE SELECTION MODEL compares favorably against its competitors on SYNTHETIC DATA and REAL-WORLD APPLICATIONS , thus achieving desired goal of delivering high performance . \n",
            "this paper presents a novel NONCONVEX SPARSE GROUP FEATURE SELECTION MODEL for HIGH-DIMENSIONAL DATA . the proposed NONCONVEX SPARSE GROUP FEATURE SELECTION MODEL is based on a NONCONVEX PARADIGM , which is a generalization of the NONCONVEX SPARSE GROUP FEATURE SELECTION MODEL to the NONCONVEX PARADIGM . the proposed NONCONVEX SPARSE GROUP FEATURE SELECTION MODEL is based on the use of SPARSE GROUP FEATURE SELECTION and PARAMETER ESTIMATION . the proposed NONCONVEX SPARSE GROUP FEATURE SELECTION MODEL is based on the use of SPARSE GROUP FEATURE SELECTION and PARAMETER ESTIMATION . the proposed NONCONVEX SPARSE GROUP FEATURE SELECTION MODEL is evaluated on both SYNTHETIC DATA and REAL-WORLD APPLICATIONS and REAL-WORLD APPLICATIONS . the experimental results show that the proposed NONCONVEX SPARSE GROUP FEATURE SELECTION MODEL is effective in improving the ACCURACY and ACCURACY of the proposed NONCONVEX SPARSE GROUP FEATURE SELECTION MODEL .\n",
            "\n",
            "467 1000\n",
            "1 this paper presents an analysis of LOMBARD SPEECH produced under different types and levels of NOISE . the speech used for the analysis forms a part of the UT-SCOPE DATABASE and consists of sentences from the well-known TIMIT CORPUS , spoken in the presence of <unk> , large crowd and <unk> NOISE . differences are shown to exist in the SPEECH CHARACTERISTICS under these varying NOISE TYPES . the deterioration of the EER of an INSET SPEAKER IDENTIFICATION SYSTEM trained on neutral and tested with LOMBARD SPEECH is also illustrated . a clear <unk> between the effect of NOISE and lombard effect on NOISE is also given by testing with NOISY LOMBARD SPEECH . the effect of TEST-TOKEN DURATION on system performance under the LOMBARD CONDITION is addressed . it is seen that test duration has no effect on the EER under lombard effect . the average EER for <unk> test duration is 14 . \n",
            "this paper presents a novel approach to NOISY LOMBARD SPEECH based on LOMBARD SPEECH . the proposed INSET SPEAKER IDENTIFICATION SYSTEM is based on a INSET SPEAKER IDENTIFICATION SYSTEM trained on LOMBARD SPEECH . the proposed method uses a INSET SPEAKER IDENTIFICATION SYSTEM trained on a UT-SCOPE DATABASE of the TIMIT CORPUS . the proposed method is evaluated on the TIMIT CORPUS and the results show that the proposed method is effective in improving the EER of the INSET SPEAKER IDENTIFICATION SYSTEM . the performance of the proposed INSET SPEAKER IDENTIFICATION SYSTEM is evaluated on the TIMIT CORPUS and the results show that the proposed method is robust to NOISE and NOISE .\n",
            "\n",
            "468 1000\n",
            "<unk> speech prosody is a primary characteristic of AUTISM SPECTRUM DISORDERS , yet ATYPICAL SPEECH PROSODY is often excluded from DIAGNOSTIC INSTRUMENT ALGORITHMS due to poor subjective reliability . robust , objective prosodic cues can enhance our understanding of those aspects which are <unk> in autism . in this work , we connect objective <unk> descriptors of prosody to SUBJECTIVE PERCEPTIONS OF PROSODIC AWKWARDNESS . subjectively , more AWKWARD SPEECH is less expressive -lrb- more monotone -rrb- and more often has PERCEIVED AWKWARD RATE/RHYTHM , VOLUME , and INTONATION . we also find EXPRESSIVITY can be quantified through OBJECTIVE INTONATION VARIABILITY FEATURES , and that SPEAKING RATE and RHYTHM CUES are highly predictive of PERCEIVED AWKWARDNESS . ACOUSTIC-PROSODIC FEATURES are also able to significantly differentiate subjects with AUTISM SPECTRUM DISORDERS from typically developing -lrb- td -rrb- subjects in a CLASSIFICATION TASK , emphasizing the potential of AUTOMATED METHODS for DIAGNOSTIC EFFICIENCY and CLARITY . \n",
            "this paper presents a novel approach to SUBJECTIVE PERCEPTIONS OF PROSODIC AWKWARDNESS based on OBJECTIVE INTONATION VARIABILITY FEATURES and RHYTHM CUES . the proposed approach is based on the use of OBJECTIVE INTONATION VARIABILITY FEATURES and RHYTHM CUES . the proposed approach is based on the use of OBJECTIVE INTONATION VARIABILITY FEATURES and RHYTHM CUES . the proposed approach is based on the use of OBJECTIVE INTONATION VARIABILITY FEATURES and RHYTHM CUES . the proposed approach is evaluated on a CLASSIFICATION TASK and compared to the state-of-the-art AUTOMATED METHODS . the results show that the proposed method is effective in improving the DIAGNOSTIC EFFICIENCY and CLARITY .\n",
            "\n",
            "469 1000\n",
            "we present a UNIFIED MODEL for FACE DETECTION , POSE ESTIMATION , and LANDMARK ESTIMATION in REAL-WORLD , CLUTTERED IMAGES . our UNIFIED MODEL is based on a MIXTURES OF TREES with a shared pool of parts ; we UNIFIED MODEL every FACIAL LANDMARK as a part and use GLOBAL MIXTURES to capture TOPOLOGICAL CHANGES due to VIEWPOINT . we show that TREE-STRUCTURED MODELS are surprisingly effective at capturing GLOBAL ELASTIC DEFORMATION , while being easy to optimize unlike DENSE GRAPH STRUCTURES . we present extensive results on standard FACE BENCHMARKS , as well as a new '' in the WILD '' ANNOTATED DATASET , that suggests our UNIFIED MODEL advances the state-of-the-art , sometimes considerably , for all three tasks . though our UNIFIED MODEL is <unk> trained with hundreds of faces , UNIFIED MODEL compares favorably to COMMERCIAL SYSTEMS trained with billions of examples -lrb- such as GOOGLE PICASA and <unk> -rrb- . \n",
            "this paper presents a novel UNIFIED MODEL for POSE ESTIMATION and POSE ESTIMATION . the proposed UNIFIED MODEL is based on a UNIFIED MODEL that uses MIXTURES OF TREES to represent the FACIAL LANDMARK . the proposed UNIFIED MODEL is based on a UNIFIED MODEL , which is able to capture the GLOBAL ELASTIC DEFORMATION of the FACIAL LANDMARK . the proposed UNIFIED MODEL is evaluated on the WILD '' ANNOTATED DATASET , and the results show that the proposed UNIFIED MODEL is effective for POSE ESTIMATION and POSE ESTIMATION . in addition , the proposed UNIFIED MODEL is robust to TOPOLOGICAL CHANGES , POSE ESTIMATION and POSE ESTIMATION . the experimental results on the WILD '' ANNOTATED DATASET demonstrate the effectiveness of the proposed UNIFIED MODEL .\n",
            "\n",
            "470 1000\n",
            "we design a new LEARNING ALGORITHM for the SET COVERING MACHINE from a PAC-BAYES PERSPECTIVE and propose a <unk> risk bound which is minimized for CLASSIFIERS achieving a NON TRIVIAL MARGIN-SPARSITY TRADE-OFF . \n",
            "this paper presents a novel LEARNING ALGORITHM for the SET COVERING MACHINE . the proposed LEARNING ALGORITHM is based on a PAC-BAYES PERSPECTIVE , which is a SET COVERING MACHINE . the proposed LEARNING ALGORITHM is based on a PAC-BAYES PERSPECTIVE , which is based on the SET COVERING MACHINE . the proposed LEARNING ALGORITHM is applied to the problem of PAC-BAYES PERSPECTIVE , and the results show that the proposed LEARNING ALGORITHM is very effective .\n",
            "\n",
            "471 1000\n",
            "an approach to CLUSTERING is presented that adapts the basic TOP-DOWN INDUCTION OF DECISION TREES METHOD towards CLUSTERING . to this aim , it employs the principles of INSTANCE BASED LEARNING . the resulting methodology is implemented in the TOP-DOWN INDUCTION OF DECISION TREES METHOD -lrb- top down induction of clustering trees -rrb- system for FIRST ORDER CLUSTERING . the TOP-DOWN INDUCTION OF DECISION TREES METHOD employs the first ORDER LOGICAL DECISION TREE REPRESENTATION of the INDUCTIVE LOGIC PROGRAMMING SYSTEM <unk> . various experiments with TOP-DOWN INDUCTION OF DECISION TREES METHOD are presented , in both PROPOSITIONAL AND RE-LATIONAL DOMAINS . \n",
            "this paper presents a novel TOP-DOWN INDUCTION OF DECISION TREES METHOD for CLUSTERING . the proposed TOP-DOWN INDUCTION OF DECISION TREES METHOD is based on a TOP-DOWN INDUCTION OF DECISION TREES METHOD , which is based on a TOP-DOWN INDUCTION OF DECISION TREES METHOD . the proposed TOP-DOWN INDUCTION OF DECISION TREES METHOD is based on a TOP-DOWN INDUCTION OF DECISION TREES METHOD . the proposed TOP-DOWN INDUCTION OF DECISION TREES METHOD is based on a TOP-DOWN INDUCTION OF DECISION TREES METHOD , which is based on a TOP-DOWN INDUCTION OF DECISION TREES METHOD . the performance of the proposed INDUCTIVE LOGIC PROGRAMMING SYSTEM is evaluated on a variety of PROPOSITIONAL AND RE-LATIONAL DOMAINS . the results show that the proposed method is effective in improving the performance of the INDUCTIVE LOGIC PROGRAMMING SYSTEM .\n",
            "\n",
            "472 1000\n",
            "in this paper , we present our CROSSWORD PUZZLE RESOLUTION SYSTEM , which exploits SYNTACTIC STRUCTURES for CLUE RERANKING and ANSWER EXTRACTION . CROSSWORD PUZZLE RESOLUTION SYSTEM uses a database -lrb- db -rrb- containing previously solved <unk> in order to generate the list of candidate answers . additionally , CROSSWORD PUZZLE RESOLUTION SYSTEM uses innovative FEATURES , such as the ANSWER POSITION in the RANK and AGGREGATED INFORMATION such as the MIN , max and AVERAGE CLUE RERANKING SCORES . our CROSSWORD PUZZLE RESOLUTION SYSTEM is based on WEBCROW , one of the most advanced systems for AUTOMATIC CROSSWORD PUZZLE RESOLUTION . our extensive experiments over our two million CLUE DATASET show that our CROSSWORD PUZZLE RESOLUTION SYSTEM highly improves the quality of the ANSWER LIST , enabling the achievement of unprecedented results on the complete CP RESOLUTION TASKS , i.e. , ACCURACY of <unk> % . \n",
            "in this paper , we propose a novel CROSSWORD PUZZLE RESOLUTION SYSTEM based on CLUE RERANKING and ANSWER EXTRACTION . the proposed CROSSWORD PUZZLE RESOLUTION SYSTEM is based on the use of a set of FEATURES extracted from the ANSWER LIST of the ANSWER LIST , which are then used to estimate the ANSWER POSITION . the proposed CROSSWORD PUZZLE RESOLUTION SYSTEM is based on the use of AGGREGATED INFORMATION extracted from the ANSWER LIST of the ANSWER LIST . the proposed CROSSWORD PUZZLE RESOLUTION SYSTEM is evaluated on CP RESOLUTION TASKS and ANSWER EXTRACTION . the experimental results show that the proposed CROSSWORD PUZZLE RESOLUTION SYSTEM is effective in improving the ACCURACY of the CROSSWORD PUZZLE RESOLUTION SYSTEM in CP RESOLUTION TASKS and ANSWER EXTRACTION .\n",
            "\n",
            "473 1000\n",
            "incomplete preferences are likely to arise in REAL-WORLD PREFERENCE AGGREGATION and VOTING SYSTEMS . this paper deals with determining whether an INCOMPLETE PREFERENCE PROFILE is single-peaked . this is essential information since many intractable voting problems become tractable for SINGLE-PEAKED PROFILES . we prove that for incomplete profiles the problem of DETERMINING SINGLE-PEAKEDNESS is np-complete . despite this computational hardness result , we find two POLYNOMIAL-TIME ALGORITHMS for reasonably restricted settings . \n",
            "this paper addresses the problem of DETERMINING SINGLE-PEAKEDNESS and VOTING SYSTEMS . in particular , we consider the problem of DETERMINING SINGLE-PEAKEDNESS in the presence of INCOMPLETE PREFERENCES and VOTING SYSTEMS . we show that this problem can be viewed as a special case of VOTING SYSTEMS and VOTING SYSTEMS . we then show how this problem can be solved efficiently using POLYNOMIAL-TIME ALGORITHMS and VOTING SYSTEMS .\n",
            "\n",
            "474 1000\n",
            "we investigate the problem of reducing the COMPLEXITY of a GRAPHICAL MODEL -lrb- G , p G -rrb- by finding a SUBGRAPH H OF G , chosen from a class of SUBGRAPHS H , such that h is optimal with respect to KL-DIVERGENCE . we do this by first defining a DECOMPOSITION TREE REPRESENTATION for G , which is closely related to the JUNCTION-TREE REPRESENTATION for G . we then give an algorithm which uses this DECOMPOSITION TREE REPRESENTATION to compute the optimal h ∈ h. <unk> -lsb- 2 -rsb- and <unk> -lsb- 3 -rsb- have used GRAPH SEPARATION PROPERTIES to solve several COMBINATORIAL OPTIMIZATION PROBLEMS when the size of the MINIMAL SEPARATORS in the G is bounded . we present an extension of this technique which applies to some important choices of h even when the size of the MINIMAL SEPARATORS of G are arbitrarily large . in particular , this applies to problems such as finding an optimal SUBGRAPHICAL MODEL over a -lrb- k − 1 -rrb- - TREE of a GRAPHICAL MODEL over a <unk> -lrb- for arbitrary k -rrb- and selecting an optimal SUBGRAPHICAL MODEL with -lrb- a constant -rrb- d fewer edges with respect to KL-DIVERGENCE can be solved in time polynomial in | v -lrb- G -rrb- | using this formulation . \n",
            "this paper addresses the problem of COMBINATORIAL OPTIMIZATION PROBLEMS from a GRAPHICAL MODEL . we propose a method to estimate the SUBGRAPH H OF G of the TREE of the TREE , which is a SUBGRAPH H OF G of the TREE of the TREE . we show that the G of the TREE can be approximated by a SUBGRAPH H OF G , which is a function of the COMPLEXITY of the TREE . we show that the COMPLEXITY of the TREE can be reduced by a factor of o -lrb- 1 / √ t -rrb- , where n is the number of G , and the number of samples n is the same order of magnitude .\n",
            "\n",
            "475 1000\n",
            "multilingual applications frequently involve dealing with proper names , but names are often missing in BILINGUAL LEXICONS . this MULTILINGUAL APPLICATIONS is exacerbated for applications involving translation between LATIN-SCRIPTED LANGUAGES and ASIAN LANGUAGES such as CHINESE , JAPANESE and korean -lrb- <unk> -rrb- where simple STRING COPYING is not a solution . we present a novel approach for generating the IDEOGRAPHIC REPRESENTATIONS of a CJK NAME written in a LATIN SCRIPT . the proposed approach involves first identifying the origin of the name , and then <unk> the name to all possible CHINESE CHARACTERS using LANGUAGE-SPECIFIC MAPPINGS . to reduce the massive number of possibilities for COMPUTATION , we apply a THREE-TIER FILTERING PROCESS by filtering first through a set of ATTESTED BIGRAMS , then through a set of attested terms , and lastly through the WWW for a final validation . we illustrate the approach with ENGLISH-TO-JAPANESE BACK-TRANSLITERATION . against test sets of JAPANESE given names and <unk> , we have achieved AVERAGE PRECISIONS of 73 % and 90 % , respectively . \n",
            "this paper presents a novel approach to the problem of CHINESE CHARACTERS in a THREE-TIER FILTERING PROCESS . the proposed approach is based on the use of ATTESTED BIGRAMS in the form of a THREE-TIER FILTERING PROCESS . the proposed approach is based on the use of ATTESTED BIGRAMS in the form of a THREE-TIER FILTERING PROCESS . the proposed approach is based on the use of ATTESTED BIGRAMS in the THREE-TIER FILTERING PROCESS . the proposed approach is based on the use of ATTESTED BIGRAMS in the THREE-TIER FILTERING PROCESS . the proposed approach is evaluated on a variety of ASIAN LANGUAGES , including JAPANESE , CHINESE and JAPANESE . the results show that the proposed approach is able to detect CHINESE CHARACTERS , and outperforms the other methods in the literature .\n",
            "\n",
            "476 1000\n",
            "our study deals with a SILENT SPEECH INTERFACE based on MAPPING SURFACE ELECTROMYOGRAPHIC SIGNALS to SPEECH WAVEFORMS . ELECTROMYOGRAPHIC SIGNALS recorded from the FACIAL MUSCLES capture the activity of the HUMAN ARTICULATORY APPARATUS and therefore allow to <unk> speech , even when no AUDIBLE SIGNAL is produced . the MAPPING OF EMG SIGNALS to speech is done via a GAUSSIAN MIXTURE MODEL - based conversion technique . in this paper , we follow the lead of EMG-BASED SPEECH-TO-TEXT SYSTEMS and apply two major recent technological advances to our system , namely , we consider EMG-BASED SPEECH-TO-TEXT SYSTEMS , which are robust against ELECTRODE REPOSITIONING , and we show that MAPPING the EMG SIGNAL to WHISPERED SPEECH creates a better SPEECH SIGNAL than a MAPPING to normally SPOKEN SPEECH . we objectively evaluate the performance of our systems using a SPECTRAL DISTORTION MEASURE . \n",
            "this paper presents a novel approach to MAPPING SURFACE ELECTROMYOGRAPHIC SIGNALS from SPOKEN SPEECH . the proposed approach is based on a GAUSSIAN MIXTURE MODEL that uses a GAUSSIAN MIXTURE MODEL to estimate the AUDIBLE SIGNAL . the proposed method is based on a GAUSSIAN MIXTURE MODEL that uses a GAUSSIAN MIXTURE MODEL to estimate the AUDIBLE SIGNAL . the proposed method is based on the use of a GAUSSIAN MIXTURE MODEL as a MAPPING for ELECTRODE REPOSITIONING . the experimental results show that the proposed method is effective in improving the MAPPING OF EMG SIGNALS performance of SPOKEN SPEECH . the proposed method is robust to SPOKEN SPEECH in the presence of SPOKEN SPEECH .\n",
            "\n",
            "477 1000\n",
            "large-scale clustering has found wide applications in many fields and received much attention in recent years . however , most existing LARGE-SCALE CLUSTERING METHODS can only achieve <unk> performance , because LARGE-SCALE CLUSTERING METHODS are sensitive to the unavoidable presence of NOISE in the LARGE-SCALE DATA . to address this challenging problem , we thus propose a LARGE-SCALE SPARSE CLUSTERING ALGORITHM . in this paper , we choose a TWO-STEP OPTIMIZATION STRATEGY for LARGE-SCALE SPARSE CLUSTERING : 1 -rrb- k-means clustering over the LARGE-SCALE DATA to obtain the initial clustering results ; 2 -rrb- CLUSTERING REFINEMENT over the initial results by developing a SPARE CODING ALGORITHM . to guarantee the <unk> of the second step for LARGE-SCALE DATA , we also utilize NONLINEAR APPROXIMATION and DIMENSION REDUCTION TECHNIQUES to speed up the SPARE CODING ALGORITHM . experimental results on both SYNTHETIC AND REAL-WORLD DATASETS demonstrate the promising performance of our LARGE-SCALE SPARSE CLUSTERING ALGORITHM . \n",
            "this paper presents a novel LARGE-SCALE SPARSE CLUSTERING ALGORITHM for LARGE-SCALE SPARSE CLUSTERING . the proposed LARGE-SCALE SPARSE CLUSTERING ALGORITHM is based on the SPARE CODING ALGORITHM and the DIMENSION REDUCTION TECHNIQUES . the proposed SPARE CODING ALGORITHM is based on the use of NONLINEAR APPROXIMATION and DIMENSION REDUCTION TECHNIQUES . the proposed SPARE CODING ALGORITHM is based on the use of NONLINEAR APPROXIMATION and DIMENSION REDUCTION TECHNIQUES to estimate the NOISE . the proposed SPARE CODING ALGORITHM is evaluated on both SYNTHETIC AND REAL-WORLD DATASETS . the experimental results show that the proposed SPARE CODING ALGORITHM is effective in improving the NOISE and NOISE .\n",
            "\n",
            "478 1000\n",
            "we present a NONLINEAR FORWARD-SEARCH METHOD suitable for planning the reactions of an agent operating in a highly unpredictable environment . we show that this NONLINEAR FORWARD-SEARCH METHOD is more eecient than existing LINEAR METHODS . we then introduce the notion of SAFETY AND LIVENESS RULES . this makes possible a sharper exploitation of the information retrieved when exploring the future of the agent . \n",
            "this paper proposes a novel NONLINEAR FORWARD-SEARCH METHOD , which is based on a NONLINEAR FORWARD-SEARCH METHOD . the proposed NONLINEAR FORWARD-SEARCH METHOD is based on the use of a set of SAFETY AND LIVENESS RULES , each of which is a set of SAFETY AND LIVENESS RULES . the proposed NONLINEAR FORWARD-SEARCH METHOD is applied to the problem of SAFETY AND LIVENESS RULES . experimental results show that the proposed NONLINEAR FORWARD-SEARCH METHOD outperforms the existing LINEAR METHODS .\n",
            "\n",
            "479 1000\n",
            "in this paper , we develop a new method for WEIGHTED LEAST SQUARES 2D LINEAR-PHASE FIR FILTER DESIGN . it poses the problem of FILTER DESIGN as the problem of projecting the DESIRED FREQUENCY RESPONSE onto the SUBSPACE spanned by an appropriate ORTHONORMAL BASIS . we show how to compute the ORTHONORMAL BASIS efficiently in the cases of QUADRANTALLY-SYMMETRIC FILTER DESIGN and CENTRO-SYMMETRIC FILTER DESIGN . the design examples show that the proposed method is faster than a conventional WEIGHTED LEAST SQUARES FILTER DESIGN METHOD . also , the amount of STORAGE required to compute the FILTER COEFFICIENTS is greatly reduced . \n",
            "this paper addresses the problem of QUADRANTALLY-SYMMETRIC FILTER DESIGN and CENTRO-SYMMETRIC FILTER DESIGN . in particular , we consider the problem of CENTRO-SYMMETRIC FILTER DESIGN and QUADRANTALLY-SYMMETRIC FILTER DESIGN . we propose a new algorithm , called the WEIGHTED LEAST SQUARES FILTER DESIGN METHOD , to estimate the FILTER COEFFICIENTS of the FILTER COEFFICIENTS . the proposed algorithm is based on the use of a WEIGHTED LEAST SQUARES FILTER DESIGN METHOD and a WEIGHTED LEAST SQUARES 2D LINEAR-PHASE FIR FILTER DESIGN . the proposed algorithm is based on the WEIGHTED LEAST SQUARES FILTER DESIGN METHOD and the QUADRANTALLY-SYMMETRIC FILTER DESIGN . the performance of the proposed algorithm is demonstrated on a variety of STORAGE .\n",
            "\n",
            "480 1000\n",
            "<unk> and STOCHASTIC EQUIVALENCE are two primary features of interest in SOCIAL NETWORKS . recently , the MULTIPLICATIVE LATENT FACTOR MODEL is proposed to model SOCIAL NETWORKS with DIRECTED LINKS . although MULTIPLICATIVE LATENT FACTOR MODEL can capture STOCHASTIC EQUIVALENCE , it can not model well <unk> in NETWORK STRUCTURE . however , many REAL-WORLD NETWORKS exhibit HOMOPHILY or both HOMOPHILY and STOCHASTIC EQUIVALENCE , and hence the NETWORK STRUCTURE of these NETWORK STRUCTURE can not be mod-eled well by MULTIPLICATIVE LATENT FACTOR MODEL . in this paper , we propose a novel model , called generalized latent factor model -lrb- <unk> -rrb- , for SOCIAL NETWORK ANALYSIS by enhancing HOMOPHILY MODELING in MULTIPLICATIVE LATENT FACTOR MODEL . we devise a MINORIZATION-MAXIMIZATION ALGORITHM with LINEAR-TIME COMPLEXITY and CONVERGENCE GUARANTEE to learn the MODEL PARAMETERS . extensive experiments on some REAL-WORLD NETWORKS show that <unk> can effectively model HOMOPHILY to dramatically outperform state-of-the-art methods . \n",
            "this paper addresses the problem of HOMOPHILY MODELING for SOCIAL NETWORKS . in particular , we propose a novel MULTIPLICATIVE LATENT FACTOR MODEL for SOCIAL NETWORKS . the proposed MULTIPLICATIVE LATENT FACTOR MODEL is based on the use of STOCHASTIC EQUIVALENCE and CONVERGENCE GUARANTEE . the proposed MULTIPLICATIVE LATENT FACTOR MODEL is based on the use of STOCHASTIC EQUIVALENCE and CONVERGENCE GUARANTEE . the proposed MULTIPLICATIVE LATENT FACTOR MODEL is applied to the problem of HOMOPHILY MODELING , which is based on the MULTIPLICATIVE LATENT FACTOR MODEL . the proposed MULTIPLICATIVE LATENT FACTOR MODEL is applied to the problem of HOMOPHILY MODELING , and the experimental results demonstrate the effectiveness of the proposed MULTIPLICATIVE LATENT FACTOR MODEL .\n",
            "\n",
            "481 1000\n",
            "in WIRELESS SYSTEMS where half <unk> transceivers are employed , most existing practical COOPERATIVE PROTOCOLS achieve a SPECTRAL EFFICIENCY of 0.5 symbols per channel use -lrb- <unk> -rrb- . recently a DECODE-AND-FORWARD PROTOCOL was developed to achieve a SPECTRAL EFFICIENCY of 2/3 symbols <unk> . but there is no practical AMPLIFY-AND-FORWARD PROTOCOL that can achieve a SPECTRAL EFFICIENCY higher than 0.5 symbols <unk> . in this paper , we develop a NONORTHOGONAL AF PROTOCOL which achieves a SPECTRAL EFFICIENCY of 2/3 symbols <unk> and provides almost the same bit ERROR RATE as the traditional ORTHOGONAL AF which has a SPECTRAL EFFICIENCY of 0.5 symbols <unk> . \n",
            "this paper presents a new approach to WIRELESS SYSTEMS for WIRELESS SYSTEMS . the proposed approach is based on the use of a set of COOPERATIVE PROTOCOLS , which are used in a DECODE-AND-FORWARD PROTOCOL . the proposed COOPERATIVE PROTOCOLS is evaluated in terms of the ERROR RATE of the ORTHOGONAL AF . the results show that the proposed COOPERATIVE PROTOCOLS can reduce the ERROR RATE by about 10 % when compared to a NONORTHOGONAL AF PROTOCOL . the ERROR RATE of the proposed method is comparable to that of the conventional COOPERATIVE PROTOCOLS .\n",
            "\n",
            "482 1000\n",
            "it is a common practice to approximate '' complicated '' functions with more friendly ones . in LARGE-SCALE MACHINE LEARNING APPLICATIONS , NONSMOOTH LOSSES/REGULARIZERS that entail great COMPUTATIONAL CHALLENGES are usually approximated by SMOOTH FUNCTIONS . we reexamine this powerful methodology and point out a NONSMOOTH APPROXIMATION which simply <unk> the linearity of the PROXI-MAL MAP . the new approximation is justified using a recent CONVEX ANALYSIS TOOL -- proximal average , and yields a novel PROXIMAL GRADIENT ALGORITHM that is strictly better than the one based on SMOOTHING , without incurring any extra OVERHEAD . numerical experiments conducted on two important applications , OVERLAPPING GROUP LASSO and <unk> fused lasso , corroborate the theoretical claims . \n",
            "this paper addresses the problem of OVERLAPPING GROUP LASSO in the presence of SMOOTH FUNCTIONS . in particular , we focus on the problem of OVERLAPPING GROUP LASSO , where the number of SMOOTH FUNCTIONS grows exponentially with the number of SMOOTH FUNCTIONS . we propose a new algorithm , called the PROXIMAL GRADIENT ALGORITHM , which is able to efficiently solve the problem of SMOOTHING . the algorithm is based on the use of SMOOTHING , which is an extension of the well-known PROXIMAL GRADIENT ALGORITHM . the algorithm is based on the use of SMOOTHING , which is a generalization of the standard PROXIMAL GRADIENT ALGORITHM . the algorithm is tested on a variety of COMPUTATIONAL CHALLENGES , and the results show that the proposed algorithm is able to achieve the same performance as the standard PROXIMAL GRADIENT ALGORITHM .\n",
            "\n",
            "483 1000\n",
            "this work is a contribution to the analysis of the procedure , based on WAVELET COEECIENT PARTITION FUNCTIONS , commonly used to estimate the LEGENDRE MULTIFRACTAL SPECTRUM . the procedure is applied to two examples , a FRACTIONAL BROWNIAN MOTION in MULTIFRACTAL TIME and a SELF-SIMILAR-STABLE PROCESS , whose SAMPLE PATHS exhibit irregularities that by eye appear very close . we observe that , for the second example , this analysis results in a qualitatively inaccurate estimation of its MULTIFRACTAL SPECTRUM , and a related masking of <unk> nature of the process . we explain the origin of this error through a detailed analysis of the PARTITION FUNCTIONS of the SELF-SIMILAR-STABLE PROCESS . such a study is made possible by the speciic properties of the WAVELET CO-EECIENTS of such processes . we indicate how the ESTIMATION PROCEDURE might be modiied to avoid such errors . \n",
            "this paper addresses the problem of recovering the MULTIFRACTAL SPECTRUM of a MULTIFRACTAL SPECTRUM under a SELF-SIMILAR-STABLE PROCESS . the main contribution of this paper is to show that the LEGENDRE MULTIFRACTAL SPECTRUM of the LEGENDRE MULTIFRACTAL SPECTRUM can be approximated by a SELF-SIMILAR-STABLE PROCESS . the algorithm is based on a SELF-SIMILAR-STABLE PROCESS and a SELF-SIMILAR-STABLE PROCESS . the algorithm is based on a SELF-SIMILAR-STABLE PROCESS and a SELF-SIMILAR-STABLE PROCESS . the algorithm is based on a SELF-SIMILAR-STABLE PROCESS and a SELF-SIMILAR-STABLE PROCESS . the performance of the proposed algorithm is demonstrated on a variety of SAMPLE PATHS .\n",
            "\n",
            "484 1000\n",
            "joint object recognition and POSE ESTIMATION solely from RANGE IMAGES is an important task e.g. in ROBOTICS APPLICATIONS and in AUTOMATED MANUFACTURING ENVIRONMENTS . the lack of COLOR INFORMATION and limitations of current COMMODITY DEPTH SENSORS make this task a challenging COMPUTER VISION PROBLEM , and a standard RANDOM SAMPLING BASED APPROACH is prohibitively time-consuming . we propose to address this difficult problem by generating promising inlier sets for POSE ESTIMATION by early rejection of CLEAR OUTLIERS with the help of LOCAL BELIEF PROPAGATION -lrb- or DYNAMIC PROGRAMMING -rrb- . by exploiting <unk> our method is fast , and we also do not rely on a COMPUTATIONALLY EXPENSIVE TRAINING PHASE . we demonstrate state-of-the art performance on a standard dataset and illustrate our approach on challenging REAL SEQUENCES . \n",
            "this paper addresses the problem of POSE ESTIMATION and ROBOTICS APPLICATIONS in AUTOMATED MANUFACTURING ENVIRONMENTS . we propose a novel approach to JOINT OBJECT RECOGNITION and ROBOTICS APPLICATIONS . the proposed approach is based on the use of COMMODITY DEPTH SENSORS , which is able to deal with CLEAR OUTLIERS and AUTOMATED MANUFACTURING ENVIRONMENTS . the proposed approach is based on the use of COMMODITY DEPTH SENSORS , which is able to deal with CLEAR OUTLIERS and CLEAR OUTLIERS . the proposed approach is based on the use of COMMODITY DEPTH SENSORS and DYNAMIC PROGRAMMING . the proposed approach is evaluated on a variety of REAL SEQUENCES and REAL SEQUENCES . the experimental results show that the proposed method outperforms the state-of-the-art methods in terms of POSE ESTIMATION and ROBOTICS APPLICATIONS .\n",
            "\n",
            "485 1000\n",
            "enlarging or reducing the TEMPLATE SIZE by adding new parts , or removing parts of the template , according to their suitability for TRACKING , requires the ability to deal with the variation of the TEMPLATE SIZE . for instance , REAL-TIME TEMPLATE TRACKING using LINEAR PREDICTORS , although fast and reliable , requires using templates of fixed size and does not allow ON-LINE MODIFICATION of the predictor . to solve this problem we propose the ADAPTIVE LINEAR PREDICTORS which enable fast online modifications of PRE-LEARNED LINEAR PREDICTORS . instead of applying a FULL MATRIX INVERSION for every modification of the TEMPLATE SHAPE as standard approaches to learning LINEAR PREDICTORS do , we just perform a fast update of this inverse . this allows us to learn the ADAPTIVE LINEAR PREDICTORS in a much shorter time than standard LEARNING APPROACHES while performing equally well . we performed exhaustive evaluation of our ADAPTIVE LINEAR PREDICTORS and compared ADAPTIVE LINEAR PREDICTORS to standard LINEAR PREDICTORS and other state of the art approaches . \n",
            "in this paper , we propose a novel approach to REAL-TIME TEMPLATE TRACKING based on FULL MATRIX INVERSION . the proposed ADAPTIVE LINEAR PREDICTORS is based on the use of PRE-LEARNED LINEAR PREDICTORS to estimate the TEMPLATE SHAPE . the proposed method is based on the use of PRE-LEARNED LINEAR PREDICTORS to estimate the TEMPLATE SHAPE of the TEMPLATE SHAPE . the proposed ADAPTIVE LINEAR PREDICTORS are compared to the traditional LINEAR PREDICTORS , and the results show that the proposed ADAPTIVE LINEAR PREDICTORS are more effective than conventional LEARNING APPROACHES . the proposed ADAPTIVE LINEAR PREDICTORS is more effective than conventional LINEAR PREDICTORS , especially in the presence of TEMPLATE SIZE .\n",
            "\n",
            "486 1000\n",
            "we introduce a novel approach to the CEREBRAL WHITE MATTER CONNECTIVITY MAPPING from DIFFUSION TENSOR MRI . DT-MRI is the unique NON-INVASIVE TECHNIQUE capable of probing and quantifying the ANISOTROPIC DIFFUSION OF WATER MOLECULES in BIOLOGICAL TISSUES . we address the problem of CONSISTENT NEURAL FIBERS RECONSTRUCTION in areas of COMPLEX DIFFUSION PROFILES with potentially multiple fibers orientations . our method relies on a GLOBAL MODELIZATION of the acquired mri volume as a RIEMANNIAN MANIFOLD M and proceeds in 4 <unk> steps : first , we establish the link between BROWNIAN MOTION and diffusion mri by using the LAPLACE-BELTRAMI OPERATOR on m . we then expose how the sole knowledge of the diffusion properties of water molecules on m is sufficient to infer its geometry . there exists a DIRECT MAPPING between the DIFFUSION TENSOR and the metric of m. next , having access to that metric , we propose a novel LEVEL SET FORMULATION SCHEME to approximate the DISTANCE FUNCTION related to a RADIAL BROWNIAN MOTION on m. finally , a rigorous NUMERICAL SCHEME using the EXPONENTIAL MAP is derived to estimate the GEODESICS OF M , seen as the DIFFUSION PATHS OF WATER MOLECULES . numerical experimentations conducted on SYNTHETIC AND REAL DIFFUSION MRI DATASETS illustrate the potentialities of this LEVEL SET FORMULATION SCHEME . \n",
            "this paper presents a novel LEVEL SET FORMULATION SCHEME for DIFFUSION TENSOR MRI . the proposed LEVEL SET FORMULATION SCHEME is based on a NON-INVASIVE TECHNIQUE , which is based on a NON-INVASIVE TECHNIQUE . the proposed LEVEL SET FORMULATION SCHEME is based on a NON-INVASIVE TECHNIQUE , which is based on the LAPLACE-BELTRAMI OPERATOR . the proposed LEVEL SET FORMULATION SCHEME is based on the use of the LAPLACE-BELTRAMI OPERATOR to estimate the LAPLACE-BELTRAMI OPERATOR of the LAPLACE-BELTRAMI OPERATOR . the proposed LEVEL SET FORMULATION SCHEME is based on the RIEMANNIAN MANIFOLD M , which is based on the RIEMANNIAN MANIFOLD M . the proposed LEVEL SET FORMULATION SCHEME is applied to the SYNTHETIC AND REAL DIFFUSION MRI DATASETS , and the experimental results show that the proposed LEVEL SET FORMULATION SCHEME is effective in reducing the BROWNIAN MOTION . the proposed LEVEL SET FORMULATION SCHEME is applied to SYNTHETIC AND REAL DIFFUSION MRI DATASETS , and the results show that the proposed LEVEL SET FORMULATION SCHEME is able to recover the DIFFUSION PATHS OF WATER MOLECULES of the DIFFUSION TENSOR .\n",
            "\n",
            "487 1000\n",
            "<unk> detection for MIXED-TYPE DATA is an important problem that has not been well addressed in the MACHINE LEARNING FIELD . there are two challenging issues for MIXED-TYPE DATASETS , namely modeling mutual correlations between MIXED-TYPE ATTRIBUTES and capturing large variations due to anomalies . this paper presents BUFFDETECT , a robust ERROR BUFFERING APPROACH for ANOMALY DETECTION in MIXED-TYPE DATASETS . a new variant of the GENERALIZED LINEAR MODEL is proposed to GENERALIZED LINEAR MODEL the dependency between MIXED-TYPE ATTRIBUTES . the GENERALIZED LINEAR MODEL incorporates an ERROR BUFFERING COMPONENT based on STUDENT-T DISTRIBUTION to absorb the variations caused by anomalies . however , because of the NON-GAUSSIAN DESIGN , the problem becomes analytically intractable . we propose a novel BAYESIAN INFERENCE APPROACH , which integrates LAPLACE APPROXIMATION and several COMPUTATIONAL OPTIMIZATIONS , and is able to efficiently approximate the POSTERIOR OF HIGH DIMENSIONAL LATENT VARIABLES by iteratively updating the LATENT VARIABLES in groups . extensive experimental evaluations based on 13 benchmark datasets demonstrate the effectiveness and efficiency of BUFFDETECT . \n",
            "in this paper , we propose a novel BAYESIAN INFERENCE APPROACH based on LAPLACE APPROXIMATION and COMPUTATIONAL OPTIMIZATIONS . the proposed BAYESIAN INFERENCE APPROACH consists of two steps : -lrb- 1 -rrb- a GENERALIZED LINEAR MODEL that combines a GENERALIZED LINEAR MODEL with a GENERALIZED LINEAR MODEL , and -lrb- 2 -rrb- a GENERALIZED LINEAR MODEL for ANOMALY DETECTION . the proposed BAYESIAN INFERENCE APPROACH consists of two steps : -lrb- 1 -rrb- a GENERALIZED LINEAR MODEL that combines a GENERALIZED LINEAR MODEL with a GENERALIZED LINEAR MODEL , and -lrb- 2 -rrb- a GENERALIZED LINEAR MODEL based on LAPLACE APPROXIMATION . the proposed BAYESIAN INFERENCE APPROACH consists of two steps : -lrb- 1 -rrb- a GENERALIZED LINEAR MODEL based on a GENERALIZED LINEAR MODEL , and -lrb- 2 -rrb- a GENERALIZED LINEAR MODEL based on a GENERALIZED LINEAR MODEL . the proposed GENERALIZED LINEAR MODEL is applied to the task of ANOMALY DETECTION in the context of ANOMALY DETECTION . the experimental results show that the proposed ERROR BUFFERING APPROACH is effective in improving the ANOMALY DETECTION performance .\n",
            "\n",
            "488 1000\n",
            "a blending of PHONOLOGICAL CONCEPTS and TECHNICAL ANALYSIS is proposed to yield a better MODELING AND UNDERSTANDING OF PHONOLOGICAL PROCESSES . based on the manual segmentation and labeling of the ITALIAN CLIPS CORPUS we automatically derive a probabilistic set of PHONOLOGICAL PRONUNCIATION RULES : a new ALIGNMENT TECHNIQUE is used to map the PHONOLOGICAL FORM OF SPONTANEOUS SENTENCES onto the PHONETIC SURFACE FORM . a MACHINE-LEARNING ALGORITHM then calculates a set of PHONOLOGI-CAL REPLACEMENT RULES together with their CONDITIONAL PROBABILITIES . a critical analysis of the resulting PROBABILISTIC RULE SET is presented and discussed with regard to REGIONAL ITALIAN ACCENTS . the RULE set presented here is also applied in the newly published <unk> <unk> that allows a user to segment and phonetically label ITALIAN SPEECH via a simple WEB-INTERFACE . \n",
            "in this paper , we propose a novel MACHINE-LEARNING ALGORITHM for MODELING AND UNDERSTANDING OF PHONOLOGICAL PROCESSES and TECHNICAL ANALYSIS . the proposed ALIGNMENT TECHNIQUE is based on a MACHINE-LEARNING ALGORITHM for MODELING AND UNDERSTANDING OF PHONOLOGICAL PROCESSES and TECHNICAL ANALYSIS . the proposed ALIGNMENT TECHNIQUE is based on a MACHINE-LEARNING ALGORITHM for MODELING AND UNDERSTANDING OF PHONOLOGICAL PROCESSES and TECHNICAL ANALYSIS . the proposed ALIGNMENT TECHNIQUE is based on the use of a WEB-INTERFACE , which is a WEB-INTERFACE . the proposed ALIGNMENT TECHNIQUE is applied to MODELING AND UNDERSTANDING OF PHONOLOGICAL PROCESSES and TECHNICAL ANALYSIS . experimental results on the ITALIAN CLIPS CORPUS show that the proposed approach is effective for MODELING AND UNDERSTANDING OF PHONOLOGICAL PROCESSES and ITALIAN SPEECH .\n",
            "\n",
            "489 1000\n",
            "this paper provides an analysis of the STEADY-STATE BEHAVIOR of the FILTERED-X AFFINE PROJECTION ALGORITHM . this efficient AFFINE PROJECTION ALGORITHM for ACTIVE NOISE CONTROL APPLICATIONS is based on the FILTERED-X SCHEME , unlike most AP ALGORITHMS based on the more COMPUTATIONALLY DEMANDING MODIFIED FILTERED-X SCHEME . this study depends on ENERGY CONSERVATION ARGUMENTS and does not require an specific SIGNAL DISTRIBUTION . the THEORETICAL EXPRESSIONS derived for the MEAN SQUARE ERROR allowed to accurately predict the steady-state performance of the AFFINE PROJECTION ALGORITHM for meaningful practical cases . simulation results of a SINGLE-CHANNEL ANC SYSTEM validate the analysis and the THEORETICAL EXPRESSIONS derived . \n",
            "this paper presents a novel COMPUTATIONALLY DEMANDING MODIFIED FILTERED-X SCHEME for ACTIVE NOISE CONTROL APPLICATIONS . the proposed COMPUTATIONALLY DEMANDING MODIFIED FILTERED-X SCHEME is based on a COMPUTATIONALLY DEMANDING MODIFIED FILTERED-X SCHEME for ACTIVE NOISE CONTROL APPLICATIONS . the proposed FILTERED-X AFFINE PROJECTION ALGORITHM is based on a COMPUTATIONALLY DEMANDING MODIFIED FILTERED-X SCHEME , which is based on a COMPUTATIONALLY DEMANDING MODIFIED FILTERED-X SCHEME . the proposed COMPUTATIONALLY DEMANDING MODIFIED FILTERED-X SCHEME is based on a COMPUTATIONALLY DEMANDING MODIFIED FILTERED-X SCHEME , which is based on a COMPUTATIONALLY DEMANDING MODIFIED FILTERED-X SCHEME . the proposed COMPUTATIONALLY DEMANDING MODIFIED FILTERED-X SCHEME is based on a COMPUTATIONALLY DEMANDING MODIFIED FILTERED-X SCHEME , which is based on a COMPUTATIONALLY DEMANDING MODIFIED FILTERED-X SCHEME . the proposed COMPUTATIONALLY DEMANDING MODIFIED FILTERED-X SCHEME is applied to ACTIVE NOISE CONTROL APPLICATIONS . the experimental results show that the proposed COMPUTATIONALLY DEMANDING MODIFIED FILTERED-X SCHEME is effective in improving the STEADY-STATE BEHAVIOR of the SINGLE-CHANNEL ANC SYSTEM .\n",
            "\n",
            "490 1000\n",
            "examples of FIGURATIVE LANGUAGE can range from the explicit and the obvious to the implicit and <unk> <unk> . some simpler forms , like SIMILE , often wear their meanings on their <unk> , while more challenging forms , like METAPHOR , can make CRYPTIC ALLUSIONS more akin to those of <unk> or CROSSWORD PUZZLES . in this paper we argue that because the same concepts and properties are described in either case , a COMPUTATIONAL AGENT can learn from the easy cases -lrb- explicit <unk> -rrb- how to comprehend and generate the HARD CASES -LRB- NON-EXPLICIT METAPHORS -rrb- . we demonstrate that the MARKED-NESS OF SIMILES allows for a large CASE-BASE of ILLUSTRATIVE EXAMPLES to be easily acquired from the web , and present a system , called <unk> , that uses this CASE-BASE both to understand PROPERTY-ATTRIBUTION METAPHORS and to generate <unk> <unk> for a given target on demand . in each case , we show how the text of the web is used as a source of TACIT KNOWLEDGE about what CATE-GORIZATIONS are allowable and what properties are most contextually appropriate . overall , we demonstrate that by using the web as a primary knowledge source , a system can achieve a robust and scalable competence with METAPHOR while minimizing the need for HAND-CRAFTED RESOURCES like WORDNET . \n",
            "in this paper , we present a novel approach to the problem of HARD CASES -LRB- NON-EXPLICIT METAPHORS in the context of FIGURATIVE LANGUAGE . the proposed approach is based on the use of a set of PROPERTY-ATTRIBUTION METAPHORS , a METAPHOR , and a METAPHOR . the proposed approach is based on the use of a set of PROPERTY-ATTRIBUTION METAPHORS , which are used in conjunction with a CRYPTIC ALLUSIONS . the proposed approach is based on the use of a METAPHOR , which allows for the MARKED-NESS OF SIMILES of the FIGURATIVE LANGUAGE . the proposed method is evaluated on a variety of ILLUSTRATIVE EXAMPLES . the experimental results show that the proposed method is able to achieve better performance in comparison with the state-of-the-art methods .\n",
            "\n",
            "491 1000\n",
            "in this paper , we tackle the problem of CO-LOCALIZATION in REAL-WORLD IMAGES . CO-LOCALIZATION is the problem of simultaneously localizing -lrb- with bounding boxes -rrb- objects of the same class across a set of distinct images . although similar problems such as CO-SEGMENTATION and WEAKLY SUPERVISED LOCALIZATION have been previously studied , we focus on being able to perform CO-LOCALIZATION in REAL-WORLD SETTINGS , which are typically characterized by large amounts of INTRA-CLASS VARIATION , INTER-CLASS DIVERSITY , and ANNOTATION NOISE . to address these issues , we present a JOINT IMAGE-BOX FORMULATION for solving the CO-LOCALIZATION PROBLEM , and show how JOINT IMAGE-BOX FORMULATION can be relaxed to a CONVEX QUADRATIC PROGRAM which can be efficiently solved . we perform an extensive evaluation of our JOINT IMAGE-BOX FORMULATION compared to previous state-of-the-art approaches on the challenging pascal voc 2007 and OBJECT DISCOVERY DATASETS . in addition , we also present a large-scale study of CO-LOCALIZATION on IMAGENET , involving GROUND-TRUTH ANNOTATIONS for <unk> classes and approximately 1 million images . \n",
            "this paper addresses the problem of WEAKLY SUPERVISED LOCALIZATION in REAL-WORLD IMAGES . we propose a novel method to address the problem of ANNOTATION NOISE in the presence of ANNOTATION NOISE , INTER-CLASS DIVERSITY , and ANNOTATION NOISE . the proposed JOINT IMAGE-BOX FORMULATION is based on a novel JOINT IMAGE-BOX FORMULATION , which is able to deal with INTRA-CLASS VARIATION , INTER-CLASS DIVERSITY , and ANNOTATION NOISE . the proposed JOINT IMAGE-BOX FORMULATION is based on a novel JOINT IMAGE-BOX FORMULATION , which is robust to ANNOTATION NOISE , INTER-CLASS DIVERSITY , and ANNOTATION NOISE . experimental results on REAL-WORLD IMAGES demonstrate the effectiveness of the proposed JOINT IMAGE-BOX FORMULATION .\n",
            "\n",
            "492 1000\n",
            "vector taylor series -lrb- vts -rrb- model based compensation is a powerful approach for NOISE ROBUST SPEECH RECOGNITION . an important extension to this approach is VTS ADAPTIVE TRAINING , which allows CANONICAL MODELS to be estimated on DIVERSE NOISE-DEGRADED TRAINING DATA . these CANONICAL MODELS can be estimated using EM-BASED APPROACHES , allowing simple extensions to DISCRIMINATIVE VAT . however to ensure a DIAGONAL CORRUPTED SPEECH COVARIANCE MATRIX the JACO-BIAN -LRB- LOADING MATRIX -rrb- relating the NOISE and CLEAN SPEECH is <unk> . in this work an approach for yielding optimal DIAGONAL LOADING MATRICES based on minimising the expected KL-DIVERGENCE between the DIAGONAL LOADING MATRIX and '' correct '' distributions is proposed . the performance of DISCRIMINATIVE VAT using the standard and OPTIMAL DIAGONALISATION was evaluated on both IN-CAR COLLECTED DATA and the AURORA4 TASK . \n",
            "in this paper , we propose a novel approach to NOISE ROBUST SPEECH RECOGNITION based on CANONICAL MODELS . the proposed approach is based on the use of DIAGONAL LOADING MATRICES to estimate the JACO-BIAN -LRB- LOADING MATRIX of the CANONICAL MODELS . the proposed method is based on the use of DIAGONAL LOADING MATRICES to estimate the JACO-BIAN -LRB- LOADING MATRIX and the NOISE . the proposed method is based on the use of DIAGONAL LOADING MATRICES to estimate the JACO-BIAN -LRB- LOADING MATRIX of the CANONICAL MODELS . the proposed method is evaluated on both IN-CAR COLLECTED DATA and IN-CAR COLLECTED DATA . the results show that the proposed KL-DIVERGENCE is effective in improving the NOISE ROBUST SPEECH RECOGNITION performance in terms of NOISE ROBUST SPEECH RECOGNITION and NOISE .\n",
            "\n",
            "493 1000\n",
            "we propose a STATISTICAL FORMULATION for 2-D HUMAN POSE ESTIMATION from SINGLE IMAGES . the HUMAN BODY CONFIGURATION is modeled by a MARKOV NETWORK and the ESTIMATION PROBLEM is to infer pose parameters from IMAGE CUES such as APPEARANCE , SHAPE , EDGE , and color . from a set of HAND LABELED IMAGES , we accumulate prior knowledge of 2-D BODY SHAPES by learning their LOW-DIMENSIONAL REPRESENTATIONS for inference of pose parameters . a DATA DRIVEN BELIEF PROPAGATION MONTE CARLO ALGORITHM , utilizing IMPORTANCE SAMPLING FUNCTIONS built from BOTTOM-UP VISUAL CUES , is proposed for efficient PROBABILISTIC INFERENCE . contrasted to the few sequential statistical formulations in the literature , our STATISTICAL FORMULATION integrates both top-down as well as BOTTOM-UP REASONING MECHANISMS , and can carry out the INFERENCE TASKS in parallel . experimental results demonstrate the <unk> and effectiveness of the proposed STATISTICAL FORMULATION in ESTIMATING 2-D HUMAN POSE from SINGLE IMAGES . \n",
            "in this paper , we propose a novel DATA DRIVEN BELIEF PROPAGATION MONTE CARLO ALGORITHM for ESTIMATING 2-D HUMAN POSE in SINGLE IMAGES . the proposed DATA DRIVEN BELIEF PROPAGATION MONTE CARLO ALGORITHM is based on a MARKOV NETWORK that uses BOTTOM-UP VISUAL CUES to represent 2-D BODY SHAPES . the proposed DATA DRIVEN BELIEF PROPAGATION MONTE CARLO ALGORITHM is based on a MARKOV NETWORK that exploits the BOTTOM-UP VISUAL CUES between the 2-D BODY SHAPES and the EDGE . the proposed DATA DRIVEN BELIEF PROPAGATION MONTE CARLO ALGORITHM is based on the use of BOTTOM-UP VISUAL CUES extracted from the IMAGE CUES , and the EDGE between the 2-D BODY SHAPES and the EDGE . the proposed DATA DRIVEN BELIEF PROPAGATION MONTE CARLO ALGORITHM is applied to SINGLE IMAGES , and the experimental results demonstrate the effectiveness of the proposed DATA DRIVEN BELIEF PROPAGATION MONTE CARLO ALGORITHM .\n",
            "\n",
            "494 1000\n",
            "several paradigms for HIGH-LEVEL MUSIC DESCRIPTIONS have been proposed to develop effective system for browsing and retrieving MUSICAL CONTENT in large repositories . such paradigms are based on either CATEGORICAL OR DIMENSIONAL MODELS . the interest in DIMENSIONAL MODELS has recently grown a great deal , as they define a SEMANTIC RELATION between concepts through GRADED DESCRIPTIONS . one problem that affects SEMANTIC DESCRIPTIONS is the ambiguity that often arises from using the same descriptor in different contexts . in order to overcome this difficulty , it is important to DIMENSIONAL CONTEXTUAL SEMANTIC MODEL and address POLYSEMY , which is the property of words to take on different meanings depending on the <unk> . in this paper we propose a DIMENSIONAL CONTEXTUAL SEMANTIC MODEL for defining SEMANTIC RELATIONS among descriptors in a CONTEXT-AWARE FASHION . this DIMENSIONAL CONTEXTUAL SEMANTIC MODEL is here used for developing a SEMANTIC MUSIC SEARCH ENGINE . in order to evaluate the effectiveness of our DIMENSIONAL CONTEXTUAL SEMANTIC MODEL , we compare this DIMENSIONAL CONTEXTUAL SEMANTIC MODEL with two systems that are based on different DESCRIPTION MODELS . \n",
            "this paper presents a novel DIMENSIONAL CONTEXTUAL SEMANTIC MODEL for SEMANTIC MUSIC SEARCH ENGINE . the proposed DIMENSIONAL CONTEXTUAL SEMANTIC MODEL is based on the use of a DIMENSIONAL CONTEXTUAL SEMANTIC MODEL to estimate the SEMANTIC RELATIONS of the SEMANTIC RELATION . the proposed DIMENSIONAL CONTEXTUAL SEMANTIC MODEL is based on the use of a DIMENSIONAL CONTEXTUAL SEMANTIC MODEL to estimate the SEMANTIC RELATIONS . the proposed DIMENSIONAL CONTEXTUAL SEMANTIC MODEL is based on the use of a DIMENSIONAL CONTEXTUAL SEMANTIC MODEL to estimate the SEMANTIC RELATIONS . the proposed DIMENSIONAL CONTEXTUAL SEMANTIC MODEL is applied to the SEMANTIC MUSIC SEARCH ENGINE of the SEMANTIC MUSIC SEARCH ENGINE . the experimental results show that the proposed DIMENSIONAL CONTEXTUAL SEMANTIC MODEL significantly improves the performance of the SEMANTIC MUSIC SEARCH ENGINE .\n",
            "\n",
            "495 1000\n",
            "the paper addresses LANGUAGE MODEL ADAPTATION for AUTOMATIC LECTURE TRANSCRIPTION by fully exploiting PRESENTATION SLIDE INFORMATION used in the lecture . as the text in the presentation slides is small in its size and <unk> in its content , a ROBUST ADAPTATION SCHEME is addressed by focusing on the KEYWORD AND TOPIC INFORMATION . several methods are investigated and combined ; first , GLOBAL TOPIC ADAPTATION is conducted based on PLSA -LRB- PROBABILISTIC LATENT SEMANTIC ANALYSIS -rrb- using KEYWORDS appearing in all slides . WEB TEXT is also retrieved to enhance the relevant text . then , LOCAL PREFERENCE of the KEYWORDS are reflected with a CACHE MODEL by referring to the slide used during each utterance . experimental evaluations on REAL LECTURES show that the proposed method combining the GLOBAL AND LOCAL SLIDE INFORMATION achieves a significant improvement of RECOGNITION ACCURACY , especially in the DETECTION RATE OF CONTENT KEYWORDS . \n",
            "this paper addresses the problem of AUTOMATIC LECTURE TRANSCRIPTION for AUTOMATIC LECTURE TRANSCRIPTION . we propose a novel ROBUST ADAPTATION SCHEME based on PLSA -LRB- PROBABILISTIC LATENT SEMANTIC ANALYSIS . in the proposed approach , a CACHE MODEL is used to estimate the KEYWORD AND TOPIC INFORMATION of the WEB TEXT . the proposed approach is based on the use of LOCAL PREFERENCE and PRESENTATION SLIDE INFORMATION . the proposed approach is based on the use of PLSA -LRB- PROBABILISTIC LATENT SEMANTIC ANALYSIS , which is based on a CACHE MODEL . the proposed approach is evaluated on REAL LECTURES and REAL LECTURES . the results show that the proposed method is effective in improving the RECOGNITION ACCURACY and RECOGNITION ACCURACY of the proposed method .\n",
            "\n",
            "496 1000\n",
            "we present a new DISCRIMINATIVE FEATURE transform approach to LARGE VOCABULARY CONTINUOUS SPEECH RECOGNITION using gaussian mixture density hidden markov models -lrb- GMM-HMMS -rrb- for ACOUSTIC MODELING . the feature transform is formulated with a set of CONTEXT-EXPANDED REGION-DEPENDENT LINEAR TRANSFORMS utilizing both LONG-SPAN FEATURES and CONTEXTUAL WEIGHT EXPANSION . the CONTEXT-EXPANDED REGION-DEPENDENT LINEAR TRANSFORMS are estimated by LATTICE-FREE , TIED-STATE BASED DISCRIMINATIVE TRAINING using MAXIMUM MUTUAL INFORMATION CRITERION , while the GMM-HMMS are trained by conventional LATTICE-BASED , BOOSTED MMI TRAINING . compared with two baseline systems , which use CONTEXT-EXPANDED REGION-DEPENDENT LINEAR TRANSFORMS with either LONG-SPAN FEATURES or WEIGHT EXPANSION only and are trained using the conventional LATTICE-BASED DISCRIMINATIVE TRAINING for both CONTEXT-EXPANDED REGION-DEPENDENT LINEAR TRANSFORMS and HMMS , the proposed approach achieves a RELATIVE WORD ERROR RATE REDUCTION of 10 % and 6 % respectively on SWITCHBOARD-1 CONVERSATIONAL TELEPHONE SPEECH TRANSCRIPTION TASK . \n",
            "this paper addresses the problem of LARGE VOCABULARY CONTINUOUS SPEECH RECOGNITION for LARGE VOCABULARY CONTINUOUS SPEECH RECOGNITION . in this paper , we propose a novel approach to ACOUSTIC MODELING for LARGE VOCABULARY CONTINUOUS SPEECH RECOGNITION . the proposed CONTEXT-EXPANDED REGION-DEPENDENT LINEAR TRANSFORMS is based on the use of CONTEXT-EXPANDED REGION-DEPENDENT LINEAR TRANSFORMS and WEIGHT EXPANSION . the proposed CONTEXT-EXPANDED REGION-DEPENDENT LINEAR TRANSFORMS is based on the use of CONTEXT-EXPANDED REGION-DEPENDENT LINEAR TRANSFORMS and WEIGHT EXPANSION . the proposed CONTEXT-EXPANDED REGION-DEPENDENT LINEAR TRANSFORMS is based on a MAXIMUM MUTUAL INFORMATION CRITERION , and the CONTEXTUAL WEIGHT EXPANSION is applied to the problem of ACOUSTIC MODELING . experimental results show that the proposed method is effective in improving the RELATIVE WORD ERROR RATE REDUCTION of the SWITCHBOARD-1 CONVERSATIONAL TELEPHONE SPEECH TRANSCRIPTION TASK .\n",
            "\n",
            "497 1000\n",
            "this paper presents a CLOSED FORM RECURSIVE SOLUTION for training ADAPTIVE FILTERS using the MAXIMUM CORRENTROPY CRITERION . CORRENTROPY has been recently proposed as a robust similarity measure between two RANDOM VARIABLES or signals , when the PDF S involved are heavy <unk> and NON-GAUSSIAN . maximizing the CROSS-CORRENTROPY between the output of an ADAPTIVE FILTERS and the desired response leads to the MAXIMUM CORRENTROPY CRITERION for ADAPTIVE SYSTEMS TRAINING . we show that a CLOSED FORM , CLOSED FORM RECURSIVE SOLUTION of the FILTER WEIGHTS using this CLOSED FORM yields a simple WEIGHTED LEAST SQUARES like formulation . our simulations show that training the FILTER WEIGHTS using this CLOSED FORM RECURSIVE SOLUTION is much faster than GRADIENT BASED TRAINING , and more accurate than the RLS ALGORITHM in cases where the ERROR PDF is NON-GAUSSIAN and heavy <unk> . \n",
            "this paper proposes a novel CLOSED FORM RECURSIVE SOLUTION for ADAPTIVE SYSTEMS TRAINING . the proposed CLOSED FORM RECURSIVE SOLUTION is based on a MAXIMUM CORRENTROPY CRITERION , which is a generalization of the standard RLS ALGORITHM . the proposed CLOSED FORM RECURSIVE SOLUTION is based on the MAXIMUM CORRENTROPY CRITERION , which is a MAXIMUM CORRENTROPY CRITERION of the MAXIMUM CORRENTROPY CRITERION . the proposed CLOSED FORM RECURSIVE SOLUTION is based on the MAXIMUM CORRENTROPY CRITERION , which is a MAXIMUM CORRENTROPY CRITERION of the PDF S . the proposed CLOSED FORM RECURSIVE SOLUTION is compared to the standard RLS ALGORITHM , and the proposed CLOSED FORM RECURSIVE SOLUTION is compared to the standard RLS ALGORITHM . the proposed CLOSED FORM RECURSIVE SOLUTION is compared to the standard RLS ALGORITHM and the RLS ALGORITHM is much faster than the conventional RLS ALGORITHM . in addition , the proposed CLOSED FORM RECURSIVE SOLUTION is much faster than the conventional RLS ALGORITHM . the proposed CLOSED FORM RECURSIVE SOLUTION is compared to the standard RLS ALGORITHM , and is much faster than the conventional RLS ALGORITHM .\n",
            "\n",
            "498 1000\n",
            "we resolve a <unk> old open question in VISIBILITY-BASED PURSUIT EVASION : how many pursuers are needed to capture an evader in an ARBITRARY POLYGONAL ENVIRONMENT with obstacles ? the evader is assumed to be adversarial , moves with the same MAXIMUM SPEED as pursuers , and is '' sensed '' by a <unk> only when it lies in <unk> of that <unk> . the players move in DISCRETE TIME STEPS , and the capture occurs when a <unk> reaches the position of the evader on its move . our main result is that o -lrb- √ h + log n -rrb- pursuers can always win the game with a DETERMINISTIC SEARCH STRATEGY in any <unk> with n vertices and h obstacles -lrb- holes -rrb- . in order to achieve this bound , however , we argue that the environment must satisfy a MINIMUM FEATURE SIZE PROPERTY , which essentially requires the MINIMUM DISTANCE between any two vertices to be of the same order as the speed of the players . without the MINIMUM FEATURE SIZE ASSUMPTION , we show that ω -lrb- n / log n -rrb- pursuers are needed in the worst-case even for SIMPLY-CONNECTED POLYGONS OF N VERTICES ! this reveals an unexpected <unk> that seems to have been overlooked in previous work <unk> that O -LRB- LOG N -RRB- PURSUERS can always win in SIMPLY-CONNECTED N-GONS . our lower bound also shows that capturing an evader is inherently more difficult than just '' seeing '' it because O -LRB- LOG N -RRB- PURSUERS are prov-ably sufficient for LINE-OF-SIGHT DETECTION even against an arbitrarily fast evader in simple <unk> . \n",
            "in this paper , we propose a novel method for LINE-OF-SIGHT DETECTION . the proposed method is based on the MINIMUM FEATURE SIZE ASSUMPTION , which is based on the MINIMUM FEATURE SIZE ASSUMPTION . the proposed method is based on the MINIMUM FEATURE SIZE ASSUMPTION , which is based on the MINIMUM FEATURE SIZE ASSUMPTION . the proposed method is based on the MINIMUM FEATURE SIZE ASSUMPTION , which is based on the MINIMUM FEATURE SIZE ASSUMPTION . in the proposed method , the proposed method is based on the MINIMUM FEATURE SIZE ASSUMPTION . the proposed method can be applied to LINE-OF-SIGHT DETECTION . the experimental results show that the proposed method is effective in improving the MAXIMUM SPEED of the LINE-OF-SIGHT DETECTION . the performance of the proposed method is compared with the best known results for LINE-OF-SIGHT DETECTION .\n",
            "\n",
            "499 1000\n",
            "this work introduces a modified <unk> multiple to multiple EM-DRIVEN ALIGNMENT ALGORITHM for GRAPHEME-TO-PHONEME CONVERSION , and preliminary experimental results applying a RECURRENT NEURAL NETWORK LANGUAGE MODEL as an N-BEST RESCORING MECHANISM for G2P CONVERSION . the ALIGNMENT ALGORITHM leverages the WFST-BASED G2P FRAMEWORK and introduces several simple STRUCTURAL CONSTRAINTS which yield a small but consistent improvement in WORD ACCURACY on a selection of standard base-lines . the RECURRENT NEURAL NETWORK LANGUAGE MODEL further extends these gains and achieves state-of-the-art performance on four standard G2P DATASETS . the system is also shown to be significantly faster than existing solutions . finally , the complete WFST-BASED G2P FRAMEWORK is provided as an OPEN-SOURCE TOOLKIT . \n",
            "this paper presents a novel RECURRENT NEURAL NETWORK LANGUAGE MODEL for GRAPHEME-TO-PHONEME CONVERSION . the proposed RECURRENT NEURAL NETWORK LANGUAGE MODEL is based on a EM-DRIVEN ALIGNMENT ALGORITHM for G2P CONVERSION . the proposed RECURRENT NEURAL NETWORK LANGUAGE MODEL is based on a EM-DRIVEN ALIGNMENT ALGORITHM for GRAPHEME-TO-PHONEME CONVERSION . the proposed RECURRENT NEURAL NETWORK LANGUAGE MODEL is based on a EM-DRIVEN ALIGNMENT ALGORITHM , which is based on a EM-DRIVEN ALIGNMENT ALGORITHM . the proposed RECURRENT NEURAL NETWORK LANGUAGE MODEL is evaluated on a standard G2P DATASETS , and the results show that the proposed RECURRENT NEURAL NETWORK LANGUAGE MODEL is effective in improving the WORD ACCURACY performance in terms of WORD ACCURACY . the proposed RECURRENT NEURAL NETWORK LANGUAGE MODEL is evaluated on the G2P DATASETS , and the results show that the proposed RECURRENT NEURAL NETWORK LANGUAGE MODEL is effective in improving the WORD ACCURACY performance .\n",
            "\n",
            "500 1000\n",
            "the INTERVAL ALGEBRA and a subset of the REGION CONNECTION CALCULUS , namely RCC-8 , are the dominant ARTIFICIAL INTELLIGENCE APPROACHES for representing and reasoning about QUALITATIVE TEMPORAL AND TOPOLOGICAL RELATIONS respectively . such QUALITATIVE TEMPORAL AND TOPOLOGICAL RELATIONS can be formulated as a QUALITATIVE CONSTRAINT NETWORK . in this paper , we focus on the MINIMAL LABELING PROBLEM and we propose an algorithm to efficiently derive all the feasible base relations of a QUALITATIVE CONSTRAINT NETWORK . our algorithm considers CHORDAL QCNS and a new form of PARTIAL CONSISTENCY which we define as ◆ G-CONSISTENCY . further , the proposed algorithm uses tractable subclasses of relations having a specific PATCHWORK PROPERTY for <unk> implies the consistency of the input QUALITATIVE CONSTRAINT NETWORK . <unk> with QUALITATIVE CONSTRAINT NETWORK of INTERVAL ALGEBRA and RCC-8 show the importance and efficiency of this new approach . \n",
            "in this paper , we propose a novel approach to the MINIMAL LABELING PROBLEM . the proposed method is based on the use of a QUALITATIVE CONSTRAINT NETWORK , called ◆ G-CONSISTENCY , which is based on a QUALITATIVE CONSTRAINT NETWORK . the proposed method is based on a QUALITATIVE CONSTRAINT NETWORK , called ◆ G-CONSISTENCY , which is based on a QUALITATIVE CONSTRAINT NETWORK . the proposed method is based on the REGION CONNECTION CALCULUS , which is a MINIMAL LABELING PROBLEM . the proposed method can be applied to other ARTIFICIAL INTELLIGENCE APPROACHES such as INTERVAL ALGEBRA and INTERVAL ALGEBRA . the experimental results demonstrate the effectiveness of the proposed method in comparison to the existing methods .\n",
            "\n",
            "501 1000\n",
            "many state-of-the-art OPTICAL FLOW ESTIMATION ALGORITHMS optimize the DATA AND REGULARIZATION TERMS to solve ILL-POSED PROBLEMS . in this paper , in contrast to the conventional OPTICAL FLOW FRAMEWORK that uses a SINGLE OR FIXED DATA MODEL , we study a novel framework that employs LOCALLY VARYING DATA TERM that adaptively combines different multiple types of DATA MODELS . the locally adaptive data term greatly reduces the MATCHING AMBIGUITY due to the complementary nature of the MULTIPLE DATA MODELS . the optimal number of COMPLEMENTARY DATA MODELS is learnt by minimizing the redundancy among them under the MINIMUM DESCRIPTION LENGTH CONSTRAINT . from these chosen DATA MODELS , a new OPTICAL FLOW ESTIMATION ENERGY MODEL is designed with the weighted sum of the MULTIPLE DATA MODELS , and a convex <unk> highly effective and practical solution that finds the OPTICAL FLOW , as well as the weights is proposed . comparative experimental results on the MIDDLEBURY OPTICAL FLOW BENCHMARK show that the proposed method using the COMPLEMENTARY DATA MODELS outperforms the STATE-OF-THE ART METHODS . \n",
            "this paper addresses the problem of OPTICAL FLOW in the context of MULTIPLE DATA MODELS . we propose a novel approach to the problem of OPTICAL FLOW in the context of MULTIPLE DATA MODELS . the proposed approach is based on the use of a LOCALLY VARYING DATA TERM , a LOCALLY VARYING DATA TERM , and a MINIMUM DESCRIPTION LENGTH CONSTRAINT . the proposed approach is based on the use of MULTIPLE DATA MODELS to estimate the OPTICAL FLOW of the DATA MODELS . the proposed approach is evaluated on the MIDDLEBURY OPTICAL FLOW BENCHMARK and the results show that the proposed method outperforms the existing STATE-OF-THE ART METHODS .\n",
            "\n",
            "502 1000\n",
            "we propose a method for extracting SEMANTIC ORIENTATIONS OF WORDS : desirable or undesirable . regarding SEMANTIC ORI-ENTATIONS as SPINS OF ELECTRONS , we use the MEAN FIELD APPROXIMATION to compute the APPROXIMATE PROBABILITY FUNCTION of the system instead of the INTRACTABLE ACTUAL PROBABILITY FUNCTION . we also propose a criterion for PARAMETER SELECTION on the basis of MAGNETIZATION . given only a small number of SEED WORDS , the proposed method extracts SEMANTIC ORIENTA-TIONS with high ACCURACY in the experiments on ENGLISH LEXICON . the result is comparable to the best value ever reported . \n",
            "this paper presents a novel method for PARAMETER SELECTION in a ENGLISH LEXICON . the proposed method is based on a MEAN FIELD APPROXIMATION , which is based on a MEAN FIELD APPROXIMATION . the proposed method is based on a MEAN FIELD APPROXIMATION , which is based on a MEAN FIELD APPROXIMATION . the proposed method is based on a MEAN FIELD APPROXIMATION , which is based on a MEAN FIELD APPROXIMATION . the proposed method is based on a SPINS OF ELECTRONS , which is based on the MEAN FIELD APPROXIMATION . the proposed method is evaluated in terms of both ACCURACY and ACCURACY .\n",
            "\n",
            "503 1000\n",
            "in this paper we consider the problem of CLASSIFICATION in the presence of PAIRWISE CONSTRAINTS , which consist of pairs of examples as well as a BINARY VARIABLE indicating whether they belong to the same class or not . we propose a method which can effectively utilize PAIRWISE CONSTRAINTS to construct an estimator of the DECISION BOUNDARY , and we show that the resulting estimator is <unk> consistent with respect to the OPTIMAL LINEAR DECISION BOUNDARY . we also study the ASYMPTOTIC VARIANCE of the estimator and extend the method to handle both LABELED AND PAIRWISE EXAMPLES in a natural way . several experiments on SIMULATED DATASETS and REAL WORLD CLASSIFICATION DATASETS are conducted . the results not only verify the theoretical properties of the proposed method but also demonstrate its practical value in applications . \n",
            "this paper addresses the problem of CLASSIFICATION and CLASSIFICATION . we propose a new method to estimate the OPTIMAL LINEAR DECISION BOUNDARY of a set of LABELED AND PAIRWISE EXAMPLES . the proposed method is based on the idea of PAIRWISE CONSTRAINTS , which is a BINARY VARIABLE . the proposed method is based on the idea that the OPTIMAL LINEAR DECISION BOUNDARY of the signal is unknown . the proposed method is applied to the problem of CLASSIFICATION and REAL WORLD CLASSIFICATION DATASETS . the experimental results show that the proposed method outperforms the existing methods in terms of both LABELED AND PAIRWISE EXAMPLES and REAL WORLD CLASSIFICATION DATASETS .\n",
            "\n",
            "504 1000\n",
            "we propose a simple GENERATIVE , SYNTACTIC LANGUAGE MODEL that conditions on OVERLAPPING WINDOWS OF TREE CONTEXT -lrb- or <unk> -rrb- in the same way that N-GRAM LANGUAGE MODELS condition on OVERLAPPING WINDOWS OF LINEAR CONTEXT . we estimate the parameters of our GENERATIVE , SYNTACTIC LANGUAGE MODEL by collecting counts from AUTOMATICALLY PARSED TEXT using standard N-GRAM LANGUAGE MODEL ESTIMATION TECHNIQUES , allowing us to train a GENERATIVE , SYNTACTIC LANGUAGE MODEL on over one billion tokens of data using a single machine in a matter of hours . we evaluate on perplexity and a range of GRAMMATICALITY TASKS , and find that we perform as well or better than N-GRAM MODELS and other GENERATIVE BASELINES . our GENERATIVE , SYNTACTIC LANGUAGE MODEL even competes with state-of-the-art DISCRIMINATIVE MODELS <unk> for the GRAMMATICALITY TASKS , despite training on POSITIVE DATA alone . we also show fluency improvements in a preliminary machine translation experiment . \n",
            "this paper presents a novel GENERATIVE , SYNTACTIC LANGUAGE MODEL for AUTOMATICALLY PARSED TEXT . the proposed GENERATIVE , SYNTACTIC LANGUAGE MODEL is based on the use of a GENERATIVE , SYNTACTIC LANGUAGE MODEL , which is based on a GENERATIVE , SYNTACTIC LANGUAGE MODEL . the proposed GENERATIVE , SYNTACTIC LANGUAGE MODEL is based on the use of DISCRIMINATIVE MODELS and GENERATIVE BASELINES . the proposed GENERATIVE , SYNTACTIC LANGUAGE MODEL is applied to the task of AUTOMATICALLY PARSED TEXT , which is based on a GENERATIVE , SYNTACTIC LANGUAGE MODEL . the experimental results show that the proposed GENERATIVE , SYNTACTIC LANGUAGE MODEL outperforms the traditional GENERATIVE BASELINES and the GENERATIVE BASELINES .\n",
            "\n",
            "505 1000\n",
            "continuous speech input for ASR PROCESSING is usually <unk> into speech <unk> by pauses . in this paper , we propose that smaller , PROSODICALLY DEFINED UNITS can be identified by tackling the problem on IMBALANCED PROSODIC UNIT BOUNDARY DETECTION using five MACHINE LEARNING TECHNIQUES . a parsimonious set of LINGUISTICALLY MOTIVATED PROSODIC FEATURES has been proven to be useful to characterize PROSODIC BOUNDARY INFORMATION . furthermore , BMPM is prone to have true positive rate on the MINORITY CLASS , i.e. the DEFINED PROSODIC UNITS . as a whole , the DECISION TREE CLASSIFIER , C4 .5 , reaches a more stable performance than the other algorithms . \n",
            "in this paper , we propose a novel approach to IMBALANCED PROSODIC UNIT BOUNDARY DETECTION for IMBALANCED PROSODIC UNIT BOUNDARY DETECTION . the proposed approach is based on the use of LINGUISTICALLY MOTIVATED PROSODIC FEATURES for IMBALANCED PROSODIC UNIT BOUNDARY DETECTION . the proposed approach is based on the use of LINGUISTICALLY MOTIVATED PROSODIC FEATURES extracted from the DEFINED PROSODIC UNITS . the proposed method is based on the use of LINGUISTICALLY MOTIVATED PROSODIC FEATURES for IMBALANCED PROSODIC UNIT BOUNDARY DETECTION . the proposed method is evaluated in the context of IMBALANCED PROSODIC UNIT BOUNDARY DETECTION for IMBALANCED PROSODIC UNIT BOUNDARY DETECTION . the experimental results show that the proposed method is effective in improving the ASR PROCESSING performance .\n",
            "\n",
            "506 1000\n",
            "we investigate the problem of learning the STRUCTURE of an ARTICULATED OBJECT , i.e. its KINEMATIC CHAIN , from FEATURE TRA-JECTORIES under AFFINE PROJECTIONS . we demonstrate this possibility by proposing an algorithm which first segments the trajectories by LOCAL SAMPLING and SPECTRAL CLUSTERING , then builds the KINEMATIC CHAIN as a MINIMUM SPANNING TREE of a GRAPH constructed from the SEGMENTED MOTION SUBSPACES . we test our method in challenging DATA SETS and demonstrate the ability to automatically build the KINEMATIC CHAIN of an ARTICULATED OBJECT from FEATURE TRAJECTORIES . the algorithm also works when there are multiple ARTICULATED OBJECTS in the scene . furthermore , we take into account NON-RIGID ARTICULATED PARTS that exist in HUMAN MOTIONS . we believe this advance will have impact on ARTICULATED OBJECT TRACKING and dynamical STRUCTURE from motion . \n",
            "in this paper , we propose a novel approach to ARTICULATED OBJECT TRACKING and SPECTRAL CLUSTERING . the method is based on the use of AFFINE PROJECTIONS extracted from AFFINE PROJECTIONS extracted from the GRAPH . the proposed method consists of two steps : -lrb- 1 -rrb- a MINIMUM SPANNING TREE , a MINIMUM SPANNING TREE , and -lrb- 2 -rrb- a MINIMUM SPANNING TREE . the proposed method is based on the use of AFFINE PROJECTIONS extracted from the KINEMATIC CHAIN of the GRAPH . the proposed method is based on the use of AFFINE PROJECTIONS extracted from the GRAPH of the GRAPH . the method is applied to ARTICULATED OBJECT TRACKING and SPECTRAL CLUSTERING . the experimental results show that the proposed method outperforms the state-of-the-art methods in terms of ARTICULATED OBJECT TRACKING and SPECTRAL CLUSTERING .\n",
            "\n",
            "507 1000\n",
            "in this paper we introduce CONTEXT-SENSITIVE DECISION FORESTS-A NEW PERSPECTIVE to exploit CONTEXTUAL INFORMATION in the popular DECISION FOREST FRAMEWORK for the OBJECT DETECTION PROBLEM . they are TREE-STRUCTURED CLASSIFIERS with the ability to access INTERMEDIATE PREDICTION -lrb- here : CLASSIFICATION and regression -rrb- information during TRAINING and INFERENCE TIME . this INTERMEDIATE PREDICTION is available for each sample and allows us to develop CONTEXT-BASED DECISION CRITERIA , used for refining the DECISION FOREST FRAMEWORK . in addition , we introduce a novel SPLIT CRITERION which in combination with a priority based way of constructing the TREES , allows more accurate REGRESSION MODE SELECTION and hence improves the current context information . in our experiments , we demonstrate improved results for the task of PEDESTRIAN DETECTION on the challenging TUD DATA SET when compared to state-of-the-art methods . \n",
            "in this paper , we propose a novel DECISION FOREST FRAMEWORK for PEDESTRIAN DETECTION . the proposed DECISION FOREST FRAMEWORK is based on a CONTEXT-SENSITIVE DECISION FORESTS-A NEW PERSPECTIVE , which is a CONTEXT-SENSITIVE DECISION FORESTS-A NEW PERSPECTIVE for PEDESTRIAN DETECTION . the proposed DECISION FOREST FRAMEWORK is based on the SPLIT CRITERION , which is based on the SPLIT CRITERION . the proposed DECISION FOREST FRAMEWORK is based on the SPLIT CRITERION , which is based on the SPLIT CRITERION . the proposed DECISION FOREST FRAMEWORK is applied to the OBJECT DETECTION PROBLEM in a CONTEXT-SENSITIVE DECISION FORESTS-A NEW PERSPECTIVE . the experimental results on the TUD DATA SET show that the proposed DECISION FOREST FRAMEWORK significantly improves the CLASSIFICATION performance in terms of CLASSIFICATION and CLASSIFICATION performance . the proposed DECISION FOREST FRAMEWORK is evaluated on the TUD DATA SET , and the results show that the proposed DECISION FOREST FRAMEWORK is effective in improving the CLASSIFICATION performance .\n",
            "\n",
            "508 1000\n",
            "learning semantic representations and TREE STRUCTURES of BILINGUAL PHRASES is beneficial for STATISTICAL MACHINE TRANSLATION . in this paper , we propose a new NEU-RAL NETWORK MODEL called BILINGUAL CORRESPONDENCE RECURSIVE AUTOENCODER to model BILINGUAL PHRASES in translation . we incorporate WORD ALIGNMENTS into BILINGUAL CORRESPONDENCE RECURSIVE AUTOENCODER to allow NEU-RAL NETWORK MODEL freely access BILINGUAL CONSTRAINTS at different levels . BILINGUAL CORRESPONDENCE RECURSIVE AUTOENCODER minimizes a JOINT OBJECTIVE on the combination of a RECURSIVE AU-TOENCODER RECONSTRUCTION ERROR , a STRUCTURAL ALIGNMENT CONSISTENCY ERROR and a CROSS-LINGUAL RECONSTRUCTION ERROR so as to not only generate ALIGNMENT-CONSISTENT PHRASE STRUCTURES , but also capture different levels of SEMANTIC RELATIONS within BILINGUAL PHRASES . in order to examine the effectiveness of BILINGUAL CORRESPONDENCE RECURSIVE AUTOENCODER , we incorporate both SEMANTIC AND STRUCTURAL SIMILARITY FEATURES built on BILINGUAL PHRASE REPRESENTATIONS and TREE STRUCTURES learned by BILINGUAL CORRESPONDENCE RECURSIVE AUTOENCODER into a state-of-the-art SMT SYSTEM . experiments on NIST CHINESE-ENGLISH TEST SETS show that our NEU-RAL NETWORK MODEL achieves a substantial improvement of up to <unk> bleu points over the baseline . \n",
            "in this paper , we propose a novel NEU-RAL NETWORK MODEL for STATISTICAL MACHINE TRANSLATION . the proposed NEU-RAL NETWORK MODEL consists of two steps : -lrb- 1 -rrb- a NEU-RAL NETWORK MODEL that combines SEMANTIC AND STRUCTURAL SIMILARITY FEATURES and BILINGUAL CONSTRAINTS into a NEU-RAL NETWORK MODEL ; -lrb- 2 -rrb- a NEU-RAL NETWORK MODEL that integrates SEMANTIC AND STRUCTURAL SIMILARITY FEATURES and BILINGUAL CONSTRAINTS into a NEU-RAL NETWORK MODEL . the proposed NEU-RAL NETWORK MODEL integrates SEMANTIC AND STRUCTURAL SIMILARITY FEATURES and BILINGUAL CONSTRAINTS into a NEU-RAL NETWORK MODEL that combines SEMANTIC AND STRUCTURAL SIMILARITY FEATURES and BILINGUAL CONSTRAINTS into the NEU-RAL NETWORK MODEL . the proposed NEU-RAL NETWORK MODEL is evaluated on both NIST CHINESE-ENGLISH TEST SETS and NIST CHINESE-ENGLISH TEST SETS . the experimental results show that the proposed NEU-RAL NETWORK MODEL outperforms the state-of-the-art methods in terms of CROSS-LINGUAL RECONSTRUCTION ERROR and CROSS-LINGUAL RECONSTRUCTION ERROR .\n",
            "\n",
            "509 1000\n",
            "in this paper , we present a novel approach for DIALOG MODELING , which extends the idea underlying the partially observable markov decision processes -lrb- pomdps -rrb- , i. e. it allows for calculating the DIALOG MODELING in real-time and thereby increases the SYSTEM FLEXIBILITY . the use of STATISTICAL DIALOG MODELS is particularly advantageous to react adequately to common errors of SPEECH RECOGNITION SYSTEMS . comparing our results to the REFERENCE SYSTEM , we achieve a relative reduction of <unk> % of the AVERAGE DIALOG LENGTH . furthermore , the proposed system shows a relative enhancement of <unk> % of the SENSITIVITY RATE in the ERROR RECOGNITION CAPABILITIES using the same SPECIFITY RATE in both systems . the achieved results are based on the AIR TRAVELLING INFORMATION SYSTEM with 21 <unk> USER UTTERANCES in 1 <unk> NATURAL SPOKEN DIALOGS . \n",
            "this paper presents a novel approach to DIALOG MODELING in NATURAL SPOKEN DIALOGS . the proposed approach is based on the use of a REFERENCE SYSTEM which is able to deal with USER UTTERANCES in the presence of USER UTTERANCES . the proposed approach is based on the use of a REFERENCE SYSTEM which is able to deal with the AVERAGE DIALOG LENGTH of the REFERENCE SYSTEM . the SENSITIVITY RATE of the proposed REFERENCE SYSTEM is evaluated in terms of the SENSITIVITY RATE and the SENSITIVITY RATE of the REFERENCE SYSTEM . the performance of the proposed REFERENCE SYSTEM is evaluated in terms of SENSITIVITY RATE and SENSITIVITY RATE .\n",
            "\n",
            "510 1000\n",
            "this paper describes a DISCRIMINATIVE APPROACH that further advances the framework for WEIGHTED FINITE STATE TRANSDUCER based decoding . the DISCRIMINATIVE APPROACH introduces additional LINEAR MODELS for adjusting the scores of a DECODING GRAPH composed of conventional INFORMATION SOURCE MODELS -lrb- e.g. , HIDDEN MARKOV MODELS and N-GRAM MODELS -rrb- , and reviews the WFST-BASED DECODING PROCESS as a LINEAR CLASSIFIER for STRUCTURED DATA -lrb- e.g. , SEQUENTIAL MULTICLASS DATA -rrb- . the difficulty with the DISCRIMINATIVE APPROACH is that the number of dimensions of the additional LINEAR MODELS becomes very large in proportion to the number of arcs in a WEIGHTED FINITE STATE TRANSDUCER , and our previous study only applied it to a small task -lrb- timit phoneme recognition -rrb- . this paper proposes a TRAINING METHOD for a LARGE-SCALE LINEAR CLASSIFIER employed in WFST-BASED DECODING by using a DISTRIBUTED PERCEPTRON ALGORITHM . the experimental results show that the proposed DISCRIMINATIVE APPROACH was successfully applied to a LARGE VOCABULARY CONTINUOUS SPEECH RECOGNITION TASK , and achieved an improvement compared with the performance of the minimum phone error based discrimina-tive training of ACOUSTIC MODELS . \n",
            "this paper presents a novel DISCRIMINATIVE APPROACH for INFORMATION SOURCE MODELS . the proposed DISCRIMINATIVE APPROACH is based on a DISTRIBUTED PERCEPTRON ALGORITHM for the WFST-BASED DECODING PROCESS . the proposed DISCRIMINATIVE APPROACH is based on the use of LINEAR MODELS and N-GRAM MODELS . the proposed DISCRIMINATIVE APPROACH is based on the use of LINEAR MODELS and N-GRAM MODELS . the proposed DISCRIMINATIVE APPROACH is based on the use of LINEAR MODELS and N-GRAM MODELS . the proposed DISCRIMINATIVE APPROACH is evaluated on a LARGE VOCABULARY CONTINUOUS SPEECH RECOGNITION TASK . the experimental results show that the proposed DISCRIMINATIVE APPROACH is effective in improving the WFST-BASED DECODING performance in the presence of STRUCTURED DATA . the performance of the proposed DISTRIBUTED PERCEPTRON ALGORITHM is demonstrated on a LARGE VOCABULARY CONTINUOUS SPEECH RECOGNITION TASK .\n",
            "\n",
            "511 1000\n",
            "we address the problem of DETECTING BATCHES OF EMAILS that have been created according to the same template . this problem is motivated by the desire to filter SPAM more effectively by exploiting COLLECTIVE INFORMATION about entire batches of JOINTLY GENERATED MESSAGES . the application matches the problem setting of SUPERVISED CLUSTERING , because examples of correct clusterings can be collected . known DECODING PROCEDURES for SUPERVISED CLUSTERING are cubic in the number of instances . when decisions can not be <unk> once they have been made -- owing to the STREAMING NATURE of the data -- then the DECODING PROBLEM can be solved in LINEAR TIME . we devise a SEQUENTIAL DECODING PROCEDURE and derive the corresponding optimization problem of SUPERVISED CLUSTERING . we study the impact of COLLECTIVE ATTRIBUTES of EMAIL BATCHES on the effectiveness of recognizing SPAM EMAILS . \n",
            "this paper addresses the problem of DETECTING BATCHES OF EMAILS in the presence of LINEAR TIME . we propose a method to estimate the COLLECTIVE INFORMATION of a scene from a single DECODING PROBLEM . we show that this problem can be viewed as a DECODING PROBLEM with COLLECTIVE INFORMATION . we show that this problem can be viewed as a DECODING PROBLEM with COLLECTIVE INFORMATION . we then show how the DECODING PROBLEM can be formulated as a DECODING PROBLEM . we show that the proposed algorithm is able to recover the COLLECTIVE ATTRIBUTES of the SPAM EMAILS , and is able to solve the DECODING PROBLEM under LINEAR TIME .\n",
            "\n",
            "512 1000\n",
            "one of the most compelling issues in the design of WIRELESS COMMUNICATION COMPONENTS is to keep POWER DISSIPATION between bounds . while LOW-POWER SOLUTIONS are readily achieved in an APPLICATION-SPECIFIC APPROACH , doing so in a PROGRAMMABLE ENVIRONMENT is a substantially harder problem . this paper presents an approach to LOW-POWER PROGRAMMABLE DSP that is based on the DYNAMIC RECONFIGURA-TION OF HARDWARE MODULES . this technique has shown to yield at least an order of magnitude of POWER REDUCTION compared to traditional INSTRUCTION-BASED ENGINES for problems in the area of WIRELESS COMMUNICATION . \n",
            "this paper presents a novel APPLICATION-SPECIFIC APPROACH for WIRELESS COMMUNICATION . the proposed APPLICATION-SPECIFIC APPROACH is based on the use of INSTRUCTION-BASED ENGINES for POWER REDUCTION . the proposed APPLICATION-SPECIFIC APPROACH is based on a DYNAMIC RECONFIGURA-TION OF HARDWARE MODULES , which is based on a DYNAMIC RECONFIGURA-TION OF HARDWARE MODULES . the proposed APPLICATION-SPECIFIC APPROACH is based on the use of INSTRUCTION-BASED ENGINES for POWER REDUCTION and POWER DISSIPATION . the performance of the proposed APPLICATION-SPECIFIC APPROACH is evaluated in terms of POWER REDUCTION and POWER DISSIPATION . the performance of the proposed APPLICATION-SPECIFIC APPROACH is evaluated in terms of the POWER DISSIPATION .\n",
            "\n",
            "513 1000\n",
            "frequency warping using ALLPASS STRUCTURES or LAGUERRE FILTERS has found increasingly applications in AUDIO SIGNAL PROCESSING due to good match with the AUDITORY FREQUENCY RESOLUTION . KAUTZ FILTERS are an extension where the FREQUENCY WARPING and RELATED RESOLUTION can have more freedom . in this paper we discuss the properties of KAUTZ FILTERS and how KAUTZ FILTERS meet typical requirements found in modeling and EQUALIZATION OF AUDIO SYSTEMS . case studies include TRANSFER FUNCTION MODELING of the GUITAR BODY and LOUDSPEAKER RESPONSE EQUALIZATION . \n",
            "this paper presents a novel approach to EQUALIZATION OF AUDIO SYSTEMS and LOUDSPEAKER RESPONSE EQUALIZATION . the proposed approach is based on the use of LAGUERRE FILTERS and LAGUERRE FILTERS to estimate the AUDITORY FREQUENCY RESOLUTION and RELATED RESOLUTION . the proposed method is based on the use of LAGUERRE FILTERS and LAGUERRE FILTERS to estimate the AUDITORY FREQUENCY RESOLUTION . the proposed method is based on the use of LAGUERRE FILTERS and KAUTZ FILTERS . the proposed method is evaluated in terms of RELATED RESOLUTION and RELATED RESOLUTION .\n",
            "\n",
            "514 1000\n",
            "in this paper we present a new approach to VARIANCE MODELLING in AUTOMATIC SPEECH RECOGNITION that is based on TANGENT DISTANCE . using TANGENT DISTANCE , CLASSIFIERS can be made invariant w.r.t. small CLASSIFIERS of the data . such CLASSIFIERS generate a MANIFOLD in a HIGH DIMENSIONAL FEATURE SPACE when applied to an OBSERVATION VECTOR . while conventional CLASSIFIERS determine the distance between an observation and a PROTOTYPE VECTOR , TANGENT DISTANCE approximates the MINIMUM DISTANCE between their manifolds , resulting in CLASSIFICATION that is invariant w.r.t. the underlying transformation . recently , this approach was successfully applied in IMAGE OBJECT RECOGNITION . in this paper we describe how TANGENT DISTANCE can be incorporated into AUTOMATIC SPEECH RECOGNITION based on GAUSSIAN MIXTURE DENSITIES . the proposed method is embedded into a PROBABILISTIC FRAMEWORK . experiments performed on the SIETILL CORPUS for TELEPHONE LINE RECORDED GERMAN DIGIT STRINGS show a significant improvement in comparison with a conventional GMD APPROACH using a comparable amount of MODEL PARAMETERS . \n",
            "in this paper , we propose a novel GMD APPROACH for IMAGE OBJECT RECOGNITION . the proposed GMD APPROACH is based on the idea of MINIMUM DISTANCE in the MANIFOLD of the MANIFOLD . the proposed GMD APPROACH is based on the TANGENT DISTANCE of the MANIFOLD , which is a PROTOTYPE VECTOR with a TANGENT DISTANCE . the proposed GMD APPROACH is based on the TANGENT DISTANCE , which is based on the TANGENT DISTANCE . the proposed GMD APPROACH is evaluated on the TELEPHONE LINE RECORDED GERMAN DIGIT STRINGS and the results show that the proposed GMD APPROACH is effective in improving the CLASSIFICATION performance . the proposed GMD APPROACH is evaluated on the TELEPHONE LINE RECORDED GERMAN DIGIT STRINGS and the results show that the proposed GMD APPROACH is effective in improving the CLASSIFICATION performance .\n",
            "\n",
            "515 1000\n",
            "this paper describes the performance of the I4U SPEAKER RECOGNITION SYSTEM in the nist 2008 speaker recognition evaluation . the I4U SPEAKER RECOGNITION SYSTEM consists of seven subsystems , each with different CEPSTRAL FEATURES and CLASSIFIERS . we describe the I4U SPEAKER RECOGNITION SYSTEM and report on its core test results as they were submitted , which were among the best-performing submissions . the <unk> effort was led by the \n",
            "this paper describes a novel approach to the I4U SPEAKER RECOGNITION SYSTEM in the I4U SPEAKER RECOGNITION SYSTEM . the proposed I4U SPEAKER RECOGNITION SYSTEM consists of two steps : -lrb- 1 -rrb- a set of CEPSTRAL FEATURES , and -lrb- 2 -rrb- a set of CEPSTRAL FEATURES . the proposed I4U SPEAKER RECOGNITION SYSTEM consists of two steps : -lrb- 1 -rrb- a set of CEPSTRAL FEATURES , and -lrb- 2 -rrb- a set of CEPSTRAL FEATURES that can be integrated into a I4U SPEAKER RECOGNITION SYSTEM . experimental results show that the proposed I4U SPEAKER RECOGNITION SYSTEM achieves better performance than the state-of-the-art methods .\n",
            "\n",
            "516 1000\n",
            "we describe our approach for generating EXPRESSIVE MUSIC PERFORMANCES OF MONOPHONIC JAZZ MELODIES . it consists of three components : -lrb- a -rrb- a MELODIC TRANSCRIPTION COMPONENT which extracts a set of ACOUSTIC FEATURES from MONOPHONIC RECORDINGS , -lrb- b -rrb- a MACHINE LEARNING COMPONENT which induces an EXPRESSIVE TRANSFORMATION MODEL from the set of EXTRACTED ACOUSTIC FEATURES , and -lrb- c -rrb- a MELODY SYNTHESIS COMPONENT which generates expressive <unk> output -lrb- <unk> or audio -rrb- from INEXPRESSIVE MELODY DESCRIPTIONS using the INDUCED EXPRESSIVE TRANSFORMATION MODEL . in this paper we concentrate on the MACHINE LEARNING COMPONENT , in particular , on the LEARNING SCHEME we use for generating EXPRESSIVE AUDIO from a score . \n",
            "in this paper , we propose a novel INDUCED EXPRESSIVE TRANSFORMATION MODEL for EXPRESSIVE MUSIC PERFORMANCES OF MONOPHONIC JAZZ MELODIES based on a MELODIC TRANSCRIPTION COMPONENT and a LEARNING SCHEME . the proposed INDUCED EXPRESSIVE TRANSFORMATION MODEL is based on the use of a MELODIC TRANSCRIPTION COMPONENT and a LEARNING SCHEME to estimate the MELODIC TRANSCRIPTION COMPONENT . the proposed INDUCED EXPRESSIVE TRANSFORMATION MODEL is based on the INDUCED EXPRESSIVE TRANSFORMATION MODEL and the EXPRESSIVE TRANSFORMATION MODEL . the proposed INDUCED EXPRESSIVE TRANSFORMATION MODEL is based on the INDUCED EXPRESSIVE TRANSFORMATION MODEL and the MELODIC TRANSCRIPTION COMPONENT . the proposed INDUCED EXPRESSIVE TRANSFORMATION MODEL is applied to the EXPRESSIVE MUSIC PERFORMANCES OF MONOPHONIC JAZZ MELODIES , and the experimental results show that the proposed INDUCED EXPRESSIVE TRANSFORMATION MODEL is effective in improving the EXPRESSIVE MUSIC PERFORMANCES OF MONOPHONIC JAZZ MELODIES .\n",
            "\n",
            "517 1000\n",
            "opponent modeling is necessary in MULTI-AGENT SETTINGS where SECONDARY AGENTS with competing goals also adapt their strategies , yet it remains challenging because strategies interact with each other and change . most previous work focuses on developing PROBABILISTIC MODELS or PARAMETERIZED STRATEGIES for specific applications . inspired by the recent success of DEEP REINFORCEMENT LEARNING , we present NEURAL-BASED MODELS that jointly learn a POLICY and the behavior of opponents . instead of explicitly predicting the opponent 's action , we encode observation of the opponents into a DEEP Q-NETWORK ; however , we retain EXPLICIT MODELING -lrb- if desired -rrb- using MULTITASKING . by using a MIXTURE-OF-EXPERTS ARCHITECTURE , our model automatically discovers different strategy patterns of opponents without extra SUPERVISION . we evaluate our models on a SIMULATED SOCCER GAME and a popular TRIVIA GAME , showing superior performance over DEEP Q-NETWORK and its variants . \n",
            "this paper addresses the problem of DEEP REINFORCEMENT LEARNING in a SIMULATED SOCCER GAME and a TRIVIA GAME . the goal of this paper is to investigate the use of DEEP REINFORCEMENT LEARNING for OPPONENT MODELING . we propose a novel approach to the problem of DEEP REINFORCEMENT LEARNING , which is based on the use of NEURAL-BASED MODELS and PROBABILISTIC MODELS . the proposed approach is based on the use of NEURAL-BASED MODELS and PROBABILISTIC MODELS . the proposed approach is based on the use of NEURAL-BASED MODELS and PROBABILISTIC MODELS . the proposed method is evaluated on a SIMULATED SOCCER GAME and a SIMULATED SOCCER GAME and a SIMULATED SOCCER GAME . the results show that the proposed method is effective in improving the quality of SECONDARY AGENTS .\n",
            "\n",
            "518 1000\n",
            "this paper provides an <unk> tutorial for the <unk> special session on '' <unk> and <unk> automatic speech recognition '' . the purpose of the special session is to bring together researchers who have special interest in novel techniques that are aimed at overcoming weaknesses of HMMS for ACOUSTIC MODELING in SPEECH RECOGNITION . numerous such approaches have been taken over the past dozen years , which can be broadly classified into <unk> -lrb- parametric -rrb- and <unk> -lrb- non-parametric -rrb- ones . in this paper , we will provide an overview of both approaches , focusing on the incorporation of long-range temporal dependencies of the SPEECH FEATURES and PHONETIC DETAIL in SPEECH RECOGNITION ALGORITHMS . we will provide a high-level survey on major existing work and systems using these two types of '' BEYOND-HMM '' FRAMEWORKS . the contributed papers in this special session will elaborate further on the related topics . \n",
            "this paper addresses the problem of ACOUSTIC MODELING for SPEECH RECOGNITION in SPEECH RECOGNITION . we propose a novel approach to ACOUSTIC MODELING based on HMMS . the proposed ACOUSTIC MODELING is based on the use of HMMS in order to improve the performance of the SPEECH RECOGNITION ALGORITHMS . the proposed approach is based on the use of HMMS , which is able to deal with PHONETIC DETAIL . the experimental results show that the proposed approach is effective in improving the SPEECH RECOGNITION performance .\n",
            "\n",
            "519 1000\n",
            "in this paper we first propose a new STATISTICAL PARSING MODEL , which is a GENERA-TIVE MODEL of LEXICALISED CONTEXT-FREE GRAMMAR . we then extend the STATISTICAL PARSING MODEL to include a PROBABILISTIC TREATMENT of both SUB-CATEGORISATION and WH-MOVEMENT . results on WALL STREET JOURNAL TEXT show that the STATISTICAL PARSING MODEL performs at <unk> / <unk> % CONSTITUENT PRECISION/RECALL , an average improvement of 2.3 % over -lrb- collins 96 -rrb- . \n",
            "this paper presents a novel STATISTICAL PARSING MODEL for STATISTICAL PARSING MODEL . the proposed STATISTICAL PARSING MODEL is based on a GENERA-TIVE MODEL of the GENERA-TIVE MODEL . the proposed STATISTICAL PARSING MODEL is based on a GENERA-TIVE MODEL of the GENERA-TIVE MODEL . the proposed STATISTICAL PARSING MODEL is evaluated on the WALL STREET JOURNAL TEXT . the experimental results show that the proposed STATISTICAL PARSING MODEL significantly improves CONSTITUENT PRECISION/RECALL on the WALL STREET JOURNAL TEXT .\n",
            "\n",
            "520 1000\n",
            "curvature has received increasing attention as an important alternative to LENGTH BASED REGULARIZATION in COMPUTER VISION . in contrast to LENGTH , it preserves ELONGATED STRUCTURES and fine details . existing approaches are either inefficient , or have LOW ANGULAR RESOLUTION and yield results with strong block artifacts . we derive a new model for COMPUTING SQUARED CURVATURE based on INTEGRAL GEOMETRY . the model counts responses of straight line triple cliques . the corresponding energy decomposes into SUBMODULAR AND SUPER-MODULAR PAIRWISE POTENTIALS . we show that this energy can be efficiently minimized even for HIGH ANGULAR RESOLUTIONS using the TRUST REGION FRAMEWORK . our results confirm that we obtain accurate and visually pleasing solutions without strong artifacts at reasonable runtimes . \n",
            "this paper addresses the problem of COMPUTING SQUARED CURVATURE in COMPUTER VISION . we propose a novel TRUST REGION FRAMEWORK for COMPUTING SQUARED CURVATURE . the proposed TRUST REGION FRAMEWORK is based on a TRUST REGION FRAMEWORK for COMPUTING SQUARED CURVATURE . the proposed TRUST REGION FRAMEWORK is based on a TRUST REGION FRAMEWORK , which is able to deal with HIGH ANGULAR RESOLUTIONS in the presence of HIGH ANGULAR RESOLUTIONS . the proposed TRUST REGION FRAMEWORK is applied to the problem of COMPUTING SQUARED CURVATURE and COMPUTING SQUARED CURVATURE . the experimental results show that the proposed TRUST REGION FRAMEWORK is able to accurately estimate the LENGTH of a scene , and the performance of the proposed method is comparable to the state of the art .\n",
            "\n",
            "521 1000\n",
            "sponsored search is an important MONETIZATION CHANNEL for SEARCH ENGINES , in which an AUCTION MECHANISM is used to select the ads shown to users and determine the prices charged from advertisers . there have been several pieces of work in the literature that investigate how to design an AUCTION MECHANISM in order to optimize the revenue of the SEARCH ENGINES . however , due to some unrealistic assumptions used , the practical values of these studies are not very clear . in this paper , we propose a novel GAME-THEORETIC MACHINE LEARNING APPROACH , which naturally combines MACHINE LEARNING and GAME THEORY , and learns the AUCTION MECHANISM using a BILEVEL OPTIMIZATION FRAMEWORK . in particular , we first learn a MARKOV MODEL from HISTORICAL DATA to describe how advertisers change their bids in response to an AUCTION MECHANISM , and then for any given AUCTION MECHANISM , we use the learnt MARKOV MODEL to predict its corresponding future bid sequences . next we learn the AUCTION MECHANISM through EMPIRICAL REVENUE MAXIMIZATION on the PREDICTED BID SEQUENCES . we show that the empirical revenue will converge when the PREDICTION PERIOD approaches infinity , and a GENETIC PROGRAMMING ALGORITHM can effectively optimize this empirical revenue . our experiments indicate that the proposed GAME-THEORETIC MACHINE LEARNING APPROACH is able to produce a much more effective AUCTION MECHANISM than several baselines . \n",
            "this paper presents a novel GAME-THEORETIC MACHINE LEARNING APPROACH based on EMPIRICAL REVENUE MAXIMIZATION and GAME THEORY . the proposed GAME-THEORETIC MACHINE LEARNING APPROACH is based on the use of EMPIRICAL REVENUE MAXIMIZATION and GAME THEORY . the proposed GAME-THEORETIC MACHINE LEARNING APPROACH is based on the use of EMPIRICAL REVENUE MAXIMIZATION and GAME THEORY . the proposed GAME-THEORETIC MACHINE LEARNING APPROACH is based on the use of EMPIRICAL REVENUE MAXIMIZATION and GAME THEORY . the proposed GAME-THEORETIC MACHINE LEARNING APPROACH is based on the use of a MARKOV MODEL , a GENETIC PROGRAMMING ALGORITHM , and a MARKOV MODEL . the proposed GAME-THEORETIC MACHINE LEARNING APPROACH is applied to the PREDICTED BID SEQUENCES , and the experimental results show that the proposed GAME-THEORETIC MACHINE LEARNING APPROACH is effective in improving the PREDICTION PERIOD .\n",
            "\n",
            "522 1000\n",
            "state-of-the-art FACTOR ANALYSIS BASED CHANNEL COMPENSATION METHODS for SPEAKER RECOGNITION are based on the assumption that SPEAKER/UTTERANCE DEPENDENT GAUSSIAN MIXTURE MODEL mean SUPER-VECTORS can be constrained to lie in a LOWER DIMENSIONAL SUBSPACE , which does not consider the fact that conventional ACOUSTIC FEATURES may also be constrained in a similar way in the FEATURE SPACE . in this study , motivated by the LOW-RANK COVARIANCE STRUCTURE OF CEPSTRAL FEATURES , we propose a FACTOR ANALYSIS MODEL in the ACOUSTIC FEATURE SPACE instead of the SUPER-VECTOR DOMAIN and derive a MIXTURE DEPENDENT FEATURE TRANSFORMATION . we demonstrate that , the proposed ACOUSTIC FACTOR ANALYSIS TRANSFORMATION performs FEATURE DIMENSIONALITY REDUCTION , DE-CORRELATION , VARIANCE NORMALIZATION and ENHANCEMENT at the same time . the transform applies a <unk> wiener gain on the ACOUSTIC FEATURE EIGENVECTOR DIRECTIONS , and is similar to the SIGNAL SUB-SPACE BASED SPEECH ENHANCEMENT SCHEMES . we also propose several methods of adaptively selecting the AFA PARAMETER for each mixture . the proposed feature transform is applied using a PROBABILISTIC MIXTURE ALIGNMENT , and is integrated with a conventional I-VECTOR SYSTEM . experimental results on the telephone trials of the nist sre 2010 demonstrate the effectiveness of the proposed FACTOR ANALYSIS MODEL . \n",
            "in this paper , we propose a novel approach to FEATURE DIMENSIONALITY REDUCTION based on PROBABILISTIC MIXTURE ALIGNMENT . the proposed approach is based on a MIXTURE DEPENDENT FEATURE TRANSFORMATION , which is based on a MIXTURE DEPENDENT FEATURE TRANSFORMATION . the proposed SPEAKER/UTTERANCE DEPENDENT GAUSSIAN MIXTURE MODEL is based on a MIXTURE DEPENDENT FEATURE TRANSFORMATION , which is based on a MIXTURE DEPENDENT FEATURE TRANSFORMATION . the proposed method is based on a SPEAKER/UTTERANCE DEPENDENT GAUSSIAN MIXTURE MODEL , which is based on a SPEAKER/UTTERANCE DEPENDENT GAUSSIAN MIXTURE MODEL . the proposed method is based on the LOW-RANK COVARIANCE STRUCTURE OF CEPSTRAL FEATURES , which is a MIXTURE DEPENDENT FEATURE TRANSFORMATION . the proposed method is evaluated in terms of SPEAKER RECOGNITION and ENHANCEMENT . the experimental results show that the proposed method outperforms the state-of-the-art methods in terms of SPEAKER RECOGNITION and ENHANCEMENT .\n",
            "\n",
            "523 1000\n",
            "a KNOWLEDGE-BASED PROGRAM defines the behavior of an agent by combining PRIMITIVE ACTIONS , PROGRAMMING CONSTRUCTS and TEST CONDITIONS that make explicit reference to the AGENT 'S KNOWLEDGE . in this paper we consider a setting where an agent is equipped with a DESCRIPTION LOGIC KNOWLEDGE BASE providing GENERAL DOMAIN KNOWLEDGE and an incomplete description of the initial situation . we introduce a corresponding new DL-BASED ACTION LANGUAGE that allows for representing both PHYSICAL AND SENSING ACTIONS , and that we then use to build KNOWLEDGE-BASED PROGRAMS with TEST CONDITIONS expressed in the EPISTEMIC DL . after proving undecidability for the general case , we then discuss a RESTRICTED FRAGMENT where VERIFICATION becomes decidable . the provided proof is constructive and comes with an upper bound on the PROCE-DURE 'S COMPLEXITY . \n",
            "this paper addresses the problem of VERIFICATION from a DESCRIPTION LOGIC KNOWLEDGE BASE . we propose a novel approach to the problem of VERIFICATION from a DESCRIPTION LOGIC KNOWLEDGE BASE . the proposed approach is based on a DESCRIPTION LOGIC KNOWLEDGE BASE , which is based on a DESCRIPTION LOGIC KNOWLEDGE BASE . the proposed approach is based on the use of a DESCRIPTION LOGIC KNOWLEDGE BASE and a KNOWLEDGE-BASED PROGRAM . the proposed approach is based on the use of a DESCRIPTION LOGIC KNOWLEDGE BASE and a KNOWLEDGE-BASED PROGRAM . the proposed approach is evaluated on a DESCRIPTION LOGIC KNOWLEDGE BASE , and the results show that the proposed method is effective in improving the VERIFICATION performance .\n",
            "\n",
            "524 1000\n",
            "we propose a method that uses a FILLER PREDICTION MODEL for building a LANGUAGE MODEL that includes FILLERS from a corpus without FILLERS . in our method , a FILLER PREDICTION MODEL is trained from a corpus that does not cover DOMAIN-RELEVANT TOPICS . it recovers FILLERS in inexact TRANSCRIBED CORPORA in the target domain , and then a LANGUAGE MODEL that includes FILLERS is built from the corpora . the results of an evaluation of the JAPANESE NATIONAL DIET RECORD showed that a model using our method achieves higher RECOGNITION performance than conventional ones . \n",
            "this paper presents a novel approach to RECOGNITION based on the FILLER PREDICTION MODEL . the proposed approach is based on the use of a set of DOMAIN-RELEVANT TOPICS , which are then used for RECOGNITION . the FILLER PREDICTION MODEL is based on the use of a FILLER PREDICTION MODEL to estimate the FILLERS of the LANGUAGE MODEL . the performance of the proposed approach is evaluated on a number of TRANSCRIBED CORPORA . the results show that the proposed method is effective in RECOGNITION .\n",
            "\n",
            "525 1000\n",
            "most INFORMATION EXTRACTION SYSTEMS treat separate potential <unk> as independent . however , in many cases , considering influences between different potential <unk> could improve OVERALL ACCURACY . STATISTICAL METHODS based on UNDIRECTED GRAPHICAL MODELS , such as CONDITIONAL RANDOM FIELDS , have been shown to be an effective approach to learning accurate IE SYSTEMS . we present a new ie method that employs RELATIONAL MARKOV NETWORKS -lrb- a GENERALIZATION OF CRFS -RRB- , which can represent ARBITRARY DEPENDENCIES between <unk> . this allows for '' collective information extraction '' that exploits the mutual influence between possible <unk> . experiments on learning to extract PROTEIN NAMES from BIOMEDICAL TEXT demonstrate the advantages of this approach . \n",
            "this paper presents a novel approach to INFORMATION EXTRACTION SYSTEMS based on RELATIONAL MARKOV NETWORKS . the proposed approach is based on the use of RELATIONAL MARKOV NETWORKS , such as RELATIONAL MARKOV NETWORKS . the proposed method is based on the use of RELATIONAL MARKOV NETWORKS , such as RELATIONAL MARKOV NETWORKS , to estimate ARBITRARY DEPENDENCIES . the proposed method is based on the use of RELATIONAL MARKOV NETWORKS , which are used in conjunction with a GENERALIZATION OF CRFS -RRB- . the experimental results show that the proposed method is effective in improving the OVERALL ACCURACY and OVERALL ACCURACY of the proposed method .\n",
            "\n",
            "526 1000\n",
            "in this paper , we present a HYBRID SPEECH RECOGNIZER combining HIDDEN MARKOV MODELS and a POLYNOMIAL CLASSIFIER . in our HYBRID SPEECH RECOGNIZER the EMISSION PROBABILITIES are not modeled as a mixture of GAUS-SIANS but are calculated by the POLYNOMIAL CLASSIFIER . however , we do not apply the CLASSIFIER directly to the FEATURE VECTOR but we make use of the DENSITY VALUES of cents gaussians clustering the FEATURE SPACE . that means we model the EMISSION PROBABILITY as a POLYNOMIAL OF GAUSSIAN DISTRIBUTIONS of # - th degree . as most of these DENSITY VALUES are approximately zero for a single FEATURE VECTOR the calculation of a POLYNOMIAL can be done very efficiently . the usefulness of this HYBRID SPEECH RECOGNIZER was successfully tested on a large CONVERSATIONAL SPEECH RECOGNITION TASK . \n",
            "in this paper , we propose a novel approach to HIDDEN MARKOV MODELS in a CONVERSATIONAL SPEECH RECOGNITION TASK . the proposed HYBRID SPEECH RECOGNIZER consists of two steps : -lrb- 1 -rrb- a FEATURE VECTOR in the FEATURE SPACE , and -lrb- 2 -rrb- a POLYNOMIAL CLASSIFIER to estimate the DENSITY VALUES . the proposed method consists of two steps : -lrb- 1 -rrb- a FEATURE VECTOR in the FEATURE SPACE , and -lrb- 2 -rrb- a POLYNOMIAL CLASSIFIER to estimate the DENSITY VALUES . the proposed method consists of two steps : a POLYNOMIAL CLASSIFIER and a POLYNOMIAL CLASSIFIER . the proposed method consists of a HYBRID SPEECH RECOGNIZER and a POLYNOMIAL CLASSIFIER . the proposed method is evaluated on a CONVERSATIONAL SPEECH RECOGNITION TASK . the experimental results show that the proposed method achieves better performance than the state-of-the-art methods .\n",
            "\n",
            "527 1000\n",
            "many PITCH TRACKERS based on DYNAMIC PROGRAMMING require <unk> design of local cost and TRANSITION COST FUNCTIONS . the forms of these functions are often empirically determined and their parameters are tuned accordingly . PARAMETER TUNING usually requires great effort without a guarantee of optimal performance . this work presents a GRAPHICAL MODEL FRAMEWORK to automatically optimize PITCH TRACKING PARAMETERS in the MAXIMUM LIKELIHOOD SENSE . therein , PROBABILISTIC DEPENDENCIES between pitch , PITCH TRANSITION and acoustical observations are expressed using the language of GRAPHICAL MODELS , and PROBABILISTIC INFERENCE is accomplished using the GRAPHI-CAL MODEL TOOLKIT . experiments show that this GRAPHICAL MODEL FRAMEWORK not only <unk> the design of a PITCH TRACKERS , but also yields remarkably good performance for both PITCH ESTIMATION and VOICING DECISION . \n",
            "in this paper , we propose a novel GRAPHICAL MODEL FRAMEWORK for PITCH ESTIMATION . the proposed GRAPHICAL MODEL FRAMEWORK is based on a GRAPHICAL MODEL FRAMEWORK for PROBABILISTIC INFERENCE . the proposed GRAPHICAL MODEL FRAMEWORK is based on the use of a GRAPHICAL MODEL FRAMEWORK , which is based on a GRAPHICAL MODEL FRAMEWORK . the proposed GRAPHICAL MODEL FRAMEWORK is based on a GRAPHICAL MODEL FRAMEWORK , which is based on a GRAPHICAL MODEL FRAMEWORK . the proposed GRAPHICAL MODEL FRAMEWORK is applied to the GRAPHI-CAL MODEL TOOLKIT for PITCH ESTIMATION . the experimental results show that the proposed GRAPHICAL MODEL FRAMEWORK is effective in improving the performance of PITCH ESTIMATION .\n",
            "\n",
            "528 1000\n",
            "this paper proposes a novel approach for effectively utilizing UNSUPERVISED DATA in addition to SUPERVISED DATA for SUPERVISED LEARNING . we use UNSUPERVISED DATA to generate INFORMATIVE ` CONDENSED FEATURE REPRESEN-TATIONS ' from the original feature set used in SUPERVISED NLP SYSTEMS . the main contribution of our method is that it can offer DENSE AND LOW-DIMENSIONAL FEATURE SPACES for NLP TASKS while maintaining the state-of-the-art performance provided by the recently developed high-performance SEMI-SUPERVISED LEARNING TECHNIQUE . our method matches the results of current state-of-the-art systems with very few FEATURES , i.e. , f-score <unk> with <unk> FEATURES for CONLL-2003 NER DATA , and <unk> <unk> with 12.5 k FEATURES for DEPENDENCY PARSING DATA derived from PTB-III . \n",
            "this paper addresses the problem of SUPERVISED LEARNING from SUPERVISED DATA and SUPERVISED DATA . we propose a novel approach to INFORMATIVE ` CONDENSED FEATURE REPRESEN-TATIONS based on SUPERVISED DATA and SUPERVISED DATA . the proposed approach is based on the use of SUPERVISED DATA and SUPERVISED DATA . the proposed approach is based on the use of SUPERVISED DATA and SUPERVISED DATA . the proposed approach is based on the use of SUPERVISED DATA and SUPERVISED DATA . the experimental results show that the proposed method is effective in improving the performance of SUPERVISED NLP SYSTEMS .\n",
            "\n",
            "529 1000\n",
            "ordinary least squares -lrb- ORDINARY LEAST SQUARES -rrb- is the DEFAULT METHOD for fitting LINEAR MODELS , but is not applicable for problems with DIMENSIONALITY larger than the SAMPLE SIZE . for these problems , we advocate the use of a GENERALIZED VERSION of ORDINARY LEAST SQUARES motivated by RIDGE REGRESSION , and propose two novel THREE-STEP ALGORITHMS involving LEAST SQUARES FITTING and HARD THRESHOLDING . the THREE-STEP ALGORITHMS are <unk> simple to understand intuitively , computationally easy to implement efficiently , and theoretically appealing for choosing models consistently . NUMERICAL EXERCISES comparing our methods with PENALIZATION-BASED APPROACHES in simulations and data analyses illustrate the great potential of the proposed THREE-STEP ALGORITHMS . \n",
            "in this paper , we propose a novel DEFAULT METHOD for LEAST SQUARES FITTING . the proposed DEFAULT METHOD is based on LEAST SQUARES FITTING and LEAST SQUARES FITTING . the proposed DEFAULT METHOD is based on the use of LEAST SQUARES FITTING and HARD THRESHOLDING . the proposed DEFAULT METHOD is based on the use of LEAST SQUARES FITTING and HARD THRESHOLDING . the proposed DEFAULT METHOD is based on the use of LEAST SQUARES FITTING and HARD THRESHOLDING . the performance of the proposed DEFAULT METHOD is demonstrated on a variety of NUMERICAL EXERCISES .\n",
            "\n",
            "530 1000\n",
            "we investigate the effect of CORPUS SIZE in combining SUPERVISED AND UNSUPER-VISED LEARNING for two types of ATTACHMENT DECISIONS : RELATIVE CLAUSE ATTACHMENT and PREPOSITIONAL PHRASE ATTACHMENT . the SUPERVISED COMPONENT is COLLINS ' PARSER , trained on the WALL STREET JOURNAL . the UNSUPERVISED COMPONENT gathers LEXICAL STATISTICS from an UNANNOTATED CORPUS OF NEWSWIRE TEXT . we find that the combined system only improves the performance of the COLLINS ' PARSER for small training sets . surprisingly , the size of the UNANNOTATED CORPUS has little effect due to the <unk> of the LEXICAL STATISTICS acquired by UNSUPERVISED LEARNING . \n",
            "in this paper , we propose a novel approach to PREPOSITIONAL PHRASE ATTACHMENT based on SUPERVISED AND UNSUPER-VISED LEARNING . the proposed COLLINS ' PARSER is based on a UNANNOTATED CORPUS OF NEWSWIRE TEXT , which is based on a COLLINS ' PARSER . the proposed COLLINS ' PARSER is based on the use of a UNANNOTATED CORPUS OF NEWSWIRE TEXT and a COLLINS ' PARSER . the proposed COLLINS ' PARSER is applied to the WALL STREET JOURNAL , which is based on a COLLINS ' PARSER . the experimental results show that the proposed COLLINS ' PARSER is effective in improving the RELATIVE CLAUSE ATTACHMENT in the WALL STREET JOURNAL . the performance of the proposed COLLINS ' PARSER is evaluated on a UNANNOTATED CORPUS OF NEWSWIRE TEXT and a UNANNOTATED CORPUS OF NEWSWIRE TEXT . the results show that the proposed COLLINS ' PARSER is effective in improving the ATTACHMENT DECISIONS performance .\n",
            "\n",
            "531 1000\n",
            "automatic topic segmentation is an important t e c <unk> for MULTIMEDIA ARCHIVAL AND RETRIEVAL SYSTEMS . in this paper we present an algorithm for TOPIC SEGMENTATION which uses a combination of MACHINE LEARNING , STATISTICAL NATURAL LANGUAGE PROCESSING , and INFORMATION RETRIEVAL TECHNIQUES . the performance of this algorithm is measured by considering the misses and false alarms on a MANUALLY SEGMENTED CORPUS . we present our results on the widely used <unk> and <unk> corpora provided by NIST . most of the techniques described are independent of the source language . we demonstrate this by applying the algorithm on both the ENGLISH AND MANDARIN TDT3 CORPORA with only minor changes . \n",
            "this paper addresses the problem of AUTOMATIC TOPIC SEGMENTATION and INFORMATION RETRIEVAL TECHNIQUES . we propose a novel approach to AUTOMATIC TOPIC SEGMENTATION , which is based on AUTOMATIC TOPIC SEGMENTATION and INFORMATION RETRIEVAL TECHNIQUES . the proposed approach is based on the use of TOPIC SEGMENTATION and INFORMATION RETRIEVAL TECHNIQUES . the proposed approach is based on the use of AUTOMATIC TOPIC SEGMENTATION , STATISTICAL NATURAL LANGUAGE PROCESSING , and INFORMATION RETRIEVAL TECHNIQUES . the experimental results on the ENGLISH AND MANDARIN TDT3 CORPORA demonstrate the effectiveness of the proposed approach .\n",
            "\n",
            "532 1000\n",
            "recognizing materials in REAL-WORLD IMAGES is a challenging task . REAL-WORLD MATERIALS have RICH SURFACE TEXTURE , GEOMETRY , LIGHTING CONDITIONS , and CLUTTER , which combine to make the problem particularly difficult . in this paper , we introduce a new , LARGE-SCALE , OPEN DATASET OF MATERIALS in the wild , the MATERIALS IN CONTEXT DATABASE , and combine this dataset with DEEP LEARNING to achieve MATERIAL RECOGNITION and segmentation of images in the wild . MATERIALS IN CONTEXT DATABASE is an order of magnitude larger than previous MATERIAL DATABASES , while being more diverse and <unk> across its 23 categories . using MATERIALS IN CONTEXT DATABASE , we train CONVOLU-TIONAL NEURAL NETWORKS -lrb- cnns -rrb- for two tasks : CLASSIFYING MATERIALS from PATCHES , and SIMULTANEOUS MATERIAL RECOGNITION and segmentation in FULL IMAGES . for PATCH-BASED CLASSIFICATION on MATERIALS IN CONTEXT DATABASE we found that the best performing MATERIALS IN CONTEXT DATABASE can achieve <unk> % MEAN CLASS ACCURACY . we convert these trained CNN CLASSIFIERS into an efficient fully convolutional framework combined with a fully connected conditional random field -lrb- crf -rrb- to predict the material at every PIXEL in an image , achieving <unk> % MEAN CLASS ACCURACY . our experiments demonstrate that having a large , WELL-SAMPLED DATASET such as MATERIALS IN CONTEXT DATABASE is crucial for REAL-WORLD MATERIAL RECOGNITION and segmentation . \n",
            "this paper addresses the problem of CLASSIFYING MATERIALS in REAL-WORLD IMAGES . in particular , we propose a novel approach to CLASSIFYING MATERIALS in REAL-WORLD IMAGES . the proposed approach is based on the use of PATCHES , GEOMETRY , GEOMETRY , GEOMETRY , and FULL IMAGES . the proposed approach is based on the use of PATCHES , GEOMETRY , GEOMETRY , and CLUTTER . the proposed approach is evaluated on a variety of MATERIAL DATABASES , including LARGE-SCALE , OPEN DATASET OF MATERIALS , REAL-WORLD MATERIAL RECOGNITION , and FULL IMAGES . the experimental results show that the proposed method outperforms the state-of-the-art methods in terms of MEAN CLASS ACCURACY and CLUTTER .\n",
            "\n",
            "533 1000\n",
            "the ideal BINARY MASK , often used in robust speech recognition applications , requires an estimate of the LOCAL SNR in each TIME-FREQUENCY UNIT . a DATA-DRIVEN APPROACH is proposed for estimating the INSTANTANEOUS SNR of each t-f unit . by assuming that the a PRIORI SNR and a POSTERIORI SNR are uniformly distributed within a small region , the INSTANTANEOUS SNR is estimated by minimizing the LOCALIZED BAYES RISK . the BINARY MASK ESTIMATOR derived by the proposed DATA-DRIVEN APPROACH is evaluated in terms of hit and FALSE ALARM RATES . compared to the BINARY MASK ESTIMATOR that uses the DECISION-DIRECTED APPROACH to compute the SNR , the proposed DATA-DRIVEN APPROACH yielded substantial improvements -lrb- up to 40 % -rrb- in CLASSIFICATION performance , when assessed in terms of a SENSITIVITY METRIC which is based on the difference between the hit and FALSE ALARM RATES . \n",
            "in this paper , we propose a novel DATA-DRIVEN APPROACH for CLASSIFICATION and CLASSIFICATION . the proposed DATA-DRIVEN APPROACH is based on a DATA-DRIVEN APPROACH for CLASSIFICATION . the proposed DATA-DRIVEN APPROACH is based on a DATA-DRIVEN APPROACH that uses a BINARY MASK ESTIMATOR to estimate the INSTANTANEOUS SNR and the INSTANTANEOUS SNR . the proposed DATA-DRIVEN APPROACH is applied to the problem of CLASSIFICATION and CLASSIFICATION . the proposed DATA-DRIVEN APPROACH is evaluated in terms of CLASSIFICATION and FALSE ALARM RATES . the experimental results show that the proposed DATA-DRIVEN APPROACH is effective in reducing the FALSE ALARM RATES and FALSE ALARM RATES .\n",
            "\n",
            "534 1000\n",
            "in the JAPANESE LANGUAGE , as a predicate is placed at the end of a sentence , the content of a sentence can not be inferred until reaching the end . however , when the content is complicated and the sentence is long , people want to know at an earlier stage in the sentence whether the content is negative , affirmative , or <unk> . in JAPANESE , the GRAMMATICAL FORM called the KO-OU RELATION exists . the KO-OU RELATION is a kind of CONCORD . if a KO ELEMENT appears , then an OU ELEMENT appears in the latter part of a sentence . it is being pointed out that the KO-OU RELATION gives advance notice to the element that appears in the latter part of a sentence . in this paper , we present the method of extracting automatically the KO-OU EXPRESSION DATA from LARGE-SCALE ELECTRONIC CORPUS and verify the usefulness of the KO-OU EXPRESSION DATA . \n",
            "in this paper , we propose a novel approach to the problem of JAPANESE . the proposed method is based on the use of a KO-OU RELATION , a KO ELEMENT , and a KO ELEMENT . the proposed method is based on a KO-OU RELATION , a KO ELEMENT , and a KO ELEMENT . the proposed method is evaluated on a LARGE-SCALE ELECTRONIC CORPUS and on a LARGE-SCALE ELECTRONIC CORPUS . the results show that the proposed method is able to achieve the same performance as the current state of the art .\n",
            "\n",
            "535 1000\n",
            "we approach the ZERO-ANAPHORA RESOLUTION PROBLEM by decomposing ZERO-ANAPHORA RESOLUTION PROBLEM into INTRA-SENTENTIAL AND INTER-SENTENTIAL ZERO-ANAPHORA RESOLUTION . for the former problem , SYNTACTIC PATTERNS of the appearance of ZERO-PRONOUNS and their antecedents are useful clues . taking JAPANESE as a target language , we empirically demonstrate that incorporating RICH SYNTACTIC PATTERN FEATURES in a state-of-the-art LEARNING-BASED ANAPHORA RESOLUTION MODEL dramatically improves the ACCURACY of INTRA-SENTENTIAL ZERO-ANAPHORA , which consequently improves the overall performance of ZERO-ANAPHORA RESOLUTION . \n",
            "in this paper , we propose a novel LEARNING-BASED ANAPHORA RESOLUTION MODEL for JAPANESE . the proposed LEARNING-BASED ANAPHORA RESOLUTION MODEL is based on a novel LEARNING-BASED ANAPHORA RESOLUTION MODEL , which is able to deal with RICH SYNTACTIC PATTERN FEATURES . the proposed LEARNING-BASED ANAPHORA RESOLUTION MODEL is based on a RICH SYNTACTIC PATTERN FEATURES , which is a ZERO-ANAPHORA RESOLUTION PROBLEM . the proposed LEARNING-BASED ANAPHORA RESOLUTION MODEL is applied to the ZERO-ANAPHORA RESOLUTION PROBLEM of the ZERO-ANAPHORA RESOLUTION PROBLEM . the proposed LEARNING-BASED ANAPHORA RESOLUTION MODEL is evaluated on a variety of JAPANESE , and the results show that the proposed LEARNING-BASED ANAPHORA RESOLUTION MODEL is effective in reducing the ACCURACY and ACCURACY of the proposed LEARNING-BASED ANAPHORA RESOLUTION MODEL .\n",
            "\n",
            "536 1000\n",
            "we propose a GESTURE RECOGNITION SYSTEM based primarily on a single 3-AXIS ACCELEROMETER . the GESTURE RECOGNITION SYSTEM employs DYNAMIC TIME WARPING and AFFINITY PROPAGATION ALGORITHMS for training and utilizes the sparse nature of the GESTURE SEQUENCE by implementing COMPRESSIVE SENSING for GESTURE RECOGNITION . a dictionary of 18 gestures or classes is defined and a database of over <unk> repetitions is created from 7 users . our DICTIONARY OF GESTURES is the largest in PUBLISHED STUDIES related to ACCELERATION-BASED GESTURE RECOGNITION , to the best of our knowledge . the proposed GESTURE RECOGNITION SYSTEM achieves almost perfect USER-DEPENDENT RECOGNITION and a USER-INDEPENDENT RECOGNITION ACCURACY that is competitive with the STATISTICAL METHODS that require significantly a large number of training samples and with the other ACCELEROMETER-BASED GESTURE RECOGNITION SYSTEMS available in literature . \n",
            "this paper presents a novel GESTURE RECOGNITION SYSTEM based on DYNAMIC TIME WARPING and AFFINITY PROPAGATION ALGORITHMS . the proposed GESTURE RECOGNITION SYSTEM is based on DYNAMIC TIME WARPING and AFFINITY PROPAGATION ALGORITHMS . the proposed GESTURE RECOGNITION SYSTEM is based on DYNAMIC TIME WARPING and AFFINITY PROPAGATION ALGORITHMS . the proposed GESTURE RECOGNITION SYSTEM is based on the use of DYNAMIC TIME WARPING and AFFINITY PROPAGATION ALGORITHMS . the proposed GESTURE RECOGNITION SYSTEM is evaluated in terms of USER-INDEPENDENT RECOGNITION ACCURACY and USER-INDEPENDENT RECOGNITION ACCURACY . the performance of the proposed GESTURE RECOGNITION SYSTEM is evaluated in terms of USER-INDEPENDENT RECOGNITION ACCURACY and USER-INDEPENDENT RECOGNITION ACCURACY . the performance of the proposed GESTURE RECOGNITION SYSTEM is evaluated in terms of USER-INDEPENDENT RECOGNITION ACCURACY and USER-INDEPENDENT RECOGNITION ACCURACY .\n",
            "\n",
            "537 1000\n",
            "in beijing , most TAXI DRIVERS intentionally avoid working during peak hours despite of the huge customer demand within these peak periods . this dilemma is mainly due to the fact that TAXI DRIVERS ' CONGESTION COSTS are not reflected in the current TAXI FARE STRUCTURE . to resolve this problem , we propose a new ATOM SCHEDULE METHOD to provide TAXI DRIVERS with extra incentives to work during peak hours . this differs from previous studies of <unk> market by considering MARKET VARIANCE over multiple periods , TAXI DRIVERS ' PROFIT-DRIVEN DECISIONS , and their SCHEDULING CONSTRAINTS regarding the interdependence among different periods . the major challenge of this research is the COMPUTATIONAL INTEN-SIVENESS to identify optimal strategy due to the exponentially large size of a TAXI DRIVER 'S STRATEGY SPACE and the SCHEDULING CONSTRAINTS . we develop an ATOM SCHEDULE METHOD to overcome these issues . it reduces the magnitude of the problem while satisfying the constraints to filter out INFEASIBLE PURE STRATEGIES . simulation results based on real data show the effectiveness of the proposed ATOM SCHEDULE METHOD , which opens up a new door to improving the efficiency of <unk> market in <unk> -lrb- e.g. , beijing -rrb- . \n",
            "in this paper , we propose a novel ATOM SCHEDULE METHOD for TAXI DRIVERS . the proposed ATOM SCHEDULE METHOD is based on a ATOM SCHEDULE METHOD , which is based on a ATOM SCHEDULE METHOD . the proposed ATOM SCHEDULE METHOD is based on a ATOM SCHEDULE METHOD of the TAXI DRIVER 'S STRATEGY SPACE . the proposed ATOM SCHEDULE METHOD is based on a ATOM SCHEDULE METHOD , which is based on a ATOM SCHEDULE METHOD . the proposed ATOM SCHEDULE METHOD is applied to the problem of TAXI DRIVERS , and the experimental results show that the proposed ATOM SCHEDULE METHOD is effective in reducing the number of INFEASIBLE PURE STRATEGIES .\n",
            "\n",
            "538 1000\n",
            "we describe a SPOKEN DIALOGUE SYSTEM which can engage in CALL FOR FIRE RADIO DIALOGUES to help train <unk> in proper procedures for requesting ARTILLERY FIRE MISSIONS . we describe the domain , an INFORMATION-STATE DIALOGUE MANAGER with a novel system of INTERACTIVE INFORMATION COMPONENTS , and provide evaluation results . \n",
            "this paper addresses the problem of CALL FOR FIRE RADIO DIALOGUES in a SPOKEN DIALOGUE SYSTEM with INTERACTIVE INFORMATION COMPONENTS . in particular , we focus on the problem of CALL FOR FIRE RADIO DIALOGUES in the context of a SPOKEN DIALOGUE SYSTEM with INTERACTIVE INFORMATION COMPONENTS . in this paper , we show how this problem can be applied to a SPOKEN DIALOGUE SYSTEM with INTERACTIVE INFORMATION COMPONENTS . the results show that the proposed method is able to achieve the same performance as the current state of the art .\n",
            "\n",
            "539 1000\n",
            "<unk> patients -lrb- <unk> <unk> <unk> with PRIMARY CLOSURE , the red lines indicate the <unk> of the <unk> -rrb- abstract <unk> changes properties of the tongue and negatively affects patients ' speech production . among the most difficult consonants to produce in the POST-GLOSSECTOMY SPEAKERS , the SIBILANT FRICATIVES / s / and / <unk> / are often problematic . to better understand these problems in production , this study analyzed ACOUSTIC AND ARTICULATORY DATA of / s / and / <unk> / from three subjects : one normal speaker and two POST-GLOSSECTOMY SPEAKERS with abnormal / s / or / <unk> . based on CINE MAGNETIC RESONANCE IMAGES , three DIMENSIONAL VOCAL TRACT RECONSTRUCTIONS , TONGUE SURFACE SHAPES behind CONSTRICTIONS , and AREA FUNCTIONS were analyzed . our results show that in each patient , contrary to normal , / s / and / <unk> / were quite similar in ACOUSTIC SPECTRA , TONGUE SURFACE SHAPES , and CONSTRICTION LOCATIONS . in the abnormal / s / , the MISSING UNILATERAL TONGUE TISSUE created an AIR FLOW BYPASS which made the CONSTRICTION further backward . the abnormal / <unk> / may be explained by the lack of PRECISE TONGUE CONTROL after surgery . in addition , the TONGUE SURFACES in the patients were more asymmetric in the back and were not <unk> for / s / <unk> to the CONSTRICTION . \n",
            "in this paper we present a novel method for PRECISE TONGUE CONTROL in CINE MAGNETIC RESONANCE IMAGES . the method is based on the use of AIR FLOW BYPASS , TONGUE SURFACE SHAPES , TONGUE SURFACE SHAPES and CONSTRICTION LOCATIONS . the proposed approach is based on the use of a set of POST-GLOSSECTOMY SPEAKERS , TONGUE SURFACE SHAPES , TONGUE SURFACE SHAPES , and CONSTRICTION LOCATIONS . the proposed approach is based on the use of AIR FLOW BYPASS , TONGUE SURFACE SHAPES , TONGUE SURFACE SHAPES , and CONSTRICTION LOCATIONS . the proposed approach is evaluated on a variety of CINE MAGNETIC RESONANCE IMAGES . the experimental results show that the proposed method outperforms the existing methods in terms of both ACOUSTIC AND ARTICULATORY DATA and TONGUE SURFACES .\n",
            "\n",
            "540 1000\n",
            "in this paper , we describe a new JAVA FRAMEWORK for an easy and efficient way of developing new GUI BASED SPEECH PROCESSING APPLICATIONS . STANDARD COMPONENTS are provided to display the SPEECH SIGNAL , the POWER PLOT , and the SPECTROGRAM . furthermore , a component to create a new TRANSCRIPTION and to display and manipulate an existing TRANSCRIPTION is provided , as well as a component to display and manually correct EXTERNAL PITCH VALUES . these STANDARD COMPONENTS can be easily embedded into own JAVA PROGRAMS . STANDARD COMPONENTS can be synchronized to display the same region of the SPEECH FILE . the OBJECT-ORIENTED DESIGN provides base classes for rapid development of own components . \n",
            "in this paper , we present a novel JAVA FRAMEWORK for GUI BASED SPEECH PROCESSING APPLICATIONS . the proposed JAVA FRAMEWORK is based on a JAVA FRAMEWORK for JAVA PROGRAMS . the proposed JAVA FRAMEWORK is based on a JAVA FRAMEWORK that exploits the EXTERNAL PITCH VALUES between the SPEECH SIGNAL and the SPEECH SIGNAL . the proposed JAVA FRAMEWORK is based on a JAVA FRAMEWORK , which is based on a JAVA FRAMEWORK . the proposed JAVA FRAMEWORK is applied to the SPECTROGRAM and the SPECTROGRAM of the STANDARD COMPONENTS . the experimental results show that the proposed JAVA FRAMEWORK is effective in improving the TRANSCRIPTION performance .\n",
            "\n",
            "541 1000\n",
            "in this article we propose a NETWORK TOPOLOGY ESTIMATION STRATEGY using UNICAST END-TO-END PACKET PAIR DELAY MEASUREMENTS that is based on MIXTURE MODELS for the DELAY CO-VARIANCES . an UNSUPERVISED LEARNING ALGORITHMS is applied to estimate the number of MIXTURE COMPONENTS and delay covariances . the LEAF PAIRS are clustered by a MAP CRITERION and passed to a HIERARCHICAL TOPOLOGY CONSTRUCTION ALGORITHM to rebuild the TREE . results from an ns simulation show that our NETWORK TOPOLOGY ESTIMATION STRATEGY can identify a NETWORK TREE with 8 LEAF NODES . \n",
            "in this paper , we propose a novel NETWORK TOPOLOGY ESTIMATION STRATEGY based on MIXTURE MODELS . the proposed HIERARCHICAL TOPOLOGY CONSTRUCTION ALGORITHM is based on a HIERARCHICAL TOPOLOGY CONSTRUCTION ALGORITHM , which is based on a HIERARCHICAL TOPOLOGY CONSTRUCTION ALGORITHM . the proposed NETWORK TOPOLOGY ESTIMATION STRATEGY is based on a HIERARCHICAL TOPOLOGY CONSTRUCTION ALGORITHM , which is based on a HIERARCHICAL TOPOLOGY CONSTRUCTION ALGORITHM . the proposed NETWORK TOPOLOGY ESTIMATION STRATEGY is based on a HIERARCHICAL TOPOLOGY CONSTRUCTION ALGORITHM , which is based on a HIERARCHICAL TOPOLOGY CONSTRUCTION ALGORITHM . the proposed NETWORK TOPOLOGY ESTIMATION STRATEGY is applied to the TREE of the TREE and the UNICAST END-TO-END PACKET PAIR DELAY MEASUREMENTS is used to estimate the MIXTURE COMPONENTS . the experimental results show that the proposed NETWORK TOPOLOGY ESTIMATION STRATEGY is effective in improving the DELAY CO-VARIANCES of the NETWORK TREE .\n",
            "\n",
            "542 1000\n",
            "speech SPEECH RECOGNITION continues to be a challenging problem particularly for SPEECH RECOGNITION in NOISY ENVIRONMENTS . in this paper , we address this problem from the point of view of <unk> and chaos . by studying RECURRENCE TIME STATISTICS for CHAOTIC SYSTEMS , we find the nonstationarity and <unk> in a TIME SERIES are due to <unk> and lack of FRACTAL STRUCTURE in the signal . a POINCARÉ RECURRENCE METRIC is designed to determine the STATIONARITY CHANGE for SPEECH RECOGNITION . we consider the small area of beginning and ending of an utterance as transient . for NONSTATIONARY AND TRANSIENT TIME SERIES , we expect the average number of POINCARÉ RECURRENCE POINTS for each given small block will be different for different blocks of data subsets . however , the average number of RECURRENCE POINTS will stay nearly constant . the resulting RECURRENCE POINT VARIABILITY ALGORITHM is shown to be well suited for the DETECTION OF STATE TRANSITIONS in a TIME SERIES and is very robust for different types of NOISE , especially for LOW SNR . \n",
            "this paper presents a novel approach to SPEECH ENDPOINT DETECTION in NOISY ENVIRONMENTS . the proposed method is based on the use of a RECURRENCE POINT VARIABILITY ALGORITHM for SPEECH ENDPOINT DETECTION in NOISY ENVIRONMENTS . the proposed method is based on a RECURRENCE POINT VARIABILITY ALGORITHM for SPEECH ENDPOINT DETECTION in NOISY ENVIRONMENTS . the proposed method is based on a RECURRENCE POINT VARIABILITY ALGORITHM , which is able to deal with NOISE in the presence of NOISE in NOISY ENVIRONMENTS . the proposed method is evaluated in the context of SPEECH RECOGNITION in NOISY ENVIRONMENTS . the experimental results show that the proposed method outperforms the state-of-the-art methods in terms of SPEECH RECOGNITION performance .\n",
            "\n",
            "543 1000\n",
            "this paper explores the use of ANSWER SET PROGRAMMING in solving DISTRIBUTED CONSTRAINT OPTIMIZATION PROBLEMS . it makes the following contributions : -lrb- i -rrb- it shows how one can formulate ANSWER SET PROGRAMMING as LOGIC PROGRAMS ; -lrb- ii -rrb- it introduces ASP-DPOP , the first DCOP ALGORITHM that is based on LOGIC PROGRAMMING ; -lrb- iii -rrb- it experimentally shows that ASP-DPOP can be up to two orders of magnitude faster than ANSWER SET PROGRAMMING -lrb- its <unk> counterpart -rrb- as well as solve some problems that ANSWER SET PROGRAMMING fails to solve due to MEMORY LIMITATIONS ; and -lrb- iv -rrb- it demonstrates the applicability of ANSWER SET PROGRAMMING in the wide array of MULTI-AGENT PROBLEMS currently modeled as ANSWER SET PROGRAMMING . \n",
            "this paper addresses the problem of DISTRIBUTED CONSTRAINT OPTIMIZATION PROBLEMS for MULTI-AGENT PROBLEMS . in particular , we consider the problem of ANSWER SET PROGRAMMING , where the goal is to minimize the total number of LOGIC PROGRAMS . we show that this problem can be solved efficiently using LOGIC PROGRAMMING . we show that ANSWER SET PROGRAMMING can be solved efficiently using LOGIC PROGRAMMING . we also show that ANSWER SET PROGRAMMING can be used to solve the problem of ANSWER SET PROGRAMMING . we also show that ANSWER SET PROGRAMMING can be used to solve the problem of ANSWER SET PROGRAMMING .\n",
            "\n",
            "544 1000\n",
            "a new BOOSTING ALGORITHM of <unk> and <unk> is used to improve the performance of DECISION TREES which are constructed <unk> : the INFORMATION RATIO CRITERION of QUINLAN 'S C4 .5 ALGORITHM . this BOOSTING ALGORITHM iteratively constructs a series of DECISION TREES , each DECISION TREE being trained and pruned on examples that have been filtered by previously trained TREES . examples that have been incorrectly classified by the previous TREES in the ensemble are resampled with higher probability to give a new PROBABILITY DISTRIBUTION for the next ace in the ensemble to <unk> on . results from optical <unk> : er <unk> ~ tion -lrb- ocr -rrb- , and KNOWLEDGE DISCOVERY and DATA MINING PROBLEMS show that in comparison to single TREES , or to TREES trained <unk> <unk> or to TREES trained on subsets of the feature space , the BOOSRING ENSEMBLE is much better . \n",
            "this paper proposes a novel BOOSTING ALGORITHM for KNOWLEDGE DISCOVERY and DATA MINING PROBLEMS . the proposed BOOSTING ALGORITHM is based on a BOOSTING ALGORITHM , which is based on a BOOSTING ALGORITHM . the proposed QUINLAN 'S C4 .5 ALGORITHM is based on a BOOSTING ALGORITHM , which is based on the QUINLAN 'S C4 .5 ALGORITHM . the proposed BOOSTING ALGORITHM is based on a BOOSTING ALGORITHM , which is a generalization of the QUINLAN 'S C4 .5 ALGORITHM . the proposed QUINLAN 'S C4 .5 ALGORITHM is applied to the problem of KNOWLEDGE DISCOVERY and DATA MINING PROBLEMS . the experimental results show that the proposed QUINLAN 'S C4 .5 ALGORITHM is effective in improving the KNOWLEDGE DISCOVERY and DATA MINING PROBLEMS .\n",
            "\n",
            "545 1000\n",
            "in many MULTILINGUAL TEXT CLASSIFICATION PROBLEMS , the documents in different languages often share the same set of categories . to reduce the LABELING COST of training a CLASSIFICATION MODEL for each individual language , it is important to transfer the LABEL KNOWLEDGE gained from one language to another language by conducting CROSS LANGUAGE CLASSIFICATION . in this paper we develop a novel SUBSPACE CO-REGULARIZED MULTI-VIEW LEARNING METHOD for CROSS LANGUAGE TEXT CLASSIFICATION . this SUBSPACE CO-REGULARIZED MULTI-VIEW LEARNING METHOD is built on PARALLEL CORPORA produced by MACHINE TRANSLATION . SUBSPACE CO-REGULARIZED MULTI-VIEW LEARNING METHOD jointly minimizes the training error of each CLASSIFIER in each language while penalizing the distance between the subspace representations of PARALLEL DOCUMENTS . our empirical study on a large set of CROSS LANGUAGE TEXT CLASSIFICATION TASKS shows the proposed SUBSPACE CO-REGULARIZED MULTI-VIEW LEARNING METHOD consistently outperforms a number of INDUCTIVE METHODS , DOMAIN ADAPTATION METHODS , and MULTI-VIEW LEARNING METHODS . \n",
            "this paper presents a novel SUBSPACE CO-REGULARIZED MULTI-VIEW LEARNING METHOD based on MACHINE TRANSLATION and MULTI-VIEW LEARNING METHODS . the proposed SUBSPACE CO-REGULARIZED MULTI-VIEW LEARNING METHOD is based on the use of PARALLEL CORPORA and DOMAIN ADAPTATION METHODS . the proposed SUBSPACE CO-REGULARIZED MULTI-VIEW LEARNING METHOD is based on the use of PARALLEL CORPORA and DOMAIN ADAPTATION METHODS . the proposed SUBSPACE CO-REGULARIZED MULTI-VIEW LEARNING METHOD is based on the use of PARALLEL CORPORA and DOMAIN ADAPTATION METHODS . the proposed SUBSPACE CO-REGULARIZED MULTI-VIEW LEARNING METHOD is evaluated on the CROSS LANGUAGE TEXT CLASSIFICATION TASKS , and the results show that the proposed SUBSPACE CO-REGULARIZED MULTI-VIEW LEARNING METHOD is effective in improving the LABELING COST and LABELING COST of the proposed SUBSPACE CO-REGULARIZED MULTI-VIEW LEARNING METHOD . the proposed SUBSPACE CO-REGULARIZED MULTI-VIEW LEARNING METHOD is evaluated in the context of MULTILINGUAL TEXT CLASSIFICATION PROBLEMS . the experimental results show that the proposed SUBSPACE CO-REGULARIZED MULTI-VIEW LEARNING METHOD is able to significantly improve the performance of the proposed SUBSPACE CO-REGULARIZED MULTI-VIEW LEARNING METHOD in terms of LABELING COST and LABELING COST .\n",
            "\n",
            "546 1000\n",
            "in this paper a novel method is introduced for SEMI-SUPERVISED DIMENSIONALITY REDUCTION on FACIAL IMAGES extracted from STEREO VIDEOS . it operates on IMAGE DATA with multiple representations and calculates a PROJECTION MATRIX that preserves LOCALITY INFORMATION and a PRIORI PAIRWISE INFORMATION , in the form of MUST-LINK AND CANNOT-LINK CONSTRAINTS between the various DATA REPRESENTATIONS , as well as LABEL INFORMATION for a percentage of the data . the final DATA REPRESENTATIONS is a LINEAR COMBINATION of the projections of all DATA REPRESENTATIONS . the performance of the proposed semi-supervised multiple locality preserving projections method was evaluated in PERSON IDENTITY LABEL PROPAGATION on FACIAL IMAGES extracted from STEREO MOVIES . experimental results showed that the proposed method outperforms STATE OF THE ART METHODS . \n",
            "this paper addresses the problem of SEMI-SUPERVISED DIMENSIONALITY REDUCTION from STEREO VIDEOS . we propose a novel approach to SEMI-SUPERVISED DIMENSIONALITY REDUCTION based on DATA REPRESENTATIONS . the proposed approach is based on the use of a PROJECTION MATRIX and a PRIORI PAIRWISE INFORMATION for SEMI-SUPERVISED DIMENSIONALITY REDUCTION . the proposed approach is based on the use of MUST-LINK AND CANNOT-LINK CONSTRAINTS and LABEL INFORMATION . the proposed approach is evaluated on a variety of STEREO MOVIES . the experimental results show that the proposed method outperforms the existing methods in terms of both MUST-LINK AND CANNOT-LINK CONSTRAINTS and LABEL INFORMATION .\n",
            "\n",
            "547 1000\n",
            "we show how DEEP LEARNING METHODS can be applied in the context of CROWDSOURCING and UNSUPERVISED ENSEMBLE LEARNING . first , we prove that the popular model of <unk> and <unk> , which assumes that all CLASSIFIERS are conditionally independent , is equivalent to a RESTRICTED BOLTZMANN MACHINE with a single HIDDEN NODE . hence , under this model , the POSTERIOR PROBABILITIES of the true labels can be instead estimated via a trained RESTRICTED BOLTZMANN MACHINE . next , to address the more general case , where CLASSIFIERS may strongly violate the CONDITIONAL INDEPENDENCE ASSUMPTION , we propose to apply RBM-BASED DEEP NEURAL NET . experimental results on various SIMULATED AND REAL-WORLD DATASETS demonstrate that our proposed dnn approach outperforms other state-of-the-art methods , in particular when the data violates the CONDITIONAL INDEPENDENCE ASSUMPTION . \n",
            "in this paper , we propose a novel method for UNSUPERVISED ENSEMBLE LEARNING . the proposed approach is based on the use of a RESTRICTED BOLTZMANN MACHINE to estimate the POSTERIOR PROBABILITIES of the CLASSIFIERS . the proposed method is based on the use of a RESTRICTED BOLTZMANN MACHINE , which is able to estimate the POSTERIOR PROBABILITIES of the CLASSIFIERS . the proposed method is based on the CONDITIONAL INDEPENDENCE ASSUMPTION of the RESTRICTED BOLTZMANN MACHINE . the proposed method is evaluated on both SIMULATED AND REAL-WORLD DATASETS . the experimental results show that the proposed method outperforms the existing methods in terms of both SIMULATED AND REAL-WORLD DATASETS .\n",
            "\n",
            "548 1000\n",
            "we consider an important class of SIGNAL PROCESSING PROBLEMS where the SIGNAL OF INTEREST is known to be sparse , and can be recovered from data given AUXILIARY INFORMATION about how this data was generated . for example , a SPARSE GREEN 'S FUNCTION may be recovered from SEISMIC EXPERIMENTAL DATA using SPARSITY OPTIMIZATION when the SOURCE SIGNATURE is known . unfortunately , in practice this information is often missing , and must be recovered from data along with the signal using DECONVOLUTION TECHNIQUES . in this paper , we present a novel methodology to simultaneously solve for the SPARSE SIGNAL AND AUXILIARY PARAMETERS using a recently proposed VARIABLE PROJECTION TECHNIQUE . our main contribution is to combine VARIABLE PROJECTION with SPAR-SITY PROMOTING OPTIMIZATION , obtaining an efficient algorithm for LARGE-SCALE SPARSE DECONVOLUTION PROBLEMS . we demonstrate the algorithm on a SEISMIC IMAGING EXAMPLE . \n",
            "this paper addresses the problem of LARGE-SCALE SPARSE DECONVOLUTION PROBLEMS from SEISMIC EXPERIMENTAL DATA . in particular , we propose a novel VARIABLE PROJECTION TECHNIQUE for LARGE-SCALE SPARSE DECONVOLUTION PROBLEMS and SPAR-SITY PROMOTING OPTIMIZATION . the proposed VARIABLE PROJECTION TECHNIQUE is based on a VARIABLE PROJECTION TECHNIQUE , which is based on a VARIABLE PROJECTION TECHNIQUE . the proposed VARIABLE PROJECTION TECHNIQUE is based on a VARIABLE PROJECTION TECHNIQUE , which is able to deal with the SIGNAL OF INTEREST . the proposed VARIABLE PROJECTION TECHNIQUE is based on a VARIABLE PROJECTION TECHNIQUE , which is able to deal with the SIGNAL OF INTEREST . the proposed VARIABLE PROJECTION TECHNIQUE is evaluated on a SEISMIC IMAGING EXAMPLE and on a SEISMIC IMAGING EXAMPLE .\n",
            "\n",
            "549 1000\n",
            "this paper studies conditions under which a signal can be reconstructed from PARTIAL FREQUENCY CONTENT . we focus on signals in SHIFT-INVARIANT SPACES generated by multiple generators . for these signals , we derive a lower bound on the necessary SIGNAL BANDWIDTH as well as sufficient conditions on the generators such that SIGNAL RECOVERY is possible . when the available FREQUENCY CONTENT is not sufficient to recover the signal , we propose appropriate PRE-PROCESSING that can improve the RECONSTRUCTION ABILITY . \n",
            "this paper addresses the problem of SIGNAL RECOVERY in the presence of PARTIAL FREQUENCY CONTENT . we propose a method to estimate the RECONSTRUCTION ABILITY of a scene , based on a PRE-PROCESSING of the FREQUENCY CONTENT . the method is based on a PRE-PROCESSING of the signal to the FREQUENCY CONTENT . the proposed method is based on the use of PRE-PROCESSING to estimate the RECONSTRUCTION ABILITY and the RECONSTRUCTION ABILITY of the target signal . experimental results show that the proposed method outperforms the existing methods in terms of RECONSTRUCTION ABILITY and RECONSTRUCTION ABILITY .\n",
            "\n",
            "550 1000\n",
            "two approaches for parallelization of h. 264 decoder , DATA PARTITION and FUNCTION PARTITION , are realized on a PAC DUO PLATFORM , which contains two parallel architecture core digital signal processors -lrb- <unk> 's -rrb- . eight baseline <unk> sequences are decoded and their execution cycles and waiting cycles are examined . there are three roots hindering the performance of DUAL-CORE DECODERS : INTER-CORE SYNCHRONIZATION , RESOURCE CONTENTION , and CACHE MISS . through the WAITING CYCLE ANALYSIS , the major reasons causing the degradation of DUAL CORE h. <unk> decoders are found . the INTER CORE SYNCHRONIZATION and RESOURCE CONTENTION principally slow down the EXECUTION SPEED of the DUAL CORE with FUNCTION PARTITION and DUAL CORE DATA PARTITION , respectively . the precious experience and analysis will help the software and hardware designers explore the mechanisms to improve performance of the MULTI CORE SCENARIOS . \n",
            "this paper addresses the problem of WAITING CYCLE ANALYSIS in the presence of MULTI CORE SCENARIOS , RESOURCE CONTENTION , RESOURCE CONTENTION , and INTER CORE SYNCHRONIZATION . in particular , we consider the problem of INTER-CORE SYNCHRONIZATION in the presence of INTER CORE SYNCHRONIZATION , RESOURCE CONTENTION , RESOURCE CONTENTION , and INTER CORE SYNCHRONIZATION . in particular , we consider the problem of INTER-CORE SYNCHRONIZATION in the presence of CACHE MISS , RESOURCE CONTENTION , RESOURCE CONTENTION , and INTER CORE SYNCHRONIZATION . we show that this problem can be solved efficiently by using the DUAL CORE DATA PARTITION . we then show that the PAC DUO PLATFORM can be solved efficiently . we also show that the proposed algorithm can be applied to a wide range of MULTI CORE SCENARIOS , including INTER-CORE SYNCHRONIZATION , RESOURCE CONTENTION , and INTER CORE SYNCHRONIZATION .\n",
            "\n",
            "551 1000\n",
            "pathological speech usually refers to the VOICE DISORDERS resulting from <unk> in VOICE AND/OR in the ARTICULATORY MECHANISMS due to DISEASE , ILLNESS or other PHYSICAL PROBLEM in the SPEECH PRODUCTION SYSTEM . it may increase UNHEALTHY SOCIAL BEHAVIOR and VOICE ABUSE , and dramatically affect the patients ' quality of life . therefore , AUTOMATIC INTEL-LIGIBILITY DETECTION of PATHOLOGICAL SPEECH has an important role in the <unk> treatment of PATHOLOGICAL VOICES . this paper proposes to use asymmetric sparse kernel partial least squares classifier -lrb- <unk> -rrb- for INTELLIGIBILITY DETECTION of PATHOLOGICAL SPEECH . the proposed approach achieves an UN-WEIGHTED ACCURACY of <unk> % , which is <unk> % relative improvement of baseline system of an UN-WEIGHTED ACCURACY of <unk> % for the <unk> sub-challenge of interspeech 2012 speaker trait challenge . \n",
            "this paper presents a novel approach to AUTOMATIC INTEL-LIGIBILITY DETECTION in PATHOLOGICAL SPEECH . the proposed approach is based on the use of ARTICULATORY MECHANISMS and VOICE ABUSE . the proposed method is based on the use of ARTICULATORY MECHANISMS and ILLNESS . the proposed method is based on the use of ARTICULATORY MECHANISMS and ILLNESS . the proposed approach is evaluated on a variety of PATHOLOGICAL SPEECH , and the results show that the proposed method is effective in improving the INTELLIGIBILITY DETECTION performance in terms of UN-WEIGHTED ACCURACY and VOICE AND/OR .\n",
            "\n",
            "552 1000\n",
            "we use a combination of LINEAR SUPPORT VECTOR MACHINES and HIDDEN MARKOV MODELS for DIALOG ACT TAGGING in the HCRC MAPTASK CORPUS , and obtain better results than those previously reported . SUPPORT VECTOR MACHINES allow easy integration of SPARSE HIGH-DIMENSIONAL TEXT FEATURES and DENSE LOW-DIMENSIONAL ACOUSTIC FEATURES , and produce POSTERIOR PROBABILITIES usable by SEQUENCE LABELLING ALGORITHMS . the relative contribution of TEXT AND ACOUSTIC FEATURES for each class of dialog act is analyzed . \n",
            "this paper addresses the problem of DIALOG ACT TAGGING in the presence of SPARSE HIGH-DIMENSIONAL TEXT FEATURES and DENSE LOW-DIMENSIONAL ACOUSTIC FEATURES . we propose a novel approach to DIALOG ACT TAGGING based on HIDDEN MARKOV MODELS and LINEAR SUPPORT VECTOR MACHINES . the proposed approach is based on the use of LINEAR SUPPORT VECTOR MACHINES and LINEAR SUPPORT VECTOR MACHINES . the proposed approach is based on the use of LINEAR SUPPORT VECTOR MACHINES and HIDDEN MARKOV MODELS . the proposed method is evaluated on the HCRC MAPTASK CORPUS , and the results show that the proposed method outperforms the state-of-the-art methods .\n",
            "\n",
            "553 1000\n",
            "efficient LEARNING with NON-LINEAR KERNELS is often based on extracting FEATURES from the data that '' <unk> '' the kernel . while most constructions aim at obtaining LOW-DIMENSIONAL AND DENSE FEATURES , in this work we explore HIGH-DIMENSIONAL AND SPARSE ONES . we give a method to compute SPARSE FEATURES for ARBITRARY KERNELS , <unk> as a special case a popular map for the INTERSECTION KERNEL and extending it to ARBITRARY ADDITIVE KERNELS . we show that BUNDLE OPTIMISATION METHODS can handle efficiently these SPARSE FEATURES in LEARNING . as an application , we show that PRODUCT QUANTISATION can be interpreted as a SPARSE FEATURE ENCODING , and use this to significantly accelerate LEARNING with this technique . we demonstrate these ideas on IMAGE CLASSIFICATION with FISHER KERNELS and OBJECT DETECTION with DEFORMABLE PART MODELS on the challenging PASCAL VOC DATA , obtaining five to tenfold speed-ups as well as reducing MEMORY USE by an order of magnitude . \n",
            "in this paper , we propose a novel approach to OBJECT DETECTION and OBJECT DETECTION . the proposed approach is based on the use of FISHER KERNELS for OBJECT DETECTION and OBJECT DETECTION . the proposed approach is based on the use of FISHER KERNELS for LEARNING and OBJECT DETECTION . the proposed method is based on the use of FISHER KERNELS for IMAGE CLASSIFICATION and OBJECT DETECTION . the proposed method is evaluated on PASCAL VOC DATA and PASCAL VOC DATA . the experimental results show that the proposed method outperforms the state-of-the-art methods in terms of IMAGE CLASSIFICATION and IMAGE CLASSIFICATION .\n",
            "\n",
            "554 1000\n",
            "we consider the WEIGHTED SUM RATE OPTIMIZATION of WEIGHTED SUM RATE OPTIMIZATION in a MIMO INTERFERING MULTIPLE ACCESS CHANNEL . we propose to jointly optimize the users ' LINEAR PROCODERS as well as their base station -lrb- bs -rrb- associations . this approach enables the users to avoid CONGESTED BSS and can improve system performance as well as USER FAIRNESS . we formulate the WEIGHTED SUM RATE OPTIMIZATION into a NONCOOPERATIVE GAME , and develop an algorithm that allows the players to <unk> reach the NASH EQUILIBRIUM of the game . we show that every NE of the game is a STATIONARY SOLUTION of the WEIGHTED SUM RATE OPTIMIZATION PROBLEM , and propose an algorithm to compute the NE of the game . simulation results show that the proposed algorithm performs well in the presence of BS CONGESTION . \n",
            "this paper addresses the problem of CONGESTED BSS in a NONCOOPERATIVE GAME . we propose a novel method to estimate the BS CONGESTION of the MIMO INTERFERING MULTIPLE ACCESS CHANNEL . the proposed WEIGHTED SUM RATE OPTIMIZATION PROBLEM is based on a WEIGHTED SUM RATE OPTIMIZATION , which is based on a WEIGHTED SUM RATE OPTIMIZATION . the proposed WEIGHTED SUM RATE OPTIMIZATION PROBLEM is based on a LINEAR PROCODERS , which is based on the LINEAR PROCODERS . the proposed WEIGHTED SUM RATE OPTIMIZATION is applied to the WEIGHTED SUM RATE OPTIMIZATION PROBLEM in a NONCOOPERATIVE GAME . the performance of the proposed WEIGHTED SUM RATE OPTIMIZATION is evaluated in terms of NE , and the performance of the proposed method is evaluated on a NONCOOPERATIVE GAME .\n",
            "\n",
            "555 1000\n",
            "examples are often used along with TEXTUAL DESCRIPTIONS to help convey particular <unk> in INSTRUCTIONAL OR EXPLANATORY CONTEXTS . these accompanying examples reflect information in the surrounding text , and in turn , also influence the text . sometimes , examples replace possible -lrb- textual -rrb- <unk> in the description . it is thus clear that if OBJECT DESCRIPTIONS are to be generated , the system must incorporate strategies to handle examples . in this work , we shall investigate some of these issues in the generation of OBJECT DESCRIPTIONS . \n",
            "in this paper , we present a method to detect OBJECT DESCRIPTIONS in INSTRUCTIONAL OR EXPLANATORY CONTEXTS . our approach is based on the use of INSTRUCTIONAL OR EXPLANATORY CONTEXTS and TEXTUAL DESCRIPTIONS . we show that our approach is able to detect and track moving objects in a scene from a single image .\n",
            "\n",
            "556 1000\n",
            "in applications of GAUSSIAN PROCESSES where QUANTIFICATION OF UNCERTAINTY is of primary interest , it is necessary to accurately characterize the POSTERIOR DISTRIBUTION over COVARIANCE PARAMETERS . this paper proposes an adaptation of the STOCHASTIC GRADIENT LANGEVIN DYNAMICS ALGORITHM to draw samples from the POSTERIOR DISTRIBUTION over COVARIANCE PARAMETERS with NEGLIGIBLE BIAS and without the need to compute the MARGINAL LIKELIHOOD . in GAUSSIAN PROCESS REGRESSION , this has the enormous advantage that STOCHASTIC GRADIENTS can be computed by solving LINEAR SYSTEMS only . a novel UNBIASED LINEAR SYSTEMS SOLVER based on PARALLELIZABLE COVARIANCE MATRIX-VECTOR PRODUCTS is developed to accelerate the UNBIASED ESTIMATION OF GRADIENTS . the results demonstrate the possibility to enable scal-able and exact -lrb- in a monte carlo sense -rrb- <unk> of uncertainty in GAUSSIAN PROCESSES without imposing any special structure on the COVARI-ANCE or reducing the number of input vectors . \n",
            "in this paper , we propose a novel UNBIASED LINEAR SYSTEMS SOLVER for GAUSSIAN PROCESS REGRESSION . the proposed STOCHASTIC GRADIENT LANGEVIN DYNAMICS ALGORITHM is based on a STOCHASTIC GRADIENT LANGEVIN DYNAMICS ALGORITHM , which is based on a STOCHASTIC GRADIENT LANGEVIN DYNAMICS ALGORITHM . the proposed STOCHASTIC GRADIENT LANGEVIN DYNAMICS ALGORITHM is based on a STOCHASTIC GRADIENT LANGEVIN DYNAMICS ALGORITHM of the POSTERIOR DISTRIBUTION . the proposed STOCHASTIC GRADIENT LANGEVIN DYNAMICS ALGORITHM is based on the use of STOCHASTIC GRADIENTS , which is a NEGLIGIBLE BIAS . the proposed STOCHASTIC GRADIENT LANGEVIN DYNAMICS ALGORITHM is based on a STOCHASTIC GRADIENT LANGEVIN DYNAMICS ALGORITHM . the proposed STOCHASTIC GRADIENT LANGEVIN DYNAMICS ALGORITHM is applied to the problem of UNBIASED ESTIMATION OF GRADIENTS in LINEAR SYSTEMS . the experimental results show that the proposed STOCHASTIC GRADIENT LANGEVIN DYNAMICS ALGORITHM is effective in improving the QUANTIFICATION OF UNCERTAINTY of the UNBIASED LINEAR SYSTEMS SOLVER .\n",
            "\n",
            "557 1000\n",
            "<unk> rules , also known as DATA DEPENDENCIES in DATABASES , have been recently rediscovered as a promising family of languages for ONTOLOGY-BASED QUERY ANSWERING . in this paper , we prove that DISJUNCTIVE EMBEDDED DEPENDENCIES exactly capture the class of recursively <unk> ontologies in ONTOLOGY-BASED CONJUNCTIVE QUERY ANSWERING . our EXPRESSIVE COMPLETENESS result does not rely on any BUILT-IN LINEAR ORDER on the database . to establish the EXPRESSIVE COMPLETENESS , we introduce a novel SEMANTIC DEFINITION for ONTOLOGY-BASED CONJUNCTIVE QUERY ANSWERING . we also show that neither the CLASS OF DISJUNC-TIVE TUPLE-GENERATING DEPENDENCIES nor the CLASS OF EMBEDDED DEPENDENCIES is <unk> complete for recursively <unk> OCQA ONTOLOGIES . \n",
            "in this paper , we propose a novel approach to ONTOLOGY-BASED CONJUNCTIVE QUERY ANSWERING in ONTOLOGY-BASED CONJUNCTIVE QUERY ANSWERING . the proposed approach is based on the use of EXISTENTIAL RULES , such as DATA DEPENDENCIES and DATA DEPENDENCIES . the proposed approach is based on the use of EXISTENTIAL RULES , such as CLASS OF EMBEDDED DEPENDENCIES and DATA DEPENDENCIES . the proposed approach is based on the use of a BUILT-IN LINEAR ORDER , which is able to deal with DISJUNCTIVE EMBEDDED DEPENDENCIES . we demonstrate the effectiveness of the proposed method in the context of ONTOLOGY-BASED CONJUNCTIVE QUERY ANSWERING and ONTOLOGY-BASED QUERY ANSWERING .\n",
            "\n",
            "558 1000\n",
            "we consider the problem of the SHAPE RECONSTRUCTION of a COMPACT OBJECT in X RAY TOMOGRAPHY when the CONTOUR of the object is modeled by a POLYGON . the problem is then to estimate the vertices of that POLYGON from a limited number of projections . the main objectives of this paper are : -lcb- to show how this SHAPE RECONSTRUCTION becomes equivalent to a generic MATHEMATICAL INVERSION PROBLEM which arises also in LINEAR ANTENNA ARRAY PROCESSING ; -lcb- to evaluate the performances of the CLASSICAL AP TECHNIQUES to handle with this SHAPE RECONSTRUCTION , and , -lcb- to propose a new method based on BAYESIAN ESTIMATION APPROACH for the resolution of this INVERSE PROBLEM . \n",
            "in this paper , we propose a novel BAYESIAN ESTIMATION APPROACH for SHAPE RECONSTRUCTION . the proposed BAYESIAN ESTIMATION APPROACH is based on a BAYESIAN ESTIMATION APPROACH to the INVERSE PROBLEM . the proposed BAYESIAN ESTIMATION APPROACH is based on a BAYESIAN ESTIMATION APPROACH to the INVERSE PROBLEM . the proposed BAYESIAN ESTIMATION APPROACH is based on a BAYESIAN ESTIMATION APPROACH , which is based on a BAYESIAN ESTIMATION APPROACH . the proposed BAYESIAN ESTIMATION APPROACH is applied to the INVERSE PROBLEM in a X RAY TOMOGRAPHY . the proposed BAYESIAN ESTIMATION APPROACH is applied to the problem of SHAPE RECONSTRUCTION in X RAY TOMOGRAPHY .\n",
            "\n",
            "559 1000\n",
            "the ability to MAP DESCRIPTIONS OF SCENES to 3D GEOMETRIC REPRESENTATIONS has many applications in areas such as ART , EDUCATION , and ROBOTICS . however , prior work on the text to 3D SCENE GENERATION TASK has used MANUALLY SPECIFIED OBJECT CATEGORIES and language that identifies them . we introduce a dataset of 3d scenes annotated with NATURAL LANGUAGE DESCRIPTIONS and learn from this data how to ground TEX-TUAL DESCRIPTIONS to PHYSICAL OBJECTS . our method successfully grounds a variety of LEXICAL TERMS to CONCRETE REFERENTS , and we show quantitatively that our method improves 3D SCENE GENERATION over previous work using purely RULE-BASED METHODS . we evaluate the fidelity and <unk> of 3d scenes generated with our GROUNDING APPROACH through HUMAN JUDGMENTS . to ease evaluation on this task , we also introduce an GROUNDING APPROACH that strongly correlates with HUMAN JUDGMENTS . \n",
            "this paper presents a novel GROUNDING APPROACH for 3D SCENE GENERATION and EDUCATION . the proposed GROUNDING APPROACH is based on the use of a set of TEX-TUAL DESCRIPTIONS , which are then used to estimate the 3D GEOMETRIC REPRESENTATIONS . the proposed GROUNDING APPROACH is based on the use of a set of LEXICAL TERMS , which are then used for 3D SCENE GENERATION and EDUCATION . the proposed GROUNDING APPROACH is evaluated on a 3D SCENE GENERATION TASK and a 3D SCENE GENERATION TASK . the results show that the proposed GROUNDING APPROACH is able to detect PHYSICAL OBJECTS , and is robust to PHYSICAL OBJECTS , EDUCATION and ROBOTICS .\n",
            "\n",
            "560 1000\n",
            "layered LAYERED MODELS are a powerful way of describing NATURAL SCENES containing SMOOTH SURFACES that may overlap and occlude each other . for IMAGE MOTION ESTIMATION , such LAYERED MODELS have a long history but have not achieved the wide use or ACCURACY of NON-LAYERED METHODS . we present a new PROBABILISTIC MODEL of OPTICAL FLOW IN LAYERS that addresses many of the shortcomings of previous approaches . in particular , we define a PROBABILISTIC GRAPHICAL MODEL that explicitly captures : 1 -rrb- OCCLUSIONS and DISOCCLUSIONS ; 2 -rrb- depth ordering of the layers ; 3 -rrb- TEMPORAL CONSISTENCY of the LAYER SEGMENTATION . additionally the OPTICAL FLOW in each layer is modeled by a combination of a PARAMETRIC MODEL and a SMOOTH DEVIATION based on an MRF with a ROBUST SPATIAL PRIOR ; the resulting PROBABILISTIC GRAPHICAL MODEL allows <unk> in layers . finally , a key contribution is the formulation of the layers using an IMAGE-DEPENDENT HIDDEN FIELD PRIOR based on recent LAYERED MODELS for STATIC SCENE SEGMENTATION . the PROBABILISTIC GRAPHICAL MODEL achieves state-of-the-art results on the MIDDLEBURY BENCHMARK and produces MEANINGFUL SCENE SEGMENTATIONS as well as DETECTED OCCLUSION REGIONS . \n",
            "this paper presents a novel PROBABILISTIC GRAPHICAL MODEL for IMAGE MOTION ESTIMATION . the proposed PROBABILISTIC GRAPHICAL MODEL is based on a PARAMETRIC MODEL of the OPTICAL FLOW IN LAYERS of the OPTICAL FLOW IN LAYERS . the proposed PROBABILISTIC GRAPHICAL MODEL is based on the use of LAYERED MODELS to estimate the OPTICAL FLOW IN LAYERS of the OPTICAL FLOW IN LAYERS . the proposed PROBABILISTIC GRAPHICAL MODEL is based on a PARAMETRIC MODEL that exploits the TEMPORAL CONSISTENCY and SMOOTH DEVIATION . the proposed PROBABILISTIC GRAPHICAL MODEL is evaluated on the MIDDLEBURY BENCHMARK , and the results show that the proposed PROBABILISTIC GRAPHICAL MODEL is robust to OCCLUSIONS , DISOCCLUSIONS , and DISOCCLUSIONS . the ACCURACY of the proposed PROBABILISTIC GRAPHICAL MODEL is comparable to the state of the art on the MIDDLEBURY BENCHMARK .\n",
            "\n",
            "561 1000\n",
            "speech separation is a challenging problem at LOW SIGNAL-TO-NOISE RATIOS . SEPARATION can be formulated as a CLASSIFICATION PROBLEM . in this study , we focus on the SNR LEVEL OF-5 DB in which SPEECH is generally dominated by BACKGROUND NOISE . in such a LOW SNR CONDITION , extracting ROBUST FEATURES from a NOISY MIXTURE is crucial for successful CLASSIFICATION . using a common NEURAL NETWORK CLASSIFIER , we systematically compare separation performance of many MONAURAL FEATURES . in addition , we propose a new MULTI-RESOLUTION COCHLEAGRAM called MULTI-RESOLUTION COCHLEAGRAM , which is extracted from four <unk> of different resolutions to capture both LOCAL INFORMATION and <unk> context . comparisons using two NON-STATIONARY NOISES show a range of FEATURE ROBUSTNESS for SPEECH SEPARATION with the proposed MULTI-RESOLUTION COCHLEAGRAM performing the best . we also find that MULTI-RESOLUTION COCHLEAGRAM , a POST-PROCESSING TECHNIQUE previously used for ROBUST SPEECH RECOGNITION , improves SPEECH SEPARATION performance by smoothing the TEMPORAL TRAJECTORIES OF FEATURE DIMENSIONS . \n",
            "this paper presents a novel POST-PROCESSING TECHNIQUE for ROBUST SPEECH RECOGNITION in SPEECH SEPARATION . the proposed POST-PROCESSING TECHNIQUE is based on a POST-PROCESSING TECHNIQUE for ROBUST SPEECH RECOGNITION . the proposed POST-PROCESSING TECHNIQUE is based on the use of a NOISY MIXTURE and a MULTI-RESOLUTION COCHLEAGRAM for CLASSIFICATION . the proposed POST-PROCESSING TECHNIQUE is based on a POST-PROCESSING TECHNIQUE that exploits the LOCAL INFORMATION in the NOISY MIXTURE . the proposed POST-PROCESSING TECHNIQUE is applied to the CLASSIFICATION PROBLEM for ROBUST SPEECH RECOGNITION in SPEECH . the experimental results show that the proposed POST-PROCESSING TECHNIQUE is effective in improving the FEATURE ROBUSTNESS of the NEURAL NETWORK CLASSIFIER in terms of FEATURE ROBUSTNESS and LOW SNR CONDITION .\n",
            "\n",
            "562 1000\n",
            "we construct a mixture of LOCALLY LINEAR GENERATIVE MODELS of a collection of PIXEL-BASED IMAGES OF DIGITS , and use LOCALLY LINEAR GENERATIVE MODELS for RECOGNITION . different models of a given digit are used to capture different styles of writing , and new images are classified by evaluating their <unk> under each LOCALLY LINEAR GENERATIVE MODELS . we use an EM-BASED ALGORITHM in which the M-STEP is computationally straightforward principal components analysis -lrb- M-STEP -rrb- . incorporating <unk> information -lsb- 12 -rsb- about expected LOCAL DEFORMATIONS only requires adding TANGENT VECTORS into the SAMPLE COVARIANCE MATRICES for the M-STEP , and it demonstrably improves performance . \n",
            "in this paper , we propose a novel approach to RECOGNITION based on LOCALLY LINEAR GENERATIVE MODELS . the proposed method is based on the use of SAMPLE COVARIANCE MATRICES in the SAMPLE COVARIANCE MATRICES of the SAMPLE COVARIANCE MATRICES . the proposed method is based on the use of LOCALLY LINEAR GENERATIVE MODELS for RECOGNITION . the proposed method is based on the use of SAMPLE COVARIANCE MATRICES in the SAMPLE COVARIANCE MATRICES of the SAMPLE COVARIANCE MATRICES . the experimental results show that the proposed method is effective in improving the RECOGNITION performance .\n",
            "\n",
            "563 1000\n",
            "an empirical comparison of CFG FILTERING TECHNIQUES for LTAG and HPSG is presented . we demonstrate that an approximation of HPSG produces a more effective CFG FILTER than that of LTAG . we also investigate the reason for that difference . \n",
            "this paper presents a new approach to HPSG for HPSG . the CFG FILTER is based on the use of CFG FILTERING TECHNIQUES for HPSG . the CFG FILTER is based on the use of CFG FILTERING TECHNIQUES for HPSG . the experimental results show that the proposed LTAG is more effective than HPSG for HPSG .\n",
            "\n",
            "564 1000\n",
            "parallel named entity PARALLEL NAMED ENTITY PAIRS are important resources in several NLP TASKS , such as , <unk> and mt systems . further , such PARALLEL NAMED ENTITY PAIRS may also be used for TRAINING TRANSLITERATION SYSTEMS , if PARALLEL NAMED ENTITY PAIRS are <unk> of each other . in this paper , we profile the performance of a MINING METHODOLOGY in mining PARALLEL NAMED ENTITY TRANSLITERATION PAIRS in ENGLISH and an INDIAN LANGUAGE , TAMIL , leveraging LINGUISTIC TOOLS in ENGLISH , and ARTICLE-ALIGNED COMPARABLE CORPORA in the two languages . we adopt a MINING METHODOLOGY parallel to that of -lsb- <unk> and <unk> , 2006 -rsb- , but we focus instead on mining PARALLEL NAMED ENTITY TRANSLITERATION PAIRS , using a WELL-TRAINED LINEAR CLASSIFIER to identify TRANSLITERATION PAIRS . we profile the performance at several operating parameters of our MINING METHODOLOGY and present the results that show the potential of the MINING METHODOLOGY in MINING TRANSLITERATIONS PAIRS ; in addition , we uncover a host of issues that need to be resolved , for effective mining of PARALLEL NAMED ENTITY TRANSLITERATION PAIRS . \n",
            "this paper presents a novel approach to PARALLEL NAMED ENTITY TRANSLITERATION PAIRS , which is based on a WELL-TRAINED LINEAR CLASSIFIER . the approach is based on the use of a WELL-TRAINED LINEAR CLASSIFIER , a WELL-TRAINED LINEAR CLASSIFIER , and a WELL-TRAINED LINEAR CLASSIFIER for TAMIL . the proposed approach is based on the use of a WELL-TRAINED LINEAR CLASSIFIER , which is able to capture the PARALLEL NAMED ENTITY PAIRS in the INDIAN LANGUAGE . the proposed approach is evaluated on a variety of NLP TASKS , including PARALLEL NAMED ENTITY PAIRS , and PARALLEL NAMED ENTITY TRANSLITERATION PAIRS . the experimental results show that the proposed approach is able to perform well in NLP TASKS , such as PARALLEL NAMED ENTITY TRANSLITERATION PAIRS .\n",
            "\n",
            "565 1000\n",
            "in many ESTIMATION PROBLEMS , the set of UNKNOWN PARAMETERS can be divided into a subset of desired parameters and a subset of NUISANCE PARAMETERS . using a maximum a POS-TERIORI APPROACH to PARAMETER ESTIMATION , these NUISANCE PARAMETERS are integrated out in the ESTIMATION PROCESS . this can result in an extremely COMPUTATIONALLY-INTENSIVE ESTI-MATOR . this paper proposes a method by which COMPUTATIONALLY-INTENSIVE I N TEGRATIONS over the NUISANCE PARAMETERS required in BAYESIAN ESTIMATION may be avoided under certain conditions . the <unk> method is an APPROXIMATE MAP ESTIMATOR which is much more compu-tationally ecient than direct , or even MONTE CARLO , integration of the joint posteriori distribution of the desired and NUISANCE PARAMETERS . as an example of its eciency , we apply the fast algorithm to MATCHED-ELD SOURCE LOCALIZA-TION in an uncertain environment . \n",
            "this paper addresses the problem of MATCHED-ELD SOURCE LOCALIZA-TION in a POS-TERIORI APPROACH . we propose a novel approach to the problem of MATCHED-ELD SOURCE LOCALIZA-TION in a POS-TERIORI APPROACH . the proposed method consists of two steps : -lrb- 1 -rrb- a POS-TERIORI APPROACH is proposed to incorporate a POS-TERIORI APPROACH into the ESTIMATION PROCESS ; -lrb- 2 -rrb- a POS-TERIORI APPROACH is proposed to solve the problem of PARAMETER ESTIMATION in the ESTIMATION PROCESS . the proposed algorithm is based on a POS-TERIORI APPROACH , which is a generalization of the COMPUTATIONALLY-INTENSIVE ESTI-MATOR to the ESTIMATION PROCESS . the performance of the proposed algorithm is demonstrated on a variety of ESTIMATION PROBLEMS . the experimental results show that the proposed approach is effective in reducing the number of NUISANCE PARAMETERS in the ESTIMATION PROCESS .\n",
            "\n",
            "566 1000\n",
            "in this paper we are studying the use of two microphones for ACOUSTIC FEEDBACK CANCELLATION in HEARING AIDS . with the two MICROPHONES APPROACH , an additional microphone is employed to provide added information about the signals which is then utilized to obtain an INCOMING SIGNAL ESTIMATE . this INCOMING SIGNAL ESTIMATE is removed from the ERROR SIGNAL prior to adapting the CANCELER , thus removing the UNDESIRED SIGNAL CORRELATION . in this paper , we propose to use ORTHOGONAL TRANSFORMS with the two MICROPHONES APPROACH . the discrete fourier transform and the DISCRETE COSINE TRANSFORM are implemented to transform the ADAPTIVE FILTER SIGNALS . also , a bank of ADAPTIVE FILTERS is employed , each adapting to different portions of the spectrum for a finer control of the ADAPTATION PROCESS . simulation results based on REAL MEASURED FEEDBACK PATHS and SPEECH SIGNALS show improved CONVERGENCE RATES and stable solutions . \n",
            "in this paper , we propose a novel approach to ACOUSTIC FEEDBACK CANCELLATION in HEARING AIDS . the proposed approach is based on a DISCRETE COSINE TRANSFORM , which is based on a DISCRETE COSINE TRANSFORM . the proposed approach is based on the use of a DISCRETE COSINE TRANSFORM , which is a CANCELER . the proposed approach is based on the DISCRETE COSINE TRANSFORM of the DISCRETE COSINE TRANSFORM . the proposed approach is based on the use of a DISCRETE COSINE TRANSFORM , which is a CANCELER . the proposed method can be applied to SPEECH SIGNALS and SPEECH SIGNALS . experimental results show that the proposed method is effective in improving the CONVERGENCE RATES and CONVERGENCE RATES of the proposed method .\n",
            "\n",
            "567 1000\n",
            "goal recognition design -lrb- <unk> -rrb- problems involve identifying the best ways to modify the underlying environment that the agents operate in , typically by making a subset of feasible actions infeasible , in such a way that agents are forced to reveal their goals as early as possible . thus far , existing work assumes that the outcomes of the actions of the agents are deterministic , which might be unrealistic in REAL-WORLD PROBLEMS . for example , WHEEL SLIPPAGE in robots cause the outcomes of their movements to be stochastic . in this paper , we generalize the GOAL RECOGNITION DESIGN PROBLEMS to STOCHASTIC GRD PROBLEMS , which handle STOCHASTIC ACTION OUTCOMES . we also generalize the WORST-CASE DISTINCTIVENESS MEASURE , which measures the goodness of a solution , to take STOCHASTIC-ITY into account . finally , we introduce MARKOV DECISION PROCESS BASED ALGORITHMS to compute the WCD and minimize MARKOV DECISION PROCESS BASED ALGORITHMS by making up to k actions infeasible . \n",
            "this paper addresses the problem of GOAL RECOGNITION DESIGN PROBLEMS for STOCHASTIC GRD PROBLEMS . in particular , we focus on the problem of STOCHASTIC GRD PROBLEMS in the context of STOCHASTIC GRD PROBLEMS for STOCHASTIC GRD PROBLEMS . we show that this problem can be solved efficiently using MARKOV DECISION PROCESS BASED ALGORITHMS . we then show how this problem can be solved efficiently using MARKOV DECISION PROCESS BASED ALGORITHMS . we also show that the WORST-CASE DISTINCTIVENESS MEASURE of the algorithm is bounded by a factor of o -lrb- log -lrb- 1 / √ t -rrb- -rrb- and o -lrb- log -lrb- 1 / √ t -rrb- -rrb- , where the number of STOCHASTIC-ITY is bounded by a factor of o -lrb- 1 / √ t -rrb- .\n",
            "\n",
            "568 1000\n",
            "we show that the approaches to 3D RECONSTRUCTION that use VOLUMETRIC GRAPH CUTS to minimize a COST FUNCTION over the OBJECT SURFACE have two types of biases , the MINIMAL SURFACE BIAS and the DISCRETIZATION BIAS . these biases make it difficult to recover SURFACE EXTRUSIONS and other details , especially when a NON-LAMBERTIAN PHOTO-CONSISTENCY MEASURE is used . to reduce these biases , we propose a new ITERATIVE GRAPH CUTS BASED ALGORITHM that operates on the SURFACE DISTANCE GRID , which is a special discretization of the 3D SPACE , constructed using a SIGNED DISTANCE TRANSFORM of the current surface estimate . it can be shown that SURFACE DISTANCE GRID significantly reduces the MINIMAL SURFACE BIAS , and transforms the DISCRETIZATION BIAS into a controllable degree of SURFACE SMOOTHNESS . experiments on 3D RECONSTRUCTION OF NON-LAMBERTIAN OBJECTS confirm the effectiveness of our ITERATIVE GRAPH CUTS BASED ALGORITHM over previous methods . \n",
            "this paper proposes a novel ITERATIVE GRAPH CUTS BASED ALGORITHM for 3D RECONSTRUCTION OF NON-LAMBERTIAN OBJECTS . the proposed ITERATIVE GRAPH CUTS BASED ALGORITHM is based on the SIGNED DISTANCE TRANSFORM and the DISCRETIZATION BIAS of the OBJECT SURFACE . the proposed ITERATIVE GRAPH CUTS BASED ALGORITHM is based on the SIGNED DISTANCE TRANSFORM and the DISCRETIZATION BIAS . the proposed ITERATIVE GRAPH CUTS BASED ALGORITHM is based on the SIGNED DISTANCE TRANSFORM and the DISCRETIZATION BIAS . the proposed ITERATIVE GRAPH CUTS BASED ALGORITHM is based on the SIGNED DISTANCE TRANSFORM and the MINIMAL SURFACE BIAS . the proposed ITERATIVE GRAPH CUTS BASED ALGORITHM is evaluated on a 3D RECONSTRUCTION OF NON-LAMBERTIAN OBJECTS and on a set of 3D RECONSTRUCTION OF NON-LAMBERTIAN OBJECTS . the results show that the proposed ITERATIVE GRAPH CUTS BASED ALGORITHM is able to recover the OBJECT SURFACE of the OBJECT SURFACE in the 3D SPACE .\n",
            "\n",
            "569 1000\n",
            "<unk> is a useful SIMRANK-LIKE MEASURE OF SIMILARITY based on GRAPH STRUCTURE . the existing method iteratively computes each pair of CO-SIMRANK SCORE from a dot product of two PAGERANK VECTORS , <unk> o -lrb- log -lrb- 1 / ǫ -rrb- n 3 -rrb- time to compute all pairs of CO-SIMRANK SCORE in a GRAPH with n nodes , to attain a desired accuracy ǫ . in this study , we devise a model , CO-SIMRANK , to speed up the retrieval of all pairs of CO-SIMRANK SCORE to o -lrb- log 2 -lrb- log -lrb- 1 / ǫ -rrb- -rrb- n 3 -rrb- time . moreover , we show the optimality of CO-SIMRANK among other <unk> - -lrb- u k -rrb- variations , and integrate it with a MATRIX DECOMPOSITION BASED METHOD on SINGULAR GRAPHS to attain higher efficiency . the viable experiments verify the superiority of CO-SIMRANK to others . \n",
            "in this paper , we propose a novel MATRIX DECOMPOSITION BASED METHOD based on GRAPH STRUCTURE . the proposed MATRIX DECOMPOSITION BASED METHOD is based on a SIMRANK-LIKE MEASURE OF SIMILARITY , which is a generalization of the MATRIX DECOMPOSITION BASED METHOD . the proposed MATRIX DECOMPOSITION BASED METHOD is based on a SIMRANK-LIKE MEASURE OF SIMILARITY , which is a SIMRANK-LIKE MEASURE OF SIMILARITY . the proposed MATRIX DECOMPOSITION BASED METHOD is based on a SIMRANK-LIKE MEASURE OF SIMILARITY , which is a SIMRANK-LIKE MEASURE OF SIMILARITY . the proposed MATRIX DECOMPOSITION BASED METHOD is applied to the problem of SIMRANK-LIKE MEASURE OF SIMILARITY in the GRAPH . experimental results show that the proposed method is effective in improving the CO-SIMRANK SCORE of the proposed MATRIX DECOMPOSITION BASED METHOD .\n",
            "\n",
            "570 1000\n",
            "automated musical accompaniment of human performers often requires an agent be able to follow a musical score with similar facility to that of a HUMAN PERFORMER . systems described in the literature represent musical scores in a way that assumes no LARGE-SCALE STRUCTURAL VARIATION of the piece during performance . if the performer deviates from the expected path by <unk> or repeating a section , the system may become lost . we describe a way to automatically generate a MARKOV MODEL from a WRITTEN SCORE that models the SCORE FORM , and an ON-LINE ALGORITHM to align a performance to a score . the resulting system can follow performances that take alternate paths through the score without losing its place . we compare the performance of our system to that of SEQUENCE-BASED SCORE FOLLOWERS on a MELODIC CORPUS OF 98 JAZZ MELODIES . results show that explicitly representing the BRANCHING STRUCTURE of a score significantly improves score following when the branch a performer may take is unknown beforehand . \n",
            "this paper presents a novel approach to AUTOMATED MUSICAL ACCOMPANIMENT OF HUMAN PERFORMERS in the context of AUTOMATED MUSICAL ACCOMPANIMENT OF HUMAN PERFORMERS . the proposed approach is based on the use of a MARKOV MODEL to estimate the BRANCHING STRUCTURE of the MARKOV MODEL . the proposed method is based on a MARKOV MODEL , which is based on a MARKOV MODEL . the proposed method is evaluated on the MELODIC CORPUS OF 98 JAZZ MELODIES . the results show that the proposed SEQUENCE-BASED SCORE FOLLOWERS is effective in improving the AUTOMATED MUSICAL ACCOMPANIMENT OF HUMAN PERFORMERS performance .\n",
            "\n",
            "571 1000\n",
            "this paper describes the application of RAO-BLACKWELLISED GIBBS SAMPLING to SPEECH RECOGNITION using SWITCHING LINEAR DY-NAMICAL SYSTEMS -lrb- RAO-BLACKWELLISED GIBBS SAMPLING -rrb- . the RAO-BLACKWELLISED GIBBS SAMPLING is a hybrid of standard HIDDEN MARKOV MODELS and LINEAR DYNAMICAL SYSTEMS . RAO-BLACKWELLISED GIBBS SAMPLING is an extension of the STOCHASTIC SEGMENT MODEL as RAO-BLACKWELLISED GIBBS SAMPLING relaxes the ASSUMPTION OF INDEPENDENT SEGMENTS . RAO-BLACKWELLISED GIBBS SAMPLING explicitly take into account the strong co-articulation present in SPEECH . unfortunately , INFERENCE in RAO-BLACKWELLISED GIBBS SAMPLING is intractable unless the DISCRETE STATE SEQUENCE is known . RAO-BLACKWELLISED GIBBS SAMPLING is one approach that may be applied for both improved training and decoding for this form of INTRACTABLE MODEL . the theory of RAO-BLACKWELLISED GIBBS SAMPLING and RAO-BLACKWELLISED GIBBS SAMPLING is described , along with an efficient PROPOSAL MECHANISM . the performance of the RAO-BLACKWELLISED GIBBS SAMPLING using RAO-BLACKWELLISED GIBBS SAMPLING for training and INFERENCE is evaluated on the ARPA RESOURCE MANAGEMENT TASK . \n",
            "in this paper , we propose a novel STOCHASTIC SEGMENT MODEL for SPEECH RECOGNITION and HIDDEN MARKOV MODELS . the proposed STOCHASTIC SEGMENT MODEL is based on the use of RAO-BLACKWELLISED GIBBS SAMPLING and HIDDEN MARKOV MODELS . the proposed STOCHASTIC SEGMENT MODEL is based on the use of RAO-BLACKWELLISED GIBBS SAMPLING and HIDDEN MARKOV MODELS . the proposed STOCHASTIC SEGMENT MODEL is based on the use of RAO-BLACKWELLISED GIBBS SAMPLING and HIDDEN MARKOV MODELS . the proposed RAO-BLACKWELLISED GIBBS SAMPLING is evaluated on the ARPA RESOURCE MANAGEMENT TASK , and the results show that the proposed RAO-BLACKWELLISED GIBBS SAMPLING is effective in improving SPEECH RECOGNITION performance . moreover , the proposed RAO-BLACKWELLISED GIBBS SAMPLING can also be applied to other LINEAR DYNAMICAL SYSTEMS . the experimental results on the ARPA RESOURCE MANAGEMENT TASK show that the proposed RAO-BLACKWELLISED GIBBS SAMPLING outperforms the state-of-the-art methods in terms of SPEECH RECOGNITION and SWITCHING LINEAR DY-NAMICAL SYSTEMS .\n",
            "\n",
            "572 1000\n",
            "energy detection is widely used by COGNITIVE RADIOS for SPECTRUM SENSING . during a silent period , SECONDARY USERS are kept silent so that the ENERGY DETECTOR does not confuse SU SIGNALS for primary user -lrb- pu -rrb- signals . due to imperfect coordination , an SECONDARY USERS may transmit during a silent period and cause possible false alarms . we propose to leverage matched filters that already exist in many SUS to alleviate the impact of such SU INTERFERENCE by combining the matched filtering result and the ENERGY DETECTION result . the analysis shows that for practical purposes , our algorithm virtually eliminates all of the negative impact of SU INTERFERENCE with only negligible penalty in DELAY and energy consumption . \n",
            "this paper addresses the problem of ENERGY DETECTION in COGNITIVE RADIOS such as ENERGY DETECTION and ENERGY DETECTION . we propose a novel approach to ENERGY DETECTION based on SPECTRUM SENSING . the proposed approach is based on the use of SUS , which is able to deal with SU INTERFERENCE in the presence of SU INTERFERENCE . the proposed approach is based on the use of SUS , which is able to deal with SU INTERFERENCE in the presence of SU INTERFERENCE . the experimental results show that the proposed approach is effective in improving the performance of ENERGY DETECTION in COGNITIVE RADIOS .\n",
            "\n",
            "573 1000\n",
            "this paper proposes a DICTIONARY LEARNING FRAMEWORK that combines the proposed <unk> -lrb- <unk> -rrb- or RECONSTRUCTED BLOCK/GROUP SPARSE CODING SCHEMES with the novel <unk> coherence suppression dictionary learning -lrb- <unk> -rrb- algorithm . an important and distinguishing DICTIONARY BLOCKS of the proposed DICTIONARY LEARNING FRAMEWORK is that all DICTIONARY BLOCKS are trained simultaneously with respect to each data group while the INTRA-BLOCK COHERENCE being explicitly minimized as an important objective . we provide both empirical evidence and heuristic support for this DICTIONARY BLOCKS that can be considered as a direct consequence of incorporating both the GROUP STRUCTURE for the INPUT DATA and the BLOCK STRUCTURE for the dictionary in the LEARNING PROCESS . the OPTIMIZATION PROBLEMS for both the DICTIONARY LEARNING and SPARSE CODING can be solved efficiently using BLOCK-GRADIENT DESCENT , and the details of the OPTIMIZATION ALGORITHMS are presented . we evaluate the proposed methods using WELL-KNOWN DATASETS , and favorable comparisons with state-of-the-art DICTIONARY LEARNING METHODS demonstrate the viability and validity of the proposed DICTIONARY LEARNING FRAMEWORK . \n",
            "this paper addresses the problem of SPARSE CODING and SPARSE CODING in WELL-KNOWN DATASETS . in particular , we propose a novel approach to DICTIONARY LEARNING for SPARSE CODING . the proposed approach is based on the use of BLOCK-GRADIENT DESCENT , BLOCK STRUCTURE , and BLOCK STRUCTURE . the proposed approach is based on the use of BLOCK-GRADIENT DESCENT , BLOCK STRUCTURE , and BLOCK STRUCTURE . the proposed approach is based on the use of BLOCK-GRADIENT DESCENT and BLOCK STRUCTURE . the proposed approach is evaluated on WELL-KNOWN DATASETS , and the results show that the proposed method is effective in improving the performance of the proposed RECONSTRUCTED BLOCK/GROUP SPARSE CODING SCHEMES .\n",
            "\n",
            "574 1000\n",
            "we propose an ADAPTIVE DIFFUSION STRATEGY with LIMITED COMMUNICATION OVERHEAD by cutting off all links but one for each NODE in the network . we keep the '' best '' neighbor that has the smallest estimated <unk> measure and ignore the other neighbors . the COMBINATION COEFFICIENTS for the INTERACTING NODES are calculated via a MAXIMAL-RATIO-COMBINING RULE to minimize the STEADY-STATE MEAN-SQUARE-DEVIATION . simulation results illustrate that , with less COMMUNICATION OVERHEAD and less computations , the proposed ADAPTIVE DIFFUSION STRATEGY performs well and outperforms other related methods with similar overheads . \n",
            "in this paper , we consider the problem of STEADY-STATE MEAN-SQUARE-DEVIATION in the presence of INTERACTING NODES . we first show that the COMBINATION COEFFICIENTS of the COMBINATION COEFFICIENTS can be reduced to the MAXIMAL-RATIO-COMBINING RULE of the INTERACTING NODES . we then show that the COMBINATION COEFFICIENTS of the COMBINATION COEFFICIENTS can be approximated by a MAXIMAL-RATIO-COMBINING RULE . we then show that the COMBINATION COEFFICIENTS is equivalent to the MAXIMAL-RATIO-COMBINING RULE of the MAXIMAL-RATIO-COMBINING RULE . we also show that the MAXIMAL-RATIO-COMBINING RULE can be approximated by a MAXIMAL-RATIO-COMBINING RULE .\n",
            "\n",
            "575 1000\n",
            "conventional VIDEO SUMMARIZATION METHODS <unk> -rrb- cus predominantly on summarizing videos along the TIME AXIS , such as building a MOVIE TRAILER : the resulting VIDEO TRAILER tends to retain much EMPTY SPUCE in the background of the video <unk> while discarding much <unk> video content due lo size limit . in this <unk> we propose a novel SPACE-TIME VIDEO SUMMARIZATION METHOD which we call SPACE-TIME VIDEO MONTAGE . the SPACE-TIME VIDEO SUMMARIZATION METHOD simultaneously analyzes both the SPATIAL AND TEMPORAL INJBRMATION DISTRIBUTION in a VIDEO SEQUENCE , and <unk> the visually informative space-time portions of the input videos . the informative video <unk> are represented in VOLUMETRIC LA.YERS . the LAYERS are then <unk> together in a SMULL OUZPUT VIDEO VOLUME such <unk> the total amount of VISUAL INFORMATION in the VIDEO VOLUME is maximized . to achieve the PACKING PROCESS , we develop a new SPACE-TIME VIDEO SUMMARIZATION METHOD based upon THE3RST-JT UND GRAPH CUT OPTIMIZATION TECHNIQUES . since our SPACE-TIME VIDEO SUMMARIZATION METHOD is <unk> to <unk> <unk> spatially <unk> temporally less informative portions , it is <unk> to <unk> much more <unk> yet highly informative output videos . the <unk> -lrb- $ our SPACE-TIME VIDEO SUMMARIZATION METHOD is validated by extensive experiments over a wide variety c ~ videos . \n",
            "in this paper , we propose a novel SPACE-TIME VIDEO SUMMARIZATION METHOD based on THE3RST-JT UND GRAPH CUT OPTIMIZATION TECHNIQUES . the proposed SPACE-TIME VIDEO SUMMARIZATION METHOD is based on a THE3RST-JT UND GRAPH CUT OPTIMIZATION TECHNIQUES of the SPATIAL AND TEMPORAL INJBRMATION DISTRIBUTION of the VIDEO SEQUENCE . the proposed SPACE-TIME VIDEO SUMMARIZATION METHOD is based on the use of THE3RST-JT UND GRAPH CUT OPTIMIZATION TECHNIQUES to estimate the TIME AXIS of the VIDEO SEQUENCE . the proposed SPACE-TIME VIDEO SUMMARIZATION METHOD is based on a THE3RST-JT UND GRAPH CUT OPTIMIZATION TECHNIQUES of the SPATIAL AND TEMPORAL INJBRMATION DISTRIBUTION of the LAYERS . the proposed SPACE-TIME VIDEO SUMMARIZATION METHOD is based on a THE3RST-JT UND GRAPH CUT OPTIMIZATION TECHNIQUES of the SPATIAL AND TEMPORAL INJBRMATION DISTRIBUTION of the VIDEO SEQUENCE . the proposed method is evaluated on a SMULL OUZPUT VIDEO VOLUME and a SMULL OUZPUT VIDEO VOLUME . the results show that the proposed SPACE-TIME VIDEO SUMMARIZATION METHOD is able to accurately estimate the TIME AXIS of a scene from a VIDEO SEQUENCE .\n",
            "\n",
            "576 1000\n",
            "this paper addresses the SEARCH PROBLEM in TEXTUAL INFERENCE , where systems need to infer one piece of text from another . a prominent approach to this SEARCH PROBLEM is attempts to transform one text into the other through a sequence of INFERENCE-PRESERVING TRANSFORMATIONS , a.k.a. a proof , while estimating the proof 's validity . this raises a search challenge of finding the best possible proof . we explore this challenge through a comprehensive investigation of prominent SEARCH ALGORITHMS and propose two novel ALGORITHMIC COMPONENTS specifically designed for TEXTUAL INFERENCE : a GRADIENT-STYLE EVALUATION FUNCTION , and a LOCAL-LOOKAHEAD NODE EXPANSION METHOD . evaluations , using the OPEN-SOURCE SYSTEM , BIUTEE , show the contribution of these ideas to search efficiency and PROOF QUALITY . \n",
            "this paper presents a novel approach to the SEARCH PROBLEM . the proposed approach is based on the use of INFERENCE-PRESERVING TRANSFORMATIONS , a LOCAL-LOOKAHEAD NODE EXPANSION METHOD , and a LOCAL-LOOKAHEAD NODE EXPANSION METHOD . the proposed approach is based on the use of INFERENCE-PRESERVING TRANSFORMATIONS , such as the LOCAL-LOOKAHEAD NODE EXPANSION METHOD , and the GRADIENT-STYLE EVALUATION FUNCTION . the proposed approach is based on the use of INFERENCE-PRESERVING TRANSFORMATIONS , such as the GRADIENT-STYLE EVALUATION FUNCTION , and the GRADIENT-STYLE EVALUATION FUNCTION . the proposed method is evaluated in terms of the PROOF QUALITY and the PROOF QUALITY of the proposed method .\n",
            "\n",
            "577 1000\n",
            "this paper presents a new ANALYTICAL MODEL for the normalized least mean square -lrb- nlms -rrb- adaptive algorithm . the new ANALYTICAL MODEL is derived using a STOCHASTIC DIFFERENTIAL EQUATION APPROACH . an accurate estimate of the STEADY-STATE WEIGHT-ERROR CORRELATIONS is also derived , which leads to an improved ANALYTICAL MODEL performance for medium and large step sizes . NUMERICAL SIMULATIONS compare the new ANALYTICAL MODEL with existing ANALYTICAL MODEL and show better agreement with MONTE CARLO SIMULATIONS . \n",
            "this paper proposes a novel ANALYTICAL MODEL for STOCHASTIC DIFFERENTIAL EQUATION APPROACH . the proposed STOCHASTIC DIFFERENTIAL EQUATION APPROACH is based on a STOCHASTIC DIFFERENTIAL EQUATION APPROACH , which is a generalization of the existing STOCHASTIC DIFFERENTIAL EQUATION APPROACH . the proposed ANALYTICAL MODEL is based on a STOCHASTIC DIFFERENTIAL EQUATION APPROACH , which is a generalization of the existing ANALYTICAL MODEL . the proposed ANALYTICAL MODEL is evaluated on a number of NUMERICAL SIMULATIONS . the experimental results show that the proposed ANALYTICAL MODEL outperforms the existing ANALYTICAL MODEL in terms of both NUMERICAL SIMULATIONS and the ANALYTICAL MODEL .\n",
            "\n",
            "578 1000\n",
            "machine learning contains many COMPUTATIONAL BOTTLENECKS in the form of NESTED SUMMATIONS OVER DATASETS . computation of these MACHINE LEARNING is typically o -lrb- n 2 -rrb- or higher , which severely limits application to large datasets . we present a MULTI-STAGE STRATIFIED MONTE CARLO METHOD for approximating such MACHINE LEARNING with PROBABILISTIC RELATIVE ERROR CONTROL . the essential idea is fast approximation by sampling in trees . this MULTI-STAGE STRATIFIED MONTE CARLO METHOD differs from many previous SCALABILITY TECHNIQUES -lrb- such as MULTI-TREE METHODS -rrb- in that its error is stochastic , but we derive conditions for ERROR CONTROL and demonstrate that they work . further , we give a THEORETICAL SAMPLE COMPLEXITY for the MULTI-STAGE STRATIFIED MONTE CARLO METHOD that is independent of DATASET SIZE , and show that this appears to hold in experiments , where SPEEDUPS reach as high as 10 14 , many orders of magnitude beyond the previous state of the art . \n",
            "this paper proposes a novel MULTI-STAGE STRATIFIED MONTE CARLO METHOD for MACHINE LEARNING in MACHINE LEARNING . the proposed MULTI-STAGE STRATIFIED MONTE CARLO METHOD is based on the MULTI-STAGE STRATIFIED MONTE CARLO METHOD , which is a generalization of the existing MULTI-TREE METHODS . the proposed MULTI-STAGE STRATIFIED MONTE CARLO METHOD is based on the MULTI-STAGE STRATIFIED MONTE CARLO METHOD , which is a generalization of the MULTI-STAGE STRATIFIED MONTE CARLO METHOD . the proposed MULTI-STAGE STRATIFIED MONTE CARLO METHOD is based on the MULTI-STAGE STRATIFIED MONTE CARLO METHOD , which is a generalization of the existing MULTI-TREE METHODS . the proposed MULTI-STAGE STRATIFIED MONTE CARLO METHOD is evaluated on the NESTED SUMMATIONS OVER DATASETS , and the results show that the proposed MULTI-STAGE STRATIFIED MONTE CARLO METHOD is effective in improving the THEORETICAL SAMPLE COMPLEXITY of the MULTI-TREE METHODS . moreover , the proposed MULTI-STAGE STRATIFIED MONTE CARLO METHOD can also be applied to other MULTI-TREE METHODS such as MULTI-TREE METHODS .\n",
            "\n",
            "579 1000\n",
            "we present in this paper a novel approach for training DETERMINISTIC AUTO-ENCODERS . we show that by adding a well chosen penalty term to the CLASSICAL RECONSTRUCTION COST FUNCTION , we can achieve results that equal or surpass those attained by other REGULARIZED AUTO-ENCODERS as well as DENOISING AUTO-ENCODERS on a range of datasets . this penalty term corresponds to the FROBENIUS NORM of the JACOBIAN MATRIX of the ENCODER ACTIVATIONS with respect to the input . we show that this penalty term results in a LOCALIZED SPACE CONTRACTION which in turn yields robust FEATURES on the ACTIVATION LAYER . furthermore , we show how this penalty term is related to both REGULARIZED AUTO-ENCODERS and DENOISING AUTO-ENCODERS and how it can be seen as a link between DETERMINISTIC AND NON-DETERMINISTIC AUTO-ENCODERS . we find empirically that this penalty helps to <unk> a representation that better captures the LOCAL DIRECTIONS OF VARIATION dictated by the data , corresponding to a LOWER-DIMENSIONAL NON-LINEAR MANIFOLD , while being more invariant to the vast majority of directions orthogonal to the MANIFOLD . finally , we show that by using the learned FEATURES to initialize a MLP , we achieve state of the art classification error on a range of datasets , surpassing other methods of PRE-TRAINING . \n",
            "this paper addresses the problem of PRE-TRAINING and DETERMINISTIC AUTO-ENCODERS . in particular , we propose a novel method to estimate the ENCODER ACTIVATIONS of the MANIFOLD , which is a LOWER-DIMENSIONAL NON-LINEAR MANIFOLD of the MANIFOLD of the MANIFOLD . the proposed algorithm is based on the use of DETERMINISTIC AUTO-ENCODERS and DETERMINISTIC AUTO-ENCODERS to estimate the JACOBIAN MATRIX of the MANIFOLD . the proposed method is based on the use of FEATURES and REGULARIZED AUTO-ENCODERS . the proposed method is based on the DETERMINISTIC AND NON-DETERMINISTIC AUTO-ENCODERS and the DETERMINISTIC AUTO-ENCODERS . the proposed method is applied to the DETERMINISTIC AND NON-DETERMINISTIC AUTO-ENCODERS and the DENOISING AUTO-ENCODERS . the experimental results show that the proposed method outperforms the state-of-the-art methods .\n",
            "\n",
            "580 1000\n",
            "we present an overview of recent work in which EYE MOVEMENTS are monitored as people follow SPOKEN INSTRUCTIONS to move objects or pictures in a VISUAL WORKSPACE . subjects naturally make SACCADIC EYE-MOVEMENTS to objects that are closely <unk> to relevant information in the instruction . thus the <unk> provide a WINDOW into the RAPID MENTAL PROCESSES that underlie SPOKEN LANGUAGE COMPREHENSION . we review studies of REFERENCE RESOLUTION , WORD RECOGNITION , and pragmatic effects on SYNTACTIC AMBIGUITY RESOLUTION . our studies show that people seek to establish reference with respect to their behavioral goals during the earliest moments of LINGUISTIC PROCESSING . moreover , <unk> relevant NON-LINGUISTIC INFORMATION immediately affects how the LINGUISTIC INPUT is initially structured . \n",
            "this paper addresses the problem of SPOKEN LANGUAGE COMPREHENSION in SPOKEN LANGUAGE COMPREHENSION . we propose a method to estimate the NON-LINGUISTIC INFORMATION of a scene from a single image . our approach is based on the use of a WINDOW and a SYNTACTIC AMBIGUITY RESOLUTION . we show that the proposed approach is able to recover the NON-LINGUISTIC INFORMATION of a scene from a single image , and to estimate the NON-LINGUISTIC INFORMATION of a scene from a single image . we show that the proposed approach is able to detect and track moving objects in a scene from a single image . we demonstrate the effectiveness of our method on a variety of SPOKEN INSTRUCTIONS , including WORD RECOGNITION , WORD RECOGNITION and WORD RECOGNITION .\n",
            "\n",
            "581 1000\n",
            "matrix factorization techniques have been frequently applied in INFORMATION PROCESSING TASKS . among them , NON-NEGATIVE MATRIX FACTORIZATION have received considerable attentions due to its psychological and physiological interpretation of naturally occurring data whose representation may be <unk> in HUMAN BRAIN . on the other hand , from GEOMETRIC PERSPECTIVE the data is usually sampled from a LOW DIMENSIONAL MANIFOLD embedded in HIGH DIMENSIONAL AMBIENT SPACE . one hopes then to find a COMPACT REPRESENTATION which uncovers the HIDDEN TOPICS and simultaneously respects the INTRINSIC GEOMETRIC STRUCTURE . in this paper , we propose a novel algorithm , called locality preserving non-negative matrix factorization -lrb- <unk> -rrb- , for this purpose . for two data points , we use KL-DIVERGENCE to evaluate their similarity on the HIDDEN TOPICS . the optimal maps are obtained such that the FEATURE VALUES on HIDDEN TOPICS are restricted to be non-negative and vary smoothly along the GEODESICS OF THE DATA MANIFOLD . our empirical study shows the encouraging results of the proposed algorithm in comparisons to the state-of-the-art algorithms on two large HIGH-DIMENSIONAL DATABASES . \n",
            "this paper proposes a novel approach to INFORMATION PROCESSING TASKS from HIGH-DIMENSIONAL DATABASES . the proposed method is based on the use of NON-NEGATIVE MATRIX FACTORIZATION to estimate the INTRINSIC GEOMETRIC STRUCTURE of the HIDDEN TOPICS . the proposed method is based on the use of MATRIX FACTORIZATION TECHNIQUES to estimate the INTRINSIC GEOMETRIC STRUCTURE of the HIDDEN TOPICS . the proposed method is based on the use of MATRIX FACTORIZATION TECHNIQUES to estimate the INTRINSIC GEOMETRIC STRUCTURE . the proposed method is based on the use of MATRIX FACTORIZATION TECHNIQUES to estimate the INTRINSIC GEOMETRIC STRUCTURE of the HIDDEN TOPICS . the proposed method is applied to the problem of INFORMATION PROCESSING TASKS in HIGH-DIMENSIONAL DATABASES .\n",
            "\n",
            "582 1000\n",
            "kernel functions have become an extremely popular tool in MACHINE LEARNING , with an attractive theory as well . this theory views a kernel as implicitly mapping data points into a possibly very high dimensional space , and describes a KERNEL FUNCTION as being good for a given LEARNING PROBLEM if data is separable by a large margin in that IMPLICIT SPACE . however , while quite elegant , this theory does not directly correspond to one 's intuition of a good kernel as a good SIMILARITY FUNCTION . furthermore , it may be difficult for a domain expert to use the theory to help design an appropriate kernel for the LEARNING PROBLEM at hand since the IMPLICIT MAPPING may not be easy to calculate . finally , the requirement of POSITIVE SEMI-DEFINITENESS may rule out the most NATURAL PAIRWISE SIMILARITY FUNCTIONS for the given problem <unk> this work we develop an alternative , more general theory of learning with SIMILARITY FUNCTIONS -lrb- i.e. , sufficient conditions for a SIMILARITY FUNCTION to allow one to learn well -rrb- that does not require reference to IMPLICIT SPACES , and does not require the function to be positive semi-definite -lrb- or even symmetric -rrb- . our results also generalize the standard theory in the sense that any good KERNEL FUNCTION under the usual definition can be shown to also be a good SIMILARITY FUNCTION under our definition -lrb- though with some loss in the parameters -rrb- . in this way , we provide the first steps towards a THEORY OF KERNELS that describes the effectiveness of a given KERNEL FUNCTION in terms of NATURAL SIMILARITY-BASED PROPERTIES . \n",
            "this paper addresses the problem of MACHINE LEARNING in MACHINE LEARNING . the LEARNING PROBLEM consists of a set of KERNEL FUNCTIONS , each of which is a SIMILARITY FUNCTION of the SIMILARITY FUNCTION . the LEARNING PROBLEM is formulated as a LEARNING PROBLEM , where the LEARNING PROBLEM is solved by a KERNEL FUNCTION , and the LEARNING PROBLEM is solved by a THEORY OF KERNELS . the algorithm is based on a THEORY OF KERNELS , where the number of SIMILARITY FUNCTIONS is bounded by a KERNEL FUNCTION . the algorithm is based on a THEORY OF KERNELS , and the LEARNING PROBLEM is solved by a THEORY OF KERNELS . the algorithm is demonstrated on a variety of IMPLICIT SPACES , and the results show that the proposed algorithm is able to solve the problem of LEARNING PROBLEM .\n",
            "\n",
            "583 1000\n",
            "in this paper , we propose a method -- based on the DISCRETE EVOLUTIONARY TRANSFORM -- to estimate the INSTANTANEOUS FREQUENCY of a signal embedded in noise or <unk> signals . the DISCRETE EVOLUTIONARY TRANSFORM provides a REPRESENTATION for NON-STATIONARY SIGNALS and a TIME-FREQUENCY KERNEL that permit us to obtain the TIME-DEPENDENT SPECTRUM of the signal . we will show the INSTANTANEOUS PHASE and the corresponding INSTANTANEOUS FREQUENCY can also be computed from the EVOLUTIONARY KERNEL . estimation of INSTANTANEOUS FREQUENCY is of general interest in TIME-FREQUENCY ANALYSIS , and of special interest in the EXCISION OF JAMMERS in DIRECT SEQUENCE SPREAD SPECTRUM . implementation of the INSTANTANEOUS FREQUENCY is done by MASKING and a RECURSIVE NON-LINEAR CORRECTION PROCEDURE . the proposed INSTANTANEOUS FREQUENCY is valid for <unk> as well as <unk> signals in the NOISELESS AND NOISY SITUATIONS . its application to JAMMER EXCISION in DIRECT SEQUENCE SPREAD SPECTRUM COMMUNICATION is considered as an important application . the INSTANTANEOUS FREQUENCY procedure is illustrated with several examples . \n",
            "this paper addresses the problem of DIRECT SEQUENCE SPREAD SPECTRUM COMMUNICATION from a single image . in particular , we propose a novel RECURSIVE NON-LINEAR CORRECTION PROCEDURE , a RECURSIVE NON-LINEAR CORRECTION PROCEDURE and a RECURSIVE NON-LINEAR CORRECTION PROCEDURE for NON-STATIONARY SIGNALS . the proposed RECURSIVE NON-LINEAR CORRECTION PROCEDURE is based on a RECURSIVE NON-LINEAR CORRECTION PROCEDURE and a RECURSIVE NON-LINEAR CORRECTION PROCEDURE . a RECURSIVE NON-LINEAR CORRECTION PROCEDURE and a TIME-FREQUENCY KERNEL are used to estimate the INSTANTANEOUS FREQUENCY and the INSTANTANEOUS FREQUENCY . the proposed RECURSIVE NON-LINEAR CORRECTION PROCEDURE is applied to the INSTANTANEOUS PHASE and the TIME-DEPENDENT SPECTRUM of the TIME-DEPENDENT SPECTRUM and the TIME-DEPENDENT SPECTRUM . the proposed RECURSIVE NON-LINEAR CORRECTION PROCEDURE is evaluated on a variety of NOISELESS AND NOISY SITUATIONS .\n",
            "\n",
            "584 1000\n",
            "we describe an approach to SPEED-UP INFERENCE with LATENT-VARIABLE PCFGS , which have been shown to be highly effective for NATURAL LANGUAGE PARSING . our approach is based on a TENSOR FORMULATION recently introduced for spectral estimation of LATENT-VARIABLE PCFGS coupled with a TENSOR DECOMPOSITION ALGORITHM well-known in the MULTILINEAR ALGEBRA LITERATURE . we also describe an error bound for this approximation , which gives guarantees showing that if the underlying tensors are well approximated , then the PROBABILITY DISTRIBUTION OVER TREES will also be well approximated . empirical evaluation on REAL-WORLD NATURAL LANGUAGE PARSING DATA demonstrates a significant speed-up at minimal cost for PARSING performance . \n",
            "this paper addresses the problem of NATURAL LANGUAGE PARSING from REAL-WORLD NATURAL LANGUAGE PARSING DATA . we propose a novel approach to the problem of NATURAL LANGUAGE PARSING . the proposed approach is based on a TENSOR DECOMPOSITION ALGORITHM , which is based on a TENSOR DECOMPOSITION ALGORITHM . the proposed approach is based on a TENSOR DECOMPOSITION ALGORITHM , which is based on a TENSOR DECOMPOSITION ALGORITHM . the proposed approach is based on the use of a TENSOR DECOMPOSITION ALGORITHM . the proposed approach is evaluated on the MULTILINEAR ALGEBRA LITERATURE , and the results show that the proposed method is effective in improving the PARSING performance .\n",
            "\n",
            "585 1000\n",
            "discriminative approaches to HUMAN POSE INFERENCE involve MAPPING VISUAL OBSERVATIONS to ARTICULATED BODY CONFIGURATIONS . current PROBABILISTIC APPROACHES to learn this MAPPING VISUAL OBSERVATIONS have been limited in their ability to handle domains with a large number of activities that require very large training sets . we propose an ONLINE PROBABILISTIC REGRESSION SCHEME for efficient INFERENCE of complex , high-dimensional , and multimodal mappings . our ONLINE PROBABILISTIC REGRESSION SCHEME is based on a local mixture of GAUSSIAN PROCESSES , where LOCALITY is defined based on both APPEARANCE AND POSE , and where the MAPPING HYPERPARAMETERS can vary across LOCAL NEIGHBORHOODS to better adapt to specific regions in the POSE SPACE . the MAPPING HYPERPARAMETERS are defined online in very small neighborhoods , so learning and INFERENCE is extremely efficient . when the MAPPING VISUAL OBSERVATIONS is <unk> , we derive a bound on the approximation error of LOCAL REGRESSION -lrb- vs. GLOBAL REGRESSION -rrb- for MONOTONICALLY DECREASING CO-VARIANCE FUNCTIONS . our ONLINE PROBABILISTIC REGRESSION SCHEME can determine when training examples are redundant given the rest of the database , and use this criteria for PRUNING . we report results on synthetic -lrb- <unk> -rrb- and REAL POSE DATABASES , obtaining fast and accurate pose estimates using training set sizes up to 10 5 . \n",
            "this paper proposes a novel ONLINE PROBABILISTIC REGRESSION SCHEME based on GAUSSIAN PROCESSES for HUMAN POSE INFERENCE . the proposed ONLINE PROBABILISTIC REGRESSION SCHEME is based on the use of GAUSSIAN PROCESSES for HUMAN POSE INFERENCE . the proposed ONLINE PROBABILISTIC REGRESSION SCHEME is based on the use of GAUSSIAN PROCESSES for HUMAN POSE INFERENCE . the proposed ONLINE PROBABILISTIC REGRESSION SCHEME is based on the use of GAUSSIAN PROCESSES to estimate the APPEARANCE AND POSE , and the MAPPING HYPERPARAMETERS is used to estimate the MAPPING HYPERPARAMETERS . the proposed method is evaluated on REAL POSE DATABASES and compared to the state of the art methods . the proposed method is evaluated on REAL POSE DATABASES . the results show that the proposed method is effective in improving the APPEARANCE AND POSE and LOCALITY .\n",
            "\n",
            "586 1000\n",
            "we propose a novel strategy for training NEURAL NETWORKS using SEQUENTIAL SAMPLING-IMPORTANCE RESAMPLING ALGORITHMS . this global optimisation strategy allows us to learn the PROBABILITY DISTRIBUTION of the NETWORK WEIGHTS in a SEQUENTIAL FRAMEWORK . it is well suited to applications involving on-line , nonlinear , non-gaussian or non-stationary signal processing . \n",
            "this paper addresses the problem of NEURAL NETWORKS for NEURAL NETWORKS . in particular , we consider the problem of NEURAL NETWORKS , where the number of NETWORK WEIGHTS is small and the number of NETWORK WEIGHTS is large . we show that this problem can be viewed as a special case of the SEQUENTIAL FRAMEWORK . we show that the PROBABILITY DISTRIBUTION can be approximated by a simple SEQUENTIAL FRAMEWORK . we also show that the NETWORK WEIGHTS can be obtained by solving a SEQUENTIAL FRAMEWORK .\n",
            "\n",
            "587 1000\n",
            "we present an INCREMENTAL ADAPTATION APPROACH for STATISTICAL MACHINE TRANSLATION that maintains a FLEXIBLE HIERARCHICAL DOMAIN STRUCTURE within a single CONSISTENT MODEL . both weights and RULES are updated incrementally on a STREAM OF POST-EDITS . our MULTI-LEVEL DOMAIN HIERARCHY allows the INCREMENTAL ADAPTATION APPROACH to adapt simultaneously towards LOCAL CONTEXT at different levels of GRANULARITY , including genres and individual documents . our experiments show consistent improvements in TRANSLATION QUALITY from all components of our INCREMENTAL ADAPTATION APPROACH . \n",
            "in this paper , we propose a novel INCREMENTAL ADAPTATION APPROACH for STATISTICAL MACHINE TRANSLATION . the proposed INCREMENTAL ADAPTATION APPROACH is based on the use of a STREAM OF POST-EDITS and a STREAM OF POST-EDITS . the proposed INCREMENTAL ADAPTATION APPROACH is based on the use of a STREAM OF POST-EDITS . the proposed INCREMENTAL ADAPTATION APPROACH is based on the use of a STREAM OF POST-EDITS and a STREAM OF POST-EDITS . the proposed INCREMENTAL ADAPTATION APPROACH is applied to the problem of STATISTICAL MACHINE TRANSLATION . the experimental results show that the proposed INCREMENTAL ADAPTATION APPROACH significantly improves the TRANSLATION QUALITY and TRANSLATION QUALITY of the proposed INCREMENTAL ADAPTATION APPROACH .\n",
            "\n",
            "588 1000\n",
            "we discuss a model for IMAGE SEGMENTATION that is able to overcome the SHORT-BOUNDARY BIAS observed in standard PAIRWISE RANDOM FIELD BASED APPROACHES . to <unk> , we show that a RANDOM FIELD with MULTI-LAYERED HIDDEN UNITS can encode boundary preserving higher order potentials such as the ones used in the COOPERATIVE CUTS MODEL of -lsb- 12 -rsb- while still allowing for fast and exact MAP INFERENCE . EXACT INFERENCE allows our model to outperform previous IMAGE SEG-MENTATION METHODS , and to see the true effect of COUPLING GRAPH EDGES . finally , our model can be easily extended to handle SEGMENTATION INSTANCES with multiple labels , for which it yields promising results . \n",
            "this paper proposes a novel approach to IMAGE SEGMENTATION based on MULTI-LAYERED HIDDEN UNITS . the proposed approach is based on the use of MULTI-LAYERED HIDDEN UNITS extracted from MULTI-LAYERED HIDDEN UNITS . the proposed method is based on the use of MULTI-LAYERED HIDDEN UNITS extracted from MULTI-LAYERED HIDDEN UNITS . the proposed method is based on the use of MULTI-LAYERED HIDDEN UNITS extracted from the MULTI-LAYERED HIDDEN UNITS . the proposed method is evaluated on a variety of SEGMENTATION INSTANCES . the experimental results show that the proposed method outperforms the state-of-the-art methods .\n",
            "\n",
            "589 1000\n",
            "we study PARTICLE FILTERING ALGORITHMS for TRACKING on infinite -lrb- in practice , large -rrb- dimensional state spaces . PARTICLE FILTERING -LRB- MONTE CARLO SAMPLING -RRB- from a LARGE DIMENSIONAL SYSTEM NOISE DISTRIBUTION is computationally expensive . but , in most large dim TRACKING applications , it is fair to assume that '' most of the state change '' occurs in a small dimensional basis and the basis itself may be slowly time varying -lrb- approximated as piecewise constant -rrb- . we have proposed a PF ALGORITHM with BASIS CHANGE DETECTION and RE-ESTIMATION STEPS that uses this idea . the implicit assumptions in defining this PF ALGORITHM are very strong . we study here the implications of WEAKER ASSUMPTIONS and how to handle them . we propose to use a simple modification of the ASYMPTOTICALLY STABLE ADAPTIVE PARTICLE FILTER to handle errors in estimating the basis dimension . \n",
            "this paper proposes a novel PF ALGORITHM based on PARTICLE FILTERING -LRB- MONTE CARLO SAMPLING -RRB- . the proposed ASYMPTOTICALLY STABLE ADAPTIVE PARTICLE FILTER is based on the use of BASIS CHANGE DETECTION and RE-ESTIMATION STEPS . the proposed ASYMPTOTICALLY STABLE ADAPTIVE PARTICLE FILTER is based on the use of BASIS CHANGE DETECTION and RE-ESTIMATION STEPS . the proposed PF ALGORITHM is based on the use of PARTICLE FILTERING -LRB- MONTE CARLO SAMPLING -RRB- and RE-ESTIMATION STEPS . the performance of the proposed PF ALGORITHM is evaluated in terms of TRACKING performance . the experimental results show that the proposed PF ALGORITHM is effective in improving the TRACKING performance .\n",
            "\n",
            "590 1000\n",
            "recently , the MODULATION SPECTRUM has been proposed and found to be a useful source of SPEECH INFORMATION . the MODULATION SPECTRUM represents longer term variations in the spectrum and thus implicitly requires FEATURES extracted from much longer speech segments compared to MFCCS and their delta terms . in this paper , a DISCRETE COSINE TRANSFORM ANALYSIS of the LOG MAGNITUDE SPECTRUM combined with a discrete cosine series -lrb- <unk> -rrb- expansion of dct coefficients over time is proposed as a method for capturing both the SPECTRAL AND MODULATION INFORMATION . these DCT/DCS FEATURES can be computed so as to emphasize FREQUENCY RESOLUTION or TIME RESOLUTION or a combination of the two factors . several variations of the DCT/DCS FEATURES were evaluated with PHONETIC RECOGNITION experiments using TIMIT and its telephone version -lrb- TIMIT -rrb- . best results obtained with a combined feature set are <unk> % for TIMIT and <unk> % for TIMIT . the MODULATION FEATURES are shown to be far more important than the SPECTRAL FEATURES for AUTOMATIC SPEECH RECOGNITION and far more noise robust . \n",
            "in this paper , we propose a novel approach to AUTOMATIC SPEECH RECOGNITION based on DISCRETE COSINE TRANSFORM ANALYSIS . the proposed approach is based on the use of SPECTRAL FEATURES extracted from MFCCS . the proposed method is based on the use of SPECTRAL FEATURES extracted from MFCCS . the proposed method is based on the use of DCT/DCS FEATURES extracted from MFCCS . the proposed method is evaluated in terms of AUTOMATIC SPEECH RECOGNITION and TIME RESOLUTION . the experimental results show that the proposed method outperforms the conventional MFCCS in terms of TIME RESOLUTION and TIME RESOLUTION .\n",
            "\n",
            "591 1000\n",
            "in SUPERVISED LEARNING , many techniques focus on OPTIMIZING TRAINING PHASE to increase PREDICTION performance . ACTIVE INFERENCE , a relatively novel paradigm , aims to decrease overall PREDICTION error via SELECTIVE COLLECTION of some labels based on relations among instances . in this research , we use DYNAMIC BAYESIAN NETWORKS to model TEMPORAL SYSTEMS and we apply ACTIVE INFERENCE to dynamically choose variables for observation so as to improve PREDICTION on UNOBSERVED VARIABLES . \n",
            "this paper addresses the problem of ACTIVE INFERENCE in TEMPORAL SYSTEMS . the main idea is to design a DYNAMIC BAYESIAN NETWORKS that is based on DYNAMIC BAYESIAN NETWORKS . the proposed algorithm is based on the use of DYNAMIC BAYESIAN NETWORKS to estimate the UNOBSERVED VARIABLES . the proposed algorithm is based on the use of DYNAMIC BAYESIAN NETWORKS to estimate the UNOBSERVED VARIABLES of the signal . the proposed algorithm is based on the use of DYNAMIC BAYESIAN NETWORKS to estimate the UNOBSERVED VARIABLES of the UNOBSERVED VARIABLES . the performance of the proposed algorithm is demonstrated on a variety of UNOBSERVED VARIABLES .\n",
            "\n",
            "592 1000\n",
            "as SPOKEN DIALOGUE SYSTEMS become deployed in increasingly complex domains , SPOKEN DIALOGUE SYSTEMS face rising demands on the NATURALNESS OF INTERACTION . we focus on SYSTEM RESPONSIVENESS , aiming to mimic HUMAN-LIKE DIALOGUE FLOW CONTROL by PREDICTING SPEAKER CHANGES as observed in REAL HUMAN-HUMAN CONVERSATIONS . we derive an INSTANTANEOUS VECTOR REPRESENTATION OF PITCH VARIATION and show that SPOKEN DIALOGUE SYSTEMS is amenable to standard ACOUSTIC MODELING TECHNIQUES . using a small amount of AUTOMATICALLY LABELED DATA , we train models which significantly outperform current state-of-the-art PAUSE-ONLY SYSTEMS , and replicate to within 1 % absolute the performance of our previously published HAND-CRAFTED BASELINE . the new system additionally offers scope for run-time control over the PRECISION or RECALL OF LOCATIONS at which to speak . \n",
            "this paper addresses the problem of PREDICTING SPEAKER CHANGES for SPOKEN DIALOGUE SYSTEMS . we propose a novel approach to HUMAN-LIKE DIALOGUE FLOW CONTROL based on ACOUSTIC MODELING TECHNIQUES . the proposed approach is based on the use of PAUSE-ONLY SYSTEMS to estimate the RECALL OF LOCATIONS and the RECALL OF LOCATIONS of the SPOKEN DIALOGUE SYSTEMS . the proposed approach is based on the use of PAUSE-ONLY SYSTEMS to estimate the RECALL OF LOCATIONS and RECALL OF LOCATIONS of the SPOKEN DIALOGUE SYSTEMS . the proposed approach is evaluated on a variety of REAL HUMAN-HUMAN CONVERSATIONS . the results show that the proposed approach is effective in PREDICTING SPEAKER CHANGES .\n",
            "\n",
            "593 1000\n",
            "model learning and TRACKING are two important topics in COMPUTER VISION . while there are many applications where one of them is used to support the other , there are currently only few where both aid each other simultaneously . in this work , we seek to incrementally learn a GRAPHICAL MODEL from TRACKING and to simultaneously use whatever has been learned to improve the TRACKING in the next frames . the main problem encountered in this situation is that the current GRAPHICAL MODEL may be inconsistent with future observations , creating a bias in the TRACKING results . we propose an uncertain model that explicitly accounts for such uncertainties by representing relations by an appropriately weighted sum of informative -lrb- parametric -rrb- and UNINFORMATIVE COMPONENTS . the method is completely unsupervised and operates in real time . \n",
            "this paper addresses the problem of MODEL LEARNING for TRACKING in COMPUTER VISION . in particular , we show that the GRAPHICAL MODEL can be applied to the problem of TRACKING in COMPUTER VISION . the proposed GRAPHICAL MODEL is based on the use of MODEL LEARNING in a GRAPHICAL MODEL . the proposed GRAPHICAL MODEL is applied to the problem of TRACKING in a GRAPHICAL MODEL .\n",
            "\n",
            "594 1000\n",
            "widespread use of efficient and successful solutions of COMPUTER VISION PROBLEMS based on PAIRWISE MARKOV RANDOM FIELD MODELS raises a question : does any link exist between the pairwise and higher order mrfs such that the like solutions can be applied to the latter models ? this work explores such a link for BINARY MRFS that allow us to represent GIBBS ENERGY OF SIGNAL INTERACTION with a POLYNOMIAL FUNCTION . we show how a higher ORDER POLYNOMIAL can be efficiently transformed into a QUADRATIC FUNCTION . then ENERGY MINIMIZATION TOOLS for the PAIRWISE MRF MODELS can be easily applied to the HIGHER ORDER COUNTERPARTS . also , we propose a method to analytically estimate the potential parameter of the ASYMMETRIC POTTS PRIOR . the proposed framework demonstrates very promising experimental results of IMAGE SEGMENTATION and can be used to solve other COMPUTER VISION PROBLEMS . \n",
            "in this paper , we propose a novel approach to IMAGE SEGMENTATION based on PAIRWISE MARKOV RANDOM FIELD MODELS . the proposed PAIRWISE MARKOV RANDOM FIELD MODELS is based on the use of BINARY MRFS for IMAGE SEGMENTATION . the proposed PAIRWISE MARKOV RANDOM FIELD MODELS is based on the use of BINARY MRFS for IMAGE SEGMENTATION . the proposed PAIRWISE MARKOV RANDOM FIELD MODELS is based on the use of BINARY MRFS for IMAGE SEGMENTATION . the proposed PAIRWISE MARKOV RANDOM FIELD MODELS is applied to the IMAGE SEGMENTATION of the IMAGE SEGMENTATION . the experimental results show that the proposed method is effective in improving the IMAGE SEGMENTATION performance .\n",
            "\n",
            "595 1000\n",
            "this paper proposes a novel approach that utilizes a MACHINE LEARNING METHOD to improve PIVOT-BASED STATISTICAL MACHINE TRANSLATION . for language pairs with few BILINGUAL DATA , a possible solution in PIVOT-BASED SMT using another language as a `` bridge '' to generate SOURCE-TARGET TRANSLATION . however , one of the weaknesses is that some useful SOURCE-TARGET TRANSLATIONS can not be generated if the corresponding source phrase and target phrase connect to different PIVOT PHRASES . to alleviate the problem , we utilize MARKOV RANDOM WALKS to connect possible translation phrases between source and target language . experimental results on EUROPEAN PARLIAMENT DATA , SPOKEN LANGUAGE DATA and WEB DATA show that our method leads to significant improvements on all the tasks over the baseline system . \n",
            "in this paper , we propose a novel MACHINE LEARNING METHOD for PIVOT-BASED STATISTICAL MACHINE TRANSLATION . the proposed MACHINE LEARNING METHOD is based on a MACHINE LEARNING METHOD for SOURCE-TARGET TRANSLATION and WEB DATA . the proposed MACHINE LEARNING METHOD is based on a novel MACHINE LEARNING METHOD , which can be applied to SPOKEN LANGUAGE DATA and WEB DATA . the proposed approach is based on the use of MARKOV RANDOM WALKS and MARKOV RANDOM WALKS . the proposed approach is evaluated on a variety of WEB DATA and WEB DATA . the experimental results show that the proposed method outperforms the state-of-the-art methods in terms of both SOURCE-TARGET TRANSLATION and WEB DATA .\n",
            "\n",
            "596 1000\n",
            "plant traits are a key to understanding and predicting the ADAPTATION OF ECOSYSTEMS to environmental changes , which motivates the try project aiming at constructing a GLOBAL DATABASE for PLANT TRAITS and becoming a standard resource for the ECOLOGICAL COMMUNITY . despite its unprecedented coverage , a large percentage of MISSING DATA substantially constrains JOINT TRAIT ANALYSIS . meanwhile , the TRAIT DATA is characterized by the HIERARCHICAL PHYLOGENETIC STRUCTURE of the plant <unk> . while FACTORIZATION BASED MATRIX COMPLETION TECHNIQUES have been widely used to address the MISSING DATA PROBLEM , traditional MATRIX FACTORIZATION METHODS are unable to leverage the PHYLOGENETIC STRUCTURE . we propose HIERARCHICAL PROBABILISTIC MATRIX FACTORIZATION , which effectively uses HIERARCHICAL PHYLOGENETIC INFORMATION for TRAIT PREDICTION . we demonstrate HIERARCHICAL PROBABILISTIC MATRIX FACTORIZATION 's high accuracy , effectiveness of incorporating HIERARCHICAL STRUCTURE and ability to capture TRAIT CORRELATION through experiments . \n",
            "in this paper , we propose a novel approach to JOINT TRAIT ANALYSIS based on HIERARCHICAL PROBABILISTIC MATRIX FACTORIZATION for JOINT TRAIT ANALYSIS . the proposed method is based on the use of HIERARCHICAL PROBABILISTIC MATRIX FACTORIZATION for TRAIT PREDICTION . the proposed method is based on the use of HIERARCHICAL PROBABILISTIC MATRIX FACTORIZATION for TRAIT PREDICTION . the proposed method is based on the use of HIERARCHICAL PROBABILISTIC MATRIX FACTORIZATION for TRAIT PREDICTION . the proposed method is based on the use of HIERARCHICAL PROBABILISTIC MATRIX FACTORIZATION for TRAIT PREDICTION . the proposed method is based on the use of HIERARCHICAL PROBABILISTIC MATRIX FACTORIZATION for TRAIT PREDICTION . the experimental results show that the proposed method outperforms the state-of-the-art methods in terms of TRAIT CORRELATION and MISSING DATA .\n",
            "\n",
            "597 1000\n",
            "in this paper , we present a VISION SYSTEM for OBJECT RECOGNITION in AERIAL IMAGES , which enables broader mission profiles for MICRO AIR VEHICLES . the most important factors that inform our DESIGN CHOICES are : REAL-TIME CONSTRAINTS , ROBUSTNESS to VIDEO NOISE , and COMPLEXITY OF OBJECT APPEARANCES . as such , we first propose the HSI COLOR SPACE and the COMPLEX WAVELET TRANSFORM as a set of sufficiently DISCRIMINATING FEATURES . for each FEATURE , we then build TREE-STRUCTURED BELIEF NETWORKS as our underlying STATISTICAL MODELS OF OBJECT APPEARANCES . to perform OBJECT RECOGNITION , we develop the novel MULTISCALE VITERBI CLASSIFICATION ALGORITHM , as an improvement to MULTI-SCALE BAYESIAN CLASSIFICATION . next , we show how to globally optimize TREE-STRUCTURED BELIEF NETWORKS with respect to the FEATURE SET , using an ADAPTIVE FEATURE SELECTION ALGORITHM . finally , we discuss CONTEXT-BASED OBJECT RECOGNITION , where VISUAL CONTEXTS help to disambiguate the identity of an object despite the RELATIVE POVERTY OF SCENE DETAIL in FLIGHT IMAGES , and <unk> the need for an exhaustive search of objects over various scales and locations in the image . experimental results show that the proposed VISION SYSTEM achieves smaller classification error and fewer false positives than systems using the MSBC PARADIGM on challenging REAL-WORLD TEST IMAGES . \n",
            "in this paper , we propose a novel ADAPTIVE FEATURE SELECTION ALGORITHM for OBJECT RECOGNITION . the proposed ADAPTIVE FEATURE SELECTION ALGORITHM is based on a MULTISCALE VITERBI CLASSIFICATION ALGORITHM and a COMPLEX WAVELET TRANSFORM . the proposed ADAPTIVE FEATURE SELECTION ALGORITHM is based on a MULTISCALE VITERBI CLASSIFICATION ALGORITHM and a COMPLEX WAVELET TRANSFORM . the proposed ADAPTIVE FEATURE SELECTION ALGORITHM is based on a MULTISCALE VITERBI CLASSIFICATION ALGORITHM and a COMPLEX WAVELET TRANSFORM . the proposed ADAPTIVE FEATURE SELECTION ALGORITHM is based on a MULTISCALE VITERBI CLASSIFICATION ALGORITHM and a COMPLEX WAVELET TRANSFORM . the ROBUSTNESS and ROBUSTNESS of the proposed MSBC PARADIGM is demonstrated on REAL-WORLD TEST IMAGES . the results show that the proposed ADAPTIVE FEATURE SELECTION ALGORITHM is robust to VIDEO NOISE , VIDEO NOISE , VIDEO NOISE , and VIDEO NOISE .\n",
            "\n",
            "598 1000\n",
            "this paper presents a new method for MUSICAL CHORD RECOGNITION based on a MODEL OF HUMAN PERCEPTION . we classify the chords directly from the sound without the INFORMATION OF TIMBRES and notes . a WAVELET-BASED TRANSFORM as well as a SELF-ORGANIZED MAP NEURAL NETWORK is adopted to imitate HUMAN EARS and <unk> , respectively . the resultant system can classify chords very well even in a noisy environment . \n",
            "this paper presents a novel approach to the MODEL OF HUMAN PERCEPTION . the proposed approach is based on a SELF-ORGANIZED MAP NEURAL NETWORK , which is based on a SELF-ORGANIZED MAP NEURAL NETWORK . the proposed method is based on a SELF-ORGANIZED MAP NEURAL NETWORK , which is based on a SELF-ORGANIZED MAP NEURAL NETWORK . the proposed method is based on a SELF-ORGANIZED MAP NEURAL NETWORK , which is based on a SELF-ORGANIZED MAP NEURAL NETWORK . the experimental results demonstrate the effectiveness of the proposed method .\n",
            "\n",
            "599 1000\n",
            "human speakers plan and deliver their utterances incremen-tally , <unk> , and it is obvious that their choice regarding PHONETIC DETAILS -lrb- and the details ' <unk> -rrb- is rarely determined by globally optimal solutions . in contrast , PARA-METRIC SPEECH SYNTHESIZERS use a FULL-UTTERANCE CONTEXT when optimizing VOCODING PARAMETERS and when DETERMING HMM STATES . apart from being cognitively implausible , this impedes INCREMENTAL USE-CASES , where the future context is often at least partially unavailable . this paper investigates the ` locality ' of FEATURES in PARAMETRIC SPEECH SYNTHESIS VOICES and takes some missing steps towards better HMM STATE SELECTION and PROSODY MODELLING for INCREMENTAL SPEECH SYNTHESIS . \n",
            "in this paper , we propose a novel approach to PROSODY MODELLING based on HMM STATE SELECTION and PROSODY MODELLING . the proposed approach is based on the use of DETERMING HMM STATES and PROSODY MODELLING . the proposed approach is based on the use of DETERMING HMM STATES and PROSODY MODELLING . the proposed method is based on the use of DETERMING HMM STATES and HMM STATE SELECTION to estimate the PHONETIC DETAILS . the proposed approach is evaluated on a variety of PARAMETRIC SPEECH SYNTHESIS VOICES and PARA-METRIC SPEECH SYNTHESIZERS . the experimental results show that the proposed method outperforms the state-of-the-art methods .\n",
            "\n",
            "600 1000\n",
            "burst detection is an important topic in TEMPORAL STREAM ANALYSIS . usually , only the TEXTUAL FEATURES are used in BURST DETECTION . in the theme extraction from current prevailing SOCIAL MEDIA CONTENT , it is necessary to consider not only TEXTUAL FEATURES but also the PERVASIVE COLLABORATIVE CONTEXT , e.g. , RESOURCE LIFETIME and user activity . this paper explores novel approaches to combine multiple sources of such indication for better BURST EXTRACTION . we systematically investigate the characters of COLLABORATIVE CONTEXT , i.e. , METADATA FREQUENCY , TOPIC COVERAGE and USER ATTRACTIVENESS . first , a ROBUST STATE BASED MODEL is utilized to detect bursts from individual streams . we then propose a LEARNING METHOD to combine these BURST PULSES . experiments on a large real dataset demonstrate the remarkable improvements over the traditional methods . \n",
            "this paper presents a novel approach to BURST EXTRACTION in SOCIAL MEDIA CONTENT . the proposed approach is based on the use of TEXTUAL FEATURES , TOPIC COVERAGE , and RESOURCE LIFETIME . the proposed approach is based on the use of TEXTUAL FEATURES , TOPIC COVERAGE , and COLLABORATIVE CONTEXT . the proposed approach is based on the use of TEXTUAL FEATURES , TOPIC COVERAGE , and RESOURCE LIFETIME . the proposed approach is evaluated on a variety of TEXTUAL FEATURES , including BURST DETECTION , TOPIC COVERAGE , and RESOURCE LIFETIME .\n",
            "\n",
            "601 1000\n",
            "we have trained networks of E-II UNITS with SHORT-RANGE CONNECTIONS to simulate simple CELLULAR AUTOMATA that exhibit COMPLEX OR CHAOTIC BEHAVIOUR . three levels of LEARNING are possible -lrb- in decreasing order of difficulty -rrb- : LEARNING the underlying AUTOMATON RULE , learning ASYMPTOTIC DYNAMICAL BEHAVIOUR , and LEARNING to extrapolate the training history . the levels of LEARNING achieved with and without WEIGHT SHARING for different automata provide new insight into their dynamics . \n",
            "this paper addresses the problem of LEARNING in CELLULAR AUTOMATA . we propose a novel approach to the problem of LEARNING . the proposed approach is based on the use of E-II UNITS with SHORT-RANGE CONNECTIONS . the proposed approach is based on the idea of WEIGHT SHARING in the AUTOMATON RULE . the proposed approach is based on the use of E-II UNITS with SHORT-RANGE CONNECTIONS . the proposed method is applied to the problem of LEARNING . the experimental results show that the proposed method outperforms the existing methods in terms of the quality of the E-II UNITS .\n",
            "\n",
            "602 1000\n",
            "nearest neighbor -lrb- nn -rrb- classification relies on the assumption that CLASS CONDITIONAL PROBABILITIES are locally constant . this assumption becomes false in high dimensions with FINITE SAMPLES due to the CURSE OF DIMENSIONALITY . the NEAREST NEIGHBOR CLASSIFICATION introduces SEVERE BIAS under these conditions . we propose a LOCALLY ADAPTIVE NEIGHBORHOOD MORPHING CLASSIFICATION METHOD to try to minimize BIAS . we use LOCAL SUPPORT VECTOR MACHINE LEARNING to estimate an effective metric for producing neighborhoods that are <unk> along less DISCRIMINANT FEATURE DIMENSIONS and <unk> along most discriminant ones . as a result , the CLASS CONDITIONAL PROBABILITIES can be expected to be approximately constant in the modified neighborhoods , whereby better classification performance can be achieved . the efficacy of our LOCALLY ADAPTIVE NEIGHBORHOOD MORPHING CLASSIFICATION METHOD is validated and compared against other competing techniques using a number of datasets . \n",
            "in this paper , we propose a novel LOCALLY ADAPTIVE NEIGHBORHOOD MORPHING CLASSIFICATION METHOD for NEAREST NEIGHBOR CLASSIFICATION . the proposed LOCALLY ADAPTIVE NEIGHBORHOOD MORPHING CLASSIFICATION METHOD is based on the idea of NEAREST NEIGHBOR CLASSIFICATION , which is based on a LOCALLY ADAPTIVE NEIGHBORHOOD MORPHING CLASSIFICATION METHOD . the proposed LOCALLY ADAPTIVE NEIGHBORHOOD MORPHING CLASSIFICATION METHOD is based on a novel LOCALLY ADAPTIVE NEIGHBORHOOD MORPHING CLASSIFICATION METHOD , which is based on a LOCALLY ADAPTIVE NEIGHBORHOOD MORPHING CLASSIFICATION METHOD . the proposed LOCALLY ADAPTIVE NEIGHBORHOOD MORPHING CLASSIFICATION METHOD is applied to the problem of NEAREST NEIGHBOR CLASSIFICATION . the proposed LOCALLY ADAPTIVE NEIGHBORHOOD MORPHING CLASSIFICATION METHOD is applied to the problem of NEAREST NEIGHBOR CLASSIFICATION , and the results show that the proposed LOCALLY ADAPTIVE NEIGHBORHOOD MORPHING CLASSIFICATION METHOD is effective in NEAREST NEIGHBOR CLASSIFICATION .\n",
            "\n",
            "603 1000\n",
            "deconvolution problems are encountered in SIGNAL PROCESSING APPLICATIONS where an UNKNOWN INPUT SIGNAL can only be observed after propagation through one or more NOISE CORRUPTED FIR CHANNELS . the first step in recovering the input usually entails an estimation of the FIR CHANNELS through training based or BLIND ALGORITHMS . the ` standard ' procedure then uses LEAST SQUARES ESTIMATION to recover the input . a RECURSIVE IMPLEMENTATION with CONSTANT COMPUTATIONAL COST is based on the KALMAN FILTER . in this paper we focus on a TOTAL LEAST SQUARES BASED APPROACH , which is more appropriate if errors are expected both on the output samples and the estimates of the FIR CHANNELS . we will develop a RE-CURSIVE TOTAL LEAST SQUARES ALGORITHM which closely approximates the performance of the NON-RECURSIVE TLS ALGORITHM and this at a much lower COMPUTATIONAL COST . \n",
            "this paper addresses the problem of DECONVOLUTION PROBLEMS in NOISE CORRUPTED FIR CHANNELS . we propose a novel approach to LEAST SQUARES ESTIMATION based on LEAST SQUARES ESTIMATION . the proposed approach is based on the RE-CURSIVE TOTAL LEAST SQUARES ALGORITHM , which is a RECURSIVE IMPLEMENTATION of the RE-CURSIVE TOTAL LEAST SQUARES ALGORITHM . the proposed method is based on the RE-CURSIVE TOTAL LEAST SQUARES ALGORITHM , which is a RECURSIVE IMPLEMENTATION . the proposed RE-CURSIVE TOTAL LEAST SQUARES ALGORITHM is based on a KALMAN FILTER , which is a RECURSIVE IMPLEMENTATION . the proposed RE-CURSIVE TOTAL LEAST SQUARES ALGORITHM is applied to the problem of DECONVOLUTION PROBLEMS in FIR CHANNELS . the proposed method is evaluated on a variety of NOISE CORRUPTED FIR CHANNELS . the results show that the proposed method is effective in improving the COMPUTATIONAL COST and the COMPUTATIONAL COST of the proposed method .\n",
            "\n",
            "604 1000\n",
            "an efficient realization of a LOW DELAY FILTER-BANK , termed as GENERALIZED FILTER-BANK EQUALIZER , will be proposed for NOISE REDUCTION with LOW SIGNAL DELAY . the GENERALIZED FILTER-BANK EQUALIZER is equivalent to a TIME-DOMAIN FILTER with COEFFICIENTS adapted in the FREQUENCY-DOMAIN . this FILTER-BANK STRUCTURE ensures perfect SIGNAL RECONSTRUCTION for a variety of spectral transforms with less restrictions than for an ANALYSIS-SYNTHESIS FILTER-BANK -LRB- AS FB -RRB- . a NON-UNIFORM FREQUENCY RESOLUTION can be achieved by an ALLPASS TRANSFORMATION . in this case , the GENERALIZED FILTER-BANK EQUALIZER has not only a lower signal delay than the GENERALIZED FILTER-BANK EQUALIZER , but also a lower ALGORITHMIC COMPLEXITY for most PARAMETER CONFIGURATIONS . another advantage of the GENERALIZED FILTER-BANK EQUALIZER is the lower number of required delay elements -lrb- memory -rrb- compared to the GENERALIZED FILTER-BANK EQUALIZER . the NOISE REDUCTION achieved by means of the GENERALIZED FILTER-BANK EQUALIZER and the GENERALIZED FILTER-BANK EQUALIZER is approximately equal . \n",
            "in this paper , we propose a novel GENERALIZED FILTER-BANK EQUALIZER for SIGNAL RECONSTRUCTION . the proposed GENERALIZED FILTER-BANK EQUALIZER is based on a GENERALIZED FILTER-BANK EQUALIZER , a GENERALIZED FILTER-BANK EQUALIZER , a GENERALIZED FILTER-BANK EQUALIZER , and a GENERALIZED FILTER-BANK EQUALIZER . the proposed GENERALIZED FILTER-BANK EQUALIZER is based on a GENERALIZED FILTER-BANK EQUALIZER and a GENERALIZED FILTER-BANK EQUALIZER , a GENERALIZED FILTER-BANK EQUALIZER and a GENERALIZED FILTER-BANK EQUALIZER . the ALGORITHMIC COMPLEXITY of the proposed GENERALIZED FILTER-BANK EQUALIZER is compared to a GENERALIZED FILTER-BANK EQUALIZER and the GENERALIZED FILTER-BANK EQUALIZER . the performance of the proposed GENERALIZED FILTER-BANK EQUALIZER is compared to the standard GENERALIZED FILTER-BANK EQUALIZER and the GENERALIZED FILTER-BANK EQUALIZER . the performance of the proposed GENERALIZED FILTER-BANK EQUALIZER is evaluated in terms of the ALGORITHMIC COMPLEXITY and the ALGORITHMIC COMPLEXITY of the proposed GENERALIZED FILTER-BANK EQUALIZER .\n",
            "\n",
            "605 1000\n",
            "a CLASSIFICATION METHOD is presented that detects the presence of speech embedded in a REAL ACOUSTIC BACKGROUND of NON-SPEECH SOUNDS . FEATURES used for CLASSIFICATION are MODULATION COMPONENTS extracted by computation of the AMPLITUDE MODULATION SPECTROGRAM . FEATURE SELECTION TECHNIQUES and support vector CLASSIFICATION are employed to identify MODULATION COMPONENTS most salient for the CLASSIFICATION TASK and therefore considered as highly characteristic for speech . results show that reliable DETECTION OF SPEECH can be performed with less than 10 optimally selected MODULATION FEATURES , the most important ones are located in the MODULATION FREQUENCY RANGE below 10 hz . DETECTION OF SPEECH in a background of NON-SPEECH SIGNALS is performed with about 90 % TEST-DATA ACCURACY at a SIGNAL-TO-NOISE LEVEL of 0 db . compared to standard ITU G729.B VOICE ACTIVITY DETECTION , the proposed CLASSIFICATION METHOD results in increased true positive and reduced FALSE POSITIVE RATES induced by a REAL ACOUSTIC BACKGROUND . \n",
            "in this paper , we propose a novel CLASSIFICATION METHOD based on MODULATION FEATURES extracted from the AMPLITUDE MODULATION SPECTROGRAM . the proposed CLASSIFICATION METHOD is based on the use of FEATURES extracted from the AMPLITUDE MODULATION SPECTROGRAM . the proposed CLASSIFICATION METHOD is based on the use of MODULATION FEATURES extracted from the AMPLITUDE MODULATION SPECTROGRAM . the proposed CLASSIFICATION METHOD is evaluated on ITU G729.B VOICE ACTIVITY DETECTION and compared to the state-of-the-art methods . the proposed CLASSIFICATION METHOD is evaluated on both ITU G729.B VOICE ACTIVITY DETECTION and ITU G729.B VOICE ACTIVITY DETECTION . the experimental results show that the proposed CLASSIFICATION METHOD achieves higher FALSE POSITIVE RATES in terms of TEST-DATA ACCURACY and FALSE POSITIVE RATES .\n",
            "\n",
            "606 1000\n",
            "recently , significant progress has been made on learning STRUCTURED PREDICTORS via COORDINATED TRAINING ALGORITHMS such as CONDITIONAL RANDOM FIELDS and MAXIMUM MARGIN MARKOV NETWORKS . unfortunately , these techniques are based on SPECIALIZED TRAINING ALGORITHMS , are complex to implement , and expensive to run . we present a much simpler approach to training STRUCTURED PREDICTORS by applying a BOOSTING-LIKE PROCEDURE to standard SUPERVISED TRAINING METHODS . the idea is to learn a LOCAL PREDICTOR using standard methods , such as LOGISTIC REGRESSION or SUPPORT VECTOR MACHINES , but then achieve improved STRUCTURED CLASSIFICATION by '' boosting '' the influence of MISCLASSIFIED COMPONENTS after STRUCTURED CLASSIFICATION , retraining the LOCAL PREDICTOR , and repeating . further improvement in STRUCTURED PREDICTION ACCURACY can be achieved by incorporating '' dynamic '' FEATURES -- i.e. an extension whereby the FEATURES for one predicted component can depend on the predictions already made for some other components . we apply our techniques to the problem of learning DEPENDENCY PARSERS from ANNOTATED NATURAL LANGUAGE CORPORA . by using LOGISTIC REGRESSION as an efficient BASE CLASSIFIER -lrb- for PREDICTING DEPENDENCY LINKS between word pairs -rrb- , we are able to efficiently train a DEPENDENCY PARSING MODEL , via STRUCTURED BOOSTING , that achieves state of the art results in en-glish , and surpasses state of the art in CHINESE . \n",
            "this paper proposes a novel DEPENDENCY PARSING MODEL based on STRUCTURED BOOSTING and MAXIMUM MARGIN MARKOV NETWORKS . the proposed DEPENDENCY PARSING MODEL is based on the use of STRUCTURED BOOSTING and MAXIMUM MARGIN MARKOV NETWORKS . the proposed DEPENDENCY PARSING MODEL is based on the use of LOGISTIC REGRESSION and MAXIMUM MARGIN MARKOV NETWORKS . the proposed DEPENDENCY PARSING MODEL is based on the use of LOGISTIC REGRESSION and MAXIMUM MARGIN MARKOV NETWORKS . the proposed DEPENDENCY PARSING MODEL is based on the use of LOGISTIC REGRESSION and MAXIMUM MARGIN MARKOV NETWORKS . the proposed DEPENDENCY PARSING MODEL is applied to CHINESE , and the experimental results show that the proposed DEPENDENCY PARSING MODEL is effective in improving the STRUCTURED PREDICTION ACCURACY of the BASE CLASSIFIER . the proposed DEPENDENCY PARSING MODEL is compared with other SUPERVISED TRAINING METHODS , such as SUPPORT VECTOR MACHINES and MAXIMUM MARGIN MARKOV NETWORKS .\n",
            "\n",
            "607 1000\n",
            "we present a new SAMPLING APPROACH to bayesian learning of the BAYESIAN NETWORK STRUCTURE . like some earlier SAMPLING METHODS , we sample LINEAR ORDERS on NODES rather than directed acyclic graphs -lrb- <unk> -rrb- . the key difference is that we replace the usual MARKOV CHAIN MONTE CARLO METHOD by the SAMPLING APPROACH of ANNEALED IMPORTANCE SAMPLING . we show that MARKOV CHAIN MONTE CARLO METHOD is not only competitive to MARKOV CHAIN MONTE CARLO METHOD in exploring the POSTERIOR , but also superior to MARKOV CHAIN MONTE CARLO METHOD in two ways : MARKOV CHAIN MONTE CARLO METHOD enables easy and efficient PARALLELIZATION , due to the independence of the samples , and <unk> of the marginal likelihood of the SAMPLING APPROACH with good PROBABILISTIC GUARANTEES . we also provide a principled way to correct the BIAS due to ORDER-BASED SAMPLING , by implementing a fast algorithm for counting the linear extensions of a given PARTIAL ORDER . \n",
            "in this paper , we propose a novel MARKOV CHAIN MONTE CARLO METHOD to the problem of PARALLELIZATION . the proposed MARKOV CHAIN MONTE CARLO METHOD is based on the idea that the POSTERIOR of the NODES can be approximated by a MARKOV CHAIN MONTE CARLO METHOD . the proposed MARKOV CHAIN MONTE CARLO METHOD is based on a MARKOV CHAIN MONTE CARLO METHOD , which is a generalization of the MARKOV CHAIN MONTE CARLO METHOD to the POSTERIOR . the proposed MARKOV CHAIN MONTE CARLO METHOD is based on a MARKOV CHAIN MONTE CARLO METHOD , which is a generalization of the standard MARKOV CHAIN MONTE CARLO METHOD . the proposed MARKOV CHAIN MONTE CARLO METHOD is compared to the MARKOV CHAIN MONTE CARLO METHOD and the MARKOV CHAIN MONTE CARLO METHOD . the performance of the proposed MARKOV CHAIN MONTE CARLO METHOD is compared to the standard MARKOV CHAIN MONTE CARLO METHOD . the proposed MARKOV CHAIN MONTE CARLO METHOD is compared to the standard MARKOV CHAIN MONTE CARLO METHOD , and the proposed MARKOV CHAIN MONTE CARLO METHOD is more robust to PARALLELIZATION .\n",
            "\n",
            "608 1000\n",
            "in the past researches , several kinds of information have been explored to assess the CONFIDENCE MEASURE or to select the CONFIDENCE TAG for a WORD/PHRASE . however , the CONTEXTUAL CONFIDENCE INFORMATION is little <unk> . in this paper , we propose a CONCEPT-BASED PROBABILISTIC VERIFICATION MODEL to integrate the CONTEXTUAL CONFIDENCE INFORMATION . in this CONCEPT-BASED PROBABILISTIC VERIFICATION MODEL , a concept is verified not only according to its ACOUSTIC CONFIDENCE MEASURE but also according to NEIGHBORING CONCEPTS and their confidence levels . experimental results show that the proposed CONCEPT-BASED PROBABILISTIC VERIFICATION MODEL significantly outperforms the CONCEPT-BASED PROBABILISTIC VERIFICATION MODEL using only CONFIDENCE MEASURES . the ERROR RATE of CONFIDENCE TAG is reduced from 17.7 % to <unk> % , which corresponds to an ERROR REDUCTION RATE of 14.5 % . \n",
            "in this paper , we propose a novel CONCEPT-BASED PROBABILISTIC VERIFICATION MODEL based on a CONCEPT-BASED PROBABILISTIC VERIFICATION MODEL . the proposed CONCEPT-BASED PROBABILISTIC VERIFICATION MODEL is based on the use of NEIGHBORING CONCEPTS to estimate the CONFIDENCE TAG of the target signal . the proposed CONCEPT-BASED PROBABILISTIC VERIFICATION MODEL is based on the use of NEIGHBORING CONCEPTS to estimate the CONTEXTUAL CONFIDENCE INFORMATION . the proposed CONCEPT-BASED PROBABILISTIC VERIFICATION MODEL is evaluated in terms of the ERROR RATE and the ERROR RATE of the proposed CONCEPT-BASED PROBABILISTIC VERIFICATION MODEL . the experimental results show that the proposed CONCEPT-BASED PROBABILISTIC VERIFICATION MODEL significantly improves the performance of the proposed CONCEPT-BASED PROBABILISTIC VERIFICATION MODEL .\n",
            "\n",
            "609 1000\n",
            "the paper extends the notion of LINEAR PROGRAMMING BOOSTING to handle UNEVEN DATASETS . extensive experiments with TEXT CLASSIFICATION PROBLEM compare the performance of a number of different BOOSTING STRATEGIES , concentrating on the problems posed by UNEVEN DATASETS . \n",
            "this paper addresses the problem of learning a TEXT CLASSIFICATION PROBLEM from a set of UNEVEN DATASETS . in particular , we consider the problem of LINEAR PROGRAMMING BOOSTING , where the goal is to minimize the total number of sources . we propose a method to solve this problem . our algorithm is based on the use of LINEAR PROGRAMMING BOOSTING to solve the problem of TEXT CLASSIFICATION PROBLEM . we demonstrate the effectiveness of our method on a variety of UNEVEN DATASETS , and show that our algorithm is able to solve the problem of TEXT CLASSIFICATION PROBLEM .\n",
            "\n",
            "610 1000\n",
            "lexical co-occurrence is an important cue for DETECTING WORD ASSOCIATIONS . we present a theoretical framework for discovering statistically significant LEXICAL CO-OCCURRENCES from a given corpus . in contrast with the prevalent practice of giving <unk> to UNIGRAM FREQUENCIES , we focus only on the documents containing both the terms -lrb- of a candidate <unk> -rrb- . we detect BIASES in SPAN DISTRIBUTIONS OF ASSOCIATED WORDS , while being agnostic to variations in GLOBAL UNIGRAM FREQUENCIES . our framework has the fidelity to distinguish different classes of LEXICAL CO-OCCURRENCES , based on strengths of the document and <unk> cues of co-occurrence in the data . we perform extensive experiments on BENCHMARK DATA SETS to study the performance of various CO-OCCURRENCE MEASURES that are currently known in literature . we find that a relatively obscure measure called OCHIAI , and a newly introduced measure csa capture the notion of LEXICAL CO-OCCURRENCE best , followed next by <unk> , <unk> , and TTEST , while another popular measure , <unk> , <unk> , performs poorly in the context of LEXICAL CO-OCCURRENCE . \n",
            "in this paper , we propose a novel method for DETECTING WORD ASSOCIATIONS . the proposed method is based on the use of LEXICAL CO-OCCURRENCES to represent the SPAN DISTRIBUTIONS OF ASSOCIATED WORDS of the LEXICAL CO-OCCURRENCES . the proposed method is based on the idea of LEXICAL CO-OCCURRENCE , which is a TTEST of the SPAN DISTRIBUTIONS OF ASSOCIATED WORDS . the proposed method is based on the use of LEXICAL CO-OCCURRENCES , which are used to estimate the SPAN DISTRIBUTIONS OF ASSOCIATED WORDS of the LEXICAL CO-OCCURRENCES . the experimental results on BENCHMARK DATA SETS show that the proposed method is effective in improving the performance of CO-OCCURRENCE MEASURES in DETECTING WORD ASSOCIATIONS .\n",
            "\n",
            "611 1000\n",
            "in this paper , a SESSION VARIABILITY SUBSPACE PROJECTION SVSPBASED MODEL COMPENSATION METHOD for SPEAKER VERIFICATION is proposed . during the training phase the SESSION VARIABILITY is removed from SPEAKER MODELS by projection , while during the testing phase the SESSION VARIABILITY in a test utterance is used to compensate SPEAKER MODELS . finally , the COMPENSATED SPEAKER MODELS and UBM are used to recognize the identity of the test utterance . compared with the conventional GMM-UBM SYSTEM , the RELATIVE EQUAL ERROR RATE REDUCTION of SESSION VARIABILITY SUBSPACE PROJECTION SVSPBASED MODEL COMPENSATION METHOD is <unk> % on the nist 2006 <unk> one conversation training , <unk> one conversation test . \n",
            "in this paper , we propose a novel SESSION VARIABILITY SUBSPACE PROJECTION SVSPBASED MODEL COMPENSATION METHOD for SPEAKER VERIFICATION . the proposed SESSION VARIABILITY SUBSPACE PROJECTION SVSPBASED MODEL COMPENSATION METHOD is based on the use of a UBM , UBM , and UBM . the proposed SESSION VARIABILITY SUBSPACE PROJECTION SVSPBASED MODEL COMPENSATION METHOD is based on the use of COMPENSATED SPEAKER MODELS and UBM . the proposed SESSION VARIABILITY SUBSPACE PROJECTION SVSPBASED MODEL COMPENSATION METHOD is evaluated on the GMM-UBM SYSTEM and compared with the conventional GMM-UBM SYSTEM . the experimental results show that the proposed SESSION VARIABILITY SUBSPACE PROJECTION SVSPBASED MODEL COMPENSATION METHOD is effective in improving the RELATIVE EQUAL ERROR RATE REDUCTION of the proposed SESSION VARIABILITY SUBSPACE PROJECTION SVSPBASED MODEL COMPENSATION METHOD .\n",
            "\n",
            "612 1000\n",
            "3d object reconstruction from a single 2D LINE DRAWING is an important problem in both COMPUTER VISION and graphics . many methods have been put forward to solve this problem , but they usually fail when the GEOMETRIC STRUCTURE of a 3D OBJECT becomes complex . in this paper , a novel approach based on a DIVIDE-AND-CONQUER STRATEGY is proposed to handle 3D RECONSTRUCTION OF COMPLEX MANIFOLD OBJECTS from SINGLE 2D LINE DRAWINGS . the approach consists of three steps : 1 -rrb- dividing a complex LINE DRAWING into multiple simpler line drawings based on the result of FACE IDENTIFICATION ; 2 -rrb- reconstructing the 3D SHAPES from these simpler line drawings ; and 3 -rrb- merging the 3D SHAPES into one complete object represented by the original LINE DRAWING . a number of examples are given to show that our approach can handle 3D RECONSTRUCTION OF MORE COMPLEX OBJECTS than previous methods . \n",
            "this paper presents a novel approach to 3D RECONSTRUCTION OF COMPLEX MANIFOLD OBJECTS from SINGLE 2D LINE DRAWINGS . the proposed approach is based on a DIVIDE-AND-CONQUER STRATEGY for 3D RECONSTRUCTION OF COMPLEX MANIFOLD OBJECTS from a 2D LINE DRAWING . the proposed method is based on the use of a DIVIDE-AND-CONQUER STRATEGY for 3D RECONSTRUCTION OF COMPLEX MANIFOLD OBJECTS . the proposed method is based on a DIVIDE-AND-CONQUER STRATEGY that exploits the GEOMETRIC STRUCTURE between the 3D SHAPES and the 3D SHAPES . the proposed approach is evaluated on a variety of SINGLE 2D LINE DRAWINGS . the results show that the proposed method is able to detect and track moving objects in a scene from a single image .\n",
            "\n",
            "613 1000\n",
            "<unk> of -lrb- . he key questions to be addressed in t , <unk> paper is how 1.0 estimate the DETERMINISTIC INPUT MULTIPULSE TIME SERIES from a noisy <unk> of the RESONANT TRANSFER SYSTEM iii -lrb- . lie case where <unk> SNR is low <unk> the system q -lrb- quality <unk> -rrb- of IHE TRANSFER SYSTEM is high . by <unk> the SHARP TRUUCABION employed in the standard SINGULAR-VALUE-DECOMPOSIT . ion -lrb- svd -rrb- , a TAPERING WINDOW is <unk> to t <unk> high order <unk> singular values obtained by <unk> svd , t , lie <unk> : l squared error of the <unk> due to the st , ANDARD SVD-BASED ESTIMATOR is reduced to a <unk> . this paper also <unk> a UEW METHOD to design AU OPTIMNM TAPERING WINDOW for ESTIMATIUG MULTIPULSE TIME SERIES . \n",
            "in this paper , we propose a novel IHE TRANSFER SYSTEM for DETERMINISTIC INPUT MULTIPULSE TIME SERIES . the proposed AU OPTIMNM TAPERING WINDOW is based on a UEW METHOD , which is based on a AU OPTIMNM TAPERING WINDOW . the proposed AU OPTIMNM TAPERING WINDOW is based on a ANDARD SVD-BASED ESTIMATOR , which is based on a AU OPTIMNM TAPERING WINDOW . the proposed AU OPTIMNM TAPERING WINDOW is based on a UEW METHOD , which is based on a UEW METHOD . the proposed AU OPTIMNM TAPERING WINDOW is applied to the DETERMINISTIC INPUT MULTIPULSE TIME SERIES of the RESONANT TRANSFER SYSTEM . the performance of the proposed IHE TRANSFER SYSTEM is evaluated on a ESTIMATIUG MULTIPULSE TIME SERIES . the experimental results show that the proposed IHE TRANSFER SYSTEM is effective in improving the SNR of the RESONANT TRANSFER SYSTEM .\n",
            "\n",
            "614 1000\n",
            "motion information is essential in many COMPUTER VISION and VIDEO ANALYSIS TASKS . since MPEG is still one of the most prevalent formats for representing , transferring and storing video data , the analysis of its motion field is important for real time video indexing and segmentation , EVENT ANALYSIS and SURVEILLANCE APPLICATIONS . our work considers the problem of improving the OPTICAL FLOW FIELD in MPEG SEQUENCES . we address the issues of robust , incremental , dense optical flow estimation by combining information from two different VELOCITY FIELDS : the available MPEG MOTION FIELD and the one inferred by a MULTIRESOLUTION ROBUST REGULARIZATION TECHNIQUE applied on the DC COEFFICIENTS . thus , the MULTIRESOLUTION ROBUST REGULARIZATION TECHNIQUE is based only on information that is directly available in the COMPRESSED STREAM avoiding therefore the time and MEMORY CONSUMING DECOMPRESSION . we extend standard techniques by adding a TEMPORAL CONTINUITY and an MPEG CONSISTENCY CONSTRAINT , both as MATHEMATICAL CONSTRAINTS in the OBJECTIVE FUNCTION and as hypothesis tests for the presence of MOTION DISCONTINUITIES . our approach is shown to perform well over a range of different MOTION SCENARIOS and can serve as a basis for efficient VIDEO ANALYSIS TASKS . \n",
            "this paper addresses the problem of EVENT ANALYSIS and VIDEO ANALYSIS TASKS . we propose a novel approach to the problem of EVENT ANALYSIS and VIDEO ANALYSIS TASKS . the proposed approach is based on a MULTIRESOLUTION ROBUST REGULARIZATION TECHNIQUE and a MULTIRESOLUTION ROBUST REGULARIZATION TECHNIQUE . the proposed approach is based on a MULTIRESOLUTION ROBUST REGULARIZATION TECHNIQUE and a MULTIRESOLUTION ROBUST REGULARIZATION TECHNIQUE . the proposed method consists of two steps : -lrb- 1 -rrb- a MULTIRESOLUTION ROBUST REGULARIZATION TECHNIQUE and a MULTIRESOLUTION ROBUST REGULARIZATION TECHNIQUE ; -lrb- 2 -rrb- a MULTIRESOLUTION ROBUST REGULARIZATION TECHNIQUE , which is able to deal with MOTION DISCONTINUITIES , and -lrb- 3 -rrb- a MULTIRESOLUTION ROBUST REGULARIZATION TECHNIQUE that exploits the MOTION INFORMATION and MATHEMATICAL CONSTRAINTS . the proposed method is evaluated on a variety of VIDEO ANALYSIS TASKS and SURVEILLANCE APPLICATIONS . the experimental results show that the proposed approach is effective in improving the performance of MPEG and SURVEILLANCE APPLICATIONS .\n",
            "\n",
            "615 1000\n",
            "we present a FRACTIONAL GABOR EXPANSION on a GENERAL , NON-RECTANGULAR TIME-FREQUENCY LATTICE . the traditional FRACTIONAL GABOR EXPANSION represents a signal in terms of time and frequency shifted BASIS FUNCTIONS , called GABOR LOGONS . this CONSTANT-BANDWIDTH ANALYSIS results in a fixed , rectangular time frequency plane <unk> . many of the practical signals require a more FLEXIBLE , NON-RECTANGULAR TIME-FREQUENCY LATTICE for a COMPACT REPRESENTATION . the proposed FRACTIONAL GABOR EXPANSION uses a set of BASIS FUNCTIONS that are related to the FRACTIONAL FOURIER BASIS and generate a NON-RECTANGULAR TILING . the completeness and BI-ORTHOGONALITY CONDITIONS of the new GABOR LOGONS are discussed . \n",
            "this paper addresses the problem of FRACTIONAL GABOR EXPANSION for FRACTIONAL GABOR EXPANSION . we propose a novel approach to the problem of FRACTIONAL GABOR EXPANSION , which is based on the idea of FRACTIONAL GABOR EXPANSION . the proposed approach is based on the use of BASIS FUNCTIONS for FRACTIONAL GABOR EXPANSION . the proposed approach is based on the use of BASIS FUNCTIONS in the FRACTIONAL FOURIER BASIS . the proposed approach is based on the use of BASIS FUNCTIONS , which is a COMPACT REPRESENTATION . the proposed method is applied to the problem of FRACTIONAL GABOR EXPANSION , and the results show that the proposed method is effective in reducing the number of BASIS FUNCTIONS .\n",
            "\n",
            "616 1000\n",
            "this paper considers a method for learning a DISTANCE METRIC in a FINGERPRINTING SYSTEM which identifies a QUERY CONTENT by measuring the distance between its fingerprint and a fingerprint stored in a database . a metric having a general form of the MAHALANOBIS DISTANCE is learned with the goal that the distance between fingerprints extracted from perceptually similar contents should be smaller than the distance between fingerprints extracted from perceptually dissimilar contents . the metric is learned by minimizing a COST FUNCTION designed to achieve the goal . the COST FUNCTION is convex , and the GLOBAL MINIMUM can be obtained using CONVEX OPTIMIZATION . in our experiment , the DISTANCE METRIC LEARNING is applied in an AUDIO FINGERPRINTING SYSTEM , and it is experimentally shown that the learned DISTANCE METRIC improves the IDENTIFICATION performance . \n",
            "in this paper , we propose a novel AUDIO FINGERPRINTING SYSTEM for IDENTIFICATION . the proposed AUDIO FINGERPRINTING SYSTEM is based on a CONVEX OPTIMIZATION , which is based on a CONVEX OPTIMIZATION . the proposed AUDIO FINGERPRINTING SYSTEM is based on a CONVEX OPTIMIZATION , which is based on a CONVEX OPTIMIZATION . the proposed AUDIO FINGERPRINTING SYSTEM is based on a CONVEX OPTIMIZATION , which is based on a CONVEX OPTIMIZATION . the proposed AUDIO FINGERPRINTING SYSTEM is applied to the AUDIO FINGERPRINTING SYSTEM of the AUDIO FINGERPRINTING SYSTEM . the performance of the proposed AUDIO FINGERPRINTING SYSTEM is evaluated on a FINGERPRINTING SYSTEM . the results show that the proposed AUDIO FINGERPRINTING SYSTEM is effective in improving the IDENTIFICATION performance in the presence of QUERY CONTENT .\n",
            "\n",
            "617 1000\n",
            "recently a large amount of research has been devoted to AUTOMATIC ACTIVITY ANALYSIS . typically , activities have been defined by their MOTION CHARACTERISTICS and represented by trajectories . these trajectories are collected and clustered to determine typical behaviors . this paper evaluates different SIMILARITY MEASURES and CLUSTERING METHODOLOGIES to catalog their strengths and weaknesses when utilized for the TRAJECTORY LEARNING PROBLEM . the CLUSTERING performance is measured by evaluating the correct CLUSTERING RATE on different datasets with varying characteristics . \n",
            "in this paper , we propose a novel approach to AUTOMATIC ACTIVITY ANALYSIS in the context of AUTOMATIC ACTIVITY ANALYSIS . the proposed approach is based on the use of SIMILARITY MEASURES , which is able to deal with MOTION CHARACTERISTICS . the proposed approach is based on the use of SIMILARITY MEASURES , which is able to deal with MOTION CHARACTERISTICS . the proposed approach is based on the use of SIMILARITY MEASURES . the proposed approach is evaluated in terms of CLUSTERING RATE and CLUSTERING RATE . the experimental results show that the proposed approach is effective in terms of CLUSTERING RATE and CLUSTERING RATE .\n",
            "\n",
            "618 1000\n",
            "embedded EMBEDDED SYSTEMS consisting of COLLABORATING AGENTS capable of interacting with their environment are becoming ubiquitous . it is crucial for these EMBEDDED SYSTEMS to be able to adapt to the dynamic and uncertain characteristics of an open environment . in this paper , we argue that MULTIAGENT META-LEVEL CONTROL is an effective way to determine when this ADAPTATION PROCESS should be done and how much effort should be invested in adaptation as opposed to continuing with the current ACTION PLAN . we describe a REINFORCEMENT LEARNING BASED APPROACH to learn DECENTRALIZED META-CONTROL POLICIES offline . we then propose to use the learned REINFORCEMENT LEARNING BASED APPROACH as input to a GLOBAL OPTIMIZATION ALGORITHM to avoid conflicting <unk> decisions between COORDINATING AGENTS . our initial experiments in the context of NETRADS , a MULTIAGENT TORNADO TRACKING APPLICATION show that NETRADS significantly improves performance in a 3-AGENT NETWORK . \n",
            "in this paper , we present a novel REINFORCEMENT LEARNING BASED APPROACH for MULTIAGENT META-LEVEL CONTROL . the proposed REINFORCEMENT LEARNING BASED APPROACH is based on the use of a 3-AGENT NETWORK , a GLOBAL OPTIMIZATION ALGORITHM , and a GLOBAL OPTIMIZATION ALGORITHM . the proposed REINFORCEMENT LEARNING BASED APPROACH is based on a GLOBAL OPTIMIZATION ALGORITHM , which is based on a GLOBAL OPTIMIZATION ALGORITHM . the proposed REINFORCEMENT LEARNING BASED APPROACH is based on the use of a GLOBAL OPTIMIZATION ALGORITHM , which is based on a GLOBAL OPTIMIZATION ALGORITHM . the experimental results show that the proposed REINFORCEMENT LEARNING BASED APPROACH is effective in improving the performance of EMBEDDED SYSTEMS in EMBEDDED SYSTEMS .\n",
            "\n",
            "619 1000\n",
            "the following article shows how a state-of-the-art SPEAKER DI-ARIZATION SYSTEM can be improved by combining traditional SHORT-TERM FEATURES with PROSODIC and other LONG-TERM FEATURES . first , we present a framework to study the SPEAKER DISCRIMINABILITY of 70 different LONG-TERM FEATURES . then , we show how the TOP-RANKED LONG-TERM FEATURES can be combined with SHORT-TERM FEATURES to increase the ACCURACY of SPEAKER DIARIZATION . the results were measured on standardized data sets -lrb- nist rt -rrb- and show a consistent improvement of about 30 % relative in DIARIZATION ERROR RATE compared to the best SPEAKER DI-ARIZATION SYSTEM presented at the NIST EVALUATION in 2007 . this result was also verified on a wide set of meetings , which we call <unk> , that contains 21 meetings from previous evaluations . since the PROSODIC AND LONG-TERM FEATURES were selected using a DIARIZATION-INDEPENDENT SPEAKER-DISCRIMINABILITY STUDY , we are confident that the same FEATURES are able to improve other systems that perform similar tasks \n",
            "in this paper , we propose a novel approach to SPEAKER DIARIZATION in SPEAKER DIARIZATION . the proposed approach is based on the use of SHORT-TERM FEATURES extracted from the SHORT-TERM FEATURES . the proposed method is based on the use of SHORT-TERM FEATURES extracted from the SHORT-TERM FEATURES . the proposed method is based on the use of SHORT-TERM FEATURES extracted from the TOP-RANKED LONG-TERM FEATURES of the SHORT-TERM FEATURES . the experimental results show that the proposed method is effective in improving the ACCURACY of the SPEAKER DI-ARIZATION SYSTEM in terms of both ACCURACY and ACCURACY .\n",
            "\n",
            "620 1000\n",
            "in this paper we derive PERFORMANCE BOUNDS for tracking time-varying ofdm multiple-input multiple-output -lrb- mimo -rrb- communication channel in the presence of ADDITIVE WHITE GAUSSIAN NOISE . we discuss two CHANNEL TRACKING SCHEMES . the first tracks the FILTER COEFFICIENTS directly in time-domain , while the second separately tracks each tone in the frequency-domain . the CHANNEL TRACKING SCHEMES , with KNOWN CHANNEL STATISTICS , is utilized for evaluating the PERFORMANCE BOUNDS . it is shown that the CHANNEL TRACKING SCHEMES , which exploits the sparseness of the CHANNEL IMPULSE RESPONSE , outperforms the compu-tationally more efficient , FREQUENCY-DOMAIN TRACKING SCHEME , which does not exploit the smooth frequency response of the channel . \n",
            "in this paper , we present a novel approach to CHANNEL TRACKING SCHEMES based on KNOWN CHANNEL STATISTICS . the proposed approach is based on the use of KNOWN CHANNEL STATISTICS to estimate the CHANNEL IMPULSE RESPONSE . the proposed method is based on the use of KNOWN CHANNEL STATISTICS to estimate the FILTER COEFFICIENTS . the proposed method is based on the use of KNOWN CHANNEL STATISTICS to estimate the FILTER COEFFICIENTS . experimental results show that the proposed method outperforms the existing methods in terms of PERFORMANCE BOUNDS .\n",
            "\n",
            "621 1000\n",
            "detection of perceptually important video events is formulated here on the basis of SALIENCY MODELS for the audio , visual and textual information conveyed in a VIDEO STREAM . AUDIO SALIENCY is assessed by CUES that quantify MULTIFREQUENCY WAVEFORM MODULATIONS , extracted through NONLINEAR OPERATORS and ENERGY TRACKING . VISUAL SALIENCY is measured through a SPATIOTEMPORAL ATTENTION MODEL driven by intensity , COLOR and motion . TEXT SALIENCY is extracted from PART-OF-SPEECH TAGGING on the <unk> information available with most MOVIE DISTRIBUTIONS . the various MODALITY CURVES are integrated in a single ATTENTION CURVE , where the presence of an event may be <unk> in one or multiple domains . this MULTIMODAL SALIENCY CURVE is the basis of a BOTTOM-UP VIDEO SUMMARIZATION ALGORITHM , that RESNES results from UNIMODAL OR AUDIOVISUAL-BASED SKIMMING . the BOTTOM-UP VIDEO SUMMARIZATION ALGORITHM performs favorably for VIDEO SUMMARIZATION in terms of informativeness and <unk> . \n",
            "in this paper , we present a novel BOTTOM-UP VIDEO SUMMARIZATION ALGORITHM , called the SPATIOTEMPORAL ATTENTION MODEL , for VIDEO SUMMARIZATION . the proposed BOTTOM-UP VIDEO SUMMARIZATION ALGORITHM is based on a SPATIOTEMPORAL ATTENTION MODEL called the SPATIOTEMPORAL ATTENTION MODEL , which is based on a SPATIOTEMPORAL ATTENTION MODEL . the proposed SPATIOTEMPORAL ATTENTION MODEL is based on a SPATIOTEMPORAL ATTENTION MODEL , a BOTTOM-UP VIDEO SUMMARIZATION ALGORITHM , and a SPATIOTEMPORAL ATTENTION MODEL for ENERGY TRACKING . the proposed SPATIOTEMPORAL ATTENTION MODEL is based on a SPATIOTEMPORAL ATTENTION MODEL , a BOTTOM-UP VIDEO SUMMARIZATION ALGORITHM , and a SPATIOTEMPORAL ATTENTION MODEL . the proposed BOTTOM-UP VIDEO SUMMARIZATION ALGORITHM is applied to VIDEO SUMMARIZATION and ENERGY TRACKING . the experimental results demonstrate the effectiveness of the proposed BOTTOM-UP VIDEO SUMMARIZATION ALGORITHM in VIDEO SUMMARIZATION and ENERGY TRACKING .\n",
            "\n",
            "622 1000\n",
            "efficient learning equilibrium -lrb- EFFICIENT LEARNING EQUILIBRIUM -rrb- is a NATURAL SOLUTION CONCEPT for MULTI-AGENT ENCOUNTERS with INCOMPLETE INFORMATION . it requires the LEARNING ALGORITHMS themselves to be in equilibrium for any game selected from a set of -lrb- initially unknown -rrb- games . in an optimal EFFICIENT LEARNING EQUILIBRIUM , the LEARNING ALGORITHMS would efficiently obtain the surplus the agents would obtain in an optimal nash equilibrium of the initially unknown game which is played . the crucial part is that in an ELE DEVIATIONS from the LEARNING ALGORITHMS would become <unk> after POLYNOMIAL TIME , although the game played is initially unknown . while appealing conceptually , the main challenge for establishing LEARNING ALGORITHMS based on this concept is to isolate general classes of games where an EFFICIENT LEARNING EQUILIBRIUM exists . unfortunately , it has been shown that while an EFFICIENT LEARNING EQUILIBRIUM exists for the setting in which each agent can observe all other agents ' actions and payoffs , an EFFICIENT LEARNING EQUILIBRIUM does not exist in general when the other agents ' payoffs can not be observed . in this paper we provide the first positive results on this problem , <unk> proving the existence of an optimal EFFICIENT LEARNING EQUILIBRIUM for the class of symmetric games where an agent can not observe other agents ' payoffs . \n",
            "in this paper , we propose a novel approach to MULTI-AGENT ENCOUNTERS . the proposed NATURAL SOLUTION CONCEPT is based on a novel NATURAL SOLUTION CONCEPT called the EFFICIENT LEARNING EQUILIBRIUM , which is able to deal with INCOMPLETE INFORMATION . the proposed NATURAL SOLUTION CONCEPT is based on a NATURAL SOLUTION CONCEPT , called EFFICIENT LEARNING EQUILIBRIUM , which is able to deal with INCOMPLETE INFORMATION . we show that the proposed EFFICIENT LEARNING EQUILIBRIUM can be applied to a wide range of LEARNING ALGORITHMS , such as the EFFICIENT LEARNING EQUILIBRIUM , and the EFFICIENT LEARNING EQUILIBRIUM .\n",
            "\n",
            "623 1000\n",
            "we describe an approach for UNSUPERVISED LEARNING of a GENERIC , DISTRIBUTED SENTENCE ENCODER . using the continuity of text from books , we train an ENCODER-DECODER MODEL that tries to reconstruct the surrounding sentences of an ENCODED PASSAGE . sentences that share SEMANTIC AND SYNTACTIC PROPERTIES are thus mapped to similar VECTOR REPRESENTATIONS . we next introduce a simple VOCABULARY EXPANSION METHOD to encode words that were not seen as part of training , allowing us to expand our vocabulary to a million words . after training our model , we extract and evaluate our vectors with LINEAR MODELS on 8 tasks : SEMANTIC RELATEDNESS , PARAPHRASE DETECTION , IMAGE-SENTENCE RANKING , QUESTION-TYPE CLASSIFICATION and 4 benchmark sentiment and subjectivity datasets . the end result is an off-the-shelf encoder that can produce highly GENERIC SENTENCE REPRESENTATIONS that are robust and perform well in practice . we will make our encoder publicly available . \n",
            "this paper addresses the problem of PARAPHRASE DETECTION and PARAPHRASE DETECTION in a GENERIC , DISTRIBUTED SENTENCE ENCODER . we propose a novel approach to the problem of PARAPHRASE DETECTION , which is based on the use of a GENERIC , DISTRIBUTED SENTENCE ENCODER and a VOCABULARY EXPANSION METHOD . the proposed approach is based on a GENERIC , DISTRIBUTED SENTENCE ENCODER , which is a GENERIC , DISTRIBUTED SENTENCE ENCODER , and is able to deal with SEMANTIC AND SYNTACTIC PROPERTIES , SEMANTIC RELATEDNESS , SEMANTIC RELATEDNESS , and QUESTION-TYPE CLASSIFICATION . we demonstrate the effectiveness of our approach on the task of QUESTION-TYPE CLASSIFICATION and QUESTION-TYPE CLASSIFICATION .\n",
            "\n",
            "624 1000\n",
            "we present an LFG-DOP PARSER which uses fragments from LFG-ANNOTATED SENTENCES to parse new sentences . experiments with the VERBMOBIL AND HOMECENTRE CORPORA show that -lrb- 1 -rrb- viterbi n best search performs about 100 times faster than MONTE CARLO SEARCH while both achieve the same ACCURACY ; -lrb- 2 -rrb- the DOP HYPOTHESIS which states that PARSE ACCURACY increases with increasing FRAGMENT SIZE is confirmed for LFG-DOP ; -lrb- 3 -rrb- LFG-DOP 's RELATIVE FREQUENCY ESTIMATOR performs worse than a DISCOUNTED FREQUENCY ESTIMATOR ; and -lrb- 4 -rrb- LFG-DOP significantly outperforms TREE-DOP if evaluated on TREE STRUCTURES only . \n",
            "in this paper , we propose a novel approach to MONTE CARLO SEARCH based on TREE STRUCTURES . the proposed method is based on the use of LFG-ANNOTATED SENTENCES to estimate the FRAGMENT SIZE . the proposed method is based on the use of LFG-ANNOTATED SENTENCES to estimate the FRAGMENT SIZE . the proposed LFG-DOP PARSER is based on the use of LFG-ANNOTATED SENTENCES to estimate the FRAGMENT SIZE . the proposed method is evaluated on both VERBMOBIL AND HOMECENTRE CORPORA . the experimental results show that the proposed LFG-DOP is effective in improving the ACCURACY and ACCURACY of the proposed method .\n",
            "\n",
            "625 1000\n",
            "this contribution presents a MODIFIED KALMAN FILTER APPROACH for SINGLE CHANNEL SPEECH ENHANCEMENT which is operating in the FREQUENCY DOMAIN . in the first step , TEMPORAL CORRELATION OF SUCCESSIVE FRAMES is exploited yielding estimates of the current speech and noise dft coefficients . this first prediction is updated in the second step applying an SNR DEPENDENT MMSE ESTIMATOR which is adapted to the -lrb- measured -rrb- statistics of the SPEECH PREDICTION ERROR SIGNAL . OBJECTIVE MEASUREMENTS show consistent improvements compared to ESTIMA-TORS which do not take into account the TEMPORAL CORRELATION or the influence of the input SNR on the statistics of the PREDICTION ERROR SIGNAL . \n",
            "in this paper , we propose a novel MODIFIED KALMAN FILTER APPROACH for SINGLE CHANNEL SPEECH ENHANCEMENT . the proposed MODIFIED KALMAN FILTER APPROACH is based on a MODIFIED KALMAN FILTER APPROACH , which is able to estimate the TEMPORAL CORRELATION of the SPEECH PREDICTION ERROR SIGNAL . the proposed MODIFIED KALMAN FILTER APPROACH is based on the use of a MODIFIED KALMAN FILTER APPROACH to estimate the TEMPORAL CORRELATION . the proposed MODIFIED KALMAN FILTER APPROACH is applied to the problem of SINGLE CHANNEL SPEECH ENHANCEMENT in the presence of SNR . the proposed MODIFIED KALMAN FILTER APPROACH is evaluated on OBJECTIVE MEASUREMENTS and SNR . the experimental results show that the proposed ESTIMA-TORS is effective in SINGLE CHANNEL SPEECH ENHANCEMENT .\n",
            "\n",
            "626 1000\n",
            "this paper presents a system that labels TV SHOTS either as COMMERCIAL OR PROGRAM SHOTS . the system uses two observations : LOGO PRESENCE and SHOT DURATION . this observations are modeled using HMM and the VITERBI DECODER is finally used for SHOT LABELING . the system has been tested on several hours of REAL VIDEO achieving more than 99 % of correct labeling . \n",
            "this paper addresses the problem of SHOT LABELING in the presence of COMMERCIAL OR PROGRAM SHOTS and LOGO PRESENCE . we propose a method to estimate the parameters of a VITERBI DECODER and the VITERBI DECODER . the proposed method is based on a VITERBI DECODER and a VITERBI DECODER . the performance of the proposed method is demonstrated on a variety of REAL VIDEO . the results show that the proposed method is able to detect the presence of TV SHOTS in the presence of SHOT DURATION , LOGO PRESENCE , and LOGO PRESENCE .\n",
            "\n",
            "627 1000\n",
            "deterministic parsing guided by TREEBANK-INDUCED CLASSIFIERS has emerged as a simple and efficient alternative to more complex models for DATA-DRIVEN PARSING . we present a systematic comparison of <unk> learning -lrb- <unk> -rrb- and SUPPORT VECTOR MACHINES for inducing CLASSIFIERS for DETERMINISTIC DEPENDENCY PARSING , using data from CHINESE , ENGLISH and SWEDISH , together with a variety of different FEATURE MODELS . the comparison shows that SUPPORT VECTOR MACHINES gives higher ACCURACY for <unk> articulated FEATURE MODELS across all languages , albeit with considerably longer training times . the results also confirm that CLASSIFIER-BASED DETERMINISTIC PARSING can achieve PARSING ACCURACY very close to the best results reported for more complex PARSING MODELS . \n",
            "in this paper , we propose a novel approach to DETERMINISTIC DEPENDENCY PARSING based on SUPPORT VECTOR MACHINES . in particular , we propose to use SUPPORT VECTOR MACHINES for DETERMINISTIC DEPENDENCY PARSING . the proposed FEATURE MODELS is based on the use of SUPPORT VECTOR MACHINES for DETERMINISTIC DEPENDENCY PARSING . the proposed approach is based on the use of SUPPORT VECTOR MACHINES for DETERMINISTIC DEPENDENCY PARSING . the experimental results show that the proposed SUPPORT VECTOR MACHINES is effective in improving the ACCURACY of CLASSIFIER-BASED DETERMINISTIC PARSING in terms of ACCURACY and ACCURACY . the proposed approach is evaluated in terms of PARSING ACCURACY and ACCURACY .\n",
            "\n",
            "628 1000\n",
            "this paper proposes a SEMI-SUPERVISED BOOSTING APPROACH to improve STATISTICAL WORD ALIGNMENT with LIMITED LABELED DATA and large amounts of UNLABELED DATA . the proposed SEMI-SUPERVISED BOOSTING APPROACH modifies the SUPERVISED BOOSTING ALGORITHM to a SEMI-SUPERVISED LEARNING ALGORITHM by incorporating the UNLABELED DATA . in this SEMI-SUPERVISED BOOSTING APPROACH , we build a WORD ALIGNER by using both the LABELED DATA and the UNLABELED DATA . then we build a PSEUDO REFERENCE SET for the UNLABELED DATA , and calculate the ERROR RATE of each WORD ALIGNER using only the LABELED DATA . based on this SUPERVISED BOOSTING ALGORITHM , we investigate two BOOSTING METHODS for STATISTICAL WORD ALIGNMENT . in addition , we improve the STATISTICAL WORD ALIGNMENT results by combining the results of the two SEMI-SUPERVISED BOOSTING METHODS . experimental results on STATISTICAL WORD ALIGNMENT indicate that SEMI-SUPERVISED BOOSTING achieves RELATIVE ERROR REDUCTIONS of <unk> % and <unk> % as compared with SUPERVISED BOOSTING and UNSUPERVISED BOOSTING , respectively . \n",
            "in this paper , we propose a novel SEMI-SUPERVISED BOOSTING APPROACH for STATISTICAL WORD ALIGNMENT . the proposed SEMI-SUPERVISED BOOSTING APPROACH is based on the use of a PSEUDO REFERENCE SET , which is a PSEUDO REFERENCE SET . the proposed SEMI-SUPERVISED BOOSTING APPROACH is based on the use of a PSEUDO REFERENCE SET and the UNLABELED DATA . the proposed SEMI-SUPERVISED BOOSTING APPROACH is based on the use of a PSEUDO REFERENCE SET , which is a PSEUDO REFERENCE SET . the proposed SEMI-SUPERVISED BOOSTING APPROACH is applied to the problem of STATISTICAL WORD ALIGNMENT and UNLABELED DATA . the experimental results show that the proposed SEMI-SUPERVISED BOOSTING APPROACH is effective in improving the ERROR RATE of SEMI-SUPERVISED BOOSTING in terms of ERROR RATE and LABELED DATA .\n",
            "\n",
            "629 1000\n",
            "computing the market maker price of a security in a COMBINATORIAL PREDICTION MARKET is #P - hard . we devise a fully polynomial randomized approximation scheme -lrb- <unk> -rrb- that computes the price of any security in DISJUNCTIVE NORMAL FORM within an MULTIPLICATIVE ERROR FACTOR in TIME POLYNOMIAL in 1 / / and the size of the input , with high probability and under reasonable assumptions . our algorithm is a MONTE-CARLO TECHNIQUE based on IMPORTANCE SAMPLING . the algorithm can also approximately price <unk> represented in CONJUNCTIVE NORMAL FORM with ADDITIVE ERROR BOUNDS . to illustrate the applicability of our algorithm , we show that many <unk> in yahoo! 's popular COMBINATORIAL PREDICTION MARKET GAME called CONJUNCTIVE NORMAL FORM can be represented by DNF FORMULAS OF POLYNOMIAL SIZE . \n",
            "this paper proposes a novel MONTE-CARLO TECHNIQUE , called the DNF FORMULAS OF POLYNOMIAL SIZE , for the COMBINATORIAL PREDICTION MARKET GAME . the proposed MONTE-CARLO TECHNIQUE is based on a MONTE-CARLO TECHNIQUE , which is based on the IMPORTANCE SAMPLING . the proposed MONTE-CARLO TECHNIQUE is based on a MONTE-CARLO TECHNIQUE , which is based on the IMPORTANCE SAMPLING . the proposed MONTE-CARLO TECHNIQUE is based on the IMPORTANCE SAMPLING , which is based on the IMPORTANCE SAMPLING . the proposed MONTE-CARLO TECHNIQUE is applied to the COMBINATORIAL PREDICTION MARKET GAME of the COMBINATORIAL PREDICTION MARKET GAME . the performance of the proposed MONTE-CARLO TECHNIQUE is evaluated in terms of the MULTIPLICATIVE ERROR FACTOR and the ADDITIVE ERROR BOUNDS . the performance of the proposed MONTE-CARLO TECHNIQUE is evaluated on a COMBINATORIAL PREDICTION MARKET GAME , and the results show that the proposed method is effective in reducing the number of sources .\n",
            "\n",
            "630 1000\n",
            "we replace the OVERLAP MECHANISM of the LESK ALGORITHM with a simple , GENERAL-PURPOSE NAIVE BAYES MODEL that measures MANY-TO-MANY ASSOCIATION between two sets of RANDOM VARIABLES . even with simple PROBABILITY ESTIMATES such as MAXIMUM LIKELIHOOD , the GENERAL-PURPOSE NAIVE BAYES MODEL gains significant improvement over the LESK ALGORITHM on WORD SENSE DISAMBIGUATION TASKS . with additional LEXICAL KNOWLEDGE from WORD-NET , performance is further improved to surpass the state-of-the-art results . \n",
            "this paper proposes a novel GENERAL-PURPOSE NAIVE BAYES MODEL for WORD SENSE DISAMBIGUATION TASKS . the proposed GENERAL-PURPOSE NAIVE BAYES MODEL is based on the use of a GENERAL-PURPOSE NAIVE BAYES MODEL , which is able to capture the LEXICAL KNOWLEDGE of the RANDOM VARIABLES . the proposed GENERAL-PURPOSE NAIVE BAYES MODEL is based on the use of a GENERAL-PURPOSE NAIVE BAYES MODEL , which is able to capture the LEXICAL KNOWLEDGE of the RANDOM VARIABLES . the proposed GENERAL-PURPOSE NAIVE BAYES MODEL is evaluated on WORD SENSE DISAMBIGUATION TASKS and compared with the conventional LESK ALGORITHM . the experimental results show that the proposed GENERAL-PURPOSE NAIVE BAYES MODEL outperforms the conventional LESK ALGORITHM in WORD SENSE DISAMBIGUATION TASKS .\n",
            "\n",
            "631 1000\n",
            "valued decision diagrams -lrb- VDD LANGUAGES -rrb- are DATA STRUCTURES that represent functions mapping <unk> assignments to NON-NEGATIVE REAL NUMBERS . they prove useful to compile COST FUNCTIONS , UTILITY FUNCTIONS , or PROBABILITY DISTRIBUTIONS . while the COMPLEXITY of some queries -lrb- notably optimization -rrb- and transformations -lrb- notably conditioning -rrb- on VDD LANGUAGES has been known for some time , there remain many significant queries and transformations , such as the various kinds of CUTS , MARGINALIZATIONS , and combinations , the COMPLEXITY of which has not been identified so far . this paper contributes to filling this gap and completing previous results about the time and space efficiency of VDD LANGUAGES , thus leading to a KNOWLEDGE COMPILATION MAP for REAL-VALUED FUNCTIONS . our results show that many tasks that are hard on valued CSPS are actually tractable on VDD LANGUAGES . \n",
            "this paper addresses the problem of VALUED DECISION DIAGRAMS in VDD LANGUAGES . we propose a novel method for VALUED DECISION DIAGRAMS , which is based on the use of a KNOWLEDGE COMPILATION MAP . the proposed approach is based on the use of a KNOWLEDGE COMPILATION MAP , which is able to deal with DATA STRUCTURES , REAL-VALUED FUNCTIONS , and PROBABILITY DISTRIBUTIONS . the proposed method is based on the use of a set of REAL-VALUED FUNCTIONS , PROBABILITY DISTRIBUTIONS , PROBABILITY DISTRIBUTIONS , and PROBABILITY DISTRIBUTIONS . the proposed method is evaluated on a variety of VDD LANGUAGES , and the results show that the proposed method outperforms the state-of-the-art methods .\n",
            "\n",
            "632 1000\n",
            "we describe a TRANSLATION MODEL ADAPTATION APPROACH for CONVERSATIONAL SPOKEN LANGUAGE TRANSLATION , which encourages the use of contextually appropriate translation options from relevant training conversations . our TRANSLATION MODEL ADAPTATION APPROACH employs a MONOLINGUAL LDA TOPIC MODEL to derive a SIMILARITY MEASURE between the test conversation and the set of training conversations , which is used to bias translation choices towards the current context . a significant novelty of our TRANSLATION MODEL ADAPTATION APPROACH is its INCREMENTAL NATURE ; we continuously update the TOPIC DISTRIBUTION on the evolving test conversation as new utterances become available . thus , our TRANSLATION MODEL ADAPTATION APPROACH is well-suited to the CAUSAL CONSTRAINT OF SPOKEN CONVERSATIONS . on an ENGLISH-TO-IRAQI CSLT TASK , the proposed TRANSLATION MODEL ADAPTATION APPROACH gives significant improvements over a baseline system as measured by BLEU , TER , and NIST . interestingly , the TRANSLATION MODEL ADAPTATION APPROACH outperforms a NON-INCREMENTAL ORACLE that has UP-FRONT KNOWLEDGE of the whole conversation . \n",
            "this paper presents a novel TRANSLATION MODEL ADAPTATION APPROACH for CONVERSATIONAL SPOKEN LANGUAGE TRANSLATION . the proposed TRANSLATION MODEL ADAPTATION APPROACH is based on a MONOLINGUAL LDA TOPIC MODEL for CONVERSATIONAL SPOKEN LANGUAGE TRANSLATION . the proposed TRANSLATION MODEL ADAPTATION APPROACH is based on a MONOLINGUAL LDA TOPIC MODEL , which is robust to UP-FRONT KNOWLEDGE , TER , and TER . the proposed TRANSLATION MODEL ADAPTATION APPROACH is evaluated on a ENGLISH-TO-IRAQI CSLT TASK . the experimental results show that the proposed TRANSLATION MODEL ADAPTATION APPROACH significantly outperforms the conventional NON-INCREMENTAL ORACLE in terms of both BLEU , BLEU , TER and NIST .\n",
            "\n",
            "633 1000\n",
            "the problems of DESIGNING SIGNATURE SEQUENCES and POWER ALLOCATION POLICY for <unk> multiple access -lrb- cdma -rrb- are important and have been the subject of intensive research in recent years . two different criteria adopted in such DESIGN PROBLEMS are the USER CAPACITY and the INFORMATION-THEORETIC CAPACITY . regarding the maxi-mization of the INFORMATION-THEORETIC CAPACITY , most of the previous works only consider the optimizations of SIGNATURE SEQUENCES and POWER ALLOCATION separately . in contrast , this paper presents a jointly optimal design of SIGNATURE SEQUENCES and POWER ALLOCATION under the SUM POWER CONSTRAINT . the proposed design is of closed-form and applicable for the general case of CORRELATED SIGNALS and COLORED NOISE . numerical results verify the superiority of the proposed design over the existing ones . \n",
            "this paper addresses the problem of COLORED NOISE in the presence of COLORED NOISE , COLORED NOISE , and COLORED NOISE . in particular , we consider the problem of DESIGN PROBLEMS in the presence of COLORED NOISE , COLORED NOISE , and COLORED NOISE . in particular , we consider the problem of DESIGN PROBLEMS in the presence of COLORED NOISE , COLORED NOISE , and COLORED NOISE . we show that this problem can be solved efficiently by using the SUM POWER CONSTRAINT . we show that the proposed algorithm is able to solve the problem of DESIGN PROBLEMS and COLORED NOISE .\n",
            "\n",
            "634 1000\n",
            "we investigate whether it is possible to improve the performance of AUTOMATED FACIAL FORENSIC SKETCH MATCHING by learning from examples of facial forgetting over time . FORENSIC FACIAL SKETCH RECOGNITION is a key capability for LAW ENFORCEMENT , but remains an unsolved problem . it is extremely challenging because there are three distinct <unk> to the DOMAIN GAP between FORENSIC SKETCHES and photos : the WELL-STUDIED SKETCH-PHOTO MODALITY GAP , and the less studied gaps due to -lrb- i -rrb- the FORGETTING PROCESS of the <unk> and -lrb- ii -rrb- their inability to elucidate their memory . in this paper , we address the MEMORY PROBLEM head on by introducing a database of 400 FORENSIC SKETCHES created at different <unk> . based on this database we build a model to reverse the FORGETTING PROCESS . surprisingly , we show that it is possible to systematically '' <unk> '' facial details . moreover , it is possible to apply this model to dramatically improve FORENSIC SKETCH RECOGNITION in practice : we achieve the state of the art results when matching <unk> benchmark FORENSIC SKETCHES against corresponding photos and a 10,030 MUGSHOT DATABASE . \n",
            "this paper addresses the problem of AUTOMATED FACIAL FORENSIC SKETCH MATCHING for AUTOMATED FACIAL FORENSIC SKETCH MATCHING . we propose a novel approach to the problem of AUTOMATED FACIAL FORENSIC SKETCH MATCHING for AUTOMATED FACIAL FORENSIC SKETCH MATCHING . the proposed approach is based on a FORGETTING PROCESS , which is based on a FORGETTING PROCESS . the proposed approach is based on a FORGETTING PROCESS , which is based on a FORGETTING PROCESS . the proposed approach is evaluated on a 10,030 MUGSHOT DATABASE and a 10,030 MUGSHOT DATABASE . the results show that the proposed approach is effective in improving the FORENSIC FACIAL SKETCH RECOGNITION performance .\n",
            "\n",
            "635 1000\n",
            "convexity has recently received a lot of attention in the MACHINE LEARNING COMMUNITY , and the lack of CONVEXITY has been seen as a major disadvantage of many LEARNING ALGORITHMS , such as MULTI-LAYER ARTIFICIAL NEURAL NETWORKS . we show that training MULTI-LAYER ARTIFICIAL NEURAL NETWORKS in which the number of hidden units is learned can be viewed as a CONVEX OPTIMIZATION PROBLEM . this CONVEX OPTIMIZATION PROBLEM involves an infinite number of variables , but can be solved by incrementally inserting a HIDDEN UNIT at a time , each time finding a LINEAR CLASSIFIER that minimizes a weighted sum of errors . \n",
            "this paper addresses the problem of MULTI-LAYER ARTIFICIAL NEURAL NETWORKS for MULTI-LAYER ARTIFICIAL NEURAL NETWORKS . in particular , we focus on the problem of MULTI-LAYER ARTIFICIAL NEURAL NETWORKS , where the HIDDEN UNIT is a CONVEX OPTIMIZATION PROBLEM . we show that this problem can be solved by a CONVEX OPTIMIZATION PROBLEM . we show that this problem can be solved by a CONVEX OPTIMIZATION PROBLEM . we show that the proposed MULTI-LAYER ARTIFICIAL NEURAL NETWORKS can be applied to a wide range of LEARNING ALGORITHMS such as MULTI-LAYER ARTIFICIAL NEURAL NETWORKS .\n",
            "\n",
            "636 1000\n",
            "we present an EMBEDDING of STOCHASTIC OPTIMAL CONTROL PROBLEMS , of the so called PATH INTEGRAL FORM , into reproducing kernel hilbert spaces . using consistent , sample based estimates of the EMBEDDING leads to a MODEL-FREE , NON-PARAMETRIC APPROACH for calculation of an approximate solution to the CONTROL PROBLEM . this MODEL-FREE , NON-PARAMETRIC APPROACH admits a decomposition of the problem into an INVARIANT AND TASK DEPENDENT COMPONENT . consequently , we make much more efficient use of the SAMPLE DATA compared to previous SAMPLE BASED APPROACHES in this domain , e.g. , by allowing SAMPLE RE-USE across tasks . numerical examples on test problems , which illustrate the SAMPLE EFFICIENCY , are provided . \n",
            "this paper presents a novel MODEL-FREE , NON-PARAMETRIC APPROACH for STOCHASTIC OPTIMAL CONTROL PROBLEMS . the proposed MODEL-FREE , NON-PARAMETRIC APPROACH is based on the use of a MODEL-FREE , NON-PARAMETRIC APPROACH for STOCHASTIC OPTIMAL CONTROL PROBLEMS . the proposed MODEL-FREE , NON-PARAMETRIC APPROACH is based on the use of a MODEL-FREE , NON-PARAMETRIC APPROACH , which is based on a MODEL-FREE , NON-PARAMETRIC APPROACH . the proposed MODEL-FREE , NON-PARAMETRIC APPROACH is based on a MODEL-FREE , NON-PARAMETRIC APPROACH . the proposed MODEL-FREE , NON-PARAMETRIC APPROACH is applied to the problem of STOCHASTIC OPTIMAL CONTROL PROBLEMS in a MODEL-FREE , NON-PARAMETRIC APPROACH . the experimental results show that the proposed MODEL-FREE , NON-PARAMETRIC APPROACH is effective in improving the SAMPLE EFFICIENCY and SAMPLE EFFICIENCY of the proposed SAMPLE BASED APPROACHES .\n",
            "\n",
            "637 1000\n",
            "we aim to color GREYSCALE IMAGES automatically , without any MANUAL INTERVENTION . the COLOR PROPOSITION could then be interactively corrected by USER-PROVIDED COLOR LANDMARKS if necessary . AUTOMATIC COL-ORIZATION is nontrivial since there is usually no ONE-TO-ONE CORRESPONDENCE between color and LOCAL TEXTURE . the contribution of our framework is that we deal directly with MULTIMODALITY AND ESTIMATE , for each PIXEL of the image to be colored , the PROBABILITY DISTRIBUTION of all possible colors , instead of choosing the most probable color at the LOCAL LEVEL . we also predict the expected variation of color at each PIXEL , thus defining a NON-UNIFORM SPATIAL COHERENCY CRITERION . we then use GRAPH CUTS to maximize the probability of the whole COLORED IMAGE at the GLOBAL LEVEL . we work in the L-A-B COLOR SPACE in order to approximate the HUMAN PERCEPTION OF DISTANCES between colors , and we use MACHINE LEARNING TOOLS to extract as much information as possible from a DATASET OF COLORED EXAMPLES . the resulting algorithm is fast , designed to be more robust to TEXTURE NOISE , and is above all able to deal with ambiguity , in contrary to previous approaches . \n",
            "in this paper , we propose a novel approach to HUMAN PERCEPTION OF DISTANCES in GREYSCALE IMAGES . the proposed method is based on a NON-UNIFORM SPATIAL COHERENCY CRITERION , which is based on a NON-UNIFORM SPATIAL COHERENCY CRITERION , and the PROBABILITY DISTRIBUTION of the object is estimated by a NON-UNIFORM SPATIAL COHERENCY CRITERION . the proposed approach is based on a NON-UNIFORM SPATIAL COHERENCY CRITERION , which is a NON-UNIFORM SPATIAL COHERENCY CRITERION of a COLORED IMAGE . the proposed approach is based on a NON-UNIFORM SPATIAL COHERENCY CRITERION , which is a NON-UNIFORM SPATIAL COHERENCY CRITERION of the COLORED IMAGE . the proposed method is based on a NON-UNIFORM SPATIAL COHERENCY CRITERION , which is a ONE-TO-ONE CORRESPONDENCE of the COLORED IMAGE . the proposed method is evaluated on a DATASET OF COLORED EXAMPLES and a DATASET OF COLORED EXAMPLES . the results show that the proposed method outperforms the state-of-the-art methods in terms of both MULTIMODALITY AND ESTIMATE and TEXTURE NOISE .\n",
            "\n",
            "638 1000\n",
            "in this paper we present a weighted likelihood ratio -lrb- <unk> -rrb- based hidden markov model and apply it to SPEECH RECOGNITION in noise . the <unk> measure emphasizes SPECTRAL PEAKS than VALLEYS in comparing two given SPEECH SPECTRA . the measure is more consistent with HUMAN PERCEPTION OF SPEECH FORMANTS where NATURAL RESONANCES OF VOCAL TRACK are and tends to be more robust to BROAD-BAND NOISE INTERFERENCES than other measures . a complete HMM FRAMEWORK of this measure is derived and a mixture of EXPONENTIAL KERNELS is used to model the OUTPUT PROBABILITY DENSITY FUNCTION . the new WLR-HMM is tested on the AURORA2 CONNECTED DIGITS DATABASE in noise . it shows more robust performance than the MFCC TRAINED GMM BASELINE SYSTEM . when combined with the DYNAMIC CEPSTRAL FEATURES , the MULTIPLE-STREAM WLR-HMM shows a 39 % relative improvement over the baseline system . \n",
            "in this paper , we propose a novel HMM FRAMEWORK to the NATURAL RESONANCES OF VOCAL TRACK . the proposed HMM FRAMEWORK is based on the use of EXPONENTIAL KERNELS for SPEECH RECOGNITION . the proposed HMM FRAMEWORK is based on the use of EXPONENTIAL KERNELS in the HMM FRAMEWORK . the proposed HMM FRAMEWORK is based on the use of EXPONENTIAL KERNELS in the HMM FRAMEWORK . the proposed HMM FRAMEWORK is based on the use of EXPONENTIAL KERNELS to estimate the SPECTRAL PEAKS . the proposed method is evaluated on the AURORA2 CONNECTED DIGITS DATABASE and the results show that the proposed method is effective in improving the SPEECH RECOGNITION performance . the proposed method is evaluated on the AURORA2 CONNECTED DIGITS DATABASE and the results show that the proposed method is effective in improving the SPEECH RECOGNITION performance .\n",
            "\n",
            "639 1000\n",
            "in this paper , german , <unk> , spanish , and portuguese large vocabulary continuous speech recognition -lrb- lvcsr -rrb- systems developed by the <unk> <unk> university are presented . all the above mentioned systems for the aforementioned languages are used for the QUAERO AND EU-BRIDGE PROJECT EVALUATIONS . the LVCSR SYSTEMS developed for these competitive evaluations focus on various domains like BROADCAST NEWS , PODCASTS and LECTURE DOMAIN . transcription of the speech for these tasks is challenging due to huge variability in the ACOUSTIC CONDITIONS and a significant portion of AUDIO DATA includes SPONTANEOUS SPEECH . good improvements are obtained using state-of-the-art MULTILINGUAL BOTTLENECK FEATURES , MINIMUM PHONE ERROR TRAINED ACOUSTIC MODELS , LANGUAGE MODEL ADAPTATION and CONFUSION-NETWORK BASED SYSTEM COMBINATION . in addition , an OPEN VOCABULARY APPROACH using MORPHEMIC UNITS is investigated along with the LM ADAPTATION for the GERMAN LVCSR . \n",
            "this paper presents a novel approach to LANGUAGE MODEL ADAPTATION based on a combination of MINIMUM PHONE ERROR TRAINED ACOUSTIC MODELS and LANGUAGE MODEL ADAPTATION . the proposed approach is based on the use of MORPHEMIC UNITS , MORPHEMIC UNITS , MORPHEMIC UNITS , MORPHEMIC UNITS , MORPHEMIC UNITS , and MINIMUM PHONE ERROR TRAINED ACOUSTIC MODELS . the proposed approach is based on the use of MORPHEMIC UNITS , MULTILINGUAL BOTTLENECK FEATURES , MORPHEMIC UNITS , and MINIMUM PHONE ERROR TRAINED ACOUSTIC MODELS . the experimental results show that the proposed approach is effective in improving the performance of LVCSR SYSTEMS .\n",
            "\n",
            "640 1000\n",
            "forced alignment for SPEECH SYNTHESIS traditionally aligns a PHONEME SEQUENCE predetermined by the FRONT-END TEXT PROCESSING SYSTEM . this sequence is not altered during alignment , i.e. , it is forced , despite possibly being faulty . the CONSISTENCY ASSUMPTION is the assumption that these mistakes do not degrade models , as long as the mistakes are consistent across training and synthesis . we present evidence that in the alignment of both standard READ PROMPTS and SPONTANEOUS SPEECH this PHONEME SEQUENCE is often wrong , and that this is likely to have a negative impact on ACOUSTIC MODELS . a LATTICE-BASED FORCED ALIGNMENT SYSTEM allowing for PRONUNCIATION VARIATION is implemented , resulting in improved PHONEME IDENTITY ACCURACY for both types of speech . a perceptual evaluation of HMM-BASED VOICES showed that SPONTANEOUS MODELS trained on this improved alignment also improved standard synthesis , despite breaking the CONSISTENCY ASSUMPTION . \n",
            "this paper presents a novel FRONT-END TEXT PROCESSING SYSTEM for SPEECH SYNTHESIS . the proposed FRONT-END TEXT PROCESSING SYSTEM is based on a LATTICE-BASED FORCED ALIGNMENT SYSTEM for SPEECH SYNTHESIS . the proposed FRONT-END TEXT PROCESSING SYSTEM is based on a LATTICE-BASED FORCED ALIGNMENT SYSTEM , which is based on FORCED ALIGNMENT . the proposed FRONT-END TEXT PROCESSING SYSTEM is based on a LATTICE-BASED FORCED ALIGNMENT SYSTEM , which is based on a FORCED ALIGNMENT . the proposed FRONT-END TEXT PROCESSING SYSTEM is applied to SPONTANEOUS SPEECH , and the experimental results show that the proposed LATTICE-BASED FORCED ALIGNMENT SYSTEM is effective in improving the PHONEME IDENTITY ACCURACY and PHONEME IDENTITY ACCURACY .\n",
            "\n",
            "641 1000\n",
            "image registration is one of the most important tasks in IMAGE PROCESSING . the algorithms of IMAGE REGISTRATION are classified into two categories : the FEATURE-BASED MATCHING and INTENSITY-BASED MATCHING . each of them has its strength and weakness . in this paper , by combining these two techniques together , we developed a new algorithm for IMAGE REGISTRATION . the algorithm utilises a PARAMETRIC PROJECTIVE MODEL accounting for GEOMETRICAL VARIATION and a POLYNOMIAL MODEL with a small number of POLYNOMIAL COEFFICIENTS <unk> the SMOOTH SPATIALLY VARYING ILLUMINATION VARIATION . the initial PROJECTIVE MODEL PARAMETERS are first estimated by using FEATURE-BASED APPROACH . subsequently , the coefficients of the ILLUMINATION MODEL are determined simultaneously with the PROJECTIVE TRANSFORMATION PARAMETERS through the process of INTENSITY MATCHING . the experimental results demonstrated the algorithm is of ROBUSTNESS , efficiency and ACCURACY . \n",
            "this paper addresses the problem of SMOOTH SPATIALLY VARYING ILLUMINATION VARIATION in a PARAMETRIC PROJECTIVE MODEL . we propose a novel approach to the problem of SMOOTH SPATIALLY VARYING ILLUMINATION VARIATION in IMAGE PROCESSING . the proposed approach is based on a PARAMETRIC PROJECTIVE MODEL that uses a PARAMETRIC PROJECTIVE MODEL to estimate the PROJECTIVE TRANSFORMATION PARAMETERS . the proposed approach is based on the use of a PARAMETRIC PROJECTIVE MODEL , which is able to deal with SMOOTH SPATIALLY VARYING ILLUMINATION VARIATION in the presence of SMOOTH SPATIALLY VARYING ILLUMINATION VARIATION . the proposed method is based on a PARAMETRIC PROJECTIVE MODEL , which is based on a PARAMETRIC PROJECTIVE MODEL . the experimental results show that the proposed method outperforms the state-of-the-art methods in terms of ACCURACY and ROBUSTNESS .\n",
            "\n",
            "642 1000\n",
            "we study an EXPLICIT PARAMETRIC MODEL of DOCUMENTS , QUERIES , and REL-EVANCY ASSESSMENT for INFORMATION RETRIEVAL . MEAN-FIELD METHODS are applied to analyze the model and derive efficient practical algorithms to estimate the parameters in the problem . the HYPERPARAMETERS are estimated by a fast APPROXIMATE LEAVE-ONE-OUT CROSS-VALIDATION PROCEDURE based on the CAVITY METHOD . the APPROXIMATE LEAVE-ONE-OUT CROSS-VALIDATION PROCEDURE is further evaluated on several benchmark databases by comparing with standard algorithms in IR . \n",
            "in this paper , we propose a novel APPROXIMATE LEAVE-ONE-OUT CROSS-VALIDATION PROCEDURE for INFORMATION RETRIEVAL and REL-EVANCY ASSESSMENT . the proposed APPROXIMATE LEAVE-ONE-OUT CROSS-VALIDATION PROCEDURE is based on the use of a CAVITY METHOD for INFORMATION RETRIEVAL and REL-EVANCY ASSESSMENT . the proposed APPROXIMATE LEAVE-ONE-OUT CROSS-VALIDATION PROCEDURE is based on the use of a CAVITY METHOD for INFORMATION RETRIEVAL and REL-EVANCY ASSESSMENT . the proposed APPROXIMATE LEAVE-ONE-OUT CROSS-VALIDATION PROCEDURE is applied to the problem of INFORMATION RETRIEVAL and REL-EVANCY ASSESSMENT . the experimental results show that the proposed APPROXIMATE LEAVE-ONE-OUT CROSS-VALIDATION PROCEDURE is effective in improving the INFORMATION RETRIEVAL performance .\n",
            "\n",
            "643 1000\n",
            "full covariance acoustic models trained with LIMITED TRAINING DATA generalize poorly to UNSEEN TEST DATA due to a large number of FREE PARAMETERS . we propose to use SPARSE INVERSE CO-VARIANCE MATRICES to address this problem . previous SPARSE INVERSE COVARIANCE METHODS never outperformed FULL COVARI-ANCE METHODS . we propose a method to automatically drive the structure of INVERSE COVARIANCE MATRICES to sparse during training . we use a new OBJECTIVE FUNCTION by adding L1 REG-ULARIZATION to the traditional OBJECTIVE FUNCTION for MAXIMUM LIKELIHOOD ESTIMATION.THE GRAPHIC LASSO METHOD for the estimation of a SPARSE INVERSE COVARIANCE MATRIX is incorporated into the EXPECTATION MAXIMIZATION ALGORITHM to learn parameters of HMM using the new OBJECTIVE FUNCTION . experimental results show that we only need about 25 % of the parameters of the INVERSE COVARIANCE MATRICES to be nonzero in order to achieve the same performance of a full covariance system . our proposed system using SPARSE INVERSE COVARIANCE GAUS-SIANS also significantly outperforms a system using FULL CO-VARIANCE GAUSSIANS trained on LIMITED DATA . \n",
            "in this paper , we propose a novel MAXIMUM LIKELIHOOD ESTIMATION.THE GRAPHIC LASSO METHOD for SPARSE INVERSE COVARIANCE GAUS-SIANS . the proposed MAXIMUM LIKELIHOOD ESTIMATION.THE GRAPHIC LASSO METHOD is based on a MAXIMUM LIKELIHOOD ESTIMATION.THE GRAPHIC LASSO METHOD , which is based on the EXPECTATION MAXIMIZATION ALGORITHM . the proposed MAXIMUM LIKELIHOOD ESTIMATION.THE GRAPHIC LASSO METHOD is based on a MAXIMUM LIKELIHOOD ESTIMATION.THE GRAPHIC LASSO METHOD , which is based on a MAXIMUM LIKELIHOOD ESTIMATION.THE GRAPHIC LASSO METHOD . the proposed MAXIMUM LIKELIHOOD ESTIMATION.THE GRAPHIC LASSO METHOD is based on the EXPECTATION MAXIMIZATION ALGORITHM , which is based on the EXPECTATION MAXIMIZATION ALGORITHM . the proposed MAXIMUM LIKELIHOOD ESTIMATION.THE GRAPHIC LASSO METHOD is compared to the FULL COVARI-ANCE METHODS and the FULL COVARI-ANCE METHODS . the performance of the proposed SPARSE INVERSE COVARIANCE METHODS is evaluated on UNSEEN TEST DATA and UNSEEN TEST DATA . the experimental results show that the proposed SPARSE INVERSE COVARIANCE METHODS is superior to other FULL COVARI-ANCE METHODS in terms of both FULL COVARI-ANCE METHODS and FULL COVARI-ANCE METHODS .\n",
            "\n",
            "644 1000\n",
            "it is well known that VIDEO MATERIAL with a STATIC BACKGROUND allows easier segmentation than that with a MOVING BACKGROUND . one approach to SEGMENTATION OF SEQUENCES with a MOVING BACKGROUND is to use PREPROCESSING to create a STATIC BACKGROUND , after which conventional BACKGROUND SUBTRACTION TECHNIQUES can be used for SEGMENTING FOREGROUND OBJECTS . it has been recently shown that GLOBAL MOTION ESTIMATION AND/OR BACKGROUND SPRITE GENERATION TECHNIQUES are reliable . we propose a new BACKGROUND MODELING TECHNIQUE for OBJECT SEGMENTATION using LOCAL BACKGROUND SPRITE GENERATION . experimental results show the excellent performance of this new BACKGROUND MODELING TECHNIQUE compared to recent algorithms proposed . \n",
            "this paper presents a novel BACKGROUND MODELING TECHNIQUE for SEGMENTING FOREGROUND OBJECTS from VIDEO MATERIAL . the proposed BACKGROUND MODELING TECHNIQUE is based on the use of a BACKGROUND MODELING TECHNIQUE for SEGMENTING FOREGROUND OBJECTS . the proposed BACKGROUND MODELING TECHNIQUE is based on a BACKGROUND MODELING TECHNIQUE , which is based on PREPROCESSING . the proposed BACKGROUND MODELING TECHNIQUE is based on the use of PREPROCESSING and PREPROCESSING for SEGMENTING FOREGROUND OBJECTS . the experimental results show that the proposed BACKGROUND MODELING TECHNIQUE is effective and robust to SEGMENTING FOREGROUND OBJECTS .\n",
            "\n",
            "645 1000\n",
            "bounded confidence opinion dynamic models have received much recent interest as models of INFORMATION PROPAGATION in SOCIAL NETWORKS and LOCALIZED DISTRIBUTED AVERAGING . however in the existing literature , opinions are only viewed as abstract quantities rather than as part of a DECISION-MAKING SYSTEM . in this work , OPINION DYNAMICS are examined when agents are BAYESIAN DECISION MAKERS that perform HYPOTHESIS TESTING or SIGNAL DETECTION . BOUNDED CONFIDENCE is defined on PRIOR PROBABILITIES of hypotheses through BAYES RISK ERROR DIVERGENCE , the appropriate measure between PRIORS in HYPOTHESIS TESTING . this definition contrasts with the measure used between opinions in the standard model : absolute error . it is shown that the rapid convergence of PRIOR PROBABILITIES to a small number of LIMITING VALUES is similar to that seen in the standard model . the most interesting finding in this work is that the number of these LIMITING VALUES changes with the SIGNAL-TO-NOISE RATIO in the HYPOTHESIS TESTING TASK . the number of final values or CLUSTERS is maximal at INTERMEDIATE SIGNAL-TO-NOISE RATIOS , suggesting that the most <unk> issues lead to the largest number of <unk> . \n",
            "this paper addresses the problem of SIGNAL DETECTION and SIGNAL DETECTION in SOCIAL NETWORKS . in particular , we focus on the problem of SIGNAL DETECTION and SIGNAL DETECTION . in particular , we focus on the problem of SIGNAL DETECTION and SIGNAL DETECTION . we first show that the LIMITING VALUES of the CLUSTERS can be reduced to the BAYES RISK ERROR DIVERGENCE of the HYPOTHESIS TESTING TASK . we also show that the BAYES RISK ERROR DIVERGENCE of the CLUSTERS can be reduced to the BAYES RISK ERROR DIVERGENCE of the CLUSTERS . we also show that the BAYES RISK ERROR DIVERGENCE of the CLUSTERS can be reduced to the BAYES RISK ERROR DIVERGENCE of the HYPOTHESIS TESTING TASK . we also show that the BAYES RISK ERROR DIVERGENCE of the BAYESIAN DECISION MAKERS can be reduced to the BAYES RISK ERROR DIVERGENCE of the HYPOTHESIS TESTING TASK .\n",
            "\n",
            "646 1000\n",
            "randomized features provide a computationally efficient way to approximate KERNEL MACHINES in MACHINE LEARNING TASKS . however , such methods require a USER-DEFINED KERNEL as input . we extend the RANDOMIZED-FEATURE APPROACH to the task of learning a kernel -lrb- via its associated RANDOM FEATURES -rrb- . specifically , we present an efficient OPTIMIZATION PROBLEM that learns a kernel in a SUPERVISED MANNER . we prove the consistency of the estimated kernel as well as GENERALIZATION BOUNDS for the CLASS OF ESTIMATORS induced by the optimized kernel , and we experimentally evaluate our technique on several datasets . our approach is efficient and highly scalable , and we attain competitive results with a fraction of the TRAINING COST of other techniques . \n",
            "this paper addresses the problem of KERNEL MACHINES for MACHINE LEARNING TASKS . in particular , we consider the problem of MACHINE LEARNING TASKS in a SUPERVISED MANNER . we show that the TRAINING COST of the KERNEL MACHINES can be reduced to the TRAINING COST of the RANDOMIZED-FEATURE APPROACH . we show that the TRAINING COST of the KERNEL MACHINES can be reduced to the TRAINING COST of the RANDOMIZED-FEATURE APPROACH . we also show that the TRAINING COST of the KERNEL MACHINES can be reduced to the TRAINING COST of the RANDOMIZED-FEATURE APPROACH . we also show that the proposed RANDOMIZED-FEATURE APPROACH can be applied to MACHINE LEARNING TASKS .\n",
            "\n",
            "647 1000\n",
            "the aim of this paper is to present a simple yet efficient implementation of a tool for SIMULTANEOUS RULE-BASED MORPHOSYNTACTIC TAGGING and PARTIAL PARSING FORMALISM . the parser is currently used for creating a TREE-BANK OF PARTIAL PARSES in a <unk> acquisition project over the IPI PAN CORPUS OF POLISH . \n",
            "this paper addresses the problem of SIMULTANEOUS RULE-BASED MORPHOSYNTACTIC TAGGING . we propose a novel approach to the problem of SIMULTANEOUS RULE-BASED MORPHOSYNTACTIC TAGGING , which is based on a PARTIAL PARSING FORMALISM and a PARTIAL PARSING FORMALISM . we show that our approach can be applied to the problem of SIMULTANEOUS RULE-BASED MORPHOSYNTACTIC TAGGING . we demonstrate the effectiveness of our approach on the task of SIMULTANEOUS RULE-BASED MORPHOSYNTACTIC TAGGING and IPI PAN CORPUS OF POLISH .\n",
            "\n",
            "648 1000\n",
            "when constructing a CLASSIFIER , the probability of correct classification of future data points should be maximized . in the current paper this CLASSIFIER is translated in a very direct way into an OPTIMIZATION PROBLEM , which is solved using methods from CONVEX OPTIMIZATION . we also show how to exploit MERCER KERNELS in this setting to obtain NONLINEAR DECISION BOUNDARIES . a worst-case bound on the probability of MISCLASSIFICATION OF FUTURE DATA is obtained explicitly . \n",
            "this paper addresses the problem of MISCLASSIFICATION OF FUTURE DATA in the presence of NONLINEAR DECISION BOUNDARIES . in particular , we consider the problem of MISCLASSIFICATION OF FUTURE DATA in the presence of NONLINEAR DECISION BOUNDARIES . we propose a method to estimate the NONLINEAR DECISION BOUNDARIES based on the MERCER KERNELS . the proposed algorithm is based on the use of MERCER KERNELS to estimate the NONLINEAR DECISION BOUNDARIES , and the CLASSIFIER is used to estimate the NONLINEAR DECISION BOUNDARIES . experimental results show that the proposed method can significantly improve the performance of the CLASSIFIER .\n",
            "\n",
            "649 1000\n",
            "spoken dialogue interfaces , mostly <unk> , become more visible in applications where attention needs to be shared with other tasks , such as driving a car . the deployment of the simple DIALOG SYSTEMS , instead of more sophisticated ones , is partly because the COMPUTING PLATFORMS used for such tasks have been less powerful and partly because certain issues from these cognitively challenging tasks have not been well addressed even in the most advanced DIALOG SYSTEMS . this paper reports the progress of our research effort in developing a robust , <unk> , and cognitive <unk> spoken dialog interface called CHAT : CONVERSATIONAL HELPER for AUTOMOTIVE TASKS . our research in the past few years has led to promising results , including high TASK COMPLETION RATE , DIALOG EFFICIENCY , and improved USER EXPERIENCE . \n",
            "this paper addresses the problem of COMPUTING PLATFORMS for COMPUTING PLATFORMS . the goal of this paper is to investigate the effects of SPOKEN DIALOGUE INTERFACES and USER EXPERIENCE on the performance of the DIALOG SYSTEMS . we show that the use of CHAT can improve the performance of DIALOG SYSTEMS . we also show that the use of CHAT improves the performance of SPOKEN DIALOGUE INTERFACES , TASK COMPLETION RATE , DIALOG EFFICIENCY , and USER EXPERIENCE .\n",
            "\n",
            "650 1000\n",
            "contemporary approaches to AUTOMATIC SPEECH SUMMARISATION comprise several components , among them a LINGUISTIC MODEL COMPONENT , which is unrelated to the LANGUAGE MODEL used during the RECOGNITION PROCESS . this LINGUISTIC MODEL COMPONENT assigns a probability to word sequences from the source text according to their likelihood of appearing in the SUMMARISED TEXT . in this paper we investigate LIM TOPIC AND STYLISTIC ADAPTATION using combinations of LIMS each trained on different ADAPTATION DATA . experiments are performed on 9 talks from the TED CORPUS OF EUROSPEECH CONFERENCE PRESENTATIONS , as well as 5 NEWS STORIES from CNN BROADCAST NEWS DATA , for all of which human -lrb- <unk> -rrb- and SPEECH RECOGNISER TRANSCRIPTIONS along with HUMAN SUMMARIES were used . in all asr cases , SUMMARI-SATION ACCURACY -lrb- <unk> -rrb- of AUTOMATICALLY GENERATED SUMMARIES was significantly improved by AUTOMATIC LIM ADAPTATION , with relative improvements of at least 2.5 % in all experiments . \n",
            "this paper presents a novel approach to AUTOMATIC SPEECH SUMMARISATION from CNN BROADCAST NEWS DATA . the proposed approach is based on the use of a set of HUMAN SUMMARIES , a LANGUAGE MODEL , and a LANGUAGE MODEL for the RECOGNITION PROCESS . the proposed approach is based on the use of a LANGUAGE MODEL , which is trained on a small set of ADAPTATION DATA . the proposed method is evaluated on a TED CORPUS OF EUROSPEECH CONFERENCE PRESENTATIONS and a TED CORPUS OF EUROSPEECH CONFERENCE PRESENTATIONS . the results show that the proposed method is effective in improving the SUMMARI-SATION ACCURACY and SUMMARI-SATION ACCURACY of the proposed method .\n",
            "\n",
            "651 1000\n",
            "in practice , most <unk> is done assuming a PROBABILISTIC MODEL of STOCK PRICE RETURNS known as the GEOMETRIC BROWNIAN MOTION . while often an acceptable approximation , the GEOMETRIC BROWNIAN MOTION is not always valid empirically . this motivates a WORST-CASE APPROACH to <unk> , called UNIVERSAL PORTFOLIO MANAGEMENT , where the objective is to maximize wealth relative to the wealth <unk> by the best fixed portfolio in hindsight . in this paper we tie the two approaches , and design an INVESTMENT STRATEGY which is universal in the worst-case , and yet capable of exploiting the mostly valid GEOMETRIC BROWNIAN MOTION . our INVESTMENT STRATEGY is based on new and improved REGRET BOUNDS for ONLINE CONVEX OPTIMIZATION with EXP-CONCAVE LOSS FUNCTIONS . \n",
            "this paper presents a novel INVESTMENT STRATEGY for UNIVERSAL PORTFOLIO MANAGEMENT . the proposed INVESTMENT STRATEGY is based on the WORST-CASE APPROACH , which is based on the WORST-CASE APPROACH . the proposed REGRET BOUNDS is based on the WORST-CASE APPROACH , which is based on the WORST-CASE APPROACH . the proposed REGRET BOUNDS is based on the WORST-CASE APPROACH , which is based on the WORST-CASE APPROACH . the proposed INVESTMENT STRATEGY is applied to the problem of UNIVERSAL PORTFOLIO MANAGEMENT , and the experimental results show that the proposed INVESTMENT STRATEGY is effective in reducing the number of STOCK PRICE RETURNS .\n",
            "\n",
            "652 1000\n",
            "we introduce a NONPARAMETRIC APPROACH for ESTIMATING DRIFT FUNCTIONS in systems of STOCHASTIC DIFFERENTIAL EQUATIONS from SPARSE OBSERVATIONS OF THE STATE VECTOR . using a GAUSSIAN PROCESS prior over the drift as a function of the STATE VECTOR , we develop an APPROXIMATE EM ALGORITHM to deal with the UNOBSERVED , LATENT DYNAMICS between observations . the POSTERIOR OVER STATES is approximated by a PIECEWISE LINEARIZED PROCESS of the ORNSTEIN-UHLENBECK TYPE and the MAP ESTIMATION of the drift is facilitated by a SPARSE GAUSSIAN PROCESS REGRESSION . \n",
            "in this paper , we propose a novel NONPARAMETRIC APPROACH for ESTIMATING DRIFT FUNCTIONS . the proposed NONPARAMETRIC APPROACH is based on a NONPARAMETRIC APPROACH for ESTIMATING DRIFT FUNCTIONS . the proposed NONPARAMETRIC APPROACH is based on a NONPARAMETRIC APPROACH , which is a PIECEWISE LINEARIZED PROCESS . the proposed NONPARAMETRIC APPROACH is based on a NONPARAMETRIC APPROACH , which is a PIECEWISE LINEARIZED PROCESS . the proposed NONPARAMETRIC APPROACH is based on a NONPARAMETRIC APPROACH . the proposed NONPARAMETRIC APPROACH is applied to the problem of ESTIMATING DRIFT FUNCTIONS . the experimental results show that the proposed method is effective in MAP ESTIMATION .\n",
            "\n",
            "653 1000\n",
            "although it is acknowledged that MULTI-WAY DATAFLOW CONSTRAINTS are useful in INTERACTIVE APPLICATIONS , concerns about their tractability have hindered their acceptance . certain LOCAL PROPAGATION ALGORITHMS that solve these constraints are polynomial , others -lrb- such as <unk> -rrb- are exponential . every system handles a specific problem and the influence of any particular RESTRICTION on the COMPUTATIONAL COMPLEXITY is not yet precisely determined . in this paper , we present three theoretical results that allow us to classify existing MULTI-WAY CONSTRAINT PROBLEMS . especially , we prove that the problem handled by SKYBLUE is np-hard . \n",
            "this paper addresses the problem of MULTI-WAY CONSTRAINT PROBLEMS in the presence of MULTI-WAY DATAFLOW CONSTRAINTS . in particular , we focus on the problem of MULTI-WAY CONSTRAINT PROBLEMS in the presence of MULTI-WAY DATAFLOW CONSTRAINTS . in particular , we show that the COMPUTATIONAL COMPLEXITY of the algorithm can be reduced to the problem of MULTI-WAY CONSTRAINT PROBLEMS . we also show that the COMPUTATIONAL COMPLEXITY of the algorithm is bounded by a factor of o -lrb- 1 / √ t -rrb- , where d is the number of MULTI-WAY DATAFLOW CONSTRAINTS . we also show that the COMPUTATIONAL COMPLEXITY of the algorithm is bounded by a factor of o -lrb- 1 / √ t -rrb- .\n",
            "\n",
            "654 1000\n",
            "we show how the PAD E TABLE can be utilized to develop a new LATTICE STRUCTURE for general <unk> <unk> perfect reconstruction -lrb- pr -rrb- LTER banks . this is achieved through characterization of all TWO-CHANNEL BI-ORTHOGONAL PR LTER BANKS . the PARAMETER SPACE found using this method is unique for each LTER BANK . similarly to any other LATTICE STRUCTURE , the PR PROPERTY is achieved structurally and quantization of the parameters of the LATTICE does not eeect this property . furthermore , we demonstrate that for a given LTER , the set of all COMPLEMENTARY LTERS can be uniquely speciied by two parameters , namely the END-TO-END DELAY of the system and a SCALAR QUANTITY . \n",
            "in this paper , we propose a novel method for TWO-CHANNEL BI-ORTHOGONAL PR LTER BANKS . the proposed approach is based on the PR PROPERTY of the LTER BANK . the proposed method is based on the PR PROPERTY of the LTER BANK . the proposed method is based on the PR PROPERTY of the LTER BANK . the proposed method is based on the PR PROPERTY of the LTER BANK . the proposed method is based on the PR PROPERTY of the LTER BANK . the proposed method is applied to the problem of TWO-CHANNEL BI-ORTHOGONAL PR LTER BANKS . the experimental results show that the proposed method outperforms the existing methods in terms of END-TO-END DELAY .\n",
            "\n",
            "655 1000\n",
            "it has been hypothesized that TREE ADJOINING GRAMMAR is particularly well suited for SENTENCE GENERATION . it is unclear , however , how a SENTENCE GENERATION SYSTEM based on TREE ADJOINING GRAMMAR should choose among the SYNTACTIC POSSIBILITIES made available in the GRAMMAR . in this paper we consider the question of what needs to be done to generate with TREE ADJOINING GRAMMAR and explain a GENERATION SYSTEM that provides the necessary FEATURES . this approach is compared with other TAG-BASED GENERATION SYSTEMS . particular attention is given to MUMBLE-86 which , like our GENERATION SYSTEM , makes SYNTACTIC CHOICE on sophisticated functional grounds . \n",
            "this paper presents a novel approach to SENTENCE GENERATION . the proposed approach is based on the use of a TREE ADJOINING GRAMMAR for SENTENCE GENERATION . the proposed approach is based on the use of a TREE ADJOINING GRAMMAR for SENTENCE GENERATION . the proposed method is based on the use of a TREE ADJOINING GRAMMAR for SENTENCE GENERATION . the proposed method is based on the use of a TREE ADJOINING GRAMMAR which is able to deal with the SYNTACTIC POSSIBILITIES of the target language . the proposed approach is evaluated on a number of FEATURES . the experimental results show that the proposed method outperforms the state-of-the-art methods .\n",
            "\n",
            "656 1000\n",
            "it is usually assumed that GRAMMAR PROBABILITIES and ACOUSTIC PROBABILITIES in a CONTINUOUS SPEECH RECOGNITION SYSTEM have to be incorporated to the general score with dierent w <unk> . this is an experimental fact and there is no generally accepted THEORETICAL EXPLANATION . in this paper we propose an explanation to this fact , related to the way GRAMMAR SCORING is incorporated in the SEARCHING PROCEDURE . accordingly to this explanation , we perform a set of experiments to test our hypothesis . we are also proposing a new way o f i n <unk> GRAMMARPROBABILITIES in a TREE-BASED VOCABULARY SEARCH STRATEGY , where systems are usually bound to use the worst strategy . to apply our ideas to UNIGRAMS is rather simple . for more complex LANGUAGE MODELS like BIGRAMS we h a v e t o implement a new procedure . \n",
            "this paper presents a novel approach to GRAMMAR SCORING in a CONTINUOUS SPEECH RECOGNITION SYSTEM . the proposed TREE-BASED VOCABULARY SEARCH STRATEGY consists of a set of BIGRAMS , each of which consists of a GRAMMARPROBABILITIES and an ACOUSTIC PROBABILITIES . the proposed TREE-BASED VOCABULARY SEARCH STRATEGY consists of two steps : -lrb- 1 -rrb- a TREE-BASED VOCABULARY SEARCH STRATEGY that consists of a GRAMMARPROBABILITIES and an ACOUSTIC PROBABILITIES ; -lrb- 2 -rrb- a TREE-BASED VOCABULARY SEARCH STRATEGY that integrates ACOUSTIC PROBABILITIES and ACOUSTIC PROBABILITIES into the SEARCHING PROCEDURE . experimental results show that the proposed approach is effective in improving the performance of the CONTINUOUS SPEECH RECOGNITION SYSTEM in terms of the THEORETICAL EXPLANATION and the ACOUSTIC PROBABILITIES .\n",
            "\n",
            "657 1000\n",
            "skin detection is an important preliminary process in HUMAN MOTION ANALYSIS . it is commonly performed in three steps : transforming the PIXEL COLOR to a NON-RGB COL-ORSPACE , dropping the ILLUMINANCE COMPONENT OF SKIN COLOR , and CLASSIFYING by modeling the SKIN COLOR DISTRIBUTION . in this paper , we evaluate the effect of these three steps on the SKIN DETECTION performance . the importance of this study is a new comprehensive COLORSPACE AND COLOR MODELING TESTING METHODOLOGY that would allow for making the best choices for SKIN DETECTION . combinations of nine <unk> , the presence of the absence of the ILLUMINANCE COMPONENT , and the two COLOR MODEL-ING APPROACHES are compared . the performance is measured by using a RECEIVER OPERATING CHARACTERISTIC CURVE on a large dataset of <unk> images with MANUAL GROUND TRUTH . the results reveal that -lrb- 1 -rrb- COLORSPACE TRANSFORMATIONS can improve performance in certain instances , -lrb- 2 -rrb- the absence of the ILLUMINANCE COMPONENT decreases performance , and -lrb- 3 -rrb- SKIN COLOR MODELING has a greater impact than COLORSPACE TRANSFORMATION . we found that the best performance was obtained by transforming the PIXEL COLOR to the SCT OR HSI COLORSPACES , keeping the ILLUMINANCE COMPONENT , and modeling the color with the HISTOGRAM APPROACH . \n",
            "in this paper , we propose a novel COLORSPACE AND COLOR MODELING TESTING METHODOLOGY for CLASSIFYING . the proposed COLORSPACE AND COLOR MODELING TESTING METHODOLOGY is based on a COLORSPACE AND COLOR MODELING TESTING METHODOLOGY of the SKIN COLOR DISTRIBUTION of the PIXEL COLOR . the proposed COLORSPACE AND COLOR MODELING TESTING METHODOLOGY is based on the COLORSPACE AND COLOR MODELING TESTING METHODOLOGY , which is a COLORSPACE TRANSFORMATION . the proposed COLORSPACE AND COLOR MODELING TESTING METHODOLOGY is based on a COLORSPACE AND COLOR MODELING TESTING METHODOLOGY , which is a COLORSPACE TRANSFORMATION . the proposed COLORSPACE AND COLOR MODELING TESTING METHODOLOGY is applied to SKIN DETECTION , such as SKIN DETECTION and CLASSIFYING . the experimental results show that the proposed COLORSPACE AND COLOR MODELING TESTING METHODOLOGY is more accurate than the conventional HISTOGRAM APPROACH . the proposed COLORSPACE AND COLOR MODELING TESTING METHODOLOGY is compared with the MANUAL GROUND TRUTH and the MANUAL GROUND TRUTH .\n",
            "\n",
            "658 1000\n",
            "question retrieval in current COMMUNITY-BASED QUESTION ANSWERING SERVICES does not , in general , work well for long and complex queries . one of the main difficulties lies in the WORD MISMATCH between queries and candidate questions . existing solutions try to expand the queries at WORD LEVEL , but they usually fail to consider CONCEPT LEVEL ENRICHMENT . in this paper , we explore a PIVOT LANGUAGE TRANSLATION BASED APPROACH to derive the PARAPHRASES OF KEY CONCEPTS . we further propose a UNIFIED QUESTION RETRIEVAL MODEL which integrates the key concepts and their PARAPHRASES for the QUERY QUESTION . experimental results demonstrate that the PIVOT LANGUAGE TRANSLATION BASED APPROACH significantly outperforms the state-of-the-art models in QUESTION RETRIEVAL . \n",
            "this paper presents a novel approach to QUESTION RETRIEVAL in QUESTION RETRIEVAL . the proposed approach is based on the use of PARAPHRASES extracted from the PARAPHRASES OF KEY CONCEPTS of the QUERY QUESTION . the proposed approach is based on the use of PARAPHRASES extracted from PARAPHRASES extracted from the PARAPHRASES OF KEY CONCEPTS . the proposed approach is based on the use of PARAPHRASES extracted from the PARAPHRASES OF KEY CONCEPTS of the QUERY QUESTION . the experimental results show that the proposed method outperforms the state-of-the-art methods in terms of WORD MISMATCH and WORD MISMATCH .\n",
            "\n",
            "659 1000\n",
            "conventional ACOUSTIC MODELS , such as GAUSSIAN MIXTURE MODELS or DEEP NEURAL NETWORKS , can not be reliably estimated when there are very little SPEECH TRAINING DATA , e.g. less than 1 hour . in this paper , we investigate the use of a NON-PARAMETRIC KERNEL DENSITY ESTIMATION METHOD to predict the EMISSION PROBABILITY OF HMM STATES . in addition , we introduce a DISCRIMINATIVE SCORE CALIBRATOR to improve the SPEECH CLASS POSTERIORS generated by the KERNEL DENSITY for SPEECH RECOGNITION TASK . experimental results on the WALL STREET JOURNAL TASK show that the proposed ACOUSTIC MODELS using CROSS-LINGUAL BOTTLENECK FEATURES significantly outperforms GMM AND DNN MODELS for LIMITED TRAINING DATA CASE . \n",
            "in this paper , we propose a novel NON-PARAMETRIC KERNEL DENSITY ESTIMATION METHOD for SPEECH RECOGNITION TASK . the proposed NON-PARAMETRIC KERNEL DENSITY ESTIMATION METHOD is based on a NON-PARAMETRIC KERNEL DENSITY ESTIMATION METHOD , which is based on a NON-PARAMETRIC KERNEL DENSITY ESTIMATION METHOD . the proposed NON-PARAMETRIC KERNEL DENSITY ESTIMATION METHOD is based on the use of DEEP NEURAL NETWORKS and DEEP NEURAL NETWORKS . the proposed NON-PARAMETRIC KERNEL DENSITY ESTIMATION METHOD is based on the use of DEEP NEURAL NETWORKS and DEEP NEURAL NETWORKS . the experimental results show that the proposed method is effective in improving the SPEECH RECOGNITION TASK performance of the ACOUSTIC MODELS . the proposed NON-PARAMETRIC KERNEL DENSITY ESTIMATION METHOD is evaluated on the WALL STREET JOURNAL TASK , and the results show that the proposed method is effective in improving the SPEECH RECOGNITION TASK performance .\n",
            "\n",
            "660 1000\n",
            "<unk> messages pose severe challenges for current SENTIMENT ANALYSIS TECHNIQUES due to some inherent characteristics such as the LENGTH LIMIT and INFORMAL WRITING STYLE . in this paper , we study the problem of extracting OPINION TARGETS of CHINESE MICROBLOG MESSAGES . such FINE-GRAINED WORD-LEVEL TASK has not been well investigated in CHINESE MICROBLOGS yet . we propose an UNSUPERVISED LABEL PROPAGATION ALGORITHM to address the problem . the OPINION TARGETS of all messages in a topic are collectively extracted based on the assumption that similar messages may focus on similar OPINION TARGETS . topics in CHINESE MICROBLOGS are identified by HASHTAGS or using CLUSTERING ALGORITHMS . experimental results on CHINESE MICROBLOGS show the effectiveness of our UNSUPERVISED LABEL PROPAGATION ALGORITHM and algorithms . \n",
            "this paper presents a novel UNSUPERVISED LABEL PROPAGATION ALGORITHM for CHINESE MICROBLOG MESSAGES . the proposed UNSUPERVISED LABEL PROPAGATION ALGORITHM is based on the use of HASHTAGS and OPINION TARGETS . the proposed UNSUPERVISED LABEL PROPAGATION ALGORITHM is based on the use of HASHTAGS and HASHTAGS . the proposed UNSUPERVISED LABEL PROPAGATION ALGORITHM is evaluated on CHINESE MICROBLOGS and CHINESE MICROBLOGS . the results show that the proposed UNSUPERVISED LABEL PROPAGATION ALGORITHM is effective in INFORMAL WRITING STYLE and INFORMAL WRITING STYLE . the performance of the proposed UNSUPERVISED LABEL PROPAGATION ALGORITHM is demonstrated on CHINESE MICROBLOGS and CHINESE MICROBLOGS .\n",
            "\n",
            "661 1000\n",
            "this paper proposes LEARNING-BASED METHODS for MAPPING a sparse representation of noisy SPEECH to STATE LIKELIHOODS in an AUTOMATIC SPEECH RECOGNITION SYSTEM . we represent SPEECH as a sparse linear combination of EXEMPLARS extracted from training data . the weights of EXEMPLARS are mapped to SPEECH STATE LIKELIHOODS using ordinary least squares -lrb- <unk> -rrb- and PARTIAL LEAST SQUARES REGRESSION . RECOGNITION experiments are conducted using the CHIME NOISY SPEECH DATABASE . according to the results , both LEARNING-BASED METHODS can be successfully used for training the MAPPING . we achieve improvements over the previous BINARY LABELING SYSTEM , and RECOGNITION scores close to 70 % AT-6 DB SNR . \n",
            "in this paper , we propose a novel approach to RECOGNITION in the AUTOMATIC SPEECH RECOGNITION SYSTEM . the proposed approach is based on the use of a set of EXEMPLARS , a MAPPING , and a MAPPING . the proposed method consists of two steps : -lrb- 1 -rrb- a MAPPING , and -lrb- 2 -rrb- a MAPPING based on PARTIAL LEAST SQUARES REGRESSION . the proposed method consists of two steps : -lrb- 1 -rrb- a MAPPING based on a MAPPING and PARTIAL LEAST SQUARES REGRESSION ; -lrb- 2 -rrb- a BINARY LABELING SYSTEM is proposed for RECOGNITION . experimental results show that the proposed method is effective in improving the RECOGNITION performance in terms of RECOGNITION and RECOGNITION performance .\n",
            "\n",
            "662 1000\n",
            "in this paper we present a GENERATIVE MODEL AND LEARNING PROCEDURE for UNSUPERVISED VIDEO CLUSTERING into scenes . the work addresses two important problems : realistic mod-eling of the sources of variability in the video and fast transformation invariant frame CLUSTERING . we suggest a solution to the problem of COMPUTATIONALLY INTENSIVE LEARNING in this GENERATIVE MODEL AND LEARNING PROCEDURE by combining the RECURSIVE MODEL ESTIMATION , FAST INFERENCE , and ON-LINE LEARNING . thus , we achieve real time frame CLUSTERING performance . novel aspects of this GENERATIVE MODEL AND LEARNING PROCEDURE include an algorithm for the CLUSTERING OF GAUSSIAN MIXTURES , and the fast computation of the KL DIVERGENCE between two mixtures of GAUSSIANS . the EFFICIENCY and the performance of CLUSTERING and KL APPROXIMATION METHODS are demonstrated . we also present novel VIDEO BROWSING TOOL based on the visualization of the variables in the GENERATIVE MODEL AND LEARNING PROCEDURE . \n",
            "in this paper , we propose a novel GENERATIVE MODEL AND LEARNING PROCEDURE based on RECURSIVE MODEL ESTIMATION and KL APPROXIMATION METHODS . the proposed GENERATIVE MODEL AND LEARNING PROCEDURE is based on the GENERATIVE MODEL AND LEARNING PROCEDURE and KL APPROXIMATION METHODS . the proposed GENERATIVE MODEL AND LEARNING PROCEDURE is based on RECURSIVE MODEL ESTIMATION and KL APPROXIMATION METHODS . the proposed GENERATIVE MODEL AND LEARNING PROCEDURE is based on RECURSIVE MODEL ESTIMATION and KL APPROXIMATION METHODS . the proposed GENERATIVE MODEL AND LEARNING PROCEDURE is applied to UNSUPERVISED VIDEO CLUSTERING , CLUSTERING , CLUSTERING , and FAST INFERENCE . the experimental results demonstrate the effectiveness of the proposed GENERATIVE MODEL AND LEARNING PROCEDURE .\n",
            "\n",
            "663 1000\n",
            "in this paper , the issue of NETWORK TOPOLOGY CONTROL in WIRELESS NETWORKS using a FULLY DISTRIBUTED ALGORITHM is considered . whereas the proposed FULLY DISTRIBUTED ALGORITHM is designed applying GAME THEORY CONCEPTS to design a NON-COOPERATIVE GAME , NETWORK TOPOLOGY CONTROL is guaranteed based on <unk> results of NETWORK TOPOLOGY CONTROL . simulations show that for a relatively low node density , the probability that the proposed FULLY DISTRIBUTED ALGORITHM leads to a CONNECTED NETWORK is close to one . \n",
            "this paper describes a FULLY DISTRIBUTED ALGORITHM for WIRELESS NETWORKS in WIRELESS NETWORKS . the FULLY DISTRIBUTED ALGORITHM is based on GAME THEORY CONCEPTS . the NETWORK TOPOLOGY CONTROL is based on the use of GAME THEORY CONCEPTS for NETWORK TOPOLOGY CONTROL . the FULLY DISTRIBUTED ALGORITHM is based on the use of GAME THEORY CONCEPTS , which is based on a FULLY DISTRIBUTED ALGORITHM . the proposed FULLY DISTRIBUTED ALGORITHM is applied to the NON-COOPERATIVE GAME in a NON-COOPERATIVE GAME . the experimental results show that the proposed FULLY DISTRIBUTED ALGORITHM is effective in improving the NETWORK TOPOLOGY CONTROL performance .\n",
            "\n",
            "664 1000\n",
            "this paper examines the application of LATTICE ADAPTATION TECHNIQUES to SPEAKER-DEPENDENT MODELS for the purpose of CONVERSATIONAL TELEPHONE SPEECH TRANSCRIPTION . given sufficient training data per speaker , it is feasible to build adapted SPEAKER-DEPENDENT MODELS using lattice mllr and lattice map . experiments on ITERATIVE AND CASCADED ADAPTATION are presented . additionally various strategies for THRESHOLDING FRAME POSTERIORS are investigated , and it is shown that accumulating statistics from the LOCAL BEST-CONFIDENCE PATH is sufficient to achieve optimal adaptation . overall , an ITERATIVE CASCADED LATTICE SYSTEM was able to reduce LATTICE ADAPTATION by <unk> % <unk> , which was a 0.8 % <unk> . gain over TRANSCRIPT-BASED ADAPTATION . LATTICE ADAPTATION reduced the UNSUPERVISED/SUPERVISED ADAPTATION GAP from 2.5 % to 1.7 % . \n",
            "this paper proposes a novel ITERATIVE CASCADED LATTICE SYSTEM for CONVERSATIONAL TELEPHONE SPEECH TRANSCRIPTION . the proposed LATTICE ADAPTATION TECHNIQUES is based on the use of SPEAKER-DEPENDENT MODELS for LATTICE ADAPTATION . the proposed LATTICE ADAPTATION TECHNIQUES is based on the use of SPEAKER-DEPENDENT MODELS for LATTICE ADAPTATION . the proposed LATTICE ADAPTATION TECHNIQUES is based on the use of SPEAKER-DEPENDENT MODELS for LATTICE ADAPTATION . the proposed LATTICE ADAPTATION TECHNIQUES is applied to CONVERSATIONAL TELEPHONE SPEECH TRANSCRIPTION . the experimental results show that the proposed TRANSCRIPT-BASED ADAPTATION is effective in improving the performance of TRANSCRIPT-BASED ADAPTATION in CONVERSATIONAL TELEPHONE SPEECH TRANSCRIPTION .\n",
            "\n",
            "665 1000\n",
            "for SEQUENTIAL PROBABILISTIC INFERENCE in NONLINEAR NON-GAUSSIAN SYSTEMS approximate solutions must be used . we present a novel RECURSIVE BAYESIAN ESTIMATION ALGORITHM that combines an importance sampling based MEASUREMENT UPDATE STEP with a bank of SIGMA-POINT KALMAN FILTERS for the TIME-UPDATE AND PROPOSAL DISTRIBUTION GENERATION . the POSTERIOR STATE DENSITY is represented by a GAUSSIAN MIXTURE MODEL that is recovered from the WEIGHTED PARTICLE SET of the MEASUREMENT UPDATE STEP by means of a WEIGHTED EM ALGORITHM . this RECURSIVE BAYESIAN ESTIMATION ALGORITHM replaces the RESAMPLING STAGE needed by most PARTICLE FILTERS and mitigates the '' sample <unk> '' problem . we show that this new RECURSIVE BAYESIAN ESTIMATION ALGORITHM has an improved estimation performance and reduced COMPUTATIONAL COMPLEXITY compared to other related algorithms . \n",
            "this paper proposes a novel RECURSIVE BAYESIAN ESTIMATION ALGORITHM for NONLINEAR NON-GAUSSIAN SYSTEMS . the proposed RECURSIVE BAYESIAN ESTIMATION ALGORITHM is based on a GAUSSIAN MIXTURE MODEL , which is based on a GAUSSIAN MIXTURE MODEL . the proposed RECURSIVE BAYESIAN ESTIMATION ALGORITHM is based on a GAUSSIAN MIXTURE MODEL , which is based on a GAUSSIAN MIXTURE MODEL . the proposed RECURSIVE BAYESIAN ESTIMATION ALGORITHM is based on a GAUSSIAN MIXTURE MODEL , which is based on a GAUSSIAN MIXTURE MODEL . the proposed RECURSIVE BAYESIAN ESTIMATION ALGORITHM is applied to the problem of NONLINEAR NON-GAUSSIAN SYSTEMS , and the results show that the proposed RECURSIVE BAYESIAN ESTIMATION ALGORITHM is effective in reducing the COMPUTATIONAL COMPLEXITY of the RECURSIVE BAYESIAN ESTIMATION ALGORITHM . the proposed RECURSIVE BAYESIAN ESTIMATION ALGORITHM is evaluated on the WEIGHTED PARTICLE SET , and the results show that the proposed RECURSIVE BAYESIAN ESTIMATION ALGORITHM is effective in terms of COMPUTATIONAL COMPLEXITY .\n",
            "\n",
            "666 1000\n",
            "this paper presents an exploratory study on the relations between gender and everyday <unk> . a '' DATA-MINING '' APPROACH is used to explore GENDER-SPECIFIC CHARACTERISTICS in a large number of SPONTANEOUS TELEPHONE AND FACE-TO-FACE CONVERSATIONS . our study focuses on speech rate -lrb- speaking rate and ARTICULATION RATE -rrb- , DISFLUENCIES -lrb- FILLED PAUSES and REPETITIONS -rrb- , PRONUNCIATION VARIATION -lrb- PHONEME SUBSTITUTIONS , DELETIONS and INSERTIONS -rrb- , and preferences for particular parts of speech . our study reveals interesting similarities and differences in EVERYDAY MALE AND FEMALE SPEECH , and proves that <unk> on large SPOKEN LANGUAGE CORPORA is a promising approach for obtaining information on SPONTANEOUS SPEECH PHENOMENA and for generating new hypotheses for research . \n",
            "this paper presents a novel approach to SPOKEN LANGUAGE CORPORA based on the DATA-MINING '' APPROACH . the proposed approach is based on the use of a set of PHONEME SUBSTITUTIONS , DELETIONS , DELETIONS , DELETIONS , INSERTIONS , INSERTIONS , and INSERTIONS . the proposed approach is evaluated on a variety of SPOKEN LANGUAGE CORPORA . the results show that the proposed method is robust to PRONUNCIATION VARIATION , DELETIONS , INSERTIONS , and INSERTIONS .\n",
            "\n",
            "667 1000\n",
            "in this paper , we consider a MULTIHOP WIRELESS SENSOR NETWORK with multiple relay nodes for each <unk> where the AMPLIFY-AND-FORWARD SCHEME is employed . we present a strategy to jointly design the LINEAR RECEIVER and the POWER ALLOCATION PARAMETERS via an ALTERNATING OPTIMIZATION APPROACH that maximizes the SUM-RATE of the MULTIHOP WIRELESS SENSOR NETWORK . we derive CONSTRAINED MAXIMUM SUM-RATE EXPRESSIONS along with an algorithm to compute the LINEAR RECEIVER and the POWER ALLOCATION PARAMETERS with the optimal COMPLEX AMPLIFICATION COEFFICIENTS for each RELAY NODE . computer simulations show good performance of our proposed methods in terms of SUM-RATE compared to the method with EQUAL POWER ALLOCATION . \n",
            "in this paper , we propose a novel AMPLIFY-AND-FORWARD SCHEME for the MULTIHOP WIRELESS SENSOR NETWORK . the proposed ALTERNATING OPTIMIZATION APPROACH is based on a LINEAR RECEIVER , which is based on the SUM-RATE and the POWER ALLOCATION PARAMETERS . the proposed ALTERNATING OPTIMIZATION APPROACH is based on a LINEAR RECEIVER , which is based on a LINEAR RECEIVER . the proposed ALTERNATING OPTIMIZATION APPROACH is based on the ALTERNATING OPTIMIZATION APPROACH and the POWER ALLOCATION PARAMETERS of the LINEAR RECEIVER are used to estimate the POWER ALLOCATION PARAMETERS and the POWER ALLOCATION PARAMETERS . the performance of the proposed AMPLIFY-AND-FORWARD SCHEME is evaluated on a MULTIHOP WIRELESS SENSOR NETWORK .\n",
            "\n",
            "668 1000\n",
            "this paper introduces a new ACOUSTIC MODELING METHOD called GAUSSIAN DYNAMIC WARPING . ACOUSTIC MODELING METHOD is targeting REAL WORLD APPLICATIONS such as VOICE-BASED ENTRANCE DOOR SECURITY SYSTEMS , the example presented in this paper . the proposed ACOUSTIC MODELING METHOD uses a HIERARCHICAL STATISTICAL FRAMEWORK with three levels of SPECIALIZATION for the ACOUSTIC MODELING . the highest level of SPECIALIZATION is in addition responsible for the modeling of the TEMPORAL CONSTRAINTS via a specific TEMPORAL STRUCTURE INFORMATION COMPONENT . the preliminary results show the ability of the GAUSSIAN DYNAMIC WARPING to elegantly take into account the ACOUSTIC VARIABILITY OF SPEECH while capturing important TEMPORAL CONSTRAINTS . \n",
            "this paper presents a novel ACOUSTIC MODELING METHOD , called GAUSSIAN DYNAMIC WARPING , for ACOUSTIC MODELING . the proposed ACOUSTIC MODELING METHOD is based on a HIERARCHICAL STATISTICAL FRAMEWORK with TEMPORAL CONSTRAINTS . the proposed ACOUSTIC MODELING METHOD is based on a HIERARCHICAL STATISTICAL FRAMEWORK with TEMPORAL CONSTRAINTS . the proposed ACOUSTIC MODELING METHOD is based on a HIERARCHICAL STATISTICAL FRAMEWORK with GAUSSIAN DYNAMIC WARPING . the proposed HIERARCHICAL STATISTICAL FRAMEWORK is based on a HIERARCHICAL STATISTICAL FRAMEWORK with TEMPORAL CONSTRAINTS . the proposed ACOUSTIC MODELING METHOD is applied to REAL WORLD APPLICATIONS , and the experimental results demonstrate the effectiveness of the proposed ACOUSTIC MODELING METHOD .\n",
            "\n",
            "669 1000\n",
            "in CONCEPT-BASED SUMMARIZATION , SENTENCE SELECTION is modelled as a BUDGETED MAXIMUM COVERAGE PROBLEM . as this problem is NP-HARD , pruning LOW-WEIGHT CONCEPTS is required for the SOLVER to find optimal solutions efficiently . this work shows that reducing the number of concepts in the model leads to lower rouge scores , and more importantly to the presence of multiple optimal solutions . we address these issues by extending the model to provide a single optimal solution , and eliminate the need for CONCEPT PRUNING using an APPROXIMATION ALGORITHM that achieves comparable performance to exact INFERENCE . \n",
            "this paper presents a novel APPROXIMATION ALGORITHM for SENTENCE SELECTION . the proposed APPROXIMATION ALGORITHM is based on the use of LOW-WEIGHT CONCEPTS for INFERENCE . the proposed APPROXIMATION ALGORITHM is based on the use of LOW-WEIGHT CONCEPTS for SENTENCE SELECTION . the proposed APPROXIMATION ALGORITHM is applied to the problem of SENTENCE SELECTION . the proposed APPROXIMATION ALGORITHM is applied to the problem of SENTENCE SELECTION . the experimental results show that the proposed APPROXIMATION ALGORITHM is effective in improving the performance of CONCEPT-BASED SUMMARIZATION .\n",
            "\n",
            "670 1000\n",
            "multi-resolution SUB-BAND CEPSTRAL FEATURES strive to exploit DISCRIMINATIVE CUES in LOCALISED REGIONS of the spectral domain by supplementing the FULL BANDWITH CEPSTRAL FEATURES with SUB-BAND CEPSTRAL FEATURES derived from several levels of SUB-BAND DECOMPOSITION . MULT-IRESOLUTION FEATURE VECTORS , formed by concatenation of the SUBBAND CEPSTRAL FEATURES into an extended feature vector , are shown to yield better performance than conventional MFCC FEATURES for PHONEME RECOGNITION on the TIMIT DATABASE . possible strategies for the <unk> of PARTIAL RECOGNITION SCORES from INDEPENDENT MULTI-RESOLTUION SUB-BAND MODELS are explored . by exploiting the SUB-BAND VARIATIONS in SIGNAL TO NOISE RATIO for LINEARLY WEIGHTED RECOMBINATION of the LOG LIKELIHOOD PROBABILITIES we obtained improved PHONEME RECOGNITION performance in BROADBAND NOISE compared to MFCC FEATURES . this is an advantage over a purely SUB-BAND APPROACH using NON LINEAR RECOMBINATION which is robust only to NARROW BAND NOISE . \n",
            "this paper addresses the problem of PHONEME RECOGNITION in BROADBAND NOISE . we propose a novel SUB-BAND APPROACH based on a NON LINEAR RECOMBINATION . the proposed approach is based on the use of SUBBAND CEPSTRAL FEATURES and SUBBAND CEPSTRAL FEATURES to estimate the LOG LIKELIHOOD PROBABILITIES . the proposed SUB-BAND APPROACH is based on the use of SUBBAND CEPSTRAL FEATURES and SUBBAND CEPSTRAL FEATURES to estimate the LOG LIKELIHOOD PROBABILITIES . the proposed approach is evaluated on a TIMIT DATABASE and on a TIMIT DATABASE using the TIMIT DATABASE . experimental results show that the proposed method outperforms the state-of-the-art methods in terms of SIGNAL TO NOISE RATIO and BROADBAND NOISE .\n",
            "\n",
            "671 1000\n",
            "neural network training targets for SPEECH RECOGNITION are estimated using a novel method . rather than use zero and one , CONTINUOUS TARGETS are generated using FORWARD-BACKWARD PROBABILITIES . each TRAINING PATTERN has more than one class active . experiments showed that the new method <unk> decreased the ERROR RATE by 15 % in a CONTINUOUS DIGITS RECOGNITION TASK . \n",
            "in this paper , we propose a novel approach to SPEECH RECOGNITION based on FORWARD-BACKWARD PROBABILITIES . the proposed approach is based on the use of FORWARD-BACKWARD PROBABILITIES extracted from the FORWARD-BACKWARD PROBABILITIES . the proposed approach is evaluated on a CONTINUOUS DIGITS RECOGNITION TASK and compared to the state-of-the-art methods . the proposed method is evaluated on a CONTINUOUS DIGITS RECOGNITION TASK and a CONTINUOUS DIGITS RECOGNITION TASK . the results show that the proposed method is effective in improving the ERROR RATE and ERROR RATE of the proposed method .\n",
            "\n",
            "672 1000\n",
            "in this paper , three different approaches to PRONUNCIATION MODELING are investigated . two existing PRONUNCIATION MODELING APPROACHES , namely the PRONUNCIATION MODELING and N-BEST RESCORING APPROACH are modified to work with little amount of NON-NATIVE SPEECH . we also propose a SPEAKER CLUSTERING APPROACH , which capable of grouping the speakers based on their PRONUNCIATION HABITS . given some speech , the SPEAKER CLUSTERING APPROACH can also be used for PRONUNCIATION MODELING . this SPEAKER CLUSTERING APPROACH is called LATENT PRONUNCIATION ANALYSIS . the results show that conventional PRONUNCIATION MODELING perform slightly better than N-BEST LIST RESCORING , while the LATENT PRONUNCIATION ANALYSIS has shown to be beneficial for SPEAKER CLUSTERING , and SPEAKER CLUSTERING APPROACH can produce nearly the same improvement as the PRONUNCIATION DICTIONARY APPROACH , without the need to know the origin of the speaker . \n",
            "this paper addresses the problem of PRONUNCIATION MODELING for NON-NATIVE SPEECH . we propose a novel PRONUNCIATION DICTIONARY APPROACH , which is based on a SPEAKER CLUSTERING APPROACH and an N-BEST RESCORING APPROACH . the proposed PRONUNCIATION MODELING is based on a SPEAKER CLUSTERING APPROACH and a SPEAKER CLUSTERING APPROACH . the proposed PRONUNCIATION MODELING is based on a SPEAKER CLUSTERING APPROACH and the N-BEST RESCORING APPROACH . experimental results show that the PRONUNCIATION MODELING performance of the proposed PRONUNCIATION MODELING can be improved by the proposed PRONUNCIATION DICTIONARY APPROACH .\n",
            "\n",
            "673 1000\n",
            "this paper presents a formulation for UNSUPERVISED LEARNING OF CLUSTERS reflecting multiple causal structure in BINARY DATA . unlike the standard MIXTURE MODEL , a MULTIPLE CAUSE MODEL accounts for OBSERVED DATA by combining assertions from many hidden causes , each of which can pertain to varying degree to any subset of the observable dimensions . a crucial issue is the <unk> for combining beliefs from different CLUSTER-CENTERS in order to generate DATA RECONSTRUCTIONS whose errors are minimized both during RECOGNITION and LEARNING . we demonstrate a weakness inherent to the popular WEIGHTED SUM followed by SIGMOID SQUASHING , and offer an alternative form of the NONLINEARITY . results are presented demonstrating the algorithm 's ability successfully to discover coherent multiple causal <unk> of NOISY TEST DATA and in IMAGES OF PRINTED CHARACTERS . \n",
            "in this paper , we propose a novel method for UNSUPERVISED LEARNING OF CLUSTERS and LEARNING . the proposed approach is based on the MULTIPLE CAUSE MODEL , which is based on the MULTIPLE CAUSE MODEL . the proposed method is based on the MULTIPLE CAUSE MODEL , which is based on the MULTIPLE CAUSE MODEL . the proposed method is based on the MULTIPLE CAUSE MODEL , which is based on the MULTIPLE CAUSE MODEL . the method is based on the MULTIPLE CAUSE MODEL and LEARNING . the proposed method is based on the MULTIPLE CAUSE MODEL , which is based on the MULTIPLE CAUSE MODEL and LEARNING . the proposed method is evaluated on a variety of NOISY TEST DATA , and the results show that the proposed method is effective in improving the RECOGNITION and RECOGNITION .\n",
            "\n",
            "674 1000\n",
            "in this paper we introduce a new distance for robustly matching vectors of 3d rotations . a special representation of 3d rotations , which we coin <unk> quaternion -lrb- <unk> -rrb- , allows us to express this distance as EUCLIDEAN . we apply the distance to the problems of 3D SHAPE RECOGNITION from POINT CLOUDS and 2D OBJECT TRACKING in COLOR VIDEO . for the former , we introduce a HASHING SCHEME for scale and translation which outperforms the previous state-of-the-art approach on a PUBLIC DATASET . for the latter , we incorporate ONLINE SUBSPACE LEARNING with the proposed FAQ REPRESENTATION to highlight the benefits of the new representation . \n",
            "this paper addresses the problem of 2D OBJECT TRACKING and 2D OBJECT TRACKING . we propose a novel approach to the problem of 2D OBJECT TRACKING and 2D OBJECT TRACKING . the proposed approach is based on the use of POINT CLOUDS and ONLINE SUBSPACE LEARNING . the proposed approach is based on the use of POINT CLOUDS and ONLINE SUBSPACE LEARNING . the proposed method is evaluated on a PUBLIC DATASET and a PUBLIC DATASET . the experimental results show that the proposed method is effective in improving the 3D SHAPE RECOGNITION performance .\n",
            "\n",
            "675 1000\n",
            "in this paper a general and eecient approach for representing and classifying image sequences by HIDDEN MARKOV MODELS is presented . a consistent modeling of spatial and temporal information is achieved by extracting DIIERENT LOW LEVEL IMAGE FEATURES . these implicitly convert the IMAGE INTENSITIES into PROBABILITY DENSITY VALUES , while preserving the GEOMETRY OF THE IMAGE . the resulting so called IMAGE DENSITY FUNCTIONS are contained in the states of the HIDDEN MARKOV MODELS . first results of applying the approach to the CLASSIICA-TION OF DYNAMIC HAND GESTURES demonstrate the performance of the modeling . \n",
            "this paper addresses the problem of CLASSIICA-TION OF DYNAMIC HAND GESTURES in a CLASSIICA-TION OF DYNAMIC HAND GESTURES . the main contribution of this paper is to present a method to estimate the CLASSIICA-TION OF DYNAMIC HAND GESTURES of a scene from a single IMAGE INTENSITIES . the method is based on the use of a set of IMAGE DENSITY FUNCTIONS that are used to estimate the PROBABILITY DENSITY VALUES of the IMAGE INTENSITIES . the proposed method is based on the estimation of the PROBABILITY DENSITY VALUES of the IMAGE DENSITY FUNCTIONS and the PROBABILITY DENSITY VALUES of the IMAGE INTENSITIES . the experimental results show that the proposed method outperforms the existing methods .\n",
            "\n",
            "676 1000\n",
            "this paper focuses on the novel task of AUTOMATIC EXTRACTION OF PHRASES related to causes of emotions . the analysis of emotional causes in sentences , where emotions are explicitly indicated through EMOTION KEYWORDS can provide the foundation for research on challenging task of recognition of implicit affect from text . we developed a CORPUS OF EMOTION causes specific for 22 emotions . based on the analysis of this corpus we introduce a method for the DETECTION OF THE LINGUISTIC RELATIONS between an EMOTION and its cause and the extraction of the phrases describing the EMOTION causes . the method employs SYNTACTIC AND DEPENDENCY PARSER and RULES for the analysis of eight types of the EMOTION-CAUSE LINGUISTIC RELATIONS . the results of evaluation showed that our method performed with high level of ACCURACY -lrb- 82 % -rrb- . \n",
            "this paper addresses the problem of AUTOMATIC EXTRACTION OF PHRASES from a single image . we propose a method to estimate the DETECTION OF THE LINGUISTIC RELATIONS from a set of EMOTION KEYWORDS . the proposed approach is based on the use of a set of RULES , RULES , and RULES . the proposed approach is based on the use of RULES and RULES . the proposed approach is evaluated on a number of datasets and compared with the state of the art .\n",
            "\n",
            "677 1000\n",
            "we propose a novel GEOMETRIC MIN-HASHING APPROACH for IMAGE RETRIEVAL , CLUSTERING and AUTOMATIC OBJECT DISCOVERY . unlike commonly used bag-of-words approaches , the SPATIAL EXTENT OF IMAGE FEATURES is exploited in our GEOMETRIC MIN-HASHING APPROACH . the GEOMETRIC INFORMATION is used both to construct REPEATABLE HASH KEYS and to increase the discriminability of the description . each HASH KEY combines VISUAL APPEARANCE -LRB- VISUAL WORDS -rrb- with SEMI-LOCAL GEOMETRIC INFORMATION . compared with the state-of-the-art MIN-HASH , the proposed GEOMETRIC MIN-HASHING APPROACH has both higher RECALL -LRB- PROBABILITY OF COLLISION for HASHES on the same object -rrb- and lower FALSE POSITIVE RATES -lrb- RANDOM COLLISIONS -rrb- . the advantages of GEOMETRIC MIN-HASHING APPROACH are most pronounced in the presence of VIEWPOINT and scale change , significant occlusion or small physical overlap of the viewing fields . we demonstrate the power of the proposed GEOMETRIC MIN-HASHING APPROACH on SMALL OBJECT DISCOVERY in a large unordered collection of IMAGES and on a LARGE SCALE IMAGE CLUSTERING PROBLEM . \n",
            "this paper presents a novel GEOMETRIC MIN-HASHING APPROACH for AUTOMATIC OBJECT DISCOVERY and AUTOMATIC OBJECT DISCOVERY . the proposed GEOMETRIC MIN-HASHING APPROACH is based on a GEOMETRIC MIN-HASHING APPROACH of the HASH KEY , which is a LARGE SCALE IMAGE CLUSTERING PROBLEM . the proposed GEOMETRIC MIN-HASHING APPROACH is based on a GEOMETRIC MIN-HASHING APPROACH of the HASH KEY , which is a LARGE SCALE IMAGE CLUSTERING PROBLEM . the proposed GEOMETRIC MIN-HASHING APPROACH is applied to SMALL OBJECT DISCOVERY and AUTOMATIC OBJECT DISCOVERY . the proposed GEOMETRIC MIN-HASHING APPROACH is evaluated on the LARGE SCALE IMAGE CLUSTERING PROBLEM and AUTOMATIC OBJECT DISCOVERY . the experimental results show that the proposed GEOMETRIC MIN-HASHING APPROACH significantly improves the IMAGE RETRIEVAL performance in terms of IMAGE RETRIEVAL and IMAGE RETRIEVAL .\n",
            "\n",
            "678 1000\n",
            "minimum perfect hashing -lrb- MINIMUM PERFECT HASHING -rrb- has recently been shown successful in reducing LANGUAGE MODEL LOOKAHEAD TIME in LVCSR DECODING . in this paper we propose to exploit the ORDER-PRESERVING PROPERTY of a STRING-KEY BASED MPH FUNCTION to further reduce HASHING OPERATION and speed up LM LOOKAHEAD . a SUBTREE STRUCTURE is proposed for LM LOOKAHEAD and an ORDER-PRESERVING MPH is integrated into the STRUCTURE DESIGN . SUBTREES are generated on demand and stored in caches . experiments were performed on SWITCHBOARD DATA . by using the proposed method of OP MPH AND SUBTREE CACHE STRUCTURE for both TRIGRAMS and <unk> bigrams , the LM LOOKAHEAD TIME was reduced by a factor of 2.9 in comparison with the baseline case of using MINIMUM PERFECT HASHING alone . \n",
            "this paper addresses the problem of MINIMUM PERFECT HASHING for LVCSR DECODING . we propose a novel STRING-KEY BASED MPH FUNCTION based on MINIMUM PERFECT HASHING and MINIMUM PERFECT HASHING . the proposed STRING-KEY BASED MPH FUNCTION is based on MINIMUM PERFECT HASHING and MINIMUM PERFECT HASHING . the proposed STRING-KEY BASED MPH FUNCTION is based on MINIMUM PERFECT HASHING and MINIMUM PERFECT HASHING . the proposed STRING-KEY BASED MPH FUNCTION is applied to SWITCHBOARD DATA , and the experimental results demonstrate the effectiveness of the proposed STRING-KEY BASED MPH FUNCTION .\n",
            "\n",
            "679 1000\n",
            "we consider the estimation of SPARSE GRAPHICAL MODELS that characterize the DEPENDENCY STRUCTURE of HIGH-DIMENSIONAL TENSOR-VALUED DATA . to facilitate the estimation of the PRECISION MATRIX corresponding to each way of the tensor , we assume the data follow a TENSOR NORMAL DISTRIBUTION whose covariance has a KRONECKER PRODUCT STRUCTURE . the PENALIZED MAXIMUM LIKELIHOOD ESTIMATION of this model involves minimizing a NON-CONVEX OBJECTIVE FUNCTION . in spite of the non-convexity of this ESTIMATION PROBLEM , we prove that an ALTERNATING MINIMIZATION ALGORITHM , which iteratively estimates each SPARSE PRECISION MATRIX while fixing the others , attains an ESTIMATOR with the optimal STATISTICAL RATE OF CONVERGENCE as well as CONSISTENT GRAPH RECOVERY . notably , such an ESTIMATOR achieves ESTIMATION CONSISTENCY with only one TENSOR SAMPLE , which is unobserved in previous work . our theoretical results are backed by thorough NUMERICAL STUDIES . \n",
            "this paper addresses the problem of CONSISTENT GRAPH RECOVERY in the presence of HIGH-DIMENSIONAL TENSOR-VALUED DATA in HIGH-DIMENSIONAL TENSOR-VALUED DATA with HIGH-DIMENSIONAL TENSOR-VALUED DATA . we propose a novel ESTIMATOR to the problem of CONSISTENT GRAPH RECOVERY . the proposed ESTIMATOR is based on the KRONECKER PRODUCT STRUCTURE of the TENSOR NORMAL DISTRIBUTION . the proposed ESTIMATOR is based on the KRONECKER PRODUCT STRUCTURE of the TENSOR NORMAL DISTRIBUTION . the proposed ESTIMATOR is based on the KRONECKER PRODUCT STRUCTURE of the TENSOR NORMAL DISTRIBUTION . the proposed ESTIMATOR is evaluated on a TENSOR SAMPLE with HIGH-DIMENSIONAL TENSOR-VALUED DATA . the proposed ESTIMATOR is evaluated on a TENSOR SAMPLE with HIGH-DIMENSIONAL TENSOR-VALUED DATA . the experimental results show that the proposed ESTIMATOR significantly improves the STATISTICAL RATE OF CONVERGENCE of the ESTIMATOR in terms of ESTIMATION CONSISTENCY and STATISTICAL RATE OF CONVERGENCE .\n",
            "\n",
            "680 1000\n",
            "most existing SUBSPACE ANALYSIS-BASED TRACKING ALGORITHMS utilize a FLATTENED VECTOR to represent a target , resulting in a high dimensional data learning problem . recently , SUBSPACE ANALYSIS is incorporated into the MULTILIN-EAR FRAMEWORK which offline constructs a REPRESENTATION OF IMAGE ENSEMBLES using HIGH-ORDER TENSORS . this reduces SPATIO-TEMPORAL REDUNDANCIES substantially , whereas the COMPUTATIONAL AND MEMORY COST is high . in this paper , we present an effective ONLINE TENSOR SUBSPACE LEARNING ALGORITHM which models the appearance changes of a target by incrementally learning a LOW-ORDER TENSOR EIGENSPACE REPRESENTATION through adaptively updating the sample mean and <unk> . TRACKING then is led by the STATE INFERENCE within the framework in which a PARTICLE FILTER is used for propagating sample distributions over the time . a novel LIKELIHOOD FUNCTION , based on the TENSOR RECONSTRUCTION ERROR NORM , is developed to measure the similarity between the test image and the learned TENSOR SUBSPACE MODEL during the TRACKING . THEORETIC ANALYSIS and experimental evaluations against a state-of-the-art method demonstrate the promise and effectiveness of this ONLINE TENSOR SUBSPACE LEARNING ALGORITHM . \n",
            "this paper addresses the problem of TRACKING in a LOW-ORDER TENSOR EIGENSPACE REPRESENTATION . we propose a novel ONLINE TENSOR SUBSPACE LEARNING ALGORITHM based on a LOW-ORDER TENSOR EIGENSPACE REPRESENTATION . the proposed ONLINE TENSOR SUBSPACE LEARNING ALGORITHM consists of two steps : -lrb- 1 -rrb- a TENSOR RECONSTRUCTION ERROR NORM that combines a TENSOR SUBSPACE MODEL with a TENSOR SUBSPACE MODEL ; -lrb- 2 -rrb- a TENSOR SUBSPACE MODEL based on the TENSOR RECONSTRUCTION ERROR NORM . the proposed ONLINE TENSOR SUBSPACE LEARNING ALGORITHM is based on a LOW-ORDER TENSOR EIGENSPACE REPRESENTATION , which is based on the TENSOR RECONSTRUCTION ERROR NORM . the proposed ONLINE TENSOR SUBSPACE LEARNING ALGORITHM is applied to the REPRESENTATION OF IMAGE ENSEMBLES , which is based on the TENSOR RECONSTRUCTION ERROR NORM . experimental results show that the proposed ONLINE TENSOR SUBSPACE LEARNING ALGORITHM is effective in improving the COMPUTATIONAL AND MEMORY COST of the PARTICLE FILTER .\n",
            "\n",
            "681 1000\n",
            "the purpose of this study was to examine typically developing infants ' integration of audiovisual sensory information as a fundamental process involved in EARLY WORD LEARNING . one hundred <unk> <unk> children were randomly assigned to watch one of four <unk> versions of AUDIOVISUAL VIDEO SEQUENCES . the infants ' <unk> were recorded and their looking behavior was analyzed throughout three REPETITIONS OF EXPOSURE-TEST-PHASES . the results indicate that the infants were able to learn covariance between shapes and colors of ARBITRARY GEOMETRICAL OBJECTS and to them corresponding <unk> words . implications of AUDIOVISUAL INTEGRATION in infants and in NON-HUMAN ANIMALS for modeling within SPEECH RECOGNITION SYSTEMS , NEURAL NETWORKS and ROBOTICS are discussed . \n",
            "this paper addresses the problem of EARLY WORD LEARNING and ROBOTICS . we propose a novel approach to the problem of ARBITRARY GEOMETRICAL OBJECTS in AUDIOVISUAL VIDEO SEQUENCES . the proposed approach is based on the use of NEURAL NETWORKS , NEURAL NETWORKS , NEURAL NETWORKS , and ROBOTICS . the proposed approach is based on the use of NEURAL NETWORKS , NEURAL NETWORKS , and ROBOTICS . the experimental results demonstrate the effectiveness of the proposed approach .\n",
            "\n",
            "682 1000\n",
            "segmental dynamic time warping -lrb- dtw -rrb- has been demonstrated to be a useful technique for finding acoustic similarity scores between segments of two SPEECH UTTERANCES . due to its high COMPUTATIONAL REQUIREMENTS , it had to be computed in an OFFLINE MANNER , limiting the applications of the technique . in this paper , we present results of parallelization of this task by distributing the workload in either a STATIC OR DYNAMIC WAY on an 8-PROCESSOR CLUSTER and discuss the trade-offs among different distribution schemes . we show that ONLINE UNSUPERVISED PATTERN DISCOVERY using SEGMENTAL DTW is plausible with as low as 8 processors . this brings the task within reach of today 's general purpose MULTI-CORE SERVERS . we also show results on a 32-PROCESSOR SYSTEM , and discuss factors affecting scalability of our methods . \n",
            "this paper addresses the problem of ONLINE UNSUPERVISED PATTERN DISCOVERY in the presence of SPEECH UTTERANCES . we propose a novel method for ONLINE UNSUPERVISED PATTERN DISCOVERY based on SEGMENTAL DYNAMIC TIME WARPING . the proposed approach is based on a SEGMENTAL DYNAMIC TIME WARPING , which is based on a SEGMENTAL DYNAMIC TIME WARPING . the proposed method is based on the SEGMENTAL DYNAMIC TIME WARPING , which is based on a SEGMENTAL DYNAMIC TIME WARPING . the proposed method is evaluated on a STATIC OR DYNAMIC WAY , and the results show that the proposed method outperforms the existing methods in terms of COMPUTATIONAL REQUIREMENTS and COMPUTATIONAL REQUIREMENTS .\n",
            "\n",
            "683 1000\n",
            "we give an ORACLE-BASED ALGORITHM for the ADVERSARIAL CONTEXTUAL BANDIT PROBLEM , where either contexts are drawn i.i.d. or the sequence of contexts is known a PRIORI , but where the losses are picked <unk> . our ORACLE-BASED ALGORITHM is computationally efficient , assuming access to an OFFLINE OPTIMIZATION ORACLE , and enjoys a regret of order o -lrb- -lrb- <unk> -rrb- 2 3 -lrb- log n -rrb- 1 3 -rrb- , where k is the number of actions , t is the number of ITERATIONS and n is the number of BASELINE POLICIES . our result is the first to break the O -LRB- T 3 4 -RRB- BARRIER that is achieved by recently introduced algorithms . breaking this barrier was left as a major open problem . our analysis is based on the recent RELAXATION BASED APPROACH of <unk> and <unk> -lsb- 7 -rsb- . \n",
            "in this paper , we propose a novel RELAXATION BASED APPROACH for the ADVERSARIAL CONTEXTUAL BANDIT PROBLEM . the proposed RELAXATION BASED APPROACH is based on a RELAXATION BASED APPROACH , which is a generalization of the RELAXATION BASED APPROACH to the ADVERSARIAL CONTEXTUAL BANDIT PROBLEM . the proposed RELAXATION BASED APPROACH is based on a ORACLE-BASED ALGORITHM , which is a generalization of the RELAXATION BASED APPROACH to the ADVERSARIAL CONTEXTUAL BANDIT PROBLEM . the proposed RELAXATION BASED APPROACH is applied to the problem of ADVERSARIAL CONTEXTUAL BANDIT PROBLEM . the proposed RELAXATION BASED APPROACH is applied to the problem of ADVERSARIAL CONTEXTUAL BANDIT PROBLEM . the experimental results show that the proposed RELAXATION BASED APPROACH is able to significantly reduce the number of sources and the BASELINE POLICIES .\n",
            "\n",
            "684 1000\n",
            "finding the least squares -lrb- ls -rrb- solution s to a system of linear equations <unk> = y where h , y are given and s is a VECTOR OF BINARY VARIABLES , is a well known NP-HARD PROBLEM . in this paper , we consider BINARY LS PROBLEMS under the assumption that the COEFSCIENT MATRIX H is also unknown , and lies in a given UNCERTAINTY ELLIPSOID . we show that the corresponding WORST-CASE ROBUST OPTIMIZATION PROBLEM , although NP-HARD , is still amenable to SEMIDESNITE RELAXATION - based approximations . however , the RELAXATION STEP is not obvious , and requires a certain PROBLEM REFORMULATION to be efſcient . the proposed RELAXATION STEP is motivated using LAGRANGIAN DUALITY and simulations suggest that RELAXATION STEP performs well , offering a robust alternative over the traditional SDR APPROACHES for BINARY LS PROBLEMS . \n",
            "this paper addresses the problem of PROBLEM REFORMULATION for BINARY LS PROBLEMS . in particular , we focus on the problem of PROBLEM REFORMULATION , where the UNCERTAINTY ELLIPSOID is a WORST-CASE ROBUST OPTIMIZATION PROBLEM . we show that the WORST-CASE ROBUST OPTIMIZATION PROBLEM can be solved efficiently by using LAGRANGIAN DUALITY . we show that the WORST-CASE ROBUST OPTIMIZATION PROBLEM can be efficiently solved using LAGRANGIAN DUALITY . we also show that the WORST-CASE ROBUST OPTIMIZATION PROBLEM can be efficiently solved using LAGRANGIAN DUALITY . we also show that the WORST-CASE ROBUST OPTIMIZATION PROBLEM can be efficiently solved using LAGRANGIAN DUALITY . we also show that the WORST-CASE ROBUST OPTIMIZATION PROBLEM can be reduced to a WORST-CASE ROBUST OPTIMIZATION PROBLEM .\n",
            "\n",
            "685 1000\n",
            "in this paper we give a general analysis of DYADIC DEONTIC LOGICS that were introduced in the early <unk> to formalize DEONTIC REASONING about SUBIDEAL BEHAVIOR . recently it was observed that DYADIC DEONTIC LOGICS are closely related to NON-MONOTONIC LOGICS , theories of diagnosis and DECISION THEORIES . in particular , we argue that two types of DEFEASIBILITY must be distinguished in a DEFEASIBLE DEONTIC LOGIC : OVERRIDDEN DEFEASI-BILITY that formalizes cancelling of an <unk> by other CONDITIONAL OBLIGATIONS and FACTUAL DEFEASIBILITY that formalizes <unk> of an <unk> by a violating fact . we also show that this distinction is essential for an adequate analysis of notorious ` <unk> ' of DEONTIC LOGIC such as the CHISHOLM AND FOR-RESTER ` PARADOXES ' . \n",
            "this paper addresses the problem of DEONTIC REASONING for DEFEASIBLE DEONTIC LOGIC . the main idea is to design a OVERRIDDEN DEFEASI-BILITY for the DEONTIC LOGIC , which is a OVERRIDDEN DEFEASI-BILITY . the proposed approach is based on the use of DYADIC DEONTIC LOGICS , which is a generalization of the well-known DYADIC DEONTIC LOGICS . the proposed approach is based on the use of DYADIC DEONTIC LOGICS , which is a OVERRIDDEN DEFEASI-BILITY . the proposed approach is based on the use of DYADIC DEONTIC LOGICS , which is a generalization of the OVERRIDDEN DEFEASI-BILITY . the proposed approach is evaluated on a variety of DYADIC DEONTIC LOGICS , and the results show that the proposed method is effective in reducing the number of DECISION THEORIES .\n",
            "\n",
            "686 1000\n",
            "while many recent HAND POSE ESTIMATION METHODS critically rely on a training set of labelled frames , the creation of such a dataset is a challenging task that has been overlooked so far . as a result , existing datasets are limited to a few sequences and individuals , with limited ACCURACY , and this prevents these methods from delivering their full potential . we propose a SEMI-AUTOMATED METHOD for efficiently and accurately LABELING each frame of a HAND DEPTH VIDEO with the corresponding 3D LOCATIONS of the joints : the user is asked to provide only an estimate of the 2d <unk> of the visible joints in some REFERENCE FRAMES , which are automatically selected to minimize the LABELING work by efficiently optimizing a SUB-MODULAR LOSS FUNCTION . we then exploit SPATIAL , TEMPORAL , AND APPEARANCE CONSTRAINTS to retrieve the full 3d poses of the hand over the complete sequence . we show that this data can be used to train a recent state-of-the-art HAND POSE ESTIMATION METHOD , leading to increased ACCURACY . the HAND POSE ESTIMATION METHOD and dataset can be found on our website <unk> . AT/PROJECTS/HAND _ DETECTION / . \n",
            "this paper proposes a novel HAND POSE ESTIMATION METHOD for HAND DEPTH VIDEO in HAND DEPTH VIDEO . the proposed HAND POSE ESTIMATION METHOD is based on a SUB-MODULAR LOSS FUNCTION , which is based on a SUB-MODULAR LOSS FUNCTION . the proposed HAND POSE ESTIMATION METHOD is based on a SUB-MODULAR LOSS FUNCTION , which is based on a SUB-MODULAR LOSS FUNCTION . the proposed HAND POSE ESTIMATION METHOD is applied to the HAND DEPTH VIDEO of the HAND DEPTH VIDEO . the proposed HAND POSE ESTIMATION METHOD is applied to HAND DEPTH VIDEO , and the results show that the proposed HAND POSE ESTIMATION METHOD is effective in improving the ACCURACY of the HAND POSE ESTIMATION METHODS .\n",
            "\n",
            "687 1000\n",
            "by now it is widely accepted that LEARNING a task from scratch , i.e. , without any PRIOR KNOWLEDGE , is a daunting <unk> . humans , however , rarely attempt to learn from scratch . they extract INITIAL BIASES as well as strategies how to approach a LEARNING PROBLEM from instructions and/or DEMONSTRATIONS of other humans . for LEARNING PROBLEM , this paper investigates how LEARNING from demonstration can be applied in the context of REINFORCEMENT LEARNING . we consider priming the Q-FUNCTION , the VALUE FUNCTION , the POLICY , and the model of the task dynamics as possible areas where DEMONSTRATIONS can speed up LEARNING . in general NONLINEAR LEARNING PROBLEMS , only MODEL-BASED REINFORCEMENT LEARNING shows significant speed-up after a demonstration , while in the special case of LINEAR QUADRATIC REGULATOR PROBLEMS , all methods profit from the demonstration . in an implementation of POLE BALANCING on a complex <unk> robot arm , we demonstrate that , when facing the complexities of REAL SIGNAL PROCESSING , MODEL-BASED REINFORCEMENT LEARNING offers the most ROBUSTNESS for LINEAR QUADRATIC REGULATOR PROBLEMS . using the suggested methods , the robot learns POLE BALANCING in just a single trial after a 30 second long demonstration of the HUMAN INSTRUCTOR . \n",
            "this paper addresses the problem of MODEL-BASED REINFORCEMENT LEARNING for NONLINEAR LEARNING PROBLEMS . in particular , we focus on the problem of LEARNING in the presence of INITIAL BIASES . in particular , we focus on the problem of LEARNING for LINEAR QUADRATIC REGULATOR PROBLEMS . we show that the LEARNING PROBLEM can be formulated as a LEARNING PROBLEM , which is a LEARNING PROBLEM . we show that this problem can be solved efficiently using MODEL-BASED REINFORCEMENT LEARNING . we also show that the LEARNING PROBLEM can be solved efficiently using MODEL-BASED REINFORCEMENT LEARNING . we also show that the LEARNING PROBLEM can be solved efficiently using MODEL-BASED REINFORCEMENT LEARNING .\n",
            "\n",
            "688 1000\n",
            "contrary to popular belief , we show that the optimal parameters for IBM MODEL 1 are not unique . we demonstrate that , for a large class of words , IBM MODEL 1 is <unk> among a continuum of ways to allocate probability mass to their translations . we study the magnitude of the variance in OPTIMAL MODEL PARAMETERS using a LINEAR PROGRAMMING APPROACH as well as multiple RANDOM TRIALS , and demonstrate that IBM MODEL 1 results in variance in TEST SET LOG-LIKELIHOOD and ALIGNMENT ERROR RATE . \n",
            "this paper presents a novel LINEAR PROGRAMMING APPROACH for OPTIMAL MODEL PARAMETERS . the proposed LINEAR PROGRAMMING APPROACH is based on a LINEAR PROGRAMMING APPROACH for OPTIMAL MODEL PARAMETERS . the proposed LINEAR PROGRAMMING APPROACH is based on a LINEAR PROGRAMMING APPROACH for OPTIMAL MODEL PARAMETERS . the proposed LINEAR PROGRAMMING APPROACH is evaluated in terms of ALIGNMENT ERROR RATE and ALIGNMENT ERROR RATE . the experimental results show that the proposed IBM MODEL 1 can be reduced by up to 20 % .\n",
            "\n",
            "689 1000\n",
            "from an AUDIO PERSPECTIVE , the present state of TELECONFERENCING TECHNOLOGY leaves something to be desired ; SPEAKER OVERLAP is one of the causes of this inadequate performance . to that end , this paper presents a frequency-domain implementation of CONVOLUTIVE BSS specifically designed for the nature of the <unk> environment . in addition to presenting a novel DEPERMUTATION SCHEME , this paper presents a LEAST-SQUARES POST-PROCESSING SCHEME , which exploits segments during which only a subset of all speakers are active . experiments with SIMULATED AND REAL DATA demonstrate the ability of the proposed LEAST-SQUARES POST-PROCESSING SCHEME to provide SIRS at or near that of the ADAPTIVE NOISE CANCELLATION SOLUTION which is obtained under <unk> assumptions that the ADAPTIVE NOISE CANCELLATION SOLUTION are adapted with one source being on at a time . \n",
            "this paper presents a novel LEAST-SQUARES POST-PROCESSING SCHEME for CONVOLUTIVE BSS . the proposed LEAST-SQUARES POST-PROCESSING SCHEME is based on a LEAST-SQUARES POST-PROCESSING SCHEME of the SIRS . the proposed LEAST-SQUARES POST-PROCESSING SCHEME is based on a LEAST-SQUARES POST-PROCESSING SCHEME of the SIRS . the proposed LEAST-SQUARES POST-PROCESSING SCHEME is based on a LEAST-SQUARES POST-PROCESSING SCHEME , which is based on a LEAST-SQUARES POST-PROCESSING SCHEME . the proposed LEAST-SQUARES POST-PROCESSING SCHEME is evaluated on both SIMULATED AND REAL DATA . the experimental results on SIMULATED AND REAL DATA show that the proposed LEAST-SQUARES POST-PROCESSING SCHEME is able to significantly improve the performance of the DEPERMUTATION SCHEME .\n",
            "\n",
            "690 1000\n",
            "experimental studies of INTERACTIVE LANGUAGE USE have shed light on the COGNITIVE AND INTERPERSONAL PROCESSES that shape conversation ; corpora are the emergent products of these processes . i will survey studies that focus on <unk> aspects of INTERACTIVE LANGUAGE USE , including the processing of SPONTANEOUS SPEECH and disfluencies ; METALINGUISTIC DISPLAYS such as HEDGES ; INTERACTIVE PROCESSES that affect choices of REFERRING EXPRESSIONS ; and how COMMUNICATION MEDIA SHAPE CONVERSATIONS . the findings suggest some <unk> for COMPUTATIONAL LINGUISTICS . \n",
            "this paper addresses the problem of COMMUNICATION MEDIA SHAPE CONVERSATIONS in SPONTANEOUS SPEECH , such as HEDGES and HEDGES . we present a method to estimate the REFERRING EXPRESSIONS of a scene from a single image . our approach is based on the use of a set of REFERRING EXPRESSIONS , a HEDGES , and a HEDGES . we show that our algorithm is able to detect and track objects in a scene from a single image . we demonstrate the effectiveness of our method on a variety of SPONTANEOUS SPEECH .\n",
            "\n",
            "691 1000\n",
            "recent SPIKING NETWORK MODELS of BAYESIAN INFERENCE and UNSUPERVISED LEARNING frequently assume either inputs to arrive in a special format or employ complex computations in NEURONAL ACTIVATION FUNCTIONS and SYNAPTIC PLASTICITY RULES . here we show in a rigorous MATHEMATICAL TREATMENT how HOMEOSTATIC PROCESSES , which have previously received little attention in this context , can overcome common THEORETICAL LIMITATIONS and facilitate the NEURAL IMPLEMENTATION and performance of existing models . in particular , we show that HOMEOSTATIC PLASTICITY can be understood as the enforcement of a ` BALANCING ' POSTERIOR CONSTRAINT during PROBABILIS-TIC INFERENCE and LEARNING with EXPECTATION MAXIMIZATION . we link HOMEOSTATIC DYNAMICS to the theory of VARIATIONAL INFERENCE , and show that NONTRIVIAL TERMS , which typically appear during PROBABILISTIC INFERENCE in a large class of models , drop out . we demonstrate the feasibility of our approach in a spiking <unk> architecture of BAYESIAN INFERENCE and LEARNING . finally , we sketch how the MATHEMATICAL TREATMENT can be extended to richer RECURRENT NETWORK ARCHI-TECTURES . altogether , our theory provides a novel perspective on the interplay of HOMEOSTATIC PROCESSES and SYNAPTIC PLASTICITY in CORTICAL MICROCIRCUITS , and points to an essential role of HOMEOSTASIS during INFERENCE and LEARNING in SPIKING NETWORKS . \n",
            "this paper addresses the problem of PROBABILISTIC INFERENCE in CORTICAL MICROCIRCUITS . in particular , we consider the problem of PROBABILISTIC INFERENCE in CORTICAL MICROCIRCUITS , where the number of NONTRIVIAL TERMS is large and the number of NONTRIVIAL TERMS is small . in this paper , we propose a novel algorithm that combines the advantages of EXPECTATION MAXIMIZATION and RECURRENT NETWORK ARCHI-TECTURES . the proposed algorithm is based on the use of HOMEOSTASIS and SYNAPTIC PLASTICITY RULES . the proposed algorithm is based on the use of EXPECTATION MAXIMIZATION and SYNAPTIC PLASTICITY RULES . the proposed algorithm is based on the use of HOMEOSTASIS and SYNAPTIC PLASTICITY RULES . the algorithm is based on the use of EXPECTATION MAXIMIZATION and SYNAPTIC PLASTICITY RULES . the algorithm is illustrated on a variety of SPIKING NETWORKS and PROBABILIS-TIC INFERENCE .\n",
            "\n",
            "692 1000\n",
            "from the results of the NIST SPEAKER RECOGNITION EVALUATION in <unk> years , SPEAKER RECOGNITION SYSTEMS which are mainly developed based on ENGLISH TRAINING DATA suffer the LANGUAGE GAP PROBLEM , namely , the performance of <unk> <unk> is much worse than that of ENGLISH TRAILS . this problem is addressed in this paper . based on the conventional JOINT FACTOR ANALYSIS MODEL , we enrolled in the LANGUAGE FACTORS which are mean to capture the language character of each testing and training speech utterance , and compensation was carried out by removing the LANGUAGE FACTORS in order to shrink the difference between languages . experiments on 2006 nist sre data show that , the LANGUAGE FACTOR COMPENSATION alone can reduce the gap between the performance of english and <unk> <unk> , and the score level combination with <unk> can further improve the performance of <unk> <unk> , e.g. , for female part , we observed about 19 % relatively reduction in EER , when compared with EIGENCHANNELS SESSION VARIABILITY COMPENSATION alone . \n",
            "this paper addresses the problem of EIGENCHANNELS SESSION VARIABILITY COMPENSATION in the context of SPEAKER RECOGNITION SYSTEMS . in particular , we propose a novel approach to the problem of EIGENCHANNELS SESSION VARIABILITY COMPENSATION . the proposed approach is based on the use of a JOINT FACTOR ANALYSIS MODEL which is able to deal with LANGUAGE FACTORS . the proposed approach is based on the use of a JOINT FACTOR ANALYSIS MODEL , which is able to deal with LANGUAGE FACTORS . the proposed approach is evaluated on the NIST SPEAKER RECOGNITION EVALUATION and NIST SPEAKER RECOGNITION EVALUATION . the experimental results show that the proposed approach is effective in improving the EER and EER of the proposed method .\n",
            "\n",
            "693 1000\n",
            "to learn the <unk> VISUAL ATTENTION given by humans to specific IMAGE CONTENT , we present an EYE FIXATION DATABASE compiled from a pool of <unk> images and 75 subjects . EYE FIXATIONS are an excellent modality to learn SEMANTICS-DRIVEN HUMAN UNDERSTANDING OF IMAGES , which is vastly different from FEATURE-DRIVEN APPROACHES employed by SALIENCY COMPUTATION ALGORITHMS . the EYE FIXATION DATABASE comprises FIXATION PATTERNS acquired using an EYE-TRACKER , as subjects <unk> images corresponding to many SEMANTIC CATEGORIES such as faces -lrb- human and <unk> -rrb- , <unk> and actions -lrb- look , read and <unk> -rrb- . the consistent presence of FIXATION CLUSTERS around specific image regions confirms that VISUAL ATTENTION is not subjective , but is directed towards SALIENT OBJECTS and <unk> . we then show how the FIXATION CLUSTERS can be exploited for enhancing IMAGE UNDERSTANDING , by using our EYE FIXATION DATABASE in an ACTIVE IMAGE SEGMENTATION APPLICATION . apart from proposing a mechanism to automatically determine CHARACTERISTIC FIXATION SEEDS for SEGMENTATION , we show that the use of FIXATION SEEDS generated from multiple FIXATION CLUSTERS on the SALIENT OBJECT can lead to a 10 % improvement in segmen-tation performance over the state-of-the-art . \n",
            "this paper presents a novel approach to SEMANTICS-DRIVEN HUMAN UNDERSTANDING OF IMAGES from IMAGE CONTENT . the proposed approach is based on the use of a SALIENT OBJECT to represent the IMAGE CONTENT . the proposed approach is based on a EYE-TRACKER that uses a EYE-TRACKER to estimate the IMAGE CONTENT from the IMAGE CONTENT . the proposed approach is based on the use of a SALIENT OBJECT to estimate the SALIENT OBJECTS , and the SEMANTIC CATEGORIES for SEGMENTATION . the proposed method is evaluated on a EYE FIXATION DATABASE using the EYE FIXATION DATABASE . the experimental results show that the proposed method outperforms the state-of-the-art methods in terms of SEGMENTATION and SEGMENTATION .\n",
            "\n",
            "694 1000\n",
            "-- in this paper , we review some recent advances in the design of ADCS that exploit SYSTEM-DRIVEN METRICS , such as the BIT-ERROR RATE in a COMMUNICATION LINK , or MUTUAL INFORMATION in a scheme employing FORWARD ERROR CORRECTION . we show , for example , that ADCS can be designed that maximize the INFORMATION RATE between the quantized output of the channel and the input to the channel for COMMUNICATION links with <unk> and additive noise . these ADCS dramatically outper-form -lrb- in terms of achievable information rates -rrb- traditional ADC DESIGN METHODS that are based on FIXED UNIFORM QUANTIZATION . architectures are also developed for ADCS such that ADCS can be used to dynamically adapt the structure of the ADCS to optimize APPLICATION MEANINGFUL CRITERIA , such as BIT-ERROR RATE for COMMUNICATION over INTERSYMBOL INTERFERENCE LINKS . \n",
            "in this paper , we propose a novel approach to FORWARD ERROR CORRECTION based on FIXED UNIFORM QUANTIZATION . the proposed approach is based on the use of ADCS , which is able to deal with INTERSYMBOL INTERFERENCE LINKS , such as MUTUAL INFORMATION and INTERSYMBOL INTERFERENCE LINKS . the proposed method is based on the use of ADCS , which is able to deal with INTERSYMBOL INTERFERENCE LINKS . the proposed method is based on the use of ADCS , which is able to deal with ADCS such as MUTUAL INFORMATION . the experimental results show that the proposed method achieves better performance than the state-of-the-art methods .\n",
            "\n",
            "695 1000\n",
            "traditional techniques of DENSE OPTICAL FLOW ESTIMATION do n't generally yield SYMMETRICAL SOLUTIONS : the results will differ if SYMMETRICAL SOLUTIONS are applied between IMAGES I1 and I2 or between IMAGES I2 and I1 . in this work , we present a method to recover a DENSE OPTICAL FLOW FIELD MAP from two IMAGES , while <unk> taking into account the symmetry across the IMAGES as well as possible occlusions and discontinuities in the FLOW FIELD . the idea is to consider both DISPLACEMENTS VECTORS from I1 to I2 and I2 to I1 and to minimise an ENERGY FUNCTIONAL that <unk> encodes all those properties . this VARIATIONAL PROBLEM is then solved using the GRADIENT FLOW defined by the EULER -- LAGRANGE EQUATIONS associated to the energy . in order to reduce the risk to be trapped within some irrelevant minimum , a FOCUSING STRATEGY based on a MULTI-RESOLUTION TECHNIQUE is used to converge toward the solution . promising experimental results on both SYNTHETIC AND REAL IMAGES are presented to illustrate the capabilities of this symmetrical variational approach to recover accurate OPTICAL FLOW . \n",
            "this paper addresses the problem of DENSE OPTICAL FLOW ESTIMATION from a single image . we propose a novel method to learn a FLOW FIELD from a set of DISPLACEMENTS VECTORS . the proposed method is based on a MULTI-RESOLUTION TECHNIQUE , which is based on a MULTI-RESOLUTION TECHNIQUE . the proposed approach is based on a MULTI-RESOLUTION TECHNIQUE , which is a MULTI-RESOLUTION TECHNIQUE for DENSE OPTICAL FLOW ESTIMATION . the proposed approach is based on a MULTI-RESOLUTION TECHNIQUE , which is based on a MULTI-RESOLUTION TECHNIQUE . the proposed approach is evaluated on SYNTHETIC AND REAL IMAGES , I2 , I2 , I2 , I2 , and I1 .\n",
            "\n",
            "696 1000\n",
            "we present a novel approach to RELATIVE POSE ESTIMATION which is tailored to 4D LIGHT FIELD CAMERAS . from the relationships between SCENE GEOMETRY and LIGHT FIELD STRUCTURE and an analysis of the LIGHT FIELD PROJECTION in terms of PLÜCKER RAY COORDINATES , we deduce a set of LINEAR CONSTRAINTS on RAY SPACE CORRESPONDENCES between a pair of LIGHT FIELD CAMERAS . these can be applied to infer RELATIVE POSE OF THE LIGHT FIELD CAMERAS and thus obtain a POINT CLOUD RECONSTRUCTION OF THE SCENE . while the proposed method has interesting relationships to pose estimation for GENERALIZED CAMERAS based on RAY-TO-RAY CORRESPONDENCE , our experiments demonstrate that our approach is both more accurate and computationally more efficient . it also compares favorably to DIRECT LINEAR POSE ESTIMATION based on aligning the 3D POINT CLOUDS obtained by reconstructing depth for each individual light field . to further validate the method , we employ the POSE ESTIMATES to merge LIGHT FIELDS captured with HAND-HELD CONSUMER LIGHT FIELD CAMERAS into REFOCUS-ABLE PANORAMAS . \n",
            "this paper addresses the problem of DIRECT LINEAR POSE ESTIMATION in HAND-HELD CONSUMER LIGHT FIELD CAMERAS . we propose a novel approach to DIRECT LINEAR POSE ESTIMATION in HAND-HELD CONSUMER LIGHT FIELD CAMERAS . the proposed approach is based on the use of RAY SPACE CORRESPONDENCES in the form of a POINT CLOUD RECONSTRUCTION OF THE SCENE . the proposed method consists of two steps : -lrb- 1 -rrb- the LIGHT FIELD STRUCTURE in the PLÜCKER RAY COORDINATES ; -lrb- 2 -rrb- the use of RAY SPACE CORRESPONDENCES to estimate the LIGHT FIELD STRUCTURE , and -lrb- 3 -rrb- the LIGHT FIELD STRUCTURE . the proposed method is based on the use of RAY SPACE CORRESPONDENCES in the PLÜCKER RAY COORDINATES . the proposed approach is evaluated on a variety of HAND-HELD CONSUMER LIGHT FIELD CAMERAS . the results show that the proposed method is effective and robust to SCENE GEOMETRY in the presence of 3D POINT CLOUDS .\n",
            "\n",
            "697 1000\n",
            "we find a close relationship between the DISCRETE KARHUNEN-LOEVE TRANSFORM and the DISCRETE PROLATE SPHEROIDAL WAVE FUNCTIONS . we show that the DISCRETE PROLATE SPHEROIDAL WAVE FUNCTIONS form a natural basis for an expansion of the eigenfunctions of the DISCRETE KARHUNEN-LOEVE TRANSFORM in the FREQUENCY DOMAIN , and then determine more general conditions that any set of functions must obey to be a valid basis . we also present APPROXIMATE SOLUTIONS for small , medium , and large filter orders . the MEDIUM ORDER SOLUTION suggests that the PRINCIPAL EIGENFUNC-TION is , to a high degree of APPROXIMATION , the principal DISCRETE PROLATE SPHEROIDAL WAVE FUNCTIONS modulated so that its CENTER FREQUENCY coincides with the peak of maximum energy in the SIGNAL SPECTRUM . we then use this result to propose a new basis . \n",
            "this paper addresses the problem of recovering a SIGNAL SPECTRUM from a SIGNAL SPECTRUM . in this paper , we propose a novel method to estimate the SIGNAL SPECTRUM of the SIGNAL SPECTRUM . the proposed method is based on the DISCRETE KARHUNEN-LOEVE TRANSFORM and the DISCRETE PROLATE SPHEROIDAL WAVE FUNCTIONS . the proposed method is based on the DISCRETE KARHUNEN-LOEVE TRANSFORM and the DISCRETE PROLATE SPHEROIDAL WAVE FUNCTIONS . in the proposed method , the SIGNAL SPECTRUM of the SIGNAL SPECTRUM and the DISCRETE PROLATE SPHEROIDAL WAVE FUNCTIONS are estimated by the proposed method . experimental results show that the proposed method outperforms the existing methods in terms of both CENTER FREQUENCY and DISCRETE PROLATE SPHEROIDAL WAVE FUNCTIONS .\n",
            "\n",
            "698 1000\n",
            "an INCREMENTAL NETWORK MODEL is introduced which is able to learn the important TOPOLOGICAL RELATIONS in a given set of input vectors by means of a simple HEBB-LIKE LEARNING RULE . in contrast to previous approaches like the `` NEURAL GAS '' METHOD of <unk> and <unk> -lrb- 1991 , 1994 -rrb- , this `` NEURAL GAS '' METHOD has no parameters which change over time and is able to continue learning , adding units and connections , until a PERFORMANCE CRITERION has been met . applications of the `` NEURAL GAS '' METHOD include VECTOR QUANTIZATION , CLUSTERING , and INTERPOLATION . \n",
            "this paper proposes a novel INCREMENTAL NETWORK MODEL for CLUSTERING . the proposed `` NEURAL GAS '' METHOD consists of two steps : -lrb- 1 -rrb- a PERFORMANCE CRITERION , 2 -rrb- a PERFORMANCE CRITERION , and -lrb- 2 -rrb- a PERFORMANCE CRITERION based on the INCREMENTAL NETWORK MODEL . the proposed `` NEURAL GAS '' METHOD consists of two steps : -lrb- 1 -rrb- a PERFORMANCE CRITERION , which is a HEBB-LIKE LEARNING RULE , and -lrb- 2 -rrb- an INCREMENTAL NETWORK MODEL is proposed to solve the problem of CLUSTERING . experimental results show that the proposed `` NEURAL GAS '' METHOD achieves better performance than the state-of-the-art methods .\n",
            "\n",
            "699 1000\n",
            "in this paper we propose an extension to the standard MARKOV RANDOM FIELD MODEL in order to handle LAYERS . our extension , which we call a FACTORIAL MRF , is analogous to the extension from HIDDEN MARKOV MODELS -LRB- HMM 'S -RRB- to FACTORIAL HMM 'S . we present an efficient EM-BASED ALGORITHM for INFERENCE on FACTORIAL MRF 'S . our EM-BASED ALGORITHM makes use of the fact that LAYERS are a priori independent , and that LAYERS only interact through the OBSERVABLE IMAGE . the EM-BASED ALGORITHM iterates between WIDE INFERENCE , i.e. , INFERENCE within each layer for the entire set of pixels , and DEEP INFERENCE , i.e. , INFERENCE through the LAYERS for each single pixel . the efficiency of our EM-BASED ALGORITHM is partly due to the use of GRAPH CUTS for BINARY SEGMENTATION , which is part of the WIDE INFERENCE STEP . we show experimental results for both REAL AND SYNTHETIC IMAGES . \n",
            "this paper presents a novel approach to WIDE INFERENCE based on a MARKOV RANDOM FIELD MODEL for WIDE INFERENCE . the proposed method is based on the use of a MARKOV RANDOM FIELD MODEL for INFERENCE . the proposed method is based on the use of a MARKOV RANDOM FIELD MODEL for WIDE INFERENCE . the proposed method is based on the use of GRAPH CUTS for WIDE INFERENCE . the proposed method is based on a MARKOV RANDOM FIELD MODEL , which is based on a MARKOV RANDOM FIELD MODEL . the proposed method is evaluated on both REAL AND SYNTHETIC IMAGES . the experimental results show that the proposed method outperforms the state-of-the-art methods in terms of both REAL AND SYNTHETIC IMAGES and LAYERS .\n",
            "\n",
            "700 1000\n",
            "we propose a REAL-TIME METHOD for simultaneously refining the reconstructed volume of a human body with LOOSE-FITTING CLOTHING and identifying <unk> in it . TIME-SERIES VOLUMES , which are acquired by a slow but sophisticated 3D RECONSTRUCTION ALGORITHM , with BODY-PART LABELS are obtained offline . the TIME-SERIES SAMPLE VOLUMES are represented by trajectories in the <unk> using PCA . an INPUT VISUAL HULL reconstructed online is projected into the EIGENSPACE and compared with the trajectories in order to find similar high-precision samples with BODY-PART LABELS . the HIERARCHICAL SEARCH taking into account 3d reconstruction errors can achieve robust and fast matching . experimental results demonstrate that our REAL-TIME METHOD can refine the INPUT VISUAL HULL including LOOSE-FITTING CLOTHING and identify its <unk> in real time . \n",
            "this paper presents a novel method for HIERARCHICAL SEARCH based on HIERARCHICAL SEARCH . the proposed method is based on the use of a HIERARCHICAL SEARCH to estimate the INPUT VISUAL HULL of the EIGENSPACE . the proposed 3D RECONSTRUCTION ALGORITHM is based on the use of PCA to estimate the INPUT VISUAL HULL . the proposed method is based on the use of PCA , which is based on a HIERARCHICAL SEARCH . the proposed method is based on the use of PCA , which is based on a HIERARCHICAL SEARCH . the experimental results show that the proposed method outperforms the existing methods in terms of the INPUT VISUAL HULL .\n",
            "\n",
            "701 1000\n",
            "in this paper , a novel approach for SINGLE CHANNEL SOURCE SEPARATION using a DEEP NEURAL NETWORK ARCHITECTURE is introduced . unlike previous studies in which DEEP NEURAL NETWORK ARCHITECTURE and other CLASSIFIERS were used for CLASSIFYING TIME-FREQUENCY BINS to obtain hard masks for each source , we use the DEEP NEURAL NETWORK ARCHITECTURE to classify ESTIMATED SOURCE SPECTRA to check for their validity during separation . in the TRAINING STAGE , the TRAINING DATA for the source signals are used to train a DEEP NEURAL NETWORK ARCHITECTURE . in the SEPARATION STAGE , the trained DEEP NEURAL NETWORK ARCHITECTURE is utilized to aid in estimation of each source in the mixed signal . SINGLE CHANNEL SOURCE SEPARATION PROBLEM is formulated as an ENERGY MINIMIZATION PROBLEM where each source spectra estimate is encouraged to fit the trained DEEP NEURAL NETWORK ARCHITECTURE and the MIXED SIGNAL SPECTRUM is encouraged to be written as a weighted sum of the ESTIMATED SOURCE SPECTRA . the proposed approach works regardless of the ENERGY SCALE DIFFERENCES between the source signals in the training and separation stages . NONNEGATIVE MATRIX FACTORIZATION is used to initialize the DEEP NEURAL NETWORK ARCHITECTURE for each source . the experimental results show that using DEEP NEURAL NETWORK ARCHITECTURE initialized by DEEP NEURAL NETWORK ARCHITECTURE for SOURCE SEPARATION improves the quality of the separated signal compared with using DEEP NEURAL NETWORK ARCHITECTURE for SOURCE SEPARATION . \n",
            "this paper presents a novel DEEP NEURAL NETWORK ARCHITECTURE for SINGLE CHANNEL SOURCE SEPARATION . the proposed DEEP NEURAL NETWORK ARCHITECTURE is based on the use of NONNEGATIVE MATRIX FACTORIZATION and CLASSIFIERS to estimate the ENERGY SCALE DIFFERENCES . the proposed DEEP NEURAL NETWORK ARCHITECTURE is based on a DEEP NEURAL NETWORK ARCHITECTURE , which is based on NONNEGATIVE MATRIX FACTORIZATION and CLASSIFIERS . the proposed DEEP NEURAL NETWORK ARCHITECTURE is based on a DEEP NEURAL NETWORK ARCHITECTURE , which is based on NONNEGATIVE MATRIX FACTORIZATION and CLASSIFIERS . the proposed DEEP NEURAL NETWORK ARCHITECTURE is applied to the SINGLE CHANNEL SOURCE SEPARATION PROBLEM , and the results show that the proposed DEEP NEURAL NETWORK ARCHITECTURE is effective in CLASSIFYING TIME-FREQUENCY BINS .\n",
            "\n",
            "702 1000\n",
            "in this contribution , the performance of an UPLINK CDMA SYSTEM with RANDOM SPREADING and MULTI-CELL INTERFERENCE is analyzed . a useful framework is provided in order to determine the BASE STATION COVERAGE for WIRELESS FLAT FADING CHANNELS with very dense networks -lrb- in the number of users per meter -rrb- considering different RECEIVER STRUCTURES at the base station , namely the MATCHED FILTER , the WIENER FILTER and the OPTIMUM FILTER . using ASYMPTOTIC ARGUMENTS , analytical expressions of the SPECTRAL EFFICIENCY are obtained and provide a simple expression of the NETWORK CAPACITY based only on a few meaningful parameters . \n",
            "in this paper , we present a novel approach to the problem of MULTI-CELL INTERFERENCE in WIRELESS FLAT FADING CHANNELS . the proposed approach is based on the use of a RANDOM SPREADING , a RANDOM SPREADING , a WIENER FILTER , a WIENER FILTER , and a WIENER FILTER . the proposed method is based on a RANDOM SPREADING , a WIENER FILTER and a WIENER FILTER . the proposed method is evaluated in terms of SPECTRAL EFFICIENCY and SPECTRAL EFFICIENCY .\n",
            "\n",
            "703 1000\n",
            "this paper presents a <i> local learning projection </i> -lrb- <unk> -rrb- approach for LINEAR DIMENSIONALITY REDUCTION . we first point out that the well known <I> PRINCIPAL COMPONENT ANALYSIS </I> essentially seeks the PROJECTION that has the MINIMAL <I> GLOBAL </I> ESTIMATION ERROR . then we propose a DIMENSIONALITY REDUCTION ALGORITHM that leads to the PROJECTION with the MINIMAL <I> LOCAL </I> ESTIMATION ERROR , and elucidate its advantages for CLASSIFICATION TASKS . we also indicate that <unk> keeps the LOCAL INFORMATION in the sense that the PROJECTION VALUE of each point can be well estimated based on its neighbors and their PROJECTION VALUES . experimental results are provided to validate the effectiveness of the proposed DIMENSIONALITY REDUCTION ALGORITHM . \n",
            "in this paper , we propose a novel DIMENSIONALITY REDUCTION ALGORITHM for LINEAR DIMENSIONALITY REDUCTION . the proposed DIMENSIONALITY REDUCTION ALGORITHM is based on the idea of <I> PRINCIPAL COMPONENT ANALYSIS </I> , which is a generalization of the <I> PRINCIPAL COMPONENT ANALYSIS </I> . the proposed DIMENSIONALITY REDUCTION ALGORITHM is based on the <I> PRINCIPAL COMPONENT ANALYSIS </I> , which is a generalization of the <I> PRINCIPAL COMPONENT ANALYSIS </I> . the proposed DIMENSIONALITY REDUCTION ALGORITHM is applied to the problem of LINEAR DIMENSIONALITY REDUCTION . the proposed DIMENSIONALITY REDUCTION ALGORITHM is applied to the problem of LINEAR DIMENSIONALITY REDUCTION . the proposed DIMENSIONALITY REDUCTION ALGORITHM is applied to the problem of LINEAR DIMENSIONALITY REDUCTION . the experimental results show that the proposed DIMENSIONALITY REDUCTION ALGORITHM is effective in reducing the number of PROJECTION VALUES in the CLASSIFICATION TASKS .\n",
            "\n",
            "704 1000\n",
            "the MAXIMUM LIKELIHOOD SEQUENCE ESTIMATOR is the optimal RECEIVER for the INTER-SYMBOL INTERFERENCE CHANNEL with ADDITIVE WHITE NOISE . a RECEIVER is demonstrated that estimates sequence likelihood using a VARIABLE ORDER MARKOV MODEL constructed from a CRUDELY QUANTIZED TRAINING SEQUENCE . RECEIVER PERFORMANCE is relatively unaffected by HEAVY-TAILED NOISE that can <unk> the performance of GAUSSIAN BASED ALGORITHMS such as DECISION FEEDBACK EQUALIZATION with GRADIENT BASED ADAPTATION . we consider the problem of DECODING BINARY SYMBOLS across a LINEAR ISI CHANNEL contaminated with ADDITIVE WHITE NOISE . given discrete-time observations of the channel output r n \n",
            "in this paper , we propose a novel method for DECISION FEEDBACK EQUALIZATION based on a VARIABLE ORDER MARKOV MODEL . the proposed VARIABLE ORDER MARKOV MODEL is based on a VARIABLE ORDER MARKOV MODEL , which is based on a VARIABLE ORDER MARKOV MODEL . the proposed VARIABLE ORDER MARKOV MODEL is based on a VARIABLE ORDER MARKOV MODEL , which is based on a VARIABLE ORDER MARKOV MODEL . the proposed VARIABLE ORDER MARKOV MODEL is based on a VARIABLE ORDER MARKOV MODEL , which is based on a VARIABLE ORDER MARKOV MODEL . the proposed VARIABLE ORDER MARKOV MODEL is applied to the INTER-SYMBOL INTERFERENCE CHANNEL , which is based on a VARIABLE ORDER MARKOV MODEL . the experimental results show that the proposed VARIABLE ORDER MARKOV MODEL is effective in improving the RECEIVER PERFORMANCE and RECEIVER PERFORMANCE of the RECEIVER . the proposed method is also compared to other GAUSSIAN BASED ALGORITHMS such as GRADIENT BASED ADAPTATION and GRADIENT BASED ADAPTATION .\n",
            "\n",
            "705 1000\n",
            "adaptive ridge is a special form of RIDGE REGRESSION , balancing the QUADRATIC PENALIZATION on each parameter of the model . it was shown to be equivalent to LASSO -lrb- least absolute shrinkage and selection operator -rrb- , in the sense that both procedures produce the same estimate . LASSO can thus be viewed as a particular QUADRATIC PENALIZER . from this observation , we derive a FIXED POINT ALGORITHM to compute the LASSO SOLUTION . the analogy provides also a new HYPER-PARAMETER for tuning effectively the MODEL COMPLEXITY . we finally present a SERIES OFPOSSI-BLE EXTENSIONS OFLASSO performing SPARSE REGRESSION in KERNEL SMOOTHING , ADDITIVE MODELING and NEURAL NET TRAINING . \n",
            "in this paper , we propose a novel approach to ADDITIVE MODELING in NEURAL NET TRAINING . the proposed approach is based on the use of a QUADRATIC PENALIZATION , which is a QUADRATIC PENALIZATION with a FIXED POINT ALGORITHM . the proposed method is based on a FIXED POINT ALGORITHM , which is a generalization of the QUADRATIC PENALIZATION to the RIDGE REGRESSION . the proposed method is based on the QUADRATIC PENALIZATION , which is based on a FIXED POINT ALGORITHM . the proposed method is based on the use of the QUADRATIC PENALIZATION , which is a generalization of the traditional FIXED POINT ALGORITHM . the proposed method can be applied to SPARSE REGRESSION , such as RIDGE REGRESSION , LASSO , and NEURAL NET TRAINING . the experimental results show that the proposed method is effective in reducing the MODEL COMPLEXITY of the LASSO SOLUTION .\n",
            "\n",
            "706 1000\n",
            "in the context of the <unk> workshop on NATURAL LANGUAGE PROCESSING for LESS PRIVILEGED LANGUAGES , we discuss the obstacles to research on such languages . we also briefly discuss the ways to make progress in removing these obstacles . we mention some previous work and comment on the papers selected for the workshop . \n",
            "this paper addresses the problem of NATURAL LANGUAGE PROCESSING in NATURAL LANGUAGE PROCESSING . the problem of NATURAL LANGUAGE PROCESSING for LESS PRIVILEGED LANGUAGES is a fundamental problem in NATURAL LANGUAGE PROCESSING . in this paper , we propose a novel approach to the problem of NATURAL LANGUAGE PROCESSING for LESS PRIVILEGED LANGUAGES . we show that our algorithm can be applied to the problem of NATURAL LANGUAGE PROCESSING for LESS PRIVILEGED LANGUAGES .\n",
            "\n",
            "707 1000\n",
            "in this paper we present the BEHAVIOSITE PARADIGM , a new approach to coordination and control of DISTRIBUTED AGENTS in a MULTIAGENT SYSTEM , inspired by BIOLOGICAL PARASITES with BEHAVIOR MANIPULATION PROPERTIES . BEHAVIOSITE PARADIGM are CODE MODULES that '' <unk> '' a MULTIAGENT SYSTEM , attaching themselves to agents and altering the SENSORY ACTIVITY and actions of those agents . these BEHAVIORAL CHANGES can be used to achieve altered , potentially improved , performance of the overall MULTIAGENT SYSTEM ; thus , BEHAVIOSITE PARADIGM provide a mechanism for DISTRIBUTED CONTROL over a DISTRIBUTED SYSTEM . BEHAVIOSITE PARADIGM need to be designed so that BEHAVIOSITE PARADIGM are intimately familiar with the internal <unk> of the environment and of the agents operating within it . to demonstrate our approach , we use <unk> to control the behavior of a SWARM of simple agents . with a relatively low INFECTION RATE , a few <unk> can <unk> desired behavior over the SWARM as a whole : keeping it in one place , leading it through <unk> , or moving the SWARM from one stable equilibrium to another . we contrast <unk> as a DISTRIBUTED SWARM CONTROL MECHANISM with alternatives , such as the use of group leaders , HERDERS , or SOCIAL NORMS . \n",
            "this paper addresses the problem of DISTRIBUTED CONTROL in the context of DISTRIBUTED AGENTS . we propose a novel approach to the problem of DISTRIBUTED CONTROL in the context of a MULTIAGENT SYSTEM . the proposed approach is based on the use of SOCIAL NORMS and CODE MODULES . the proposed approach is based on the use of SOCIAL NORMS and CODE MODULES . the proposed approach is based on the BEHAVIOSITE PARADIGM , which is based on the BEHAVIOSITE PARADIGM . the proposed approach is based on the use of CODE MODULES and CODE MODULES . the experimental results show that the proposed approach is effective in improving the INFECTION RATE of the MULTIAGENT SYSTEM .\n",
            "\n",
            "708 1000\n",
            "for INTERNET APPLICATIONS like SPONSORED SEARCH , <unk> need to be taken when using MACHINE LEARNING to optimize their mechanisms -lrb- e.g. , auction -rrb- since SELF-INTERESTED AGENTS in these applications may change their behaviors -lrb- and thus the DATA DISTRIBUTION -rrb- in response to the mechanisms . to tackle this problem , a framework called game-theoretic MACHINE LEARNING -lrb- <unk> -rrb- was recently proposed , which first learns a MARKOV BEHAVIOR MODEL to characterize agents behaviors , and then learns the optimal MARKOV BEHAVIOR MODEL by simulating agents ' behavior changes in response to the MARKOV BEHAVIOR MODEL . while <unk> has demonstrated practical success , its GENERALIZATION ANALYSIS is challenging because the BEHAVIOR DATA are non-i.i.d. and dependent on the MARKOV BEHAVIOR MODEL . to address this challenge , first , we decompose the GENERALIZATION ERROR for <unk> into the BEHAVIOR LEARNING ERROR and the BEHAVIOR LEARNING ERROR ; second , for the BEHAVIOR LEARNING ERROR , we obtain novel NON-ASYMPTOTIC ERROR BOUNDS for both PARAMETRIC AND NON-PARAMETRIC BEHAVIOR LEARNING METHODS ; third , for the BEHAVIOR LEARNING ERROR , we derive a UNIFORM CONVERGENCE bound based on a new concept called nested covering number of the MECHANISM SPACE and the GENERALIZATION ANALYSIS TECHNIQUES developed for mixing sequences . \n",
            "this paper addresses the problem of MACHINE LEARNING in INTERNET APPLICATIONS such as SPONSORED SEARCH . we propose a novel approach to the problem of SPONSORED SEARCH in INTERNET APPLICATIONS such as SPONSORED SEARCH . the proposed approach is based on the use of a MARKOV BEHAVIOR MODEL , which is based on a MARKOV BEHAVIOR MODEL . the proposed approach is based on a novel MARKOV BEHAVIOR MODEL , which is able to deal with BEHAVIOR DATA , such as SPONSORED SEARCH and SPONSORED SEARCH . the proposed approach is based on the use of NON-ASYMPTOTIC ERROR BOUNDS , which is a generalization of the MARKOV BEHAVIOR MODEL . the proposed approach is evaluated on a variety of INTERNET APPLICATIONS including SPONSORED SEARCH and SPONSORED SEARCH .\n",
            "\n",
            "709 1000\n",
            "-- we present a PROBABILISTIC FRAMEWORK for PHYSICAL LAYER SECRECY when the locations and channels of the <unk> are unknown . the locations are modeled by a POISSON POINT PROCESS . the channels include PATH LOSS and RAYLEIGH FADING . BEAMFORMING and <unk> of the fading channels are shown to greatly increase the probability of SECURE COMMUNICATIONS . \n",
            "this paper presents a novel PROBABILISTIC FRAMEWORK for SECURE COMMUNICATIONS . the proposed PROBABILISTIC FRAMEWORK is based on the use of a PROBABILISTIC FRAMEWORK for BEAMFORMING . the proposed PROBABILISTIC FRAMEWORK is based on a PROBABILISTIC FRAMEWORK for the POISSON POINT PROCESS . the proposed PROBABILISTIC FRAMEWORK is applied to the problem of SECURE COMMUNICATIONS and BEAMFORMING . the performance of the proposed algorithm is demonstrated on a variety of RAYLEIGH FADING and RAYLEIGH FADING .\n",
            "\n",
            "710 1000\n",
            "this paper proposes a novel ACOUSTIC MODEL based on NEURAL NETWORKS for STATISTICAL PARAMETRIC SPEECH SYNTHESIS . the NEURAL NETWORKS outputs parameters of a NON-ZERO MEAN GAUSSIAN PROCESS , which defines a PROBABILITY DENSITY FUNCTION of a SPEECH WAVEFORM given LINGUISTIC FEATURES . the MEAN AND COVARIANCE FUNCTIONS of the GAUSSIAN PROCESS represent deterministic -lrb- voiced -rrb- and STOCHASTIC COMPONENTS of a SPEECH WAVEFORM , whereas the previous approach considered the UNVOICED COMPONENT only . experimental results show that the proposed approach can generate SPEECH WAVEFORMS approximating NATURAL SPEECH WAVEFORMS . \n",
            "in this paper , we propose a novel ACOUSTIC MODEL for STATISTICAL PARAMETRIC SPEECH SYNTHESIS . the proposed ACOUSTIC MODEL is based on the use of NEURAL NETWORKS for STATISTICAL PARAMETRIC SPEECH SYNTHESIS . the proposed ACOUSTIC MODEL is based on a GAUSSIAN PROCESS that uses NEURAL NETWORKS to estimate the UNVOICED COMPONENT of the SPEECH WAVEFORM . the proposed ACOUSTIC MODEL is based on the use of NEURAL NETWORKS to estimate the UNVOICED COMPONENT of the SPEECH WAVEFORM . the proposed ACOUSTIC MODEL is applied to the SPEECH WAVEFORM of the SPEECH WAVEFORM in the SPEECH WAVEFORM . experimental results show that the proposed ACOUSTIC MODEL is effective for STATISTICAL PARAMETRIC SPEECH SYNTHESIS .\n",
            "\n",
            "711 1000\n",
            "this paper presents an AGENT-BASED MODEL that studies the emergence and evolution of a LANGUAGE SYSTEM of LOGICAL CONSTRUCTIONS , i.e. a vocabulary and a set of GRAMMATICAL CONSTRUCTIONS that allows the expression of LOGICAL COMBINATIONS OF CATEGORIES . the AGENT-BASED MODEL assumes the agents have a common vocabulary for basic categories , the ability to construct LOGICAL COMBINATIONS OF CATEGORIES using BOOLEAN FUNCTIONS , and some general purpose cognitive capacities for INVENTION , ADOPTION , INDUCTION and ADAPTATION . but it does not assume the agents have a vocabulary for BOOLEAN FUNCTIONS nor GRAMMATICAL CONSTRUCTIONS for expressing such LOGICAL COMBINATIONS OF CATEGORIES through language . the results of the experiments we have performed show that a LANGUAGE SYSTEM of LOGICAL CONSTRUCTIONS emerges as a result of a process of <unk> of the individual agents ' interactions when these agents adapt their preferences for vocabulary and GRAMMATICAL CONSTRUCTIONS to those they observe are used more often by the rest of the population , and that such a LANGUAGE SYSTEM is transmitted from one generation to the next . \n",
            "this paper addresses the problem of INVENTION and INDUCTION . the main contribution of this paper is the use of a AGENT-BASED MODEL to estimate the LOGICAL COMBINATIONS OF CATEGORIES of a set of LOGICAL CONSTRUCTIONS . a LANGUAGE SYSTEM is used to estimate the LOGICAL COMBINATIONS OF CATEGORIES of the data . the proposed approach is based on the use of BOOLEAN FUNCTIONS , ADOPTION and ADAPTATION , and ADAPTATION . the proposed approach has been tested on a variety of LOGICAL COMBINATIONS OF CATEGORIES , including ADOPTION , INDUCTION , INDUCTION and INDUCTION .\n",
            "\n",
            "712 1000\n",
            "the assumptions behind LINEAR CLASSIFIERS for CATEGORICAL DATA are examined and reformulated in the context of the MULTINOMIAL MANIFOLD , the simplex of MULTINOMIAL MODELS <unk> with the RIEMANNIAN STRUCTURE induced by the FISHER INFORMATION . this leads to a new view of HYPERPLANE CLASSIFIERS which , together with a GENERALIZED MARGIN CONCEPT , shows how to adapt existing MARGIN-BASED HYPERPLANE MODELS to MULTINOMIAL GEOMETRY . experiments show the new CLASSIFICATION FRAMEWORK to be effective for TEXT CLASSIFICATION , where the CATEGORICAL STRUCTURE of the data is modeled naturally within the MULTINOMIAL FAMILY . \n",
            "in this paper , we propose a novel CLASSIFICATION FRAMEWORK for TEXT CLASSIFICATION . the proposed CLASSIFICATION FRAMEWORK is based on a GENERALIZED MARGIN CONCEPT , which is based on a GENERALIZED MARGIN CONCEPT . the proposed CLASSIFICATION FRAMEWORK is based on a GENERALIZED MARGIN CONCEPT , which is based on a GENERALIZED MARGIN CONCEPT . in the proposed CLASSIFICATION FRAMEWORK , a GENERALIZED MARGIN CONCEPT is used to estimate the RIEMANNIAN STRUCTURE of the HYPERPLANE CLASSIFIERS . the proposed CLASSIFICATION FRAMEWORK is applied to CATEGORICAL DATA , and the experimental results demonstrate the effectiveness of the proposed CLASSIFICATION FRAMEWORK .\n",
            "\n",
            "713 1000\n",
            "we present a CENTER-REFERENCED BASIS for DISCRETE REPRESENTATION OF STEREO CORRESPONDENCE that includes new OCCLU-SION NODES . this basis improves the inclusion of constraints and the parallelism of the final algorithm . DISPARITY ESTIRNA-TION is formulated in a MAP CONTEXT and NATURAL CONSTRAINTS are incorporated , resulting in an optimal path problem in a SPARSELY CONNECTED TRELLIS . like other D P METHODS , the COMPUTATIONAL COMPLEXITY is low at -lrb- 3 -lrb- m n 2 -rrb- for m x n pixel images . however , this method is better suited to PARALLEL SOLUTION , scaling up to -lrb- 3 -lrb- m n -rrb- processors . experimental results confirm the performance of this method . \n",
            "this paper addresses the problem of DISCRETE REPRESENTATION OF STEREO CORRESPONDENCE from a single SPARSELY CONNECTED TRELLIS . in particular , we consider the problem of DISCRETE REPRESENTATION OF STEREO CORRESPONDENCE in the presence of NATURAL CONSTRAINTS . we show that the COMPUTATIONAL COMPLEXITY of the PARALLEL SOLUTION is equivalent to the MAP CONTEXT of the CENTER-REFERENCED BASIS . we show that the COMPUTATIONAL COMPLEXITY of the COMPUTATIONAL COMPLEXITY is bounded by a factor of o -lrb- log -lrb- 1 / √ t -rrb- -rrb- , where n is the number of OCCLU-SION NODES . we also show that the COMPUTATIONAL COMPLEXITY of the COMPUTATIONAL COMPLEXITY is bounded by a factor of o -lrb- log -lrb- 1 / / -rrb- -rrb- , where n is the number of OCCLU-SION NODES , and the number of OCCLU-SION NODES is bounded by a factor of o -lrb- 1 / √ t -rrb- .\n",
            "\n",
            "714 1000\n",
            "<unk> to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists , or to reuse any <unk> component of this work in other works must be obtained from the IEEE . '' ABSTRACT this paper deals with the problem of discriminating samples that contain only NOISE from samples that contain a signal embedded in NOISE . the focus is on the case when the variance of the NOISE is unknown . we derive the optimal SOFT DECISION DETECTOR using a BAYESIAN APPROACH . the COMPLEXITY of this optimal SOFT DECISION DETECTOR grows exponentially with the number of observations and as a remedy , we propose a number of approximations to SOFT DECISION DETECTOR . the problem under study is a fundamental one and SOFT DECISION DETECTOR has applications in SIGNAL DENOISING , ANOMALY DETECTION , and SPECTRUM SENSING for COGNITIVE RADIO . we illustrate the results in the context of the latter . \n",
            "this paper addresses the problem of SIGNAL DENOISING and COGNITIVE RADIO in COGNITIVE RADIO and COGNITIVE RADIO . we propose a novel BAYESIAN APPROACH for ANOMALY DETECTION and COGNITIVE RADIO . the proposed BAYESIAN APPROACH is based on a BAYESIAN APPROACH for SIGNAL DENOISING and SPECTRUM SENSING . the proposed BAYESIAN APPROACH is applied to the problem of SIGNAL DENOISING and COGNITIVE RADIO . the experimental results show that the proposed BAYESIAN APPROACH is effective in improving the COMPLEXITY of the IEEE .\n",
            "\n",
            "715 1000\n",
            "utterance UTTERANCE VERIFICATION based on N-BEST HMM SCORES has been widely used in ASR SYSTEM . there are a number of ways to calculate a MEASUREMENT SCORE for UTTERANCE VERIFICATION from n-best scores . most of proposed methods are based on the N-BEST UV APPROACH of the HYPOTHESIS TESTING . this has lead to use the second best score or an overall average of available n-best scores for <unk> . in this study we examine N-BEST UV APPROACH from a COMPETITION-BASED MEASUREMENT FRAMEWORK . with this N-BEST UV APPROACH different competitive measurements can be derived from a SEQUENCE OF SORTED LIKELIHOOD RATIOS . the evaluation results demonstrate that oov performance can be improved by using some SELECTIVE COMPONENTS in SORTED LIKELIHOOD RATIOS . in our experiments by using the first four components oov rejection errors can be reduced about 30 % in comparison with the baseline results . \n",
            "in this paper , we propose a novel approach to UTTERANCE VERIFICATION based on a COMPETITION-BASED MEASUREMENT FRAMEWORK . the proposed COMPETITION-BASED MEASUREMENT FRAMEWORK is based on the use of a SEQUENCE OF SORTED LIKELIHOOD RATIOS which is based on a SEQUENCE OF SORTED LIKELIHOOD RATIOS . the proposed COMPETITION-BASED MEASUREMENT FRAMEWORK is based on the use of the N-BEST HMM SCORES for the ASR SYSTEM . the proposed COMPETITION-BASED MEASUREMENT FRAMEWORK is applied to the N-BEST HMM SCORES of the ASR SYSTEM . the experimental results show that the proposed method is effective in improving the UTTERANCE VERIFICATION performance of the ASR SYSTEM .\n",
            "\n",
            "716 1000\n",
            "in this paper , we present a BIRDSONG-PHRASE SEGMENTATION AND VERIFICATION ALGORITHM that is robust to LIMITED TRAINING DATA , CLASS VARIABILITY , and NOISE . the BIRDSONG-PHRASE SEGMENTATION AND VERIFICATION ALGORITHM comprises a NOISE-ROBUST , DYNAMIC-TIME-WARPING - based segmentation and a DISCRIMINATIVE CLASSIFIER for OUTLIER REJECTION . the BIRDSONG-PHRASE SEGMENTATION AND VERIFICATION ALGORITHM utilizes NOISE-ROBUST , DYNAMIC-TIME-WARPING and prominent -lrb- high energy -rrb- time-frequency regions of training spectrograms to derive a reliable NOISE-ROBUST TEMPLATE for each phrase class . the resulting BIRDSONG-PHRASE SEGMENTATION AND VERIFICATION ALGORITHM is then used for SEGMENTING CONTINUOUS RECORDINGS to obtain segment candidates whose SPECTROGRAM AMPLITUDES in the prominent regions are used as FEATURES to a SUPPORT VECTOR MACHINE . the BIRDSONG-PHRASE SEGMENTATION AND VERIFICATION ALGORITHM is evaluated on the CASSIN 'S VIREO RECORDINGS ; our proposed BIRDSONG-PHRASE SEGMENTATION AND VERIFICATION ALGORITHM yields low equal error rates -lrb- eer -rrb- and SEGMENT BOUNDARIES that are close to those obtained from MANUAL ANNOTATIONS and , is better than ENERGY OR ENTROPY-BASED BIRDSONG SEGMENTATION ALGORITHMS . in the presence of ADDITIVE NOISE -lrb- <unk> to 10 db snr -rrb- , the proposed BIRDSONG-PHRASE SEGMENTATION AND VERIFICATION ALGORITHM does not degrade as significantly as the other algorithms do . \n",
            "in this paper , we propose a novel BIRDSONG-PHRASE SEGMENTATION AND VERIFICATION ALGORITHM for SEGMENTING CONTINUOUS RECORDINGS . the proposed BIRDSONG-PHRASE SEGMENTATION AND VERIFICATION ALGORITHM is based on the BIRDSONG-PHRASE SEGMENTATION AND VERIFICATION ALGORITHM and the DISCRIMINATIVE CLASSIFIER . the proposed BIRDSONG-PHRASE SEGMENTATION AND VERIFICATION ALGORITHM is based on the BIRDSONG-PHRASE SEGMENTATION AND VERIFICATION ALGORITHM and the DISCRIMINATIVE CLASSIFIER . the proposed BIRDSONG-PHRASE SEGMENTATION AND VERIFICATION ALGORITHM consists of two steps : -lrb- 1 -rrb- a NOISE-ROBUST TEMPLATE , and -lrb- 2 -rrb- an OUTLIER REJECTION . the proposed BIRDSONG-PHRASE SEGMENTATION AND VERIFICATION ALGORITHM consists of two steps : -lrb- 1 -rrb- a NOISE-ROBUST TEMPLATE , and -lrb- 2 -rrb- a BIRDSONG-PHRASE SEGMENTATION AND VERIFICATION ALGORITHM to estimate the SEGMENT BOUNDARIES . the proposed BIRDSONG-PHRASE SEGMENTATION AND VERIFICATION ALGORITHM is applied to the BIRDSONG-PHRASE SEGMENTATION AND VERIFICATION ALGORITHM and the BIRDSONG-PHRASE SEGMENTATION AND VERIFICATION ALGORITHM . experimental results show that the proposed BIRDSONG-PHRASE SEGMENTATION AND VERIFICATION ALGORITHM is able to detect SEGMENT BOUNDARIES , CLASS VARIABILITY , NOISE and NOISE .\n",
            "\n",
            "717 1000\n",
            "images taken from different views of a PLANAR OBJECT are related by PLANAR HOMOGRAPHY . recovering the parameters of such IMAGES is a fundamental problem in COMPUTER VISION with various applications . this paper proposes a novel method to estimate the parameters of a HOMOGRAPHY that aligns two BINARY IMAGES . it is obtained by solving a system of NONLINEAR EQUATIONS generated by integrating LINEARLY INDEPENDENT FUNCTIONS over the domains determined by the shapes . the advantage of the proposed solution is that it is easy to implement , less sensitive to the strength of the DEFORMATION , works without ESTABLISHED CORRESPONDENCES and robust against SEGMENTATION ERRORS . the method has been tested on synthetic as well as on REAL IMAGES and its efficiency has been demonstrated in the context of two different applications : alignment of HIP PROSTHESIS X-RAY IMAGES and MATCHING OF TRAFFIC SIGNS . \n",
            "this paper presents a novel approach to MATCHING OF TRAFFIC SIGNS in COMPUTER VISION . the proposed approach is based on the use of LINEARLY INDEPENDENT FUNCTIONS in the form of a PLANAR HOMOGRAPHY . the method is based on the use of LINEARLY INDEPENDENT FUNCTIONS in the HOMOGRAPHY . the method is based on the use of LINEARLY INDEPENDENT FUNCTIONS in the form of a PLANAR HOMOGRAPHY . the method is based on the use of LINEARLY INDEPENDENT FUNCTIONS in the DEFORMATION of the HOMOGRAPHY . the proposed method is evaluated on REAL IMAGES . the results show that the proposed method is effective in improving the SEGMENTATION ERRORS in REAL IMAGES .\n",
            "\n",
            "718 1000\n",
            "in this paper we address the problem of REDVING BLURRED POINT SOURCES in INTENSITY IMAGES . a new approach to IMAGE RESTORATION is introduced which is a 2-D GENERALIZATION UF TECHNIQUES originating from the field of direction of arrival estimation -lrb- doa -rrb- . i n the 2-D FREQUENCY DOMAIN . algorithms , such as MUSIC . may be adapted to search for these BLURRED POINT SOURCES . a generalization of ARRAY SMOOTHING <unk> on a REGULARIZATION OPERATOR is introduced for <unk> arrays in order to achieve RANK ENHANRMIENT in the SIGNAL SPACE of the COVARIANCE MUTRIR . \n",
            "this paper addresses the problem of IMAGE RESTORATION in the presence of MUSIC . we propose a method to estimate the BLURRED POINT SOURCES using a REGULARIZATION OPERATOR based on the REGULARIZATION OPERATOR . the proposed method consists of two steps : -lrb- 1 -rrb- a REGULARIZATION OPERATOR of the SIGNAL SPACE , and -lrb- 2 -rrb- a REGULARIZATION OPERATOR for ARRAY SMOOTHING . the proposed method consists of two steps : -lrb- 1 -rrb- a REGULARIZATION OPERATOR of the SIGNAL SPACE in the SIGNAL SPACE and 2 -rrb- a REGULARIZATION OPERATOR to estimate the SIGNAL SPACE of the BLURRED POINT SOURCES . the proposed method is based on the estimation of the RANK ENHANRMIENT and the RANK ENHANRMIENT of the BLURRED POINT SOURCES . experimental results demonstrate the effectiveness of the proposed method .\n",
            "\n",
            "719 1000\n",
            "the high COMPUTATIONAL COMPLEXITY of the EXPRESSIVE DESCRIPTION LOGICS that underlie the OWL STANDARD has motivated the study of their HORN FRAGMENTS , which are usually tractable in DATA COMPLEXITY and can also have lower combined COMPLEXITY , particularly for QUERY ANSWERING . in this paper we provide algorithms for answering <unk> <unk> regular path queries -lrb- <unk> -rrb- , a NON-TRIVIAL GENERALIZATION OF PLAIN CONJUNCTIVE QUERIES , in the HORN FRAGMENTS of the DLS SHOIQ and SROIQ underlying owl 1 and owl 2 . we show that the combined COMPLEXITY of the problem is <unk> for SROIQ and 2EXPTIME-COMPLETE for the more expressive SROIQ , but is <unk> in DATA COMPLEXITY for both . in contrast , even DECIDABILITY OF PLAIN CONJUNCTIVE QUERIES is still open for FULL SHOIQ and SROIQ . these are the first completeness results for QUERY ANSWERING in EXPRESSIVE DESCRIPTION LOGICS with <unk> , NOMINALS , and counting , and show that for the considered logics the problem is not more expensive than standard reasoning . \n",
            "in this paper , we present a novel approach to QUERY ANSWERING in the context of EXPRESSIVE DESCRIPTION LOGICS . the proposed EXPRESSIVE DESCRIPTION LOGICS is based on the use of HORN FRAGMENTS , SROIQ , SROIQ , and FULL SHOIQ . the proposed EXPRESSIVE DESCRIPTION LOGICS is based on the use of HORN FRAGMENTS , which is based on the DECIDABILITY OF PLAIN CONJUNCTIVE QUERIES of the OWL STANDARD . the COMPLEXITY of the proposed EXPRESSIVE DESCRIPTION LOGICS is evaluated in terms of DATA COMPLEXITY and 2EXPTIME-COMPLETE . the results show that the proposed EXPRESSIVE DESCRIPTION LOGICS can reduce the COMPUTATIONAL COMPLEXITY by up to 50 % .\n",
            "\n",
            "720 1000\n",
            "a <unk> limitation of research on SUPERVISED SENTENCE COMPRESSION is the dearth of available TRAINING DATA . we propose a new and <unk> resource for such TRAINING DATA , which we obtain by mining the revision history of wikipedia for SENTENCE COMPRESSIONS and EXPANSIONS . using only a fraction of the available WIKIPEDIA DATA , we have collected a training corpus of over <unk> sentence pairs , two orders of magnitude larger than the <unk> used ZIFF-DAVIS CORPUS . using this <unk> data , we propose a novel LEXICAL-IZED NOISY CHANNEL MODEL for SENTENCE COMPRESSION , achieving improved results in GRAM-MATICALITY AND COMPRESSION RATE CRITERIA with a slight decrease in importance . \n",
            "this paper addresses the problem of SUPERVISED SENTENCE COMPRESSION from a single image . we propose a novel approach to SUPERVISED SENTENCE COMPRESSION based on the LEXICAL-IZED NOISY CHANNEL MODEL . the proposed approach is based on the use of WIKIPEDIA DATA and EXPANSIONS . the proposed approach is based on the use of a LEXICAL-IZED NOISY CHANNEL MODEL , which is able to deal with WIKIPEDIA DATA in the presence of WIKIPEDIA DATA . the proposed method is evaluated on the ZIFF-DAVIS CORPUS , and the results show that the proposed method outperforms the state-of-the-art methods in terms of both GRAM-MATICALITY AND COMPRESSION RATE CRITERIA and EXPANSIONS performance .\n",
            "\n",
            "721 1000\n",
            "unsupervised feature learning algorithms based on CON-VOLUTIONAL FORMULATIONS of INDEPENDENT COMPONENTS ANALYSIS have been demonstrated to yield state-of-the-art results in several ACTION RECOGNITION BENCHMARKS . however , existing approaches do not allow for the number of LATENT COMPONENTS -lrb- features -rrb- to be automatically inferred from the data in an UNSUPERVISED MANNER . this is a significant disadvantage of the state-of-the-art , as it results in considerable burden imposed on researchers and practitioners , who must resort to TEDIOUS CROSS-VALIDATION PROCEDURES to obtain the optimal number of LATENT FEATURES . to resolve these issues , in this paper we introduce a convolutional <unk> bayesian sparse ica architecture for OVERCOMPLETE FEATURE LEARNING from HIGH-DIMENSIONAL DATA . our method utilizes an INDIAN BUFFET PROCESS prior to facilitate INFERENCE of the appropriate number of LATENT FEATURES under a HYBRID VARIATIONAL INFERENCE ALGORITHM , scalable to massive datasets . as we show , our model can be naturally used to obtain DEEP UNSUPERVISED HIERARCHICAL FEATURE EXTRACTORS , by greedily stacking SUCCESSIVE MODEL LAYERS , similar to existing approaches . in addition , INFERENCE for this model is completely <unk> ; thus , it obviates the need of TEDIOUS PARAMETER TUNING , which is a major challenge most deep learning approaches are faced with . we evaluate our method on several ACTION RECOGNITION BENCHMARKS , and exhibit its advantages over the state-of-the-art . \n",
            "in this paper , we propose a novel approach to OVERCOMPLETE FEATURE LEARNING based on DEEP UNSUPERVISED HIERARCHICAL FEATURE EXTRACTORS . the proposed approach is based on the use of INDEPENDENT COMPONENTS ANALYSIS , which is a HYBRID VARIATIONAL INFERENCE ALGORITHM of the INDIAN BUFFET PROCESS . the proposed HYBRID VARIATIONAL INFERENCE ALGORITHM is based on the use of INDEPENDENT COMPONENTS ANALYSIS , which is a HYBRID VARIATIONAL INFERENCE ALGORITHM of the INDIAN BUFFET PROCESS . the proposed approach is based on the use of INDEPENDENT COMPONENTS ANALYSIS , which is a HYBRID VARIATIONAL INFERENCE ALGORITHM of the INDIAN BUFFET PROCESS . the proposed HYBRID VARIATIONAL INFERENCE ALGORITHM is based on the use of CON-VOLUTIONAL FORMULATIONS to estimate the LATENT COMPONENTS . the proposed method is evaluated on the ACTION RECOGNITION BENCHMARKS and the ACTION RECOGNITION BENCHMARKS of the proposed method . the experimental results show that the proposed method is effective in improving the performance of the proposed HYBRID VARIATIONAL INFERENCE ALGORITHM .\n",
            "\n",
            "722 1000\n",
            "we experiment with extending a LATTICE PARSING METHODOLOGY for PARSING <unk> -lrb- <unk> and <unk> , 2008 ; <unk> et al. , 2009 -rrb- to make use of a stronger SYNTACTIC MODEL : the PCFG-LA BERKELEY PARSER . we show that the LATTICE PARSING METHODOLOGY is very effective : using a small training set of about <unk> trees , we construct a PARSER which parses and segments UNSEG-MENTED HEBREW TEXT with an F-SCORE of almost 80 % , an ERROR REDUCTION of over 20 % over the best previous result for this task . this result indicates that LATTICE PARSING with the PCFG-LA BERKELEY PARSER is an effective LATTICE PARSING METHODOLOGY for PARSING over UNCERTAIN INPUTS . \n",
            "in this paper , we propose a novel LATTICE PARSING METHODOLOGY for PARSING . the proposed LATTICE PARSING METHODOLOGY is based on a SYNTACTIC MODEL , a SYNTACTIC MODEL , and a SYNTACTIC MODEL for PARSING . the proposed LATTICE PARSING METHODOLOGY is based on a SYNTACTIC MODEL , a SYNTACTIC MODEL , and a PCFG-LA BERKELEY PARSER for PARSING . the proposed LATTICE PARSING METHODOLOGY is applied to the problem of PARSING and PARSING . the experimental results show that the proposed LATTICE PARSING METHODOLOGY is effective in improving the performance of the proposed LATTICE PARSING METHODOLOGY in terms of ERROR REDUCTION and F-SCORE .\n",
            "\n",
            "723 1000\n",
            "this paper provides an overview of the developments in AUDITORY VISUAL SPEECH PROCESSING , a special interest group within EUROSPEECH . i hope that this discussion will be informative and useful to readers in a variety of fields , including PSYCHOLOGY , SPEECH SCIENCE , ANIMATION , PSYCHOLINGUISTICS , HUMAN-MACHINE INTERACTION , HEARING-IMPAIRED COMMUNICATION , and numerous other fields which also share in this fruitful intersection . \n",
            "this paper addresses the problem of AUDITORY VISUAL SPEECH PROCESSING and HUMAN-MACHINE INTERACTION . we present a system that combines AUDITORY VISUAL SPEECH PROCESSING , PSYCHOLINGUISTICS , ANIMATION , ANIMATION , ANIMATION , ANIMATION , ANIMATION , ANIMATION , ANIMATION , ANIMATION , and ANIMATION . our results show that our approach is able to perform well on a variety of tasks including SPEECH SCIENCE , ANIMATION , ANIMATION , ANIMATION , ANIMATION , ANIMATION , ANIMATION , and ANIMATION .\n",
            "\n",
            "724 1000\n",
            "in this paper , we study the use of different FREQUENCY WARP-FACTORS for different ACOUSTIC CLASSES in a computationally efficient framework of VOCAL TRACT LENGTH NORMALIZATION . this is motivated by the fact that all ACOUSTIC CLASSES do not exhibit similar spectral variations as a result of PHYSIOLOGICAL DIFFERENCES IN VOCAL TRACT , and therefore , the use of a single FREQUENCY-WARP for the entire utterance may not be appropriate . we have recently proposed a VTLN METHOD that implements VOCAL TRACT LENGTH NORMALIZATION through a LINEAR-TRANSFORMATION of the conventional MFCC FEATURES and efficiently estimates the WARP-FACTOR using the same sufficient statistics as that are used in CMLLR ADAPTATION . in this paper we have shown that , in this framework of VOCAL TRACT LENGTH NORMALIZATION , and using the idea of REGRESSION CLASS TREE , we can obtain separate VOCAL TRACT LENGTH NORMALIZATION for different ACOUSTIC CLASSES . the use of REGRESSION CLASS TREE ensures that WARP-FACTOR is estimated for each class even when there is very little data available for that class . the ACOUSTIC CLASSES , in general , can be any collection of the GAUSSIAN COMPONENTS in the ACOUSTIC MODEL . we have built ACOUSTIC CLASSES by using DATA-DRIVEN APPROACH and by using PHONETIC KNOWLEDGE . using WSJ DATABASE we have shown the RECOGNITION performance of the proposed ACOUSTIC CLASS SPECIFIC WARP-FACTOR both for the data driven and the PHONETIC KNOWLEDGE BASED REGRESSION CLASS TREE DEFINITIONS and compare VTLN METHOD with the case of the single WARP-FACTOR . \n",
            "this paper presents a novel DATA-DRIVEN APPROACH for VOCAL TRACT LENGTH NORMALIZATION based on a DATA-DRIVEN APPROACH . the proposed DATA-DRIVEN APPROACH is based on the use of a REGRESSION CLASS TREE for VOCAL TRACT LENGTH NORMALIZATION . the proposed DATA-DRIVEN APPROACH is based on the use of a REGRESSION CLASS TREE for VOCAL TRACT LENGTH NORMALIZATION . the proposed DATA-DRIVEN APPROACH is based on the use of a REGRESSION CLASS TREE to estimate the ACOUSTIC CLASS SPECIFIC WARP-FACTOR of the MFCC FEATURES . the proposed DATA-DRIVEN APPROACH is evaluated on a WSJ DATABASE and on a WSJ DATABASE . the results show that the proposed DATA-DRIVEN APPROACH is effective in RECOGNITION and RECOGNITION .\n",
            "\n",
            "725 1000\n",
            "<unk> , the newly standardized <unk> codec for ENHANCED VOICE SERVICES was developed for MOBILE SERVICES such as VOLTE , where ERROR RESILIENCE is highly essential . the presented paper outlines all aspects of the advances brought during the EVS development on PACKET LOSS CONCEALMENT , by presenting a high level description of all TECHNICAL FEATURES present in the final STANDARDIZED CODEC . coupled with JITTER BUFFER MANAGEMENT , the ENHANCED VOICE SERVICES provides ROBUSTNESS against late or lost packets . the advantages of the new ENHANCED VOICE SERVICES over REFERENCE CODECS are further discussed based on listening test results . \n",
            "this paper addresses the problem of PACKET LOSS CONCEALMENT in MOBILE SERVICES such as PACKET LOSS CONCEALMENT and PACKET LOSS CONCEALMENT . we propose a novel approach to PACKET LOSS CONCEALMENT based on PACKET LOSS CONCEALMENT . the proposed approach is based on the use of ENHANCED VOICE SERVICES in order to reduce the ROBUSTNESS of the REFERENCE CODECS . the proposed approach is based on the use of ENHANCED VOICE SERVICES in order to reduce the number of TECHNICAL FEATURES in the REFERENCE CODECS . the proposed approach is evaluated on a variety of REFERENCE CODECS , including a STANDARDIZED CODEC , and PACKET LOSS CONCEALMENT .\n",
            "\n",
            "726 1000\n",
            "we present a technique for coarsely extracting the regions of NATURAL COLOR IMAGES which contain DIRECTIONAL DETAIL , e.g. , EDGES , TEXTURE , etc. , which we then use for IMAGE DATABASE INDEXING . as a measure of COLOR ACTIVITY , we use a PERCEPTUALLY MODIFIED DISTANCE MEASURE based on the SUM-OF-ANGLES CRITERION . we then apply HISTOGRAM THRESHOLDING TECHNIQUES to separate the image into smooth color regions and busy regions where EDGE , TEXTURE and colour activity exists . DATABASE INDICES are then created from the busy regions using the DIRECIONAL DETAIL HISTOGRAM TECHNIQUE and RETRIEVAL is performed using these . \n",
            "in this paper , we propose a novel method for IMAGE DATABASE INDEXING . the proposed method is based on the DIRECIONAL DETAIL HISTOGRAM TECHNIQUE , which is based on the DIRECIONAL DETAIL HISTOGRAM TECHNIQUE . the proposed method is based on the DIRECIONAL DETAIL HISTOGRAM TECHNIQUE , which is based on the DIRECIONAL DETAIL HISTOGRAM TECHNIQUE . the proposed method is based on the use of the DIRECIONAL DETAIL HISTOGRAM TECHNIQUE to estimate the DIRECTIONAL DETAIL and the TEXTURE . the proposed method is based on the DIRECIONAL DETAIL HISTOGRAM TECHNIQUE , which is based on the DIRECIONAL DETAIL HISTOGRAM TECHNIQUE . the proposed method is applied to the IMAGE DATABASE INDEXING of NATURAL COLOR IMAGES and TEXTURE . the experimental results show that the proposed method is effective in improving the RETRIEVAL performance .\n",
            "\n",
            "727 1000\n",
            "indoor FUNCTIONAL OBJECTS exhibit large view and appearance variations , thus are difficult to be recognized by the traditional APPEARANCE-BASED CLASSIFICATION PARADIGM . in this paper , we present an algorithm to parse INDOOR IMAGES based on two observations : i -rrb- the functionality is the most essential property to define an INDOOR OBJECT , e.g. '' a CHAIR to sit on '' ; ii -rrb- the GEOMETRY -LRB- 3D SHAPE -rrb- of an object is designed to serve its function . we formulate the nature of the OBJECT FUNCTION into a STOCHASTIC GRAMMAR MODEL . this model characterizes a JOINT DISTRIBUTION over the FUNCTION-GEOMETRY-APPEARANCE HIERARCHY . the JOINT DISTRIBUTION includes a SCENE CATEGORY , FUNCTIONAL GROUPS , FUNCTIONAL OBJECTS , FUNCTIONAL PARTS and 3D GEOMETRIC SHAPES . we use a SIMULATED ANNEALING MCMC ALGORITHM to find the maximum a POSTERIORI SOLUTION , i.e. a PARSE TREE . we design four DATA-DRIVEN STEPS to accelerate the search in the FGA SPACE : i -rrb- group the LINE SEGMENTS into 3D PRIMITIVE SHAPES , ii -rrb- assign FUNCTIONAL LABELS to these 3D PRIMITIVE SHAPES , iii -rrb- fill in MISSING OBJECTS/PARTS according to the FUNCTIONAL LABELS , and iv -rrb- synthesize 2D SEGMENTATION MAPS and verify the current PARSE TREE by the METROPOLIS-HASTINGS ACCEPTANCE PROBABILITY . the experimental results on several challenging INDOOR DATASETS demonstrate the proposed approach not only significantly <unk> the scope of INDOOR SCENE PARSING ALGORITHM from the SEGMENTATION and the 3D RECOVERY to the FUNCTIONAL OBJECT RECOGNITION , but also yields improved overall performance . \n",
            "in this paper , we propose a novel INDOOR SCENE PARSING ALGORITHM based on a STOCHASTIC GRAMMAR MODEL . the proposed STOCHASTIC GRAMMAR MODEL consists of two parts : 1 -rrb- a STOCHASTIC GRAMMAR MODEL of the PARSE TREE into a FGA SPACE , and 2 -rrb- a POSTERIORI SOLUTION to estimate the GEOMETRY -LRB- 3D SHAPE of the PARSE TREE . the proposed STOCHASTIC GRAMMAR MODEL consists of two parts : a STOCHASTIC GRAMMAR MODEL that consists of a JOINT DISTRIBUTION , a JOINT DISTRIBUTION , and LINE SEGMENTS , and a POSTERIORI SOLUTION . the GEOMETRY -LRB- 3D SHAPE consists of a set of LINE SEGMENTS , each of which consists of a set of LINE SEGMENTS , LINE SEGMENTS , and FUNCTIONAL LABELS . the proposed STOCHASTIC GRAMMAR MODEL is applied to the GEOMETRY -LRB- 3D SHAPE in the FGA SPACE . the proposed INDOOR SCENE PARSING ALGORITHM is evaluated on two INDOOR DATASETS . the experimental results show that the proposed INDOOR SCENE PARSING ALGORITHM is able to recover the GEOMETRY -LRB- 3D SHAPE of the object in a scene from a single image . the proposed INDOOR SCENE PARSING ALGORITHM is evaluated on two INDOOR DATASETS , and the results show that the proposed INDOOR SCENE PARSING ALGORITHM is effective in improving the SEGMENTATION and 3D RECOVERY .\n",
            "\n",
            "728 1000\n",
            "bagging is one the most classic ENSEMBLE LEARNING TECHNIQUES in the MACHINE LEARNING LITERATURE . the idea is to generate multiple subsets of the training data via BOOTSTRAPPING -LRB- RANDOM SAMPLING with replacement -rrb- , and then aggregate the output of the models trained from each subset via voting or averaging . as music is a TEMPORAL SIGNAL , we propose and study two BAGGING METHODS in this paper : the INTER-SONG INSTANCE BAGGING that BOOTSTRAPS SONG-LEVEL FEATURES , and the INTRA-SONG INSTANCE BAGGING that draws bootstrapping samples directly from SHORT-TIME FEATURES for each training song . in particular , we focus on the latter method , as BAGGING METHODS better exploits the TEMPORAL INFORMATION OF MUSIC SIGNALS . the BAGGING METHODS result in surprisingly effective models for MUSIC AUTO-TAGGING : incorporating the idea to a simple linear support vector machine -lrb- svm -rrb- based system yields ACCURACIES that are comparable or even superior to state-of-the-art , possibly more sophisticated methods for three different datasets . as the BAGGING METHODS is a META ALGORITHM , BAGGING METHODS holds the promise of improving other MIR SYSTEMS . \n",
            "this paper addresses the problem of TEMPORAL INFORMATION OF MUSIC SIGNALS in a MACHINE LEARNING LITERATURE . given a set of SHORT-TIME FEATURES , a set of BOOTSTRAPS SONG-LEVEL FEATURES are used to estimate the TEMPORAL SIGNAL of the TEMPORAL SIGNAL . the proposed ENSEMBLE LEARNING TECHNIQUES is based on the use of BAGGING METHODS to estimate the TEMPORAL SIGNAL of the TEMPORAL SIGNAL . the proposed BAGGING METHODS is based on the use of BAGGING METHODS to estimate the TEMPORAL SIGNAL of the SHORT-TIME FEATURES . the performance of the proposed BAGGING METHODS is evaluated on a variety of MACHINE LEARNING LITERATURE . the experimental results show that the proposed method is effective in reducing the number of SHORT-TIME FEATURES in the MACHINE LEARNING LITERATURE .\n",
            "\n",
            "729 1000\n",
            "in this paper , a FINGERPRINT EMBEDDING METHOD <unk> for the AND ANTI-COLLUSION CODE is proposed . the proposed FINGERPRINT EMBEDDING METHOD embeds both a CODE and an ORTHOGONAL FINGERPRINT using different BASIS VECTORS depending on the bit . although the DETECTION for the EMBEDDING METHOD is complex , the performance of the FINGERPRINTING SYSTEM using proposed EMBEDDING METHOD with the AND-ACC against average attack is improved compared with the AND ANTI-COLLUSION CODE using CODE MODULATION EMBEDDING METHOD . the FINGERPRINTING SYSTEM using the proposed EMBEDDING METHOD is robust against the LINEAR COMBINATION COLLUSION ATTACK whereas the FINGERPRINTING SYSTEM using the CODE MODULATION is not . \n",
            "in this paper , we propose a novel FINGERPRINT EMBEDDING METHOD for FINGERPRINTING SYSTEM based on CODE MODULATION . the proposed FINGERPRINT EMBEDDING METHOD is based on a LINEAR COMBINATION COLLUSION ATTACK , which is based on a LINEAR COMBINATION COLLUSION ATTACK . the proposed FINGERPRINT EMBEDDING METHOD is based on the use of a AND ANTI-COLLUSION CODE and a LINEAR COMBINATION COLLUSION ATTACK . the proposed FINGERPRINT EMBEDDING METHOD is based on the use of a AND ANTI-COLLUSION CODE and a LINEAR COMBINATION COLLUSION ATTACK . the proposed FINGERPRINT EMBEDDING METHOD is applied to the AND ANTI-COLLUSION CODE , which is based on a LINEAR COMBINATION COLLUSION ATTACK . the experimental results show that the proposed FINGERPRINT EMBEDDING METHOD is effective in improving the DETECTION performance in terms of DETECTION and CODE MODULATION .\n",
            "\n",
            "730 1000\n",
            "story generation is <unk> a <unk> , despite disappointing preliminary results from the preceding three decades . one of the principle reasons for previous <unk> was the low level of WRITING QUALITY , which resulted from the excessive focus of STORY GRAMMARS on PLOT DESIGN . although these systems leveraged NARRATIVE THEORY via CORPORA ANALYSES , they failed to thoroughly extend those analyses to all relevant linguistic levels . the end result was narratives that were recognizable as stories , but whose PROSE QUALITY was unsatisfactory . however , the <unk> for POOR WRITING QUALITY can not be laid <unk> at the feet of STORY GRAMMARS , as NATURAL LANGUAGE GENERATION has to-date not fielded systems capable of faithfully reproducing either the variety or COMPLEXITY of naturally occurring stories . this paper presents the AUTHOR ARCHITECTURE for accomplishing precisely that task , the <unk> implementation of a NARRATIVE PROSE GENERATOR , and a brief description of a formal evaluation of the stories it produces . \n",
            "this paper addresses the problem of STORY GENERATION for STORY GENERATION . a NARRATIVE PROSE GENERATOR is proposed in this paper . the proposed approach is based on the use of STORY GRAMMARS for STORY GENERATION . the proposed approach is based on the use of STORY GRAMMARS for STORY GENERATION . the proposed approach is based on the use of STORY GRAMMARS for STORY GENERATION . the experimental results show that the proposed approach is effective in improving the WRITING QUALITY of the NARRATIVE PROSE GENERATOR .\n",
            "\n",
            "731 1000\n",
            "two new approaches to ADAPTIVE BEAMFORMING in SPARSE SUBARRAY-BASED SENSOR ARRAYS are proposed . each <unk> is assumed to be well calibrated but the INTERSUBARRAY GAIN AND/OR PHASE MISMATCHES are assumed to remain unknown or imperfectly known . our first approach is based on a WORST-CASE BEAMFORMER DESIGN that , unlike the existing WORST-CASE DESIGNS , exploits a STRUCTURED ELLIPSOIDAL UNCERTAINTY MODEL for the SIGNAL STEERING VECTOR . our second approach exploits the idea of estimating the SIGNAL STEERING VECTOR by maximizing the output power of the MINIMUM VARIANCE BEAMFORMER . several modifications of our second approach are developed for the cases of GAIN-AND-PHASE AND PHASE-ONLY INTERSUBARRAY DISTORTIONS . \n",
            "this paper proposes a novel STRUCTURED ELLIPSOIDAL UNCERTAINTY MODEL for SPARSE SUBARRAY-BASED SENSOR ARRAYS . the proposed STRUCTURED ELLIPSOIDAL UNCERTAINTY MODEL is based on a STRUCTURED ELLIPSOIDAL UNCERTAINTY MODEL of the SIGNAL STEERING VECTOR of the SIGNAL STEERING VECTOR . the proposed STRUCTURED ELLIPSOIDAL UNCERTAINTY MODEL is based on a STRUCTURED ELLIPSOIDAL UNCERTAINTY MODEL . the proposed STRUCTURED ELLIPSOIDAL UNCERTAINTY MODEL is based on a STRUCTURED ELLIPSOIDAL UNCERTAINTY MODEL . the proposed STRUCTURED ELLIPSOIDAL UNCERTAINTY MODEL is applied to the GAIN-AND-PHASE AND PHASE-ONLY INTERSUBARRAY DISTORTIONS of the SIGNAL STEERING VECTOR . the performance of the proposed algorithm is demonstrated on a variety of GAIN-AND-PHASE AND PHASE-ONLY INTERSUBARRAY DISTORTIONS . the results show that the proposed STRUCTURED ELLIPSOIDAL UNCERTAINTY MODEL is able to accurately estimate the SIGNAL STEERING VECTOR of the SIGNAL STEERING VECTOR .\n",
            "\n",
            "732 1000\n",
            "large-scale distributed computing has made available the resources necessary to solve '' <unk> '' problems . as a result , it becomes feasible to automate the processing of such problems , but ACCURACY is not very high due to the conceptual difficulty of these problems . in this paper , we integrated CROWDSOURCING with MAPREDUCE to provide a scalable innovative HUMAN-MACHINE SOLUTION to AI-HARD PROBLEMS , which is called CROWDMR . in CROWDMR , the majority of problem instances are automatically processed by machine while the troublesome instances are <unk> to human via CROWDSOURCING . the results returned from CROWDSOURCING are validated in the form of CAPTCHA -LRB- COMPLETELY AUTOMATED PUBLIC TURING TEST to tell computers and humans apart -rrb- before adding to the output . an INCREMENTAL SCHEDULING METHOD was brought forward to combine the results from machine and human in a '' <unk> '' way . \n",
            "this paper presents a novel approach to LARGE-SCALE DISTRIBUTED COMPUTING based on CROWDSOURCING . the proposed HUMAN-MACHINE SOLUTION is based on the use of MAPREDUCE for LARGE-SCALE DISTRIBUTED COMPUTING . the proposed HUMAN-MACHINE SOLUTION is based on the use of MAPREDUCE for LARGE-SCALE DISTRIBUTED COMPUTING . the proposed INCREMENTAL SCHEDULING METHOD is based on the use of MAPREDUCE for LARGE-SCALE DISTRIBUTED COMPUTING . the proposed INCREMENTAL SCHEDULING METHOD is applied to the problem of LARGE-SCALE DISTRIBUTED COMPUTING , and the results show that the proposed INCREMENTAL SCHEDULING METHOD is effective in improving the ACCURACY of the HUMAN-MACHINE SOLUTION .\n",
            "\n",
            "733 1000\n",
            "a common assumption in MACHINE VISION is that the training and test samples are drawn from the same DISTRIBUTION . however , there are many problems when this assumption is <unk> violated , as in BIO-MEDICAL APPLICATIONS where different <unk> can generate drastic variations in the appearance of the data due to changing experimental conditions . this MACHINE VISION is <unk> with 3D DATA , for which ANNOTATION is very time-consuming , limiting the amount of data that can be labeled in new <unk> for training . in this paper we present a MULTI-TASK LEARNING ALGORITHM for DOMAIN ADAPTATION based on BOOSTING . unlike previous approaches that learn TASK-SPECIFIC DECISION BOUNDARIES , our MULTI-TASK LEARNING ALGORITHM learns a single DECISION BOUNDARY in a SHARED FEATURE SPACE , common to all tasks . we use the BOOSTING-TRICK to learn a NON-LINEAR MAPPING of the observations in each task , with no need for specific a-priori knowledge of its GLOBAL ANALYTICAL FORM . this yields a more PARAMETER-FREE DOMAIN ADAPTATION APPROACH that successfully leverages learning on new tasks where LABELED DATA is scarce . we evaluate our MULTI-TASK LEARNING ALGORITHM on two challenging BIO-MEDICAL DATASETS and achieve a significant improvement over the state of the art . \n",
            "this paper presents a novel MULTI-TASK LEARNING ALGORITHM for DOMAIN ADAPTATION in MACHINE VISION . the proposed PARAMETER-FREE DOMAIN ADAPTATION APPROACH is based on the use of BOOSTING to estimate the TASK-SPECIFIC DECISION BOUNDARIES of the 3D DATA . the proposed PARAMETER-FREE DOMAIN ADAPTATION APPROACH is based on the use of a NON-LINEAR MAPPING to estimate the DECISION BOUNDARY of the 3D DATA . the proposed PARAMETER-FREE DOMAIN ADAPTATION APPROACH is based on the use of BOOSTING to estimate the DECISION BOUNDARY of the 3D DATA . the proposed MULTI-TASK LEARNING ALGORITHM is evaluated on the BIO-MEDICAL DATASETS , and the results show that the proposed MULTI-TASK LEARNING ALGORITHM is effective in improving the ANNOTATION performance . the proposed MULTI-TASK LEARNING ALGORITHM is evaluated on several BIO-MEDICAL DATASETS , and the results show that the proposed MULTI-TASK LEARNING ALGORITHM is effective in improving the ANNOTATION performance .\n",
            "\n",
            "734 1000\n",
            "we describe a memory-efficient implementation of a DYNAMIC PROGRAMMING ALGORITHM for learning the optimal structure of a BAYESIAN NETWORK from training data . the DYNAMIC PROGRAMMING ALGORITHM leverages the LAYERED STRUCTURE of the DYNAMIC PROGRAMMING GRAPHS representing the RE-CURSIVE DECOMPOSITION of the problem to reduce the MEMORY REQUIREMENTS of the DYNAMIC PROGRAMMING ALGORITHM from o -lrb- n2 n -rrb- to o -lrb- c -lrb- n , <unk> -rrb- -rrb- , where c -lrb- n , <unk> -rrb- is the BINOMIAL COEFFICIENT . experimental results show that the DYNAMIC PROGRAMMING ALGORITHM runs up to an order of magnitude faster and scales to datasets with more variables than previous approaches . \n",
            "in this paper , we propose a novel DYNAMIC PROGRAMMING ALGORITHM for RE-CURSIVE DECOMPOSITION with RE-CURSIVE DECOMPOSITION . the proposed DYNAMIC PROGRAMMING ALGORITHM is based on the RE-CURSIVE DECOMPOSITION , which is based on the RE-CURSIVE DECOMPOSITION . the proposed DYNAMIC PROGRAMMING ALGORITHM is based on the RE-CURSIVE DECOMPOSITION , which is based on the RE-CURSIVE DECOMPOSITION . the proposed DYNAMIC PROGRAMMING ALGORITHM is applied to the problem of RE-CURSIVE DECOMPOSITION , which is based on the RE-CURSIVE DECOMPOSITION . experimental results show that the proposed LAYERED STRUCTURE can significantly improve the performance of the proposed DYNAMIC PROGRAMMING ALGORITHM .\n",
            "\n",
            "735 1000\n",
            "<unk> are a type of <unk> -lcb- like structures of a d dimensional image , characterized by LOCAL CONDITIONS . as CREASES tend to be at the center of ANISOTROPIC GREY -LCB- LEVEL SHAPES , CREASENESS can be considered as a type of MEDIALNESS . among the several CREASE DEENITIONS , one of the most important is based on the EXTREMA OF THE LEVEL SET CURVATURES . in 2 -lcb- d it is used the CURVATURE OF THE LEVEL CURVES of the IMAGE LANDSCAPE , however , the way it is usually computed produces a DISCON-TINUOUS CREASENESS MEASURE . the same problem arises in 3 -lcb- d with its straightforward extension and with other related CREASENESS MEASURES . in this paper , we rst present an alternative method of computing the LEVEL CURVE CURVATURE that avoids the DISCONTINUITIES . next , we propose the MEAN CURVATURE OF THE LEVEL SURFACES as CREASENESS MEASURE of 3 -lcb- d images , computed by the same method . finally , we propose a natural extension of our rst alternative method in order to enhance the CREASENESS MEASURE . \n",
            "in this paper , we propose a novel method for ANISOTROPIC GREY -LCB- LEVEL SHAPES in the presence of DISCONTINUITIES . the proposed method is based on the idea of using a DISCON-TINUOUS CREASENESS MEASURE to the MEAN CURVATURE OF THE LEVEL SURFACES of the IMAGE LANDSCAPE . the proposed method is based on the CURVATURE OF THE LEVEL CURVES of the LEVEL CURVE CURVATURE of the IMAGE LANDSCAPE . the proposed method is based on the CURVATURE OF THE LEVEL CURVES of the LEVEL CURVE CURVATURE of the IMAGE LANDSCAPE . the proposed method is based on the CURVATURE OF THE LEVEL CURVES of the LEVEL CURVE CURVATURE and the LEVEL CURVE CURVATURE of the signal . the proposed method is evaluated in terms of the EXTREMA OF THE LEVEL SET CURVATURES of the IMAGE LANDSCAPE , and the results show that the proposed method is effective in reducing the number of DISCONTINUITIES in the IMAGE LANDSCAPE .\n",
            "\n",
            "736 1000\n",
            "foreign accents in SECOND LANGUAGE PRODUCTION are caused by interference from the PHONOLOGICAL SYSTEM and phonetic realization of the speaker 's first language -lrb- l1 -rrb- , including both SEGMENTAL AND PROSODIC FEATURES . this paper examines the intonation structure of SEOUL KOREAN and its realization by AMERICAN ENGLISH SPEAKERS . four ENGLISH SPEAKERS OF KOREAN , differing in fluency , and two KOREAN SPEAKERS participated in the experiment . <unk> sentences were designed to test the realization of intonation patterns by varying the number of syllables within a word and a sentence , and by varying the conditions for the SEGMENT-TONE INTERACTION . results show that , as with SEGMENTAL DATA , more advanced L2 SPEAKERS produce more NATIVE-LIKE INTONATION PATTERNS and PROSODIC STRUCTURE than less advanced speakers . however , although advanced L2 SPEAKERS are better at grouping words into phrases , L2 SPEAKERS are not better at producing SURFACE TONAL REALIZATIONS of an accentual phrase than less advanced speakers . this suggests that PHONOLOGICAL PROPERTIES OF INTONATION are acquired earlier than PHONETIC PROPERTIES OF INTONATION . \n",
            "this paper addresses the problem of SECOND LANGUAGE PRODUCTION in KOREAN SPEAKERS . we propose a novel approach to SEGMENT-TONE INTERACTION based on SEGMENTAL DATA . the proposed approach is based on the use of SEGMENTAL AND PROSODIC FEATURES and PROSODIC STRUCTURE . the proposed approach is based on the use of SEGMENTAL AND PROSODIC FEATURES extracted from the SEGMENTAL DATA . the proposed approach is evaluated on a variety of AMERICAN ENGLISH SPEAKERS . the results show that the proposed method outperforms the existing methods in terms of both SEGMENTAL AND PROSODIC FEATURES and PROSODIC STRUCTURE .\n",
            "\n",
            "737 1000\n",
            "an analysis of the LOCAL CONVERGENCE SPEED of CONSTANT GAIN ALGORITHMS for DIRECT FORM IIR ADAPTIVE ¯ LTERS is initially presented , showing the ADVERSE E ® ECTS that result from the proximity of the poles of the MODELLED SYSTEM to the UNIT CIRCLE and , for COMPLEX POLES , to the REAL AXIS . a global analysis of the REDUCED ERROR SURFACE in these cases is also presented , which shows that , away from the global minimum , there will be regions with an almost constant error , where the convergence of CONSTANT GAIN ALGORITHMS tends to be slow . a POLYPHASE IIR ADAPTIVE ¯ LTER is then proposed and its LOCAL AND GLOBAL CONVERGENCE PROPERTIES are investigated , showing POLYPHASE IIR ADAPTIVE ¯ LTER to be specially well suited for applications with UNDERDAMPED LOW-FREQUENCY POLES . the POLYPHASE IIR ADAPTIVE ¯ LTER is tested with di ® <unk> CONSTANT GAIN ALGORITHMS in an <unk> example , attaining a gain of 14 to 70 times in GLOBAL CONVERGENCE SPEED over the DIRECT FORM , at the price of a relatively modest increase in COMPUTATIONAL COMPLEXITY . a theorem concerning the existence of stationary points for the POLYPHASE IIR ADAPTIVE ¯ LTER is also presented . \n",
            "in this paper , we present a novel method for DIRECT FORM IIR ADAPTIVE ¯ LTERS in the UNIT CIRCLE . the proposed method is based on a POLYPHASE IIR ADAPTIVE ¯ LTER of the POLYPHASE IIR ADAPTIVE ¯ LTER . the proposed method is based on a POLYPHASE IIR ADAPTIVE ¯ LTER of the POLYPHASE IIR ADAPTIVE ¯ LTER . the proposed method is based on a POLYPHASE IIR ADAPTIVE ¯ LTER , which is based on a POLYPHASE IIR ADAPTIVE ¯ LTER . the proposed method is based on a POLYPHASE IIR ADAPTIVE ¯ LTER . the proposed method is based on a POLYPHASE IIR ADAPTIVE ¯ LTER . the proposed method is evaluated in terms of the GLOBAL CONVERGENCE SPEED and the COMPUTATIONAL COMPLEXITY of the MODELLED SYSTEM . the performance of the proposed method is evaluated in terms of the GLOBAL CONVERGENCE SPEED and the COMPUTATIONAL COMPLEXITY of the MODELLED SYSTEM . the proposed method is evaluated in terms of the GLOBAL CONVERGENCE SPEED and the COMPUTATIONAL COMPLEXITY of the MODELLED SYSTEM .\n",
            "\n",
            "738 1000\n",
            "accurate localization of multiple sound sources is indispensable for the MICROPHONE ARRAY-BASED HIGH QUALITY SOUND CAPTURE . for SINGLE SOUND SOURCE LOCALIZATION , the CSP -LRB- CROSS-POWER SPECTRUM PHASE ANALYSIS -RRB- METHOD has been proposed . the CSP -LRB- CROSS-POWER SPECTRUM PHASE ANALYSIS -RRB- METHOD localizes a SOUND SOURCE as a crossing point of sound directions estimated using DIER-ENT MICROPHONE PAIRS . however , when localizing multiple sound sources , the CSP -LRB- CROSS-POWER SPECTRUM PHASE ANALYSIS -RRB- METHOD has a problem that the LOCALIZATION ACCURACY is degraded due to cross-correlation among dierent sound sources . to solve this problem , this paper proposes a new method which suppresses the UN-DESIRED CROSS-CORRELATION by SYNCHRONOUS ADDITION of CSP COECIENTS derived from multiple microphone pairs . experiment results in a real room showed that the proposed method improves the LOCALIZATION ACCURACY when increasing the number of the SYNCHRONOUS ADDITION . \n",
            "this paper presents a novel CSP -LRB- CROSS-POWER SPECTRUM PHASE ANALYSIS -RRB- METHOD for SINGLE SOUND SOURCE LOCALIZATION . the proposed CSP -LRB- CROSS-POWER SPECTRUM PHASE ANALYSIS -RRB- METHOD is based on a CSP -LRB- CROSS-POWER SPECTRUM PHASE ANALYSIS -RRB- METHOD for SINGLE SOUND SOURCE LOCALIZATION . the proposed CSP -LRB- CROSS-POWER SPECTRUM PHASE ANALYSIS -RRB- METHOD is based on a novel CSP -LRB- CROSS-POWER SPECTRUM PHASE ANALYSIS -RRB- METHOD for SINGLE SOUND SOURCE LOCALIZATION . the proposed CSP -LRB- CROSS-POWER SPECTRUM PHASE ANALYSIS -RRB- METHOD is based on a CSP -LRB- CROSS-POWER SPECTRUM PHASE ANALYSIS -RRB- METHOD for SINGLE SOUND SOURCE LOCALIZATION . the proposed CSP -LRB- CROSS-POWER SPECTRUM PHASE ANALYSIS -RRB- METHOD is applied to SINGLE SOUND SOURCE LOCALIZATION . the experimental results show that the proposed CSP -LRB- CROSS-POWER SPECTRUM PHASE ANALYSIS -RRB- METHOD is effective in improving the LOCALIZATION ACCURACY and LOCALIZATION ACCURACY .\n",
            "\n",
            "739 1000\n",
            "we present a POLICY SEARCH METHOD that uses iteratively <unk> LOCAL LINEAR MODELS to optimize TRAJECTORY DISTRIBUTIONS for large , continuous problems . these TRAJECTORY DISTRIBUTIONS can be used within the framework of GUIDED POLICY SEARCH to learn POLICIES with an ARBITRARY PARAMETERIZATION . our POLICY SEARCH METHOD fits TIME-VARYING LINEAR DYNAMICS MODELS to speed up LEARNING , but does not rely on LEARNING a GLOBAL MODEL , which can be difficult when the dynamics are complex and discontinuous . we show that this POLICY SEARCH METHOD requires many fewer samples than MODEL-FREE METHODS , and can handle complex , NONSMOOTH DYNAMICS that can pose a challenge for MODEL-BASED TECHNIQUES . we present experiments showing that our POLICY SEARCH METHOD can be used to learn complex NEURAL NETWORK POLICIES that successfully execute SIMULATED ROBOTIC MANIPULATION TASKS in partially observed environments with numerous CONTACT DISCONTINUITIES and UNDERACTUATION . \n",
            "in this paper , we propose a novel POLICY SEARCH METHOD for SIMULATED ROBOTIC MANIPULATION TASKS . the proposed GLOBAL MODEL is based on the use of TIME-VARYING LINEAR DYNAMICS MODELS , which is a GLOBAL MODEL for LEARNING . the proposed GLOBAL MODEL is based on the use of TIME-VARYING LINEAR DYNAMICS MODELS , which is a GLOBAL MODEL for GUIDED POLICY SEARCH . the proposed GLOBAL MODEL is based on the use of LOCAL LINEAR MODELS , which is a GLOBAL MODEL for LEARNING . the proposed GLOBAL MODEL is applied to the problem of GUIDED POLICY SEARCH for SIMULATED ROBOTIC MANIPULATION TASKS . the experimental results show that the proposed GLOBAL MODEL is effective for SIMULATED ROBOTIC MANIPULATION TASKS in SIMULATED ROBOTIC MANIPULATION TASKS . the proposed POLICY SEARCH METHOD is compared with other state-of-the-art methods in terms of both CONTACT DISCONTINUITIES and UNDERACTUATION .\n",
            "\n",
            "740 1000\n",
            "classification using HIDDEN MARKOV MODELS is in general done by comparing the MODEL LIKELIHOODS and choosing the class more likely to have generated the data . this work investigates a CONDITIONED HMM which additionally provides a probability for a CLASS LABEL and compares different FUSION STRATEGIES . the notion is twofold : on the one hand applications in AFFEC-TIVE COMPUTING might pass their uncertainty of the CLASSIFICATION to the next PROCESSING UNIT , on the other hand different streams might be fused to increase the performance . the data set studied incorporates two modalities and is based on a NATURALISTIC MUL-TIPARTY DIALOGUE . the goal is to discriminate between laughter and utterances . it turned out that the CONDITIONED HMM out-performs CLASSICAL HMM using different LATE FUSION APPROACHES while additionally providing a certainty about CLASS DECISION . \n",
            "this paper presents a novel approach to AFFEC-TIVE COMPUTING in NATURALISTIC MUL-TIPARTY DIALOGUE . the proposed approach is based on the use of HIDDEN MARKOV MODELS for CLASSIFICATION . the proposed approach is based on the use of HIDDEN MARKOV MODELS , which is based on a CLASSICAL HMM . the proposed approach is based on the use of HIDDEN MARKOV MODELS . the proposed approach is based on the use of HIDDEN MARKOV MODELS . the proposed method is based on the use of HIDDEN MARKOV MODELS . the proposed method is evaluated on a NATURALISTIC MUL-TIPARTY DIALOGUE and a NATURALISTIC MUL-TIPARTY DIALOGUE . the results show that the proposed method is effective in improving the CLASSIFICATION performance .\n",
            "\n",
            "741 1000\n",
            "over the years , several SPATIO-TEMPORAL INTEREST POINT DETECTORS have been proposed . while some detectors can only extract a sparse set of SCALE-INVARIANT FEATURES , others allow for the detection of a larger amount of FEATURES at USER-DEFINED SCALES . this paper presents for the first time SPATIO-TEMPORAL INTEREST POINTS that are at the same time <unk> -lrb- both spatially and temporally -rrb- and densely cover the VIDEO CONTENT . moreover , as opposed to earlier work , the FEATURES can be computed efficiently . applying SCALE-SPACE THEORY , we show that FEATURES can be achieved by using the determinant of the HESSIAN as the SALIENCY MEASURE . computations are <unk> further through the use of APPROXIMATIVE BOX-FILTER OPERATIONS on an INTEGRAL VIDEO STRUCTURE . a quantitative evaluation and experimental results on ACTION RECOGNITION show the strengths of the proposed detector in terms of repeatability , ACCURACY and SPEED , in comparison with previously proposed detectors . \n",
            "in this paper , we propose a novel approach to ACTION RECOGNITION . the proposed approach is based on the use of a SALIENCY MEASURE and a SALIENCY MEASURE for ACTION RECOGNITION . the proposed approach is based on the use of a SALIENCY MEASURE and a novel SALIENCY MEASURE for ACTION RECOGNITION . the proposed approach is based on the use of a SALIENCY MEASURE and a SALIENCY MEASURE for ACTION RECOGNITION . the experimental results show that the proposed method outperforms the state-of-the-art methods in terms of ACCURACY , SPEED and SPEED .\n",
            "\n",
            "742 1000\n",
            "we present a novel method for analyzing reflections on ARBITRARY SURFACES . we model reflections using a broader than usual class of IMAGING MODELS , which include both PERSPECTIVE AND MULTIPERSPECTIVE CAMERA TYPES . we provide an ANALYTICAL FRAMEWORK to LOCALLY MODEL REFLECTIONS as specific MULTIPERSPECTIVE CAMERAS around every ray based on a new theory of GENERAL LINEAR CAMERAS . our ANALYTICAL FRAMEWORK better characterizes the complicated IMAGE DISTORTIONS seen on IRREGULAR MIRROR SURFACES as well as the conventional CATA-DIOPTRIC MIRRORS . we show the connection between MULTIPER-SPECTIVE CAMERA MODELS and CAUSTIC SURFACES OF REFLECTIONS and demonstrate how MULTIPER-SPECTIVE CAMERA MODELS reveal important surface <unk> of the <unk> . finally , we show how to use our ANALYTICAL FRAMEWORK to assist MIRROR DESIGN and characterize distortions seen in CATADIOPTRIC IMAGING SYSTEMS . \n",
            "this paper presents a novel ANALYTICAL FRAMEWORK for GENERAL LINEAR CAMERAS in the context of CATADIOPTRIC IMAGING SYSTEMS . the proposed ANALYTICAL FRAMEWORK is based on the use of GENERAL LINEAR CAMERAS and CAUSTIC SURFACES OF REFLECTIONS . the proposed ANALYTICAL FRAMEWORK is based on the use of GENERAL LINEAR CAMERAS and CAUSTIC SURFACES OF REFLECTIONS . the proposed ANALYTICAL FRAMEWORK is based on the use of GENERAL LINEAR CAMERAS and CAUSTIC SURFACES OF REFLECTIONS . the proposed ANALYTICAL FRAMEWORK is applied to ARBITRARY SURFACES , and the results show that the proposed ANALYTICAL FRAMEWORK is capable of capturing IMAGE DISTORTIONS and CAUSTIC SURFACES OF REFLECTIONS .\n",
            "\n",
            "743 1000\n",
            "a REALTIME SOFTWARE MPEG TRANSCODER has been developed . a novel MOTION VECTOR REUSE and a SIMD OPTIMIZATION TECHNIQUES are introduced to accelerate the <unk> without any QUALITY DEGRADATION . MEAN ABSOLUTE ERROR APPROXIMATION CRITERIA are employed in the REUSE TECHNIQUE to refine SCALED MOTION VECTORS . the developed <unk> on PENTIUM II 266MHZ runs 2.5 times as fast as REALTIME , when scaling an MPEG-1 BITSTREAM to half size . \n",
            "in this paper , we propose a novel REALTIME SOFTWARE MPEG TRANSCODER for MOTION VECTOR REUSE and SIMD OPTIMIZATION TECHNIQUES . the proposed REUSE TECHNIQUE is based on a REUSE TECHNIQUE , which is based on the MEAN ABSOLUTE ERROR APPROXIMATION CRITERIA and the SIMD OPTIMIZATION TECHNIQUES . the proposed REUSE TECHNIQUE is based on the MEAN ABSOLUTE ERROR APPROXIMATION CRITERIA and the MEAN ABSOLUTE ERROR APPROXIMATION CRITERIA of the SCALED MOTION VECTORS . the performance of the proposed REALTIME SOFTWARE MPEG TRANSCODER is evaluated in terms of both QUALITY DEGRADATION and MEAN ABSOLUTE ERROR APPROXIMATION CRITERIA . the performance of the proposed algorithm is evaluated on a number of examples .\n",
            "\n",
            "744 1000\n",
            "to realize a CONVERSATIONAL INTERFACE where an AGENT SYSTEM can smoothly communicate with multiple persons , it is imperative to know how the START TIMING of speaking is decided . in this research , we demonstrate a relationship between GAZE TRANSITION PATTERNS and the START TIMING of next speaking against the end of the last speaking in MULTI-PARTY MEETINGS . then , we construct a PREDICTION MODEL for the START TIMING using GAZE TRANSITION PATTERNS near the end of an utterance . an analysis of data collected from NATURAL MULTI-PARTY MEETINGS reveals a strong relationship between GAZE TRANSITION PATTERNS of the speaker , next speaker , and listener and the START TIMING of the next speaker . on the basis of the results , we used GAZE TRANSITION PATTERNS of the speaker , next speaker , and listener and mutual gaze as variables , and devised several PREDICTION MODELS . a PREDICTION MODEL using all FEATURES performed the best and was able to predict the START TIMING well . \n",
            "this paper presents a novel approach to MULTI-PARTY MEETINGS based on GAZE TRANSITION PATTERNS . the proposed PREDICTION MODEL is based on the use of FEATURES extracted from the FEATURES . the proposed PREDICTION MODEL is based on the use of FEATURES extracted from the FEATURES of the speech signal . the proposed PREDICTION MODEL is based on the use of FEATURES extracted from the FEATURES . experimental results show that the proposed PREDICTION MODEL is effective in improving the performance of the AGENT SYSTEM .\n",
            "\n",
            "745 1000\n",
            "this paper develops a STATISTICAL INFERENCE APPROACH , STATISTICAL INFERENCE APPROACH , for STYLE TRANSFORMATION between PHOTO IMAGES and SKETCH IMAGES OF HUMAN FACES . motivated by the rationale that IMAGE APPEARANCE is determined by two COOPERATIVE FACTORS : IMAGE CONTENT and IMAGE STYLE , we first model the interaction between these factors through learning a PATCH-BASED TENSOR MODEL . second , by introducing a COMMON VARIATION SPACE , we capture the inherent connection between PHOTO PATCH SPACE and SKETCH PATCH SPACE , thus building BIDIRECTIONAL MAPPING/INFERRING between the two spaces . subsequently , we formulate a BAYESIAN APPROACH accounting for the STATISTICAL INFERENCE from sketches to their corresponding photos in terms of the learned TENSOR MODEL . comparative experiments are conducted to contrast the proposed BAYESIAN APPROACH with state-of-the-art algorithms for FACIAL SKETCH SYNTHESIS in a novel FACE HALLUCINATION SCENARIO : SKETCH-BASED FACIAL PHOTO HALLUCINATION . the encouraging results obtained convincingly validate the effectiveness of our BAYESIAN APPROACH . \n",
            "in this paper , we propose a novel STATISTICAL INFERENCE APPROACH , called SKETCH-BASED FACIAL PHOTO HALLUCINATION , for SKETCH-BASED FACIAL PHOTO HALLUCINATION . the proposed STATISTICAL INFERENCE APPROACH is based on the PATCH-BASED TENSOR MODEL , a STATISTICAL INFERENCE APPROACH , and a PATCH-BASED TENSOR MODEL to estimate the IMAGE CONTENT . the proposed STATISTICAL INFERENCE APPROACH is based on a BAYESIAN APPROACH , called PATCH-BASED TENSOR MODEL , to estimate the IMAGE CONTENT and the IMAGE CONTENT . the proposed STATISTICAL INFERENCE APPROACH is based on the PATCH-BASED TENSOR MODEL , which is based on the PATCH-BASED TENSOR MODEL . the proposed STATISTICAL INFERENCE APPROACH is based on the PATCH-BASED TENSOR MODEL , which is based on the PATCH-BASED TENSOR MODEL . the proposed STATISTICAL INFERENCE APPROACH is applied to the SKETCH IMAGES OF HUMAN FACES and the SKETCH-BASED FACIAL PHOTO HALLUCINATION . the experimental results show that the proposed BAYESIAN APPROACH is able to detect and track moving objects in a scene from a FACE HALLUCINATION SCENARIO .\n",
            "\n",
            "746 1000\n",
            "standard ADAPTIVE FEEDBACK CANCELLATION ALGORITHMS in HEARING AIDS suffer from a BIASED ADAPTATION if the input signal is spectrally colored , as it is for TONAL SIGNALS , like music . due to that , DISTORTION ARTIFACTS are generated . in this paper , a SUB-BAND FEEDBACK CANCELLATION SYSTEM is presented combined with an ADAPTATION CONTROL to deal with those signals . two CONTROL CONCEPTS for determining the VARIABLE STEP SIZES -lsb- 1 , 2 -rsb- , known from GENERAL ADAPTIVE FILTER ALGORITHMS , are theoretically and practically analyzed and evaluated for an application to FEEDBACK CANCELLATION . for FEEDBACK CANCELLATION the control is combined with known methods to reduce the BIAS , such as PREDICTION ERROR FILTERING or FREQUENCY SHIFTING . based on this combination , a completely new SETUP for FEEDBACK CANCELLATION is proposed . SETUP relies entirely on signals accessible in real systems , shows a low COMPUTATIONAL COMPLEXITY , and therefore has a strong practical relevance . \n",
            "this paper addresses the problem of FEEDBACK CANCELLATION in HEARING AIDS . we propose a novel approach to the problem of ADAPTATION CONTROL , which is based on the use of FREQUENCY SHIFTING , FREQUENCY SHIFTING and FREQUENCY SHIFTING . the proposed approach is based on the use of FREQUENCY SHIFTING , FREQUENCY SHIFTING , FREQUENCY SHIFTING , and FREQUENCY SHIFTING . the proposed approach is based on the use of FREQUENCY SHIFTING , FREQUENCY SHIFTING and FREQUENCY SHIFTING . the experimental results show that the proposed approach is effective in improving the COMPUTATIONAL COMPLEXITY of the SUB-BAND FEEDBACK CANCELLATION SYSTEM .\n",
            "\n",
            "747 1000\n",
            "boltzmann machines are able to learn highly complex , multimodal , structured and multiscale real-world data distributions . parameters of the model are usually learned by minimizing the KULLBACK-LEIBLER DIVERGENCE from training samples to the learned model . we propose in this work a novel approach for BOLTZMANN MACHINE TRAINING which assumes that a meaningful metric between observations is given . this metric can be represented by the WASSERSTEIN DISTANCE between distributions , for which we derive a GRADIENT with respect to the MODEL PARAMETERS . minimization of this new objective leads to GENERATIVE MODELS with different STATISTICAL PROPERTIES . we demonstrate their practical potential on DATA COMPLETION and DENOISING , for which the metric between observations plays a crucial role . \n",
            "this paper addresses the problem of DATA COMPLETION in the presence of DENOISING . we propose a novel method for DATA COMPLETION in the context of BOLTZMANN MACHINES . the proposed approach is based on the use of BOLTZMANN MACHINES , which is a generalization of the traditional BOLTZMANN MACHINES . the proposed approach is based on the use of BOLTZMANN MACHINES with KULLBACK-LEIBLER DIVERGENCE . the proposed method is based on the use of BOLTZMANN MACHINES with KULLBACK-LEIBLER DIVERGENCE . the proposed method can be applied to other GENERATIVE MODELS with STATISTICAL PROPERTIES . experimental results show that the proposed method outperforms the state-of-the-art methods .\n",
            "\n",
            "748 1000\n",
            "transfer learning has been used in OPINION ANALYSIS to make use of available LANGUAGE RESOURCES for other resource scarce languages . however , the CUMULATIVE CLASS NOISE in TRANSFER LEARNING adversely affects performance when more training data is used . in this paper , we propose a novel method in TRANSDUCTIVE TRANSFER LEARNING to identify NOISES through the DETECTION OF NEGATIVE TRANSFERS . evaluation on NLP&CC 2013 CROSS-LINGUAL OPINION ANALYSIS DATASET shows that our approach outperforms the state-of-the-art systems . more significantly , our system shows a MONOTONIC INCREASE TREND in performance improvement when more training data are used . \n",
            "this paper addresses the problem of DETECTION OF NEGATIVE TRANSFERS for OPINION ANALYSIS . we propose a novel approach to the problem of TRANSDUCTIVE TRANSFER LEARNING for OPINION ANALYSIS . the proposed approach is based on the use of TRANSDUCTIVE TRANSFER LEARNING to estimate the CUMULATIVE CLASS NOISE . the proposed approach is based on a MONOTONIC INCREASE TREND , which is based on a MONOTONIC INCREASE TREND . the proposed approach is evaluated on the NLP&CC 2013 CROSS-LINGUAL OPINION ANALYSIS DATASET , and the results show that the proposed method is effective in improving the DETECTION OF NEGATIVE TRANSFERS performance .\n",
            "\n",
            "749 1000\n",
            "we propose ADAPTIVE CHANNEL PREDICTORS for ORTHOGONAL FREQUENCY DIVISION MULTIPLEXING COMMUNICATIONS over TIME-VARYING CHANNELS . successful application of the NORMALIZED LEAST-MEAN-SQUARE and RECURSIVE LEAST-SQUARES ALGORITHMS is demonstrated . we also consider the use of ADAPTIVE CHANNEL PRE-DICTORS for DELAY-FREE EQUALIZATION , thereby avoiding the need for regular transmission of pilot symbols . simulation results demonstrate the good performance of the proposed techniques . \n",
            "this paper addresses the problem of DELAY-FREE EQUALIZATION in TIME-VARYING CHANNELS . we propose a novel approach to DELAY-FREE EQUALIZATION based on ADAPTIVE CHANNEL PREDICTORS and RECURSIVE LEAST-SQUARES ALGORITHMS . the proposed algorithm is based on the use of ADAPTIVE CHANNEL PREDICTORS and RECURSIVE LEAST-SQUARES ALGORITHMS . the proposed method is based on the use of ADAPTIVE CHANNEL PREDICTORS and RECURSIVE LEAST-SQUARES ALGORITHMS . the experimental results show that the proposed method is effective in improving the performance of DELAY-FREE EQUALIZATION .\n",
            "\n",
            "750 1000\n",
            "in this paper , we present a comprehensive study of the relationship between an INDI-VIDUAL 'S PERSONAL TRAITS and HIS/HER BRAND PREFERENCES . in our analysis , we included a large number of CHARACTER TRAITS such as PERSONALITY , PERSONAL VALUES and individual needs . these CHARACTER TRAITS were obtained from both a PSYCHOMETRIC SURVEY and AUTOMATED SOCIAL MEDIA ANALYTICS . we also included an extensive set of brand names from diverse product categories . from this analysis , we want to shed some light on -lrb- 1 -rrb- whether it is possible to use PERSONAL TRAITS to infer an individual 's brand preferences -lrb- 2 -rrb- whether the CHARACTER TRAITS automatically inferred from SOCIAL MEDIA are good proxies for the GROUND TRUTH CHARACTER TRAITS in BRAND PREFERENCE PREDICTION . \n",
            "this paper addresses the problem of AUTOMATED SOCIAL MEDIA ANALYTICS and AUTOMATED SOCIAL MEDIA ANALYTICS for AUTOMATED SOCIAL MEDIA ANALYTICS and AUTOMATED SOCIAL MEDIA ANALYTICS . the main contribution of this paper is twofold : -lrb- 1 -rrb- we show that it is possible to recover the CHARACTER TRAITS , and -lrb- 2 -rrb- we show that it is possible to recover the GROUND TRUTH CHARACTER TRAITS of the CHARACTER TRAITS , and to estimate the CHARACTER TRAITS , such as PERSONALITY or CHARACTER TRAITS . we also show that the proposed algorithm is able to recover the GROUND TRUTH CHARACTER TRAITS of a scene from a single image . we demonstrate the effectiveness of our method on a variety of GROUND TRUTH CHARACTER TRAITS , including PERSONALITY and GROUND TRUTH CHARACTER TRAITS .\n",
            "\n",
            "751 1000\n",
            "we describe a GENERATIVE BAYESIAN MODEL for ACTION UNDERSTANDING in which INVERSE-FORWARD INTERNAL MODEL PAIRS are considered ` HYPOTHESES ' of plausible action goals that are explored in parallel via an APPROXIMATE INFERENCE MECHANISM based on SEQUENTIAL MONTE CARLO METHODS . the REENACTMENT OF INTERNAL MODEL PAIRS can be considered a form of MOTOR SIMULATION , which supports both PERCEPTUAL PREDICTION AND ACTION UNDERSTANDING at the goal level . however , this GENERATIVE BAYESIAN MODEL is generally considered to be computationally inefficient . we present a GENERATIVE BAYESIAN MODEL that dynamically <unk> COMPUTATIONAL RESOURCES to more accurate INTERNAL MODELS depending on both the available prior information and the PREDICTION ERROR of the INVERSE-FORWARD MODELS , and which leads to successful ACTION RECOGNITION . we present experimental results that test the robustness and efficiency of our GENERATIVE BAYESIAN MODEL in REAL-WORLD SCENARIOS . \n",
            "this paper presents a novel GENERATIVE BAYESIAN MODEL for PERCEPTUAL PREDICTION AND ACTION UNDERSTANDING in REAL-WORLD SCENARIOS . the proposed GENERATIVE BAYESIAN MODEL is based on the use of SEQUENTIAL MONTE CARLO METHODS , which is a GENERATIVE BAYESIAN MODEL for ACTION UNDERSTANDING . the GENERATIVE BAYESIAN MODEL is based on a GENERATIVE BAYESIAN MODEL of the GENERATIVE BAYESIAN MODEL . the proposed GENERATIVE BAYESIAN MODEL is based on the use of ` HYPOTHESES , which is a GENERATIVE BAYESIAN MODEL of the GENERATIVE BAYESIAN MODEL . the proposed GENERATIVE BAYESIAN MODEL is evaluated on two REAL-WORLD SCENARIOS . the experimental results show that the proposed GENERATIVE BAYESIAN MODEL significantly improves the ACTION RECOGNITION performance of the proposed GENERATIVE BAYESIAN MODEL in terms of both PERCEPTUAL PREDICTION AND ACTION UNDERSTANDING and ACTION RECOGNITION .\n",
            "\n",
            "752 1000\n",
            "we study to incorporate multiple views of data in a perceptive TRANSFER LEARNING FRAMEWORK and propose a MULTI-VIEW DISCRIMINANT TRANSFER LEARNING APPROACH for DOMAIN ADAPTATION . the main idea is to find the optimal DISCRIMINANT WEIGHT VECTORS for each view such that the correlation between the TWO-VIEW PROJECTED DATA is maximized , while both the DOMAIN DISCREPANCY and the VIEW DISAGREEMENT are minimized simultaneously . furthermore , we analyze MULTI-VIEW DISCRIMINANT TRANSFER LEARNING APPROACH theoretically from DISCRIMINANT ANALYSIS PERSPECTIVE to explain the condition and reason , under which the proposed MULTI-VIEW DISCRIMINANT TRANSFER LEARNING APPROACH is not applicable . the analytical results allow us to investigate whether there exist WITHIN-VIEW AND/OR BETWEEN-VIEW CONFLICTS , and thus provides a deep insight into whether the TRANSFER LEARNING ALGORITHM work properly or not in the VIEW-BASED PROBLEMS and the combined LEARNING PROBLEM . experiments show that MULTI-VIEW DISCRIMINANT TRANSFER LEARNING APPROACH significantly outperforms the state-of-the-art baselines including some typical MULTI-VIEW LEARNING APPROACHES in SINGLE-OR CROSS-DOMAIN . \n",
            "this paper presents a novel MULTI-VIEW DISCRIMINANT TRANSFER LEARNING APPROACH for DOMAIN ADAPTATION . the proposed MULTI-VIEW DISCRIMINANT TRANSFER LEARNING APPROACH is based on a DISCRIMINANT ANALYSIS PERSPECTIVE , which is a LEARNING PROBLEM . the proposed MULTI-VIEW DISCRIMINANT TRANSFER LEARNING APPROACH is based on the use of a DISCRIMINANT ANALYSIS PERSPECTIVE and DISCRIMINANT WEIGHT VECTORS . the proposed MULTI-VIEW DISCRIMINANT TRANSFER LEARNING APPROACH is based on a DISCRIMINANT ANALYSIS PERSPECTIVE , which is a LEARNING PROBLEM . the proposed MULTI-VIEW DISCRIMINANT TRANSFER LEARNING APPROACH is applied to the problem of DOMAIN ADAPTATION and DOMAIN ADAPTATION . the experimental results show that the proposed MULTI-VIEW DISCRIMINANT TRANSFER LEARNING APPROACH can significantly improve the performance of the proposed MULTI-VIEW DISCRIMINANT TRANSFER LEARNING APPROACH .\n",
            "\n",
            "753 1000\n",
            "we propose a new method for SHAPE RECONSTRUCTION from NOISY AND UNORGANIZED POINT DATA . we represent a SHAPE through its SIGNED DISTANCE FUNCTION and formulate SHAPE RECONSTRUCTION as a CONSTRAINED ENERGY MINIMIZATION PROBLEM directly based on the OBSERVED POINT SET . the associated energy function includes both the likelihood of the observed data points and a SMOOTHNESS PRIOR on the RECONSTRUCTED SHAPE . to solve this OPTIMIZATION PROBLEM , an efficient DATA-DRIVEN LEVEL SET METHOD is developed . our method is robust to LOCAL MINIMA , CLUTTER , and NOISE . it is also applicable to situations where the data are sparse . the TOPOLOGI-CAL NATURE of the underlying SHAPE is handled automatically through the LEVEL SET FORMALISM . \n",
            "in this paper , we propose a novel DATA-DRIVEN LEVEL SET METHOD for SHAPE RECONSTRUCTION . the proposed CONSTRAINED ENERGY MINIMIZATION PROBLEM is based on a DATA-DRIVEN LEVEL SET METHOD that exploits the TOPOLOGI-CAL NATURE between the NOISY AND UNORGANIZED POINT DATA and the NOISE . the proposed DATA-DRIVEN LEVEL SET METHOD is based on a DATA-DRIVEN LEVEL SET METHOD that exploits the TOPOLOGI-CAL NATURE between the NOISY AND UNORGANIZED POINT DATA and the NOISE . the proposed DATA-DRIVEN LEVEL SET METHOD is based on a DATA-DRIVEN LEVEL SET METHOD that exploits the TOPOLOGI-CAL NATURE between the NOISY AND UNORGANIZED POINT DATA and the NOISE . the proposed DATA-DRIVEN LEVEL SET METHOD is applied to the problem of SHAPE RECONSTRUCTION and NOISE . the experimental results demonstrate the effectiveness of the proposed DATA-DRIVEN LEVEL SET METHOD .\n",
            "\n",
            "754 1000\n",
            "this paper presents our entry to a SPEECH-IN-NOISE INTELLIGIBILITY ENHANCEMENT EVALUATION : the HURRICANE CHALLENGE . the system consists of a TEXT-TO-SPEECH VOICE manipulated through a combination of ENHANCEMENT STRATEGIES , each of which is known to be individually successful : a PERCEPTUALLY-MOTIVATED SPECTRAL SHAPER based on the GLIMPSE PROPORTION MEASURE , DYNAMIC RANGE COMPRESSION , and adaptation to LOMBARD EXCITATION AND DURATION PATTERNS . we achieved substantial intelligibility improvements relative to UNMODIFIED SYNTHETIC SPEECH : 4.9 db in competing speaker and 4.1 db in SPEECH-SHAPED NOISE . an analysis conducted across this and other two similar evaluations shows that the SPECTRAL SHAPER and the COMPRESSOR -lrb- both of which are loudness <unk> -rrb- contribute most under higher SNR CONDITIONS , particularly for SPEECH-SHAPED NOISE . DURATION AND EXCITATION LOMBARD-ADAPTED CHANGES are more beneficial in lower SNR CONDITIONS , and for competing speaker noise . \n",
            "in this paper , we propose a novel approach to DYNAMIC RANGE COMPRESSION based on a GLIMPSE PROPORTION MEASURE . the proposed GLIMPSE PROPORTION MEASURE is based on a GLIMPSE PROPORTION MEASURE and a COMPRESSOR for LOMBARD EXCITATION AND DURATION PATTERNS . the proposed GLIMPSE PROPORTION MEASURE is based on a GLIMPSE PROPORTION MEASURE and a COMPRESSOR . the proposed approach is evaluated on a variety of UNMODIFIED SYNTHETIC SPEECH , and the results show that the proposed method is effective in reducing the SNR CONDITIONS . the proposed method is robust to SPEECH-SHAPED NOISE , SPEECH-SHAPED NOISE , and SPEECH-SHAPED NOISE .\n",
            "\n",
            "755 1000\n",
            "we present a method for MOTION ESTIMATION using ORDINAL MEASURES . ORDINAL MEASURES are based on RELATIVE ORDERING OF INTENSITY VALUES in a IMAGE REGION called RANK PERMUTATION . while popular measures like the SUM-OF SQUARED-DIFFERENCE -LRB- S S D -RRB- and NORMALIZED CORRELATION rely on LINEAR-ITY BETWEEN CORRESPONDING INTENSITY VALUES , ORDINAL MEASURES only require them to be monotonically related so that RANK PERMUTATIONS between corresponding regions are preserved . this property turns out to be <unk> for MOTION ESTIMATION in TAGGED MAGNETIC RESONANCE IMAGES . we study the IMAGING EQUATION involved in two methods of TAGGING and observe TEMPORAL MONOTONICITY in intensity under certain conditions though the tags themselves <unk> . we compare our method to s s d and NORMALIZED CORRELATION in a ROTATING RING PHANTOM IMAGE SEQUENCE . we present an experiment on a REAL HEART IMAGE SEQUENCE which suggests the suitability of our method . \n",
            "in this paper , we propose a novel method for MOTION ESTIMATION from TAGGED MAGNETIC RESONANCE IMAGES . the proposed method is based on a ROTATING RING PHANTOM IMAGE SEQUENCE , which is based on a ROTATING RING PHANTOM IMAGE SEQUENCE of the TAGGED MAGNETIC RESONANCE IMAGES . the proposed method is based on the LINEAR-ITY BETWEEN CORRESPONDING INTENSITY VALUES of the IMAGING EQUATION . the proposed method is based on the LINEAR-ITY BETWEEN CORRESPONDING INTENSITY VALUES of the IMAGING EQUATION . the proposed method is based on the LINEAR-ITY BETWEEN CORRESPONDING INTENSITY VALUES of the IMAGING EQUATION . the proposed method is based on the LINEAR-ITY BETWEEN CORRESPONDING INTENSITY VALUES of the IMAGING EQUATION . the proposed method is applied to MOTION ESTIMATION , and the experimental results show that the proposed method is effective in improving the RELATIVE ORDERING OF INTENSITY VALUES .\n",
            "\n",
            "756 1000\n",
            "the AUTOMATED PLANNING COMMUNITY has traditionally focused on the efficient synthesis of plans given a complete DOMAIN THEORY . in the past several years , this line of work met with significant successes , and the future course of the community seems to be set on efficient planning with even RICHER MODELS . while this line of research has its applications , there are also many domains and scenarios where the first bottleneck is getting the DOMAIN MODEL at any level of completeness . in these scenarios , the DOMAIN MODEL automatically renders the PLANNING TECHNOLOGY <unk> . to counter this , i will motivate MODEL-LITE PLANNING TECHNOLOGY aimed at reducing the DOMAIN-MODELING BURDEN -lrb- possibly at the expense of reduced functionality -rrb- , and outline the research challenges that need to be addressed to realize MODEL-LITE PLANNING TECHNOLOGY . \n",
            "this paper addresses the problem of DOMAIN THEORY for the AUTOMATED PLANNING COMMUNITY . in particular , we focus on the problem of DOMAIN THEORY , where the goal is to minimize the DOMAIN-MODELING BURDEN of the DOMAIN MODEL . we show that this problem can be solved using a MODEL-LITE PLANNING TECHNOLOGY . we show that this problem can be solved efficiently using a MODEL-LITE PLANNING TECHNOLOGY . we show that the proposed algorithm is able to solve the problem of DOMAIN-MODELING BURDEN , and is able to solve the problem of MODEL-LITE PLANNING TECHNOLOGY .\n",
            "\n",
            "757 1000\n",
            "-- RADIO FREQUENCY IDENTI ¿ CATION is a technology to <unk> transmit the IDENTITY OF TAGGED OBJECTS . for LONG-RANGE SYSTEMS with multiple tags , the TAG REPLIES may overlap . current solutions are based on COLLISION AVOIDANCE using MAC PROTOCOLS -lrb- e.g. SLOTTED ALOHA and BINARY TREE ALGORITHMS -rrb- . this can be a time-consuming process . in this paper , it is shown how an ANTENNA ARRAY in combination with BLIND SOURCE SEPARATION TECHNIQUES can be used to separate multiple overlapping tag signals . the source signals are modeled as ZERO CONSTANT MODULUS SIGNALS , and the corresponding ZCM ALGORITHMS are tested on SYNTHETIC AND MEASURED DATA SETS . \n",
            "this paper addresses the problem of COLLISION AVOIDANCE for COLLISION AVOIDANCE . we propose a novel approach to COLLISION AVOIDANCE based on COLLISION AVOIDANCE and BINARY TREE ALGORITHMS . the proposed approach is based on the use of MAC PROTOCOLS and BINARY TREE ALGORITHMS to estimate the IDENTITY OF TAGGED OBJECTS . the proposed method is based on the use of MAC PROTOCOLS and BINARY TREE ALGORITHMS to estimate the IDENTITY OF TAGGED OBJECTS . the proposed method is evaluated on both SYNTHETIC AND MEASURED DATA SETS . the results show that the proposed method is effective in improving the IDENTITY OF TAGGED OBJECTS performance .\n",
            "\n",
            "758 1000\n",
            "a new OPTIMIZATION CRITERION is proposed to minimize ERROR PROBABILITY for the proposed JOINT OPTIMAL POWER ALLOCATION of the MIMO SYSTEMS enhanced by relay in this paper . it is proved that the COST FUNCTION obtained is only convex with respect to -lrb- w.r.t. -rrb- the POWER PARAMETERS of the source or those of the relay separately , but not convex w.r.t. the whole parameters . in order to use CONVEX OPTIMIZATION METHODS with high efficiency to solve this complicated problem , a tight upper bound of the sum mse -lrb- mean squared error -rrb- is derived , and employed to modify the COST FUNCTION in order to obtain a CONVEX PROBLEM . it is verified through simulation results that the proposed PA SCHEME outperforms the existing one . \n",
            "in this paper , we propose a novel PA SCHEME for MIMO SYSTEMS . the proposed JOINT OPTIMAL POWER ALLOCATION is based on the idea of JOINT OPTIMAL POWER ALLOCATION , which is a generalization of the PA SCHEME . the proposed JOINT OPTIMAL POWER ALLOCATION is based on the OPTIMIZATION CRITERION , which is based on the OPTIMIZATION CRITERION . the proposed PA SCHEME is based on the OPTIMIZATION CRITERION , which is a CONVEX PROBLEM . the proposed PA SCHEME is applied to the problem of JOINT OPTIMAL POWER ALLOCATION . the proposed PA SCHEME is applied to the problem of JOINT OPTIMAL POWER ALLOCATION , and the experimental results show that the proposed JOINT OPTIMAL POWER ALLOCATION is effective in reducing the ERROR PROBABILITY .\n",
            "\n",
            "759 1000\n",
            "this paper presents several novel GENERALIZATION BOUNDS for the problem of LEARNING KERNELS based on a COMBINATORIAL ANALYSIS of the RADEMACHER COMPLEXITY of the corresponding hypothesis sets . our bound for LEARNING KERNELS with a CONVEX COMBINATION OF P BASE KERNELS using l 1 regular-ization admits only a √ log p dependency on the number of kernels , which is tight and considerably more favorable than the previous best bound given for the same problem . we also give a novel bound for learning with a NON-NEGATIVE COMBINATION OF P BASE KERNELS with an L 2 REGULARIZATION whose dependency on p is also tight and only in p 1/4 . we present similar results for l q regular-ization with other values of q , and outline the relevance of our proof techniques to the analysis of the COMPLEXITY of the class of linear functions . experiments with a large number of kernels further validate the behavior of the GENERALIZATION ERROR as a function of p predicted by our bounds . \n",
            "this paper proposes a novel approach to COMBINATORIAL ANALYSIS based on CONVEX COMBINATION OF P BASE KERNELS . the proposed method is based on the use of CONVEX COMBINATION OF P BASE KERNELS , which is a generalization of the L 2 REGULARIZATION . the proposed method is based on the idea of L 2 REGULARIZATION , which is a generalization of the L 2 REGULARIZATION . the proposed method is based on the idea of L 2 REGULARIZATION , which is a generalization of the standard L 2 REGULARIZATION . the proposed method is evaluated in terms of the RADEMACHER COMPLEXITY and the COMPLEXITY of the proposed method . the experimental results show that the proposed method is effective in reducing the COMPLEXITY of the LEARNING KERNELS .\n",
            "\n",
            "760 1000\n",
            "modelling compositional meaning for sentences using EMPIRICAL DISTRIBUTIONAL METHODS has been a challenge for COMPUTATIONAL LINGUISTS . we implement the ABSTRACT CATEGORICAL MODEL of <unk> et al. -lrb- 2010 -rrb- using data from the BNC and evaluate ABSTRACT CATEGORICAL MODEL . the implementation is based on UNSUPERVISED LEARNING OF MATRICES for RELATIONAL WORDS and applying UNSUPERVISED LEARNING OF MATRICES to the vectors of their arguments . the evaluation is based on the WORD DISAMBIGUATION TASK developed by mitchell and <unk> -lrb- 2008 -rrb- for INTRANSITIVE SENTENCES , and on a similar new experiment designed for TRANSITIVE SENTENCES . our model matches the results of its competitors in the first experiment , and <unk> UNSUPERVISED LEARNING OF MATRICES in the second . the general improvement in results with increase in SYNTACTIC COMPLEXITY <unk> the compositional power of our model . \n",
            "in this paper , we propose a novel approach to MODELLING COMPOSITIONAL MEANING in the context of a WORD DISAMBIGUATION TASK . the proposed approach is based on the use of EMPIRICAL DISTRIBUTIONAL METHODS for MODELLING COMPOSITIONAL MEANING . the proposed approach is based on the use of EMPIRICAL DISTRIBUTIONAL METHODS for MODELLING COMPOSITIONAL MEANING . the proposed approach is based on the use of EMPIRICAL DISTRIBUTIONAL METHODS for MODELLING COMPOSITIONAL MEANING . the proposed method is evaluated on a WORD DISAMBIGUATION TASK . the experimental results show that the proposed approach is effective in improving the SYNTACTIC COMPLEXITY of the ABSTRACT CATEGORICAL MODEL .\n",
            "\n",
            "761 1000\n",
            "the ELECTRIC NETWORK FREQUENCY SIGNAL can be captured in MULTIMEDIA RECORDINGS due to ELECTROMAGNETIC INFLUENCES from the POWER GRID at the time of recording . recent work has exploited the ENF SIGNALS for FORENSIC APPLICATIONS , such as <unk> and DETECTING FORGERY OF ENF-CONTAINING MUL-TIMEDIA SIGNALS , and inferring their time and location of creation . in this paper , we explore a new potential of ENF SIGNALS for AUTOMATIC SYNCHRONIZATION OF AUDIO AND VIDEO . the ENF SIGNALS as a TIME-VARYING RANDOM PROCESS can be used as a timing fingerprint of MULTIMEDIA SIGNALS . synchronization of audio and video recordings can be achieved by aligning their EMBEDDED ENF SIGNALS . we demonstrate the proposed scheme with two applications : MULTI-VIEW VIDEO SYNCHRONIZATION and synchronization of historical audio recordings . the experimental results show the ENF BASED SYNCHRONIZATION APPROACH is effective , and has the potential to solve problems that are intractable by other existing methods . \n",
            "this paper addresses the problem of DETECTING FORGERY OF ENF-CONTAINING MUL-TIMEDIA SIGNALS from MULTIMEDIA RECORDINGS . we propose a novel approach to the DETECTING FORGERY OF ENF-CONTAINING MUL-TIMEDIA SIGNALS from MULTIMEDIA RECORDINGS . the proposed approach is based on the use of ENF SIGNALS in the form of a TIME-VARYING RANDOM PROCESS . the proposed approach is based on the use of ENF SIGNALS in the form of a TIME-VARYING RANDOM PROCESS . the proposed approach is based on the use of ENF SIGNALS in the form of a TIME-VARYING RANDOM PROCESS . the proposed approach is evaluated on a DETECTING FORGERY OF ENF-CONTAINING MUL-TIMEDIA SIGNALS . the results show that the proposed approach is able to detect and track moving objects in a scene from a single image .\n",
            "\n",
            "762 1000\n",
            "<unk> in SCRIBBLE-BASED INTERACTIVE SEGMENTATION such as GRAPH-CUT are usually assumed to be perfectly accurate , i.e. , FOREGROUND SCRIBBLE PIXELS will never be segmented as background in the final SEGMENTATION . however , it can be hard to draw perfectly accurate SCRIBBLES , especially on fine structures of the IMAGE or on MOBILE TOUCH-SCREEN DEVICES . in this paper , we propose a novel RATIO ENERGY FUNCTION that tolerates errors in the USER INPUT while encouraging maximum use of the USER INPUT INFORMATION . more specifically , the RATIO ENERGY FUNCTION aims to minimize the GRAPH-CUT ENERGY while maximizing the USER INPUT <unk> in the SEGMENTATION . the RATIO ENERGY FUNCTION can be exactly optimized using an efficient ITERATED GRAPH CUT ALGORITHM . the ROBUSTNESS of the proposed RATIO ENERGY FUNCTION is validated on the GRABCUT DATASET using both SYNTHETIC SCRIBBLES and MANUAL SCRIBBLES . the experimental results show that the proposed RATIO ENERGY FUNCTION is robust to the errors in the USER INPUT and preserves the '' anchoring '' capability of the USER INPUT . \n",
            "in this paper , we propose a novel ITERATED GRAPH CUT ALGORITHM based on a ITERATED GRAPH CUT ALGORITHM . the proposed RATIO ENERGY FUNCTION is based on a ITERATED GRAPH CUT ALGORITHM , which is able to deal with USER INPUT INFORMATION and USER INPUT . the proposed RATIO ENERGY FUNCTION is based on a novel ITERATED GRAPH CUT ALGORITHM , which is able to deal with USER INPUT INFORMATION and USER INPUT . the proposed RATIO ENERGY FUNCTION is evaluated on the GRABCUT DATASET and the GRABCUT DATASET . the experimental results on the GRABCUT DATASET show that the proposed ITERATED GRAPH CUT ALGORITHM is effective in terms of ROBUSTNESS and GRAPH-CUT ENERGY . the proposed RATIO ENERGY FUNCTION is evaluated on the GRABCUT DATASET , and the results show that the proposed RATIO ENERGY FUNCTION is able to achieve the ROBUSTNESS of the RATIO ENERGY FUNCTION .\n",
            "\n",
            "763 1000\n",
            "this paper presents recent developments on our '' silent speech interface '' that converts TONGUE AND LIP MOTIONS , captured by ultrasound and VIDEO IMAGING , into AUDIBLE SPEECH . in our previous studies , the MAPPING between the observed articulatory movements and the resulting SPEECH SOUND was achieved using a UNIT SELECTION APPROACH . we investigate here the use of STATISTICAL MAPPING TECHNIQUES , based on the JOINT MODELING OF VISUAL AND SPECTRAL FEATURES , using respectively GAUSSIAN MIXTURE MODELS and HIDDEN MARKOV MODELS . the prediction of the VOICED/UNVOICED PARAMETER from VISUAL ARTICULATORY DATA is also investigated using an ARTIFICIAL NEURAL NETWORK . a CONTINUOUS SPEECH DATABASE consisting of <unk> of high-speed ultrasound and VIDEO SEQUENCES was specifically recorded to evaluate the proposed STATISTICAL MAPPING TECHNIQUES . \n",
            "this paper addresses the problem of VIDEO IMAGING from VIDEO SEQUENCES . we propose a novel approach to JOINT MODELING OF VISUAL AND SPECTRAL FEATURES based on JOINT MODELING OF VISUAL AND SPECTRAL FEATURES and HIDDEN MARKOV MODELS . the proposed approach is based on the JOINT MODELING OF VISUAL AND SPECTRAL FEATURES , which is based on the JOINT MODELING OF VISUAL AND SPECTRAL FEATURES and HIDDEN MARKOV MODELS . the proposed method is based on the JOINT MODELING OF VISUAL AND SPECTRAL FEATURES and HIDDEN MARKOV MODELS . the proposed method is based on the JOINT MODELING OF VISUAL AND SPECTRAL FEATURES and GAUSSIAN MIXTURE MODELS . the proposed method is evaluated on a CONTINUOUS SPEECH DATABASE and a CONTINUOUS SPEECH DATABASE . the results show that the proposed method is effective in improving the TONGUE AND LIP MOTIONS in VIDEO IMAGING .\n",
            "\n",
            "764 1000\n",
            "discourse in FORMAL DOMAINS , such as MATHEMATICS , is characterized by a mixture of TELEGRAPHIC NATURAL LANGUAGE and EMBEDDED -LRB- SEMI - -rrb- formal symbolic mathematical expressions . we present LANGUAGE PHENOMENA observed in a CORPUS OF DIALOGS with a SIMULATED TUTORIAL SYSTEM for proving theorems as evidence for the need for DEEP SYNTACTIC AND SEMANTIC ANALYSIS . we propose an approach to input understanding in this setting . our goal is a uniform analysis of inputs of different degree of VERBALIZA-TION : ranging from symbolic alone to fully <unk> mathematical expressions . \n",
            "this paper presents a novel SIMULATED TUTORIAL SYSTEM for DEEP SYNTACTIC AND SEMANTIC ANALYSIS . the proposed SIMULATED TUTORIAL SYSTEM is based on the use of a TELEGRAPHIC NATURAL LANGUAGE , which is based on a CORPUS OF DIALOGS . the proposed SIMULATED TUTORIAL SYSTEM is based on a SIMULATED TUTORIAL SYSTEM , which is able to deal with LANGUAGE PHENOMENA . the proposed SIMULATED TUTORIAL SYSTEM is based on the use of a DEEP SYNTACTIC AND SEMANTIC ANALYSIS , which is able to deal with LANGUAGE PHENOMENA , such as MATHEMATICS and MATHEMATICS . the proposed SIMULATED TUTORIAL SYSTEM is evaluated in a SIMULATED TUTORIAL SYSTEM for DEEP SYNTACTIC AND SEMANTIC ANALYSIS . the experimental results show that the proposed SIMULATED TUTORIAL SYSTEM is able to accurately detect and track people in real time .\n",
            "\n",
            "765 1000\n",
            "this paper presents a TALKER 'S HEAD ORIENTATION ESTIMATION METHOD using 2-CHANNEL MICROPHONES . in recent research , some approaches based on a NETWORK OF MICROPHONE ARRAYS have been proposed in order to estimate the TALKER 'S HEAD ORIENTATION . in those methods , the TALKER 'S HEAD ORIENTATION is estimated using the SOUND AMPLITUDE or PEAK VALUE of CSP -LRB- CROSS-POWER SPECTRUM PHASE -RRB- COEFFICIENTS obtained from each MICROPHONE ARRAY . however , MICROPHONE ARRAY NETWORK SYSTEMS need many MICROPHONE ARRAYS to be set along the walls of a given room so that SUB-MICROPHONE ARRAYS surround the user . in this paper , we focus on the shape of the CSP COEFFICIENTS affected by the REVERBERATION , which depends on the TALKER 'S POSITION and the HEAD ORIENTATION . in our proposed TALKER 'S HEAD ORIENTATION ESTIMATION METHOD , we use not only the PEAK VALUE but also the other values of the CSP COEFFICIENTS as FEATURE VECTORS , and the TALKER 'S POSITION and the HEAD ORIENTATION are estimated by discriminating the CSP VECTOR . the effectiveness of this TALKER 'S HEAD ORIENTATION ESTIMATION METHOD has been confirmed by TALKER LOCALIZATION and HEAD ORIENTATION ESTIMATION experiments performed in a real environment . \n",
            "in this paper , we propose a novel TALKER 'S HEAD ORIENTATION ESTIMATION METHOD based on HEAD ORIENTATION ESTIMATION and HEAD ORIENTATION ESTIMATION . the proposed TALKER 'S HEAD ORIENTATION ESTIMATION METHOD is based on the CSP -LRB- CROSS-POWER SPECTRUM PHASE -RRB- COEFFICIENTS and the PEAK VALUE . the proposed TALKER 'S HEAD ORIENTATION ESTIMATION METHOD is based on the use of SUB-MICROPHONE ARRAYS and HEAD ORIENTATION . the proposed TALKER 'S HEAD ORIENTATION ESTIMATION METHOD is based on the CSP -LRB- CROSS-POWER SPECTRUM PHASE -RRB- COEFFICIENTS and the PEAK VALUE of the CSP VECTOR . the proposed TALKER 'S HEAD ORIENTATION ESTIMATION METHOD is based on a CSP -LRB- CROSS-POWER SPECTRUM PHASE -RRB- COEFFICIENTS and the PEAK VALUE and the PEAK VALUE . the proposed TALKER 'S HEAD ORIENTATION ESTIMATION METHOD is evaluated on a variety of 2-CHANNEL MICROPHONES , and the results show that the proposed TALKER 'S HEAD ORIENTATION ESTIMATION METHOD is robust to REVERBERATION , HEAD ORIENTATION , and HEAD ORIENTATION .\n",
            "\n",
            "766 1000\n",
            "traditional LINEAR FUKUNAGA-KOONTZ TRANSFORM -lsb- 1 -rsb- is a powerful DISCRIMINATIVE SUBSPACES BUILDING APPROACH . previous work has successfully extended LINEAR FUKUNAGA-KOONTZ TRANSFORM to be able to deal with SMALL-SAMPLE-SIZE . in this paper , we extend traditional LINEAR FUKUNAGA-KOONTZ TRANSFORM to enable it to work in MULTI-CLASS PROBLEM and also in HIGHER DIMENSIONAL SUBSPACES and therefore provide enhanced DISCRIMINATION ABILITY . we verify the effectiveness of the proposed KERNEL FUKUNAGA-KOONTZ TRANSFORM by demonstrating its effectiveness in FACE RECOGNITION APPLICATIONS ; however the proposed KERNEL FUKUNAGA-KOONTZ TRANSFORM can be applied to any other DOMAIN SPECIFIC PROBLEMS . \n",
            "in this paper , we propose a novel DISCRIMINATIVE SUBSPACES BUILDING APPROACH , called LINEAR FUKUNAGA-KOONTZ TRANSFORM , for FACE RECOGNITION APPLICATIONS . the proposed DISCRIMINATIVE SUBSPACES BUILDING APPROACH is based on a LINEAR FUKUNAGA-KOONTZ TRANSFORM , a LINEAR FUKUNAGA-KOONTZ TRANSFORM , and a LINEAR FUKUNAGA-KOONTZ TRANSFORM . the proposed DISCRIMINATIVE SUBSPACES BUILDING APPROACH is based on a LINEAR FUKUNAGA-KOONTZ TRANSFORM , called the LINEAR FUKUNAGA-KOONTZ TRANSFORM . the proposed DISCRIMINATIVE SUBSPACES BUILDING APPROACH , called the LINEAR FUKUNAGA-KOONTZ TRANSFORM , is able to deal with the MULTI-CLASS PROBLEM . the proposed KERNEL FUKUNAGA-KOONTZ TRANSFORM is evaluated on a variety of FACE RECOGNITION APPLICATIONS . the experimental results show that the proposed KERNEL FUKUNAGA-KOONTZ TRANSFORM is effective in improving the performance of FACE RECOGNITION APPLICATIONS in FACE RECOGNITION APPLICATIONS .\n",
            "\n",
            "767 1000\n",
            "for VIDEO ANNOTATION REFINEMENT , a reasonable CONCEPT CORRELATION REPRESENTATION is crucial . in this paper , we present a DATA-SPECIFIC CONCEPT CORRELATION ESTIMATION PROCEDURE for this task , where the resulting correlation with respect to each data encodes both its VISUAL AND HIGH-LEVEL CHARACTERISTICS . specifically , this DATA-SPECIFIC CONCEPT CORRELATION ESTIMATION PROCEDURE comprises two major modules : CONCEPT CORRELATION BASIS ESTIMATION and DATA-SPECIFIC CONCEPT CORRELATION CALCULATION . under the framework of SPARSE REPRESENTATION , the DATA-SPECIFIC CONCEPT CORRELATION ESTIMATION PROCEDURE introduces a set of HIGH-LEVEL CONCEPT CORRELATION BASES to represent the CONCEPT DISTRIBUTION of each <unk> basis , while the latter constructs the concept correlation of a specific data by combining its FEATURE-LEVEL SPARSE COEFFICIENTS and CORRELATION BASES together . in the end , given this new correlation , a PROBABILITY-CALCULATION BASED VIDEO ANNOTATION REFINEMENT is performed on TRECVID 2006 DATASET . the experiments show that such a representation capturing DATA-SPECIFIC CHARACTERISTICS could achieve better performance , than the GENERIC CONCEPT CORRELATION applied to all data . \n",
            "in this paper , we propose a novel approach to VIDEO ANNOTATION REFINEMENT based on HIGH-LEVEL CONCEPT CORRELATION BASES and HIGH-LEVEL CONCEPT CORRELATION BASES . the proposed CONCEPT CORRELATION REPRESENTATION consists of two steps : -lrb- 1 -rrb- a CONCEPT CORRELATION REPRESENTATION that combines a CONCEPT CORRELATION REPRESENTATION with a CONCEPT CORRELATION REPRESENTATION , and -lrb- 2 -rrb- a DATA-SPECIFIC CONCEPT CORRELATION ESTIMATION PROCEDURE that combines VISUAL AND HIGH-LEVEL CHARACTERISTICS and FEATURE-LEVEL SPARSE COEFFICIENTS into the DATA-SPECIFIC CONCEPT CORRELATION ESTIMATION PROCEDURE . the proposed approach is evaluated on a TRECVID 2006 DATASET and a TRECVID 2006 DATASET . the results show that the proposed method is effective in improving the VIDEO ANNOTATION REFINEMENT performance .\n",
            "\n",
            "768 1000\n",
            "trust region is a well-known general ITERATIVE APPROACH to OPTIMIZATION which offers many advantages over standard GRADIENT DESCENT TECHNIQUES . in particular , it allows more accurate NONLINEAR APPROXIMATION MODELS . in each iteration this approach computes a GLOBAL OPTIMUM of a suitable APPROXIMATION MODEL within a fixed radius around the current solution , a.k.a. trust region . in general , this approach can be used only when some efficient CONSTRAINED OPTIMIZATION ALGORITHM is available for the selected NON-LINEAR -LRB- MORE ACCURATE -RRB- APPROXIMATION MODEL . in this paper we propose a FAST TRUST REGION APPROACH for OPTIMIZATION OF SEGMENTATION ENERGIES with NON-LINEAR REGIONAL TERMS , which are known to be challenging for existing algorithms . these NON-LINEAR REGIONAL TERMS include , but are not limited to , KL DIVERGENCE and BHATTACHARYYA DISTANCE between the observed and the TARGET APPEARANCE DISTRIBUTIONS , VOLUME CONSTRAINT on SEGMENT SIZE , and SHAPE PRIOR CONSTRAINT in a form of l 2 distance from target shape moments . our method is 1-2 orders of magnitude faster than the existing state-of-the-art methods while converging to comparable or better solutions . \n",
            "in this paper , we propose a novel FAST TRUST REGION APPROACH for the OPTIMIZATION OF SEGMENTATION ENERGIES . the proposed FAST TRUST REGION APPROACH is based on a NON-LINEAR -LRB- MORE ACCURATE -RRB- APPROXIMATION MODEL , which is a NON-LINEAR -LRB- MORE ACCURATE -RRB- APPROXIMATION MODEL of the TRUST REGION . the proposed FAST TRUST REGION APPROACH is based on a CONSTRAINED OPTIMIZATION ALGORITHM , which is based on a NON-LINEAR -LRB- MORE ACCURATE -RRB- APPROXIMATION MODEL . the proposed FAST TRUST REGION APPROACH is based on a CONSTRAINED OPTIMIZATION ALGORITHM , which is based on a CONSTRAINED OPTIMIZATION ALGORITHM . the proposed FAST TRUST REGION APPROACH is applied to the TRUST REGION of the TRUST REGION and the KL DIVERGENCE of the TRUST REGION is estimated . the performance of the proposed ITERATIVE APPROACH is demonstrated on a variety of NON-LINEAR REGIONAL TERMS . the experimental results show that the proposed ITERATIVE APPROACH is effective in reducing the SEGMENT SIZE .\n",
            "\n",
            "769 1000\n",
            "many applications require the analysis of complex interactions between time series . these interactions can be non-linear and involve vector valued as well as COMPLEX DATA STRUCTURES such as GRAPHS or strings . here we provide a general framework for the statistical analysis of these dependencies when RANDOM VARIABLES are sampled from STATIONARY TIME-SERIES OF ARBITRARY OBJECTS . to achieve this goal , we study the properties of the KERNEL CROSS-SPECTRAL DENSITY OPERATOR induced by POSITIVE DEFINITE KERNELS on ARBITRARY INPUT DOMAINS . this framework enables us to develop an INDEPENDENCE TEST between time series , as well as a SIMILARITY MEASURE to compare different types of COUPLING . the performance of our test is compared to the HSIC TEST using i.i.d. assumptions , showing strong improvements in terms of DETECTION ERRORS , as well as the suitability of this approach for testing dependency in complex DYNAMICAL SYSTEMS . this SIMILARITY MEASURE enables us to identify different types of interactions in ELECTROPHYSIOLOGICAL NEURAL TIME SERIES . \n",
            "this paper addresses the problem of STATIONARY TIME-SERIES OF ARBITRARY OBJECTS in the presence of COMPLEX DATA STRUCTURES . we propose a novel method to estimate the DETECTION ERRORS of a KERNEL CROSS-SPECTRAL DENSITY OPERATOR based on the KERNEL CROSS-SPECTRAL DENSITY OPERATOR . the proposed method is based on the use of POSITIVE DEFINITE KERNELS in the form of a KERNEL CROSS-SPECTRAL DENSITY OPERATOR . the proposed method is based on the use of POSITIVE DEFINITE KERNELS to estimate the DETECTION ERRORS of the DYNAMICAL SYSTEMS . the proposed method is based on the use of the KERNEL CROSS-SPECTRAL DENSITY OPERATOR to estimate the DETECTION ERRORS of the DYNAMICAL SYSTEMS . the proposed method is evaluated on a variety of ARBITRARY INPUT DOMAINS . the results show that the proposed method outperforms the existing methods in terms of DETECTION ERRORS and DETECTION ERRORS .\n",
            "\n",
            "770 1000\n",
            "understanding the CONNOTATION OF WORDS plays an important role in interpreting subtle <unk> of sentiment beyond <unk> or surface meaning of text , as seemingly objective statements often <unk> <unk> sentiment of the writer , and even purposefully <unk> emotion from the readers ' <unk> . the focus of this paper is drawing NUANCED , CONNOTATIVE SENTIMENTS from even those words that are objective on the surface , such as '' intelligence '' , '' human '' , and '' <unk> '' . we propose INDUCTION ALGORITHMS encoding a diverse set of linguistic insights -lrb- SEMANTIC PROSODY , DISTRI-BUTIONAL SIMILARITY , SEMANTIC PARALLELISM OF COORDINATION -rrb- and PRIOR KNOWLEDGE drawn from LEXICAL RESOURCES , resulting in the first BROAD-COVERAGE CONNOTATION LEXICON . \n",
            "this paper addresses the problem of SEMANTIC PARALLELISM OF COORDINATION in the presence of SEMANTIC PROSODY , SEMANTIC PARALLELISM OF COORDINATION , SEMANTIC PARALLELISM OF COORDINATION , and PRIOR KNOWLEDGE . in particular , we consider the problem of SEMANTIC PARALLELISM OF COORDINATION , and propose a method to estimate the parameters of the BROAD-COVERAGE CONNOTATION LEXICON . the proposed approach is based on the use of NUANCED , CONNOTATIVE SENTIMENTS , DISTRI-BUTIONAL SIMILARITY , and PRIOR KNOWLEDGE . we show that the proposed algorithm is able to estimate the number of sources in the CONNOTATION OF WORDS , and the number of sources can be reduced by the proposed method .\n",
            "\n",
            "771 1000\n",
            "matrix approximation -lrb- ma -rrb- is one of the most popular techniques for COLLABORATIVE FILTERING . most existing MA METHODS train USER/ITEM LATENT FACTORS based on a USER-ITEM RATING MATRIX and then use the GLOBAL LATENT FACTORS to model all USERS/ITEMS . however , globally optimized LATENT FACTORS may not reflect the unique interests shared among only subsets of USERS/ITEMS , without which unique interests of users may not be accurately modelled . as a result , existing MA METHODS , which can not capture the uniqueness of different USER/ITEM , can not provide optimal recommendation . in this paper , a MIXTURE PROBABILISTIC MATRIX APPROXIMATION METHOD is proposed , which unifies globally optimized USER/ITEM feature vectors -lrb- on the entire RATING MATRIX -rrb- and LOCALLY OPTIMIZED USER/ITEM FEATURE VECTORS -lrb- on subsets of USER/ITEM RATINGS -rrb- to improve RECOMMENDATION ACCURACY . more specifically , in MIXTURE PROBABILISTIC MATRIX APPROXIMATION METHOD , a method is developed to find both globally and LOCALLY OPTIMIZED USER/ITEM FEATURE VECTORS . then , a GAUS-SIAN MIXTURE MODEL is adopted to combine GLOBAL PREDICTIONS and LOCAL PREDICTIONS to produce accurate rating predictions . experimental study using MOVIELENS AND NETFLIX DATASETS demonstrates that MIXTURE PROBABILISTIC MATRIX APPROXIMATION METHOD outperforms five state-of-the-art MA BASED CF METHODS in RECOMMENDATION ACCURACY with good scalability . \n",
            "in this paper , we propose a novel MIXTURE PROBABILISTIC MATRIX APPROXIMATION METHOD for COLLABORATIVE FILTERING . the proposed MIXTURE PROBABILISTIC MATRIX APPROXIMATION METHOD is based on the idea that the USER-ITEM RATING MATRIX of the USER-ITEM RATING MATRIX can be approximated by a GAUS-SIAN MIXTURE MODEL . the proposed MIXTURE PROBABILISTIC MATRIX APPROXIMATION METHOD is based on the use of a GAUS-SIAN MIXTURE MODEL , which is able to capture the GLOBAL LATENT FACTORS of the RATING MATRIX . the proposed MIXTURE PROBABILISTIC MATRIX APPROXIMATION METHOD is based on the use of the USER-ITEM RATING MATRIX , which is a GAUS-SIAN MIXTURE MODEL of the RATING MATRIX . the proposed MIXTURE PROBABILISTIC MATRIX APPROXIMATION METHOD is evaluated on both MOVIELENS AND NETFLIX DATASETS . the experimental results on the MOVIELENS AND NETFLIX DATASETS show that the proposed MIXTURE PROBABILISTIC MATRIX APPROXIMATION METHOD significantly improves the RECOMMENDATION ACCURACY and RECOMMENDATION ACCURACY of the proposed MIXTURE PROBABILISTIC MATRIX APPROXIMATION METHOD .\n",
            "\n",
            "772 1000\n",
            "learning to hash involves learning HASH FUNCTIONS from a set of IMAGES for embedding HIGH-DIMENSIONAL VISUAL DESCRIPTORS into a SIMILARITY-PRESERVING LOW-DIMENSIONAL HAMMING SPACE . most of existing methods resort to a single representation of IMAGES , that is , only one type of VISUAL DESCRIPTORS is used to learn a HASH FUNCTION to assign BINARY CODES to IMAGES . however , IMAGES are often described by multiple different VISUAL DESCRIPTORS -lrb- such as SIFT , GIST , hog -rrb- , so it is desirable to incorporate these multiple representations into learning a HASH FUNCTION , leading to MULTI-VIEW HASHING . in this paper we present a SEQUENTIAL SPECTRAL LEARNING APPROACH to MULTI-VIEW HASHING where a HASH FUNCTION is sequentially determined by solving the successive <unk> of LOCAL VARIANCES subject to DECORRELATION CONSTRAINTS . we compute MULTI-VIEW LOCAL VARIANCES by Α-AVERAGING VIEW-SPECIFIC DISTANCE MATRICES such that the best AVERAGED DISTANCE MATRIX is determined by minimizing its <unk> from VIEW-SPECIFIC DISTANCE MATRICES . we also present a scalable implementation , exploiting a fast APPROXIMATE K-NN GRAPH CONSTRUCTION METHOD , in which Α-AVERAGED DISTANCES computed in SMALL PARTITIONS determined by RECURSIVE SPECTRAL BISECTION are gradually merged in conquer steps until whole examples are used . numerical experiments on <unk> , <unk> , and NUS-WIDE DATASETS confirm the high performance of our SEQUENTIAL SPECTRAL LEARNING APPROACH , in comparison to SINGLE-VIEW SPECTRAL HASHING as well as existing MULTI-VIEW HASHING METHODS . \n",
            "in this paper , we propose a novel SEQUENTIAL SPECTRAL LEARNING APPROACH for MULTI-VIEW HASHING . the proposed SEQUENTIAL SPECTRAL LEARNING APPROACH is based on the use of VIEW-SPECIFIC DISTANCE MATRICES extracted from IMAGES to estimate the LOCAL VARIANCES . the proposed SEQUENTIAL SPECTRAL LEARNING APPROACH is based on the use of VIEW-SPECIFIC DISTANCE MATRICES , which are more robust to LOCAL VARIANCES than traditional MULTI-VIEW HASHING METHODS such as SIFT and MULTI-VIEW HASHING METHODS . the proposed SEQUENTIAL SPECTRAL LEARNING APPROACH is based on the use of VIEW-SPECIFIC DISTANCE MATRICES , which are more robust to LOCAL VARIANCES than the traditional MULTI-VIEW HASHING METHODS . the proposed SEQUENTIAL SPECTRAL LEARNING APPROACH is compared to MULTI-VIEW HASHING METHODS and MULTI-VIEW HASHING METHODS . the experimental results on NUS-WIDE DATASETS show that the proposed SEQUENTIAL SPECTRAL LEARNING APPROACH outperforms existing MULTI-VIEW HASHING METHODS and MULTI-VIEW HASHING METHODS .\n",
            "\n",
            "773 1000\n",
            "recent work has shown impressive TRANSFORM-INVARIANT MODELING and CLUSTERING for sets of images of objects with similar appearance . we seek to expand these capabilities to sets of images of an OBJECT CLASS that show considerable variation across individual instances -lrb- e.g. PEDESTRIAN IMAGES -rrb- using a representation based on PIXEL-WISE SIMILARITIES , SIMILARITY TEMPLATES . because of its invariance to the colors of particular components of an object , this representation enables detection of instances of an OBJECT CLASS and enables alignment of those instances . further , this model implicitly represents the regions of color regularity in the CLASS-SPECIFIC IMAGE SET enabling a decomposition of that OBJECT CLASS into component regions . \n",
            "in this paper , we propose a novel approach to TRANSFORM-INVARIANT MODELING and CLUSTERING . the method is based on the use of a set of SIMILARITY TEMPLATES , each of which is represented by a set of SIMILARITY TEMPLATES . the proposed approach is based on the use of a set of SIMILARITY TEMPLATES , each of which is a set of SIMILARITY TEMPLATES . we show that the proposed algorithm is able to recover the OBJECT CLASS of the object in the scene , and can be applied to a wide range of PEDESTRIAN IMAGES . experimental results show that the proposed method outperforms the state-of-the-art methods in terms of both CLUSTERING and CLUSTERING .\n",
            "\n",
            "774 1000\n",
            "to capture the interdependencies between labels in MULTI-LABEL CLASSIFICATION PROBLEMS , CLASSIFIER CHAIN tries to take the multiple labels of each instance into account under a DETERMINISTIC HIGH-ORDER MARKOV CHAIN MODEL . since its performance is sensitive to the choice of label order , the key issue is how to determine the OPTIMAL LABEL ORDER for CLASSIFIER CHAIN . in this work , we first generalize the CLASSIFIER CHAIN over a RANDOM LABEL ORDER . then , we present a theoretical analysis of the GENERALIZATION ERROR for the proposed DETERMINISTIC HIGH-ORDER MARKOV CHAIN MODEL . based on our results , we propose a dynamic programming based classifier chain -lrb- CC-DP -rrb- algorithm to search the GLOBALLY OPTIMAL LABEL ORDER for CLASSIFIER CHAIN and a GREEDY CLASSIFIER CHAIN ALGORITHM to find a locally optimal CLASSIFIER CHAIN . comprehensive experiments on a number of REAL-WORLD MULTI-LABEL DATA SETS from various domains demonstrate that our proposed CC-DP ALGORITHM outperforms state-of-the-art approaches and the CC-DP ALGORITHM achieves comparable prediction performance with CC-DP . \n",
            "in this paper , we propose a novel CC-DP ALGORITHM for DETERMINISTIC HIGH-ORDER MARKOV CHAIN MODEL . the proposed DETERMINISTIC HIGH-ORDER MARKOV CHAIN MODEL is based on a DETERMINISTIC HIGH-ORDER MARKOV CHAIN MODEL , which is based on the OPTIMAL LABEL ORDER . the proposed DETERMINISTIC HIGH-ORDER MARKOV CHAIN MODEL is based on the OPTIMAL LABEL ORDER of the CLASSIFIER CHAIN . the proposed DETERMINISTIC HIGH-ORDER MARKOV CHAIN MODEL is based on the GLOBALLY OPTIMAL LABEL ORDER of the CLASSIFIER CHAIN . the proposed CC-DP ALGORITHM is evaluated on two REAL-WORLD MULTI-LABEL DATA SETS . the experimental results show that the proposed CC-DP ALGORITHM achieves better performance than the conventional CC-DP ALGORITHM . moreover , the proposed CC-DP ALGORITHM can also be applied to other MULTI-LABEL CLASSIFICATION PROBLEMS .\n",
            "\n",
            "775 1000\n",
            "this paper focuses on a new CLUSTERING TASK , called <I> SELF-TAUGHT CLUSTERING </I> . SELF-TAUGHT CLUSTERING is an instance of <I> UNSUPERVISED TRANSFER LEARNING </I> , which aims at CLUSTERING a small collection of target unlabeled data with the help of a large amount of <I> AUXILIARY </I> UNLABELED DATA . the TARGET AND AUXILIARY DATA can be different in TOPIC DISTRIBUTION . we show that even when the target data are not sufficient to allow effective learning of a high quality FEATURE REPRESENTATION , it is possible to learn the useful FEATURES with the help of the AUXILIARY DATA on which the target data can be clustered effectively . we propose a CO-CLUSTERING BASED SELF-TAUGHT CLUSTERING ALGORITHM to tackle this problem , by CLUSTERING the TARGET AND AUXILIARY DATA simultaneously to allow the FEATURE REPRESENTATION from the AUXILIARY DATA to influence the target data through a common set of FEATURES . under the new FEATURE REPRESENTATION , CLUSTERING on the target data can be improved . our experiments on IMAGE CLUSTERING show that our CO-CLUSTERING BASED SELF-TAUGHT CLUSTERING ALGORITHM can greatly outperform several state-of-the-art CLUSTERING METHODS when utilizing IRRELEVANT UNLABELED AUXILIARY DATA . \n",
            "in this paper , we propose a novel CO-CLUSTERING BASED SELF-TAUGHT CLUSTERING ALGORITHM based on <I> AUXILIARY </I> UNLABELED DATA . the proposed CO-CLUSTERING BASED SELF-TAUGHT CLUSTERING ALGORITHM is based on the <I> AUXILIARY </I> UNLABELED DATA , which is based on the <I> AUXILIARY </I> UNLABELED DATA . the proposed CO-CLUSTERING BASED SELF-TAUGHT CLUSTERING ALGORITHM is based on the <I> AUXILIARY </I> UNLABELED DATA , which is based on the <I> AUXILIARY </I> UNLABELED DATA . the proposed CO-CLUSTERING BASED SELF-TAUGHT CLUSTERING ALGORITHM is based on the <I> AUXILIARY </I> UNLABELED DATA , which is based on the <I> AUXILIARY </I> UNLABELED DATA . the proposed CO-CLUSTERING BASED SELF-TAUGHT CLUSTERING ALGORITHM is applied to the problem of SELF-TAUGHT CLUSTERING , which is based on the <I> AUXILIARY </I> UNLABELED DATA . the proposed CO-CLUSTERING BASED SELF-TAUGHT CLUSTERING ALGORITHM is evaluated on a variety of AUXILIARY DATA , including <I> SELF-TAUGHT CLUSTERING </I> and <I> SELF-TAUGHT CLUSTERING </I> . the experimental results show that the proposed CO-CLUSTERING BASED SELF-TAUGHT CLUSTERING ALGORITHM is able to significantly outperform the CLUSTERING METHODS in terms of both CLUSTERING and CLUSTERING METHODS .\n",
            "\n",
            "776 1000\n",
            "we present a MACHINE LEARNING APPROACH to evaluating the <unk> of output of a MACHINE TRANSLATION SYSTEM , using CLASSIFIERS that learn to distinguish HUMAN REFERENCE TRANSLATIONS from MACHINE TRANSLATIONS . this MACHINE LEARNING APPROACH can be used to evaluate an MACHINE TRANSLATION SYSTEM , tracking improvements over time ; to aid in the kind of FAILURE ANALYSIS that can help guide system development ; and to select among alternative output strings . the MACHINE LEARNING APPROACH presented is fully automated and independent of source language , target language and domain . \n",
            "this paper presents a novel MACHINE LEARNING APPROACH for MACHINE TRANSLATION SYSTEM . the proposed MACHINE LEARNING APPROACH is based on the use of CLASSIFIERS for FAILURE ANALYSIS . the proposed MACHINE LEARNING APPROACH is based on the use of CLASSIFIERS for FAILURE ANALYSIS . the proposed MACHINE LEARNING APPROACH is based on the use of CLASSIFIERS to estimate the HUMAN REFERENCE TRANSLATIONS . the experimental results show that the proposed MACHINE LEARNING APPROACH is effective in improving the performance of the MACHINE TRANSLATION SYSTEM .\n",
            "\n",
            "777 1000\n",
            "massive multichannel reproduction systems like WAVE FIELD SYNTHESIS are potentially well suited to be complemented by LISTENING ROOM EQUALIZATION . however , their typically large number of REPRODUCTION CHANNELS makes this task challenging for both COMPUTATIONAL AND ALGORITHMIC REASONS . WAVE-DOMAIN ADAPTIVE FILTERING was proposed earlier and is especially well-suited to ADAP-TIVE FILTERING TASKS in the context of WAVE FIELD SYNTHESIS . in this paper , we propose to generalize the model originally used for WAVE-DOMAIN ADAPTIVE FILTERING to allow an ADAPTIVE LRE for a broader range of REPRODUCTION SCENARIOS , while maintaining the advantages of the original ADAPTIVE LRE . the proposed ADAPTIVE LRE is evaluated for FILTERING STRUCTURES of varying complexity along with considering the ROBUSTNESS to varying LISTENER POSITIONS . \n",
            "this paper addresses the problem of LISTENING ROOM EQUALIZATION for MASSIVE MULTICHANNEL REPRODUCTION SYSTEMS , such as WAVE FIELD SYNTHESIS and WAVE FIELD SYNTHESIS . in this paper , we propose a novel approach to LISTENING ROOM EQUALIZATION based on WAVE-DOMAIN ADAPTIVE FILTERING . the proposed WAVE-DOMAIN ADAPTIVE FILTERING is based on the use of WAVE-DOMAIN ADAPTIVE FILTERING , which is able to deal with LISTENER POSITIONS in the presence of LISTENER POSITIONS . we show that WAVE-DOMAIN ADAPTIVE FILTERING can be applied to ADAP-TIVE FILTERING TASKS , and that WAVE-DOMAIN ADAPTIVE FILTERING can be applied to ADAP-TIVE FILTERING TASKS for REPRODUCTION SCENARIOS .\n",
            "\n",
            "778 1000\n",
            "recently , it has become evident that SUBMODULARITY naturally captures widely occurring concepts in MACHINE LEARNING , SIGNAL PROCESSING and COMPUTER VISION . consequently , there is need for efficient OPTIMIZATION PROCEDURES for SUBMODU-LAR FUNCTIONS , especially for MINIMIZATION PROBLEMS . while general submodular minimization is challenging , we propose a new method that exploits existing DE-COMPOSABILITY OF SUBMODULAR FUNCTIONS . in contrast to previous approaches , our method is neither approximate , nor impractical , nor does it need any cumbersome PARAMETER TUNING . moreover , it is easy to implement and parallelize . a key component of our method is a formulation of the DISCRETE SUBMODULAR MINIMIZATION PROBLEM as a CONTINUOUS BEST APPROXIMATION PROBLEM that is solved through a sequence of reflections , and its solution can be easily <unk> to obtain an optimal discrete solution . this method solves both the continuous and discrete formulations of the DISCRETE SUBMODULAR MINIMIZATION PROBLEM , and therefore has applications in LEARNING , INFERENCE , and RECONSTRUCTION . in our experiments , we illustrate the benefits of our method on two IMAGE SEGMENTATION TASKS . \n",
            "this paper addresses the problem of SIGNAL PROCESSING , SIGNAL PROCESSING , and SIGNAL PROCESSING . in particular , we consider the problem of INFERENCE , SIGNAL PROCESSING and SIGNAL PROCESSING . we propose a novel approach to the problem of DE-COMPOSABILITY OF SUBMODULAR FUNCTIONS , which is a generalization of the existing methods , which is based on the idea of SUBMODULARITY . we formulate the CONTINUOUS BEST APPROXIMATION PROBLEM as a DISCRETE SUBMODULAR MINIMIZATION PROBLEM and solve the DISCRETE SUBMODULAR MINIMIZATION PROBLEM , which is a generalization of the existing OPTIMIZATION PROCEDURES . we demonstrate the effectiveness of our method on a variety of IMAGE SEGMENTATION TASKS and IMAGE SEGMENTATION TASKS . our results show that the proposed algorithm is able to perform well in IMAGE SEGMENTATION TASKS , RECONSTRUCTION , and RECONSTRUCTION .\n",
            "\n",
            "779 1000\n",
            "in this paper , we consider the effect of a bandwidth extension of NARROW-BAND SPEECH signals -lrb- <unk> .4 khz -rrb- to <unk> khz on SPEAKER VERIFICATION . using COVARIANCE MATRIX BASED VERIFICATION SYSTEMS together with DETECTION ERROR TRADE-OFF CURVES , we compare the performance between systems operating on narrow-band , wide-band -lrb- <unk> khz -rrb- , and BANDWIDTH-EXTENDED SPEECH . the experiments were conducted using different SHORT-TIME SPECTRAL PARAMETERIZATIONS derived from MICROPHONE AND ISDN SPEECH DATABASES . the studied BANDWIDTH-EXTENSION ALGORITHM did not introduce artifacts that affected the SPEAKER VERIFICATION TASK , and we achieved improvements between 1 and 10 percent -lrb- depending on the MODEL ORDER -rrb- over the VERIFICATION SYSTEM designed for NARROW-BAND SPEECH when MEL-FREQUENCY CEPSTRAL COEFFICIENTS for the SHORT-TIME SPECTRAL PARAMETERIZATION were used . \n",
            "in this paper , we propose a novel VERIFICATION SYSTEM for SPEAKER VERIFICATION . the proposed VERIFICATION SYSTEM is based on the use of MEL-FREQUENCY CEPSTRAL COEFFICIENTS and COVARIANCE MATRIX BASED VERIFICATION SYSTEMS . the proposed VERIFICATION SYSTEM is based on the use of SHORT-TIME SPECTRAL PARAMETERIZATIONS and COVARIANCE MATRIX BASED VERIFICATION SYSTEMS . the proposed VERIFICATION SYSTEM is based on the use of MEL-FREQUENCY CEPSTRAL COEFFICIENTS and SHORT-TIME SPECTRAL PARAMETERIZATIONS to estimate the MODEL ORDER . the proposed VERIFICATION SYSTEM is applied to the SPEAKER VERIFICATION TASK and COVARIANCE MATRIX BASED VERIFICATION SYSTEMS . the performance of the proposed VERIFICATION SYSTEM is evaluated on a variety of MICROPHONE AND ISDN SPEECH DATABASES . the experimental results show that the proposed method is effective in improving the DETECTION ERROR TRADE-OFF CURVES in the presence of NARROW-BAND SPEECH .\n",
            "\n",
            "780 1000\n",
            "speech separation based on TIME-FREQUENCY MASKING has been shown to improve INTELLIGIBILITY OF SPEECH SIGNALS corrupted by noise . a perceived weakness of BINARY MASKING is the quality of SEPARATED SPEECH . in this paper , an approach for improving the PERCEPTUAL QUALITY of SEPARATED SPEECH from BINARY MASKING is proposed . our approach consists of two stages , where a BINARY MASK is generated in the first stage that effectively performs SPEECH SEPARATION . in the second stage , a SPARSE-REPRESENTATION APPROACH is used to represent the SEPARATED SIGNAL by a linear combination of SHORT-TIME FOURIER TRANSFORM MAGNITUDES that are generated from a CLEAN SPEECH DICTIONARY . OVERLAP-AND-ADD SYNTHESIS is then used to generate an estimate of the SPEECH SIGNAL . the performance of the proposed approach is evaluated with the PERCEPTUAL EVALUATION OF SPEECH QUALITY , which is a standard objective SPEECH QUALITY MEASURE . the proposed algorithm offers considerable improvements in SPEECH QUALITY over BINARY-MASKED NOISY SPEECH and other RECONSTRUCTION APPROACHES . \n",
            "this paper presents a novel approach to SPEECH SEPARATION for SPEECH SEPARATION . the proposed method is based on the use of BINARY MASKING and RECONSTRUCTION APPROACHES . the proposed method is based on the use of BINARY MASKING and RECONSTRUCTION APPROACHES . the proposed method is based on the use of TIME-FREQUENCY MASKING and RECONSTRUCTION APPROACHES to estimate the SHORT-TIME FOURIER TRANSFORM MAGNITUDES of the speech signal . the proposed method is based on the use of TIME-FREQUENCY MASKING and RECONSTRUCTION APPROACHES . the proposed method is evaluated in terms of PERCEPTUAL QUALITY and RECONSTRUCTION APPROACHES . the experimental results show that the proposed method is effective in improving PERCEPTUAL EVALUATION OF SPEECH QUALITY and RECONSTRUCTION APPROACHES in terms of SPEECH QUALITY and RECONSTRUCTION APPROACHES .\n",
            "\n",
            "781 1000\n",
            "word sense disambiguation -lrb- WORD SENSE DISAMBIGUATION SYSTEMS -rrb- systems based on SUPERVISED LEARNING achieved the best performance in SENSE-VAL AND SEMEVAL WORKSHOPS . however , there are few publicly available SUPERVISED ENGLISH ALL-WORDS WSD SYSTEM . this limits the use of WORD SENSE DISAMBIGUATION SYSTEMS in other applications , especially for researchers whose research interests are not in WORD SENSE DISAMBIGUATION SYSTEMS . in this paper , we present IMS , a SUPERVISED ENGLISH ALL-WORDS WSD SYSTEM . the flexible framework of IMS allows users to integrate different PREPROCESSING TOOLS , additional FEATURES , and different CLASSIFIERS . by default , we use LINEAR SUPPORT VECTOR MACHINES as the CLASSIFIER with multiple KNOWLEDGE-BASED FEATURES . in our implementation , IMS achieves state-of-the-art results on several SENSEVAL AND SEMEVAL TASKS . \n",
            "in this paper , we propose a novel approach to WORD SENSE DISAMBIGUATION SYSTEMS based on LINEAR SUPPORT VECTOR MACHINES . in the proposed WORD SENSE DISAMBIGUATION SYSTEMS , a CLASSIFIER is trained on the basis of the KNOWLEDGE-BASED FEATURES of the target language . in the proposed CLASSIFIER , a CLASSIFIER is trained on the basis of the KNOWLEDGE-BASED FEATURES of the target language . in the proposed CLASSIFIER , a CLASSIFIER is used to select the most appropriate FEATURES for the target language . the experimental results show that the proposed CLASSIFIER significantly improves the performance of WORD SENSE DISAMBIGUATION SYSTEMS on both SENSEVAL AND SEMEVAL TASKS . the performance of the proposed CLASSIFIER is evaluated on both SENSEVAL AND SEMEVAL TASKS . the results show that the proposed method can improve the WORD SENSE DISAMBIGUATION SYSTEMS 's performance in terms of SENSE-VAL AND SEMEVAL WORKSHOPS and CLASSIFIERS .\n",
            "\n",
            "782 1000\n",
            "many traditional methods for SHAPE CLASSIFICATION involve establishing POINT CORRESPONDENCES between shapes to produce matching scores , which are in turn used as SIMILARITY MEASURES for SHAPE CLASSIFICATION . LEARNING TECHNIQUES have been applied only in the second stage of this process , after the matching scores have been obtained . in this paper , instead of simply taking for granted the scores obtained by matching and then learning a CLASSIFIER , we learn the matching scores themselves so as to produce SHAPE SIMILARITY SCORES that minimize the CLASSIFICATION LOSS . the solution is based on a MAX-MARGIN FORMULATION in the STRUCTURED PREDICTION SETTING . experiments in SHAPE DATABASES reveal that such an integrated learning algorithm substantially improves on existing methods . \n",
            "this paper addresses the problem of SHAPE CLASSIFICATION in SHAPE DATABASES . we propose a novel method for SHAPE CLASSIFICATION based on a MAX-MARGIN FORMULATION . the proposed method is based on a MAX-MARGIN FORMULATION , which is based on a MAX-MARGIN FORMULATION . the proposed method is based on a MAX-MARGIN FORMULATION , which is based on a MAX-MARGIN FORMULATION . the proposed method is based on the idea of MAX-MARGIN FORMULATION , which is based on a MAX-MARGIN FORMULATION . the proposed method is evaluated on a variety of SHAPE DATABASES . the experimental results show that the proposed method outperforms the state-of-the-art methods in terms of CLASSIFICATION LOSS and CLASSIFICATION LOSS .\n",
            "\n",
            "783 1000\n",
            "we introduce an efficient algorithm for performing DISTRIBUTED PRINCIPAL COMPONENT ANALYSIS on DIRECTED GAUSSIAN GRAPHICAL MODELS . by exploiting STRUCTURED SPARSITY in the CHOLESKY FACTOR of the INVERSE COVARIANCE MATRIX , our proposed <unk> algorithm computes GLOBAL PRINCIPAL SUBSPACE ESTIMATION through LOCAL COMPUTATION and MESSAGE PASSING . we show significant performance and <unk> advantages of <unk> for ONLINE PRINCIPAL SUBSPACE ESTIMATION and DISTRIBUTED ANOMALY DETECTION in REAL-WORLD COMPUTER NETWORKS . \n",
            "this paper addresses the problem of DISTRIBUTED ANOMALY DETECTION in the presence of STRUCTURED SPARSITY . in particular , we consider the problem of ONLINE PRINCIPAL SUBSPACE ESTIMATION in the presence of STRUCTURED SPARSITY . we propose a novel approach to the problem of ONLINE PRINCIPAL SUBSPACE ESTIMATION . the proposed approach is based on the use of STRUCTURED SPARSITY , STRUCTURED SPARSITY , and MESSAGE PASSING . the proposed approach is based on the use of STRUCTURED SPARSITY and MESSAGE PASSING . the experimental results show that the proposed method outperforms the existing methods in terms of both LOCAL COMPUTATION and MESSAGE PASSING .\n",
            "\n",
            "784 1000\n",
            "there are many challenging problems for VIETNAMESE LANGUAGE PROCESSING . it will be a long time before these challenges are met . even some apparently simple problems such as SPELLING CORRECTION are quite difficult and have not been approached systematically yet . in this paper , we will discuss one aspect of this type of work : designing the so-called VIETOOLS to detect and correct spelling of vietnamese texts by using a SPELLING DATABASE based on TELEX CODE . VIETOOLS is also extended to serve many purposes in VIETNAMESE LANGUAGE PROCESSING . \n",
            "this paper presents a novel approach to VIETNAMESE LANGUAGE PROCESSING in VIETNAMESE LANGUAGE PROCESSING . the proposed approach is based on the use of a TELEX CODE for SPELLING CORRECTION . the proposed method is based on the use of a TELEX CODE for SPELLING CORRECTION . the experimental results show that the proposed method is effective in improving the performance of VIETNAMESE LANGUAGE PROCESSING .\n",
            "\n",
            "785 1000\n",
            "many practical coding scenarios deal with sources with TRANSFORM COEFFICIENTS that are well modeled as LAPLACIANS . for the WYNER-ZIV CODING PROBLEM for such sources when CORRELATED SIDE-INFORMATION is available at the DECODER , the SIDE-INFORMATION is modeled as obtained by independent additive laplacian or gaussian innovation on the source . this paper deals with the optimal choice of ENCODING PARAMETERS for practical <unk> coding in such scenarios , using the same QUANTIZER FAMILY as in the REGULAR CODEC to cover a range of RATE-DISTORTION TRADE-OFFS , given the variances of the source and innovation . using our prior analysis of a GENERAL ENCODING MODEL based on MULTI-LEVEL COSET CODES combining SOURCE AND CHANNEL CODING , we present comprehensive tables with optimal ENCODING PARAMETERS . these tables can be readily incorporated into a practical codec to read off the ENCODING PARAMETERS . \n",
            "this paper presents a novel approach to SOURCE AND CHANNEL CODING based on CORRELATED SIDE-INFORMATION . the proposed approach is based on the use of LAPLACIANS , which is based on a GENERAL ENCODING MODEL of the QUANTIZER FAMILY . the proposed approach is based on the use of CORRELATED SIDE-INFORMATION , which is a GENERAL ENCODING MODEL of the QUANTIZER FAMILY . the proposed method is based on the use of CORRELATED SIDE-INFORMATION , which is a GENERAL ENCODING MODEL of the QUANTIZER FAMILY . the proposed approach is evaluated on a REGULAR CODEC , and the results show that the proposed method is effective in improving the RATE-DISTORTION TRADE-OFFS of the DECODER .\n",
            "\n",
            "786 1000\n",
            "personalized TAG RECOMMENDATION systems recommend a list of tags to a user when he is about to annotate an item . it exploits the individual preference and the characteristic of the items . TENSOR FACTORIZATION TECHNIQUES have been applied to many applications , such as TAG RECOMMENDATION . models based on TUCKER DECOMPOSITION can achieve good performance but require a lot of COMPUTATION POWER . on the other hand , models based on CANONICAL DECOMPOSITION can run in LINEAR TIME and are more feasible for ONLINE RECOMMENDATION . in this paper , we propose a novel method for PERSONAL-IZED TAG RECOMMENDATION , which can be considered as a NONLINEAR EXTENSION OF CANONICAL DECOMPOSITION . different from LINEAR TENSOR FACTORIZATION , we exploit GAUS-SIAN RADIAL BASIS FUNCTION to increase the PERSONALIZED TAG RECOMMENDATION SYSTEMS 's capacity . the experimental results show that our proposed method outperforms the state-of-the-art methods for TAG RECOMMENDATION on REAL DATASETS and perform well even with a small number of FEATURES , which verifies that our models can make better use of FEATURES . \n",
            "this paper addresses the problem of PERSONAL-IZED TAG RECOMMENDATION in the presence of LINEAR TIME . we propose a novel approach to the problem of PERSONAL-IZED TAG RECOMMENDATION in PERSONALIZED TAG RECOMMENDATION SYSTEMS . the proposed approach is based on the use of a GAUS-SIAN RADIAL BASIS FUNCTION for PERSONAL-IZED TAG RECOMMENDATION . the proposed approach is based on the use of a GAUS-SIAN RADIAL BASIS FUNCTION , a GAUS-SIAN RADIAL BASIS FUNCTION , and a NONLINEAR EXTENSION OF CANONICAL DECOMPOSITION . the proposed approach is based on the use of a GAUS-SIAN RADIAL BASIS FUNCTION , which is able to deal with LINEAR TIME . the proposed approach is evaluated on REAL DATASETS and compared to the state of the art methods . the proposed approach is evaluated on REAL DATASETS and compared to the state-of-the-art methods .\n",
            "\n",
            "787 1000\n",
            "we present a SINGLE-IMAGE HIGHLIGHT REMOVAL METHOD that incorporates ILLUMINATION-BASED CONSTRAINTS into IMAGE IN-PAINTING . unlike OCCLUDED IMAGE REGIONS filled by traditional IMAGE IN-PAINTING , HIGHLIGHT PIXELS contain some useful information for guiding the IMAGE IN-PAINTING . ILLUMINATION CONSTRAINTS provided by observed PIXEL COLORS , HIGHLIGHT COLOR ANALYSIS and ILLUMINATION COLOR UNIFORMITY are employed in our SINGLE-IMAGE HIGHLIGHT REMOVAL METHOD to improve ESTIMATION OF THE UNDERLYING DIFFUSE COLOR . the inclusion of these ILLUMINATION CONSTRAINTS allows for better RECOVERY OF SHADING AND TEXTURES by IMAGE IN-PAINTING . experimental results are given to demonstrate the performance of our SINGLE-IMAGE HIGHLIGHT REMOVAL METHOD . \n",
            "in this paper , we propose a novel SINGLE-IMAGE HIGHLIGHT REMOVAL METHOD for RECOVERY OF SHADING AND TEXTURES in IMAGE IN-PAINTING . the proposed SINGLE-IMAGE HIGHLIGHT REMOVAL METHOD is based on the use of ILLUMINATION-BASED CONSTRAINTS , HIGHLIGHT COLOR ANALYSIS , and ILLUMINATION COLOR UNIFORMITY . the proposed SINGLE-IMAGE HIGHLIGHT REMOVAL METHOD is based on a novel SINGLE-IMAGE HIGHLIGHT REMOVAL METHOD that exploits the ILLUMINATION-BASED CONSTRAINTS between the HIGHLIGHT PIXELS and the ILLUMINATION COLOR UNIFORMITY . the proposed SINGLE-IMAGE HIGHLIGHT REMOVAL METHOD is based on a novel SINGLE-IMAGE HIGHLIGHT REMOVAL METHOD that uses ILLUMINATION-BASED CONSTRAINTS to estimate the PIXEL COLORS . the proposed SINGLE-IMAGE HIGHLIGHT REMOVAL METHOD has been tested on a variety of OCCLUDED IMAGE REGIONS , and the results show that the proposed SINGLE-IMAGE HIGHLIGHT REMOVAL METHOD is able to accurately detect and track moving objects in a scene .\n",
            "\n",
            "788 1000\n",
            "<unk> coding is a novel IMAGE CODER that utilizes the self-similarity of NATURAL IMAGES that include TEXTURES , in order to achieve STRUCTURALLY LOSSLESS COMPRESSION . the key to a high compression ratio is replacing large image blocks with previously ENCODED BLOCKS with similar structure . adjusting the lighting of the replaced block is critical for eliminating ILLUMINATION ARTIFACTS and increasing the number of matches . we propose a new ADAPTIVE LIGHTING CORRECTION METHOD that is based on the POISSON EQUATION with INCOMPLETE BOUNDARY CONDITIONS . in order to fully exploit the benefits of the ADAPTIVE POISSON LIGHTING CORRECTION , we also propose modifications of the SIDE-MATCHING ALGORITHM and STRUCTURAL TEXTURE SIMILARITY METRIC . we show that the resulting ADAPTIVE LIGHTING CORRECTION METHOD achieves better coding performance . \n",
            "in this paper , we present a novel approach to the problem of STRUCTURALLY LOSSLESS COMPRESSION in NATURAL IMAGES . the proposed approach is based on the use of a POISSON EQUATION , a STRUCTURAL TEXTURE SIMILARITY METRIC , and a STRUCTURAL TEXTURE SIMILARITY METRIC . the proposed approach is based on a novel SIDE-MATCHING ALGORITHM , called MATCHED-TEXTURE CODING , which is based on the POISSON EQUATION and the POISSON EQUATION . the proposed approach is based on a novel SIDE-MATCHING ALGORITHM , called MATCHED-TEXTURE CODING , which is based on the POISSON EQUATION and the STRUCTURAL TEXTURE SIMILARITY METRIC . experimental results show that the proposed method achieves better performance than the state-of-the-art methods in terms of ILLUMINATION ARTIFACTS and the STRUCTURAL TEXTURE SIMILARITY METRIC .\n",
            "\n",
            "789 1000\n",
            "one important class of STATE EMISSION DENSITIES of the HIDDEN MARKOV MODEL is the GAUSSIAN MIXTURE DENSITIES . the classical BAUM-WELCH ALGORITHM often fails to reliably learn the GAUSSIAN MIXTURE DENSITIES when there is INSUFFICIENT TRAINING DATA , due to the large number of FREE PARAMETERS present in the model . in this paper , we propose a novel strategy for robustly and accurately learning the GAUSSIAN MIXTURE STATE EMISSION DENSITIES of the HIDDEN MARKOV MODEL . the strategy is based on an ENSEMBLE FRAMEWORK for PROBABILITY DENSITY ESTIMATION in which the learning of the GAUSSIAN MIXTURE DENSITIES is formulated as a GRADIENT DESCENT SEARCH in a FUNCTION SPACE . the resulting LEARNING ALGORITHM is called '' the BOOSTING BAUM-WELCH ALGORITHM . '' our preliminary experiment results on EMOTION RECOGNITION from speech show that the proposed algorithm outperforms the original BAUM-WELCH ALGORITHM on this task . \n",
            "in this paper , we propose a novel LEARNING ALGORITHM for PROBABILITY DENSITY ESTIMATION in FUNCTION SPACE . the proposed LEARNING ALGORITHM is based on the idea of GRADIENT DESCENT SEARCH , which is a generalization of the BAUM-WELCH ALGORITHM to the STATE EMISSION DENSITIES . the proposed approach is based on the BOOSTING BAUM-WELCH ALGORITHM , which is based on the BOOSTING BAUM-WELCH ALGORITHM . the proposed method is based on the idea that the STATE EMISSION DENSITIES of the HIDDEN MARKOV MODEL can be obtained by using the BAUM-WELCH ALGORITHM . the proposed method is based on the BOOSTING BAUM-WELCH ALGORITHM , which is based on the BOOSTING BAUM-WELCH ALGORITHM . the proposed method is based on the use of a HIDDEN MARKOV MODEL which is able to deal with INSUFFICIENT TRAINING DATA . the proposed method is evaluated on a variety of INSUFFICIENT TRAINING DATA . the experimental results show that the proposed method outperforms the state-of-the-art methods in terms of EMOTION RECOGNITION performance .\n",
            "\n",
            "790 1000\n",
            "this paper concerns the ROBUST ESTIMATION OF NON-RIGID DEFORMATIONS from FEATURE CORRESPONDENCES . we advance the surprising view that for many realistic PHYSICAL DEFORMATIONS , the error of the mismatches -lrb- outliers -rrb- usually <unk> the effects of the curvature of the MANIFOLD on which the correct matches -lrb- inliers -rrb- lie , to the extent that one can tightly <unk> the MANIFOLD within the ERROR BOUNDS of a LOW-DIMENSIONAL HY-PERPLANE for ACCURATE OUTLIER REJECTION . this justifies a simple RANSAC-DRIVEN DEFORMABLE REGISTRATION TECHNIQUE that is at least as accurate as other methods based on the OPTIMISATION OF FULLY DEFORMABLE MODELS . we support our ideas with comprehensive experiments on SYNTHETIC AND REAL DATA typical of the deformations examined in the literature . \n",
            "this paper proposes a novel RANSAC-DRIVEN DEFORMABLE REGISTRATION TECHNIQUE based on a RANSAC-DRIVEN DEFORMABLE REGISTRATION TECHNIQUE . the proposed RANSAC-DRIVEN DEFORMABLE REGISTRATION TECHNIQUE is based on a RANSAC-DRIVEN DEFORMABLE REGISTRATION TECHNIQUE of the MANIFOLD of the MANIFOLD . the proposed RANSAC-DRIVEN DEFORMABLE REGISTRATION TECHNIQUE is based on the use of FEATURE CORRESPONDENCES to estimate the PHYSICAL DEFORMATIONS . the proposed RANSAC-DRIVEN DEFORMABLE REGISTRATION TECHNIQUE is based on a RANSAC-DRIVEN DEFORMABLE REGISTRATION TECHNIQUE , which is based on a RANSAC-DRIVEN DEFORMABLE REGISTRATION TECHNIQUE . the proposed RANSAC-DRIVEN DEFORMABLE REGISTRATION TECHNIQUE is evaluated on both SYNTHETIC AND REAL DATA . the experimental results show that the proposed RANSAC-DRIVEN DEFORMABLE REGISTRATION TECHNIQUE is robust to PHYSICAL DEFORMATIONS , and is robust to PHYSICAL DEFORMATIONS .\n",
            "\n",
            "791 1000\n",
            "it has been previously demonstrated that systems based on BLOCK WISE LOCAL FEATURES and GAUSSIAN MIXTURE MODELS are suitable for VIDEO BASED TALKING FACE VERIFICATION due to the best trade-off in terms of COMPLEXITY , ROBUSTNESS and performance . in this paper , we propose two methods to enhance the ROBUSTNESS and performance of the GMM-ZTNORM BASELINE SYSTEM . first , JOINT FACTOR ANALYSIS is performed to compensate the SESSION VARIABILITIES due to different RECORDING DEVICES , LIGHTING CONDITIONS , FACIAL EXPRESSIONS , etc. . second , the difference between the UNIVERSAL BACKGROUND MODEL and the maximum a POSTERIORI ADAPTED MODEL is mapped into the GMM MEAN SHIFTED SUPERVECTOR whose OVER-COMPLETE DICTIONARY becomes more incoherent . then , for verification purpose , the SPARSE REPRESENTATION computed by L 1-MINIMIZATION with QUADRATIC CONSTRAINTS is employed to model these GMM MEAN SHIFTED SU-PERVECTORS . experimental results show that the proposed system achieved 8.4 % -lrb- group 1 -rrb- and 10.5 % -lrb- group 2 -rrb- equal ERROR RATE on the BANCA TALKING FACE VIDEO DATABASE following the P PROTOCOL and outperformed the GMM-ZTNORM BASELINE by yielding more than 20 % RELATIVE ERROR REDUCTION . \n",
            "this paper presents a novel approach to VIDEO BASED TALKING FACE VERIFICATION in RECORDING DEVICES . the proposed approach is based on the use of GAUSSIAN MIXTURE MODELS and GAUSSIAN MIXTURE MODELS . the proposed method is based on the use of GAUSSIAN MIXTURE MODELS and GAUSSIAN MIXTURE MODELS . the proposed method is based on the use of BLOCK WISE LOCAL FEATURES and QUADRATIC CONSTRAINTS . the proposed method is evaluated on a BANCA TALKING FACE VIDEO DATABASE with FACIAL EXPRESSIONS , LIGHTING CONDITIONS and FACIAL EXPRESSIONS . experimental results show that the proposed method is effective in improving ROBUSTNESS and ROBUSTNESS .\n",
            "\n",
            "792 1000\n",
            "iterative methods that take steps in APPROXIMATE SUBGRADIENT DIRECTIONS have proved to be useful for STOCHASTIC LEARNING PROBLEMS over LARGE OR STREAMING DATA SETS . when the objective consists of a LOSS FUNCTION plus a NONSMOOTH REGULARIZATION TERM , whose purpose is to induce structure -lrb- for example , spar-sity -rrb- in the solution , the solution often lies on a LOW-DIMENSIONAL MANIFOLD along which the regularizer is smooth . this paper shows that a REGULARIZED DUAL AVERAGING ALGORITHM can identify this MANIFOLD with high probability . this observation motivates an ALGORITH-MIC STRATEGY in which , once a NEAR-OPTIMAL MANIFOLD is identified , we switch to an REGULARIZED DUAL AVERAGING ALGORITHM that searches only in this MANIFOLD , which typically has much lower INTRINSIC DIMENSION than the FULL SPACE , thus converging quickly to a NEAR-OPTIMAL POINT with the desired structure . computational results are presented to illustrate these claims . \n",
            "in this paper , we propose a novel REGULARIZED DUAL AVERAGING ALGORITHM for STOCHASTIC LEARNING PROBLEMS . the proposed REGULARIZED DUAL AVERAGING ALGORITHM is based on a REGULARIZED DUAL AVERAGING ALGORITHM , which is a NONSMOOTH REGULARIZATION TERM of the MANIFOLD . the proposed REGULARIZED DUAL AVERAGING ALGORITHM is based on a REGULARIZED DUAL AVERAGING ALGORITHM , which is based on a REGULARIZED DUAL AVERAGING ALGORITHM . the proposed REGULARIZED DUAL AVERAGING ALGORITHM is based on a REGULARIZED DUAL AVERAGING ALGORITHM , which is based on a REGULARIZED DUAL AVERAGING ALGORITHM . the proposed REGULARIZED DUAL AVERAGING ALGORITHM is based on the REGULARIZED DUAL AVERAGING ALGORITHM . the proposed REGULARIZED DUAL AVERAGING ALGORITHM is applied to the MANIFOLD of the MANIFOLD , and the LOSS FUNCTION is used to estimate the INTRINSIC DIMENSION . experimental results show that the proposed REGULARIZED DUAL AVERAGING ALGORITHM is able to recover the MANIFOLD of the MANIFOLD in the presence of LARGE OR STREAMING DATA SETS .\n",
            "\n",
            "793 1000\n",
            "in this paper we propose an algorithm to learn STATISTICAL LANGUAGE UNDERSTANDING MODELS from a CORPUS OF UNALIGNED PAIRS OF SENTENCES and their corresponding SEMANTIC REPRESENTATION . specifically , it allows to automatically map VARIABLE-LENGTH WORD SEGMENTS with their corresponding SEMANTIC UNITS and thus , the DECODING OF USER UTTERANCES to their corresponding meanings . in this way we avoid the time consuming work of manually associate semantic labels to words , process which is needed by almost all the CORPUS-BASED APPROACHES . we use the algorithm to learn the UNDERSTANDING COMPONENT of a SPOKEN DIALOG SYSTEM for RAILWAY INFORMATION RETRIEVAL in SPANISH . experiments show that the results obtained with the proposed method are very promising , whereas the effort employed to obtain the models is not comparable with this of manually segment the training corpus . \n",
            "this paper addresses the problem of RAILWAY INFORMATION RETRIEVAL from a single SPOKEN DIALOG SYSTEM . we propose a novel approach to RAILWAY INFORMATION RETRIEVAL based on a CORPUS OF UNALIGNED PAIRS OF SENTENCES . the proposed approach is based on the use of a CORPUS OF UNALIGNED PAIRS OF SENTENCES for RAILWAY INFORMATION RETRIEVAL . the proposed approach is based on the use of a CORPUS OF UNALIGNED PAIRS OF SENTENCES and a CORPUS OF UNALIGNED PAIRS OF SENTENCES for RAILWAY INFORMATION RETRIEVAL . the proposed approach is evaluated in the context of RAILWAY INFORMATION RETRIEVAL . the experimental results show that the proposed approach is effective in improving the DECODING OF USER UTTERANCES in SPANISH .\n",
            "\n",
            "794 1000\n",
            "the use of multiple features for TRACKING has been proved as an effective approach because limitation of each FEATURE could be compensated . since different types of variations such as ILLUMINATION , OCCLUSION and POSE may happen in a VIDEO SEQUENCE , especially long sequence videos , how to dynamically select the appropriate features is one of the key problems in this approach . to address this issue in MULTI-CUE VISUAL TRACKING , this paper proposes a new JOINT SPARSE REPRESENTATION MODEL for ROBUST FEATURE-LEVEL FUSION . the proposed JOINT SPARSE REPRESENTATION MODEL dynamically removes UNRELIABLE FEATURES to be fused for TRACKING by using the advantages of SPARSE REPRESENTATION . as a result , robust TRACKING performance is obtained . experimental results on publicly available videos show that the proposed JOINT SPARSE REPRESENTATION MODEL outperforms both existing SPARSE REPRESENTATION based and FUSION-BASED TRACKERS . \n",
            "in this paper , we propose a novel JOINT SPARSE REPRESENTATION MODEL for MULTI-CUE VISUAL TRACKING . the proposed JOINT SPARSE REPRESENTATION MODEL is based on a JOINT SPARSE REPRESENTATION MODEL for MULTI-CUE VISUAL TRACKING and TRACKING . the proposed JOINT SPARSE REPRESENTATION MODEL is based on a JOINT SPARSE REPRESENTATION MODEL for ROBUST FEATURE-LEVEL FUSION and TRACKING . the proposed JOINT SPARSE REPRESENTATION MODEL is applied to MULTI-CUE VISUAL TRACKING and TRACKING . the experimental results show that the proposed JOINT SPARSE REPRESENTATION MODEL outperforms the state-of-the-art methods in terms of TRACKING , OCCLUSION , and ILLUMINATION .\n",
            "\n",
            "795 1000\n",
            "person re-identification has been widely studied due to its importance in surveillance and forensics applications . in practice , GALLERY IMAGES are high-resolution -lrb- hr -rrb- while PROBE IMAGES are usually LOW-RESOLUTION in the IDENTIFICATION SCENARIOS with large variation of ILLUMINATION , weather or quality of cameras . PERSON RE-IDENTIFICATION in this kind of scenarios , which we call SUPER-RESOLUTION PERSON RE-IDENTIFICATION , has not been well studied . in this paper , we propose a <unk> low-rank discriminant dictionary learning -lrb- SLD2L -rrb- approach for SR PERSON RE-IDENTIFICATION TASK . with the hr and lr dictionary pair and MAPPING MATRICES learned from the FEATURES of HR AND LR TRAINING IMAGES , SLD2L can convert the FEATURES of lr PROBE IMAGES into HR FEATURES . to ensure that the CONVERTED FEATURES have favorable DISCRIMINATIVE CAPABILITY and the learned dictionaries can well characterize INTRINSIC FEATURE SPACES OF HR AND LR IMAGES , we design a DISCRIMINANT TERM and a LOW-RANK REGULARIZATION TERM for SLD2L . moreover , considering that low resolution results in different degrees of loss for different types of VISUAL APPEARANCE FEATURES , we propose a MULTI-VIEW SLD2L APPROACH , which can learn the TYPE-SPECIFIC DICTIONARY PAIR and mappings for each type of FEATURE . experimental results on multiple PUBLICLY AVAILABLE DATASETS demonstrate the effectiveness of our proposed approaches for the SR PERSON RE-IDENTIFICATION TASK . \n",
            "this paper addresses the problem of SUPER-RESOLUTION PERSON RE-IDENTIFICATION in the context of SUPER-RESOLUTION PERSON RE-IDENTIFICATION . we propose a novel MULTI-VIEW SLD2L APPROACH to the problem of SUPER-RESOLUTION PERSON RE-IDENTIFICATION in the context of SUPER-RESOLUTION PERSON RE-IDENTIFICATION . the proposed MULTI-VIEW SLD2L APPROACH is based on a LOW-RANK REGULARIZATION TERM and a LOW-RANK REGULARIZATION TERM for the MAPPING MATRICES . the proposed FEATURE is based on a LOW-RANK REGULARIZATION TERM and a LOW-RANK REGULARIZATION TERM for the MAPPING MATRICES . the proposed approach is evaluated on PUBLICLY AVAILABLE DATASETS , and the results show that the proposed MULTI-VIEW SLD2L APPROACH is effective in improving the DISCRIMINATIVE CAPABILITY in IDENTIFICATION SCENARIOS . in addition , the proposed method is able to recover the CONVERTED FEATURES of the scene from a single image . we demonstrate the effectiveness of the proposed method on a SR PERSON RE-IDENTIFICATION TASK . the results show that the proposed method is effective in IDENTIFICATION SCENARIOS and can be effectively used in IDENTIFICATION SCENARIOS .\n",
            "\n",
            "796 1000\n",
            "the MANIPULATION OF PROSODY , including pitch , DURATION and intensity , is one of the leading approaches in SYNTHESIZING EMOTION . this paper reports work on the development of a MALAY EMOTIONAL SYNTHESIZER capable of expressing four basic emotions , namely happiness , ANGER , SADNESS and fear for any form of TEXT INPUT with various INTONATION PATTERNS using the PROSODY MANIPULATION PRINCIPLE . the MALAY EMOTIONAL SYNTHESIZER makes use of PROSODY TEMPLATES and PROSODY PARAMETRIC MANIPULATION for different types of SENTENCE STRUCTURE . \n",
            "in this paper , we propose a novel MALAY EMOTIONAL SYNTHESIZER based on PROSODY PARAMETRIC MANIPULATION and PROSODY PARAMETRIC MANIPULATION . the proposed PROSODY MANIPULATION PRINCIPLE is based on the PROSODY MANIPULATION PRINCIPLE and PROSODY PARAMETRIC MANIPULATION . the proposed PROSODY MANIPULATION PRINCIPLE is based on the PROSODY MANIPULATION PRINCIPLE and the PROSODY PARAMETRIC MANIPULATION . the proposed PROSODY MANIPULATION PRINCIPLE is based on the PROSODY MANIPULATION PRINCIPLE and the PROSODY PARAMETRIC MANIPULATION . the proposed PROSODY MANIPULATION PRINCIPLE is based on the PROSODY MANIPULATION PRINCIPLE and PROSODY PARAMETRIC MANIPULATION . the experimental results show that the proposed MALAY EMOTIONAL SYNTHESIZER is capable of SYNTHESIZING EMOTION and SYNTHESIZING EMOTION .\n",
            "\n",
            "797 1000\n",
            "articulatory feature models have been proposed in the AUTOMATIC SPEECH RECOGNITION COMMUNITY as an alternative to PHONE-BASED MODELS OF SPEECH . in this paper , we extend this approach to the VISUAL MODALITY . specifically , we adapt a recently proposed FEATURE-BASED MODEL of pronunciation variation to VISUAL SPEECH RECOGNITION using a set of VISUALLY-SALIENT FEATURES . the model uses a DYNAMIC BAYES-IAN NETWORK to represent the evolution of the feature streams . a bank of SVM FEATURE CLASSIFIERS , with outputs converted to likelihoods , provides input to the VISUAL SPEECH RECOGNITION . we present preliminary experiments on an ISOLATED-WORD VSR TASK , comparing FEATURE-BASED AND VISEME-BASED UNITS and studying the effects of MODELING INTER-FEATURE ASYNCHRONY . \n",
            "this paper presents a novel approach to MODELING INTER-FEATURE ASYNCHRONY in a AUTOMATIC SPEECH RECOGNITION COMMUNITY . the proposed FEATURE-BASED MODEL is based on the use of VISUALLY-SALIENT FEATURES extracted from the VISUAL MODALITY . the proposed approach is based on the use of VISUALLY-SALIENT FEATURES extracted from the VISUAL MODALITY . the proposed FEATURE-BASED MODEL is based on the use of VISUALLY-SALIENT FEATURES extracted from the VISUAL MODALITY . the experimental results show that the proposed FEATURE-BASED MODEL is effective in improving the MODELING INTER-FEATURE ASYNCHRONY performance in the presence of PHONE-BASED MODELS OF SPEECH . the proposed approach is evaluated on a ISOLATED-WORD VSR TASK and a ISOLATED-WORD VSR TASK . the results show that the proposed method is effective in improving the MODELING INTER-FEATURE ASYNCHRONY performance .\n",
            "\n",
            "798 1000\n",
            "pivoting on BILINGUAL PARALLEL CORPORA is a popular approach for PARAPHRASE ACQUISITION . although such PIVOTED PARAPHRASE COLLECTIONS have been successfully used to improve the performance of several different NLP APPLICATIONS , it is still difficult to get an intrinsic estimate of the quality and coverage of the PARAPHRASES contained in these collections . we present PARAQUERY , a tool that helps a user interactively explore and characterize a given PIVOTED PARAPHRASE COLLECTION , analyze its utility for a particular domain , and compare it to other popular LEXICAL SIMILARITY RESOURCES -- all within a single interface . \n",
            "this paper addresses the problem of PARAPHRASE ACQUISITION from BILINGUAL PARALLEL CORPORA . in particular , we show that the problem of PARAPHRASE ACQUISITION can be formulated as a PIVOTED PARAPHRASE COLLECTION , where the goal is to recover the PARAPHRASES of a given set of PARAPHRASES . we show that this problem can be solved efficiently using PARAQUERY . we show that the proposed algorithm is able to learn PARAPHRASES from BILINGUAL PARALLEL CORPORA .\n",
            "\n",
            "799 1000\n",
            "we study the computation of the DUAL FRAME for OVERSAMPLED FILTER BANKS by exploiting GREVILLE 'S FORMULA , which was derived in <unk> to compute the pseudo inverse of a matrix when a new ROW is appended . in this paper , we first develop the BACKWARD GREVILLE FORMULA to handle the case of ROW DELETION . based on GREVILLE 'S FORMULA , we then study the DUAL FRAME computation of the LAPLACIAN PYRAMID . through the BACKWARD GREVILLE FORMULA , we investigate OFBS for ROBUST TRANSMISSION over ERASURE CHANNELS . the necessary and sufficient conditions for OFBS robust to one ERASURE CHANNEL are derived . a POST-FILTERING STRUCTURE is also presented to implement the DUAL FRAME when the TRANSFORM COEFFICIENTS in one SUBBAND are completely lost . \n",
            "in this paper , we propose a novel method for ROBUST TRANSMISSION in ERASURE CHANNELS . the proposed method is based on a BACKWARD GREVILLE FORMULA that uses a GREVILLE 'S FORMULA to estimate the TRANSFORM COEFFICIENTS of the SUBBAND . the proposed method is based on the BACKWARD GREVILLE FORMULA , which is based on the BACKWARD GREVILLE FORMULA . the proposed method is based on the BACKWARD GREVILLE FORMULA , which is based on the BACKWARD GREVILLE FORMULA . the proposed method is based on the BACKWARD GREVILLE FORMULA , which is based on the BACKWARD GREVILLE FORMULA . the proposed BACKWARD GREVILLE FORMULA is applied to the SUBBAND of the SUBBAND , and the TRANSFORM COEFFICIENTS of the SUBBAND are used to estimate the TRANSFORM COEFFICIENTS . the experimental results show that the proposed method outperforms the state-of-the-art methods .\n",
            "\n",
            "800 1000\n",
            "vectors of LOCALLY AGGREGATED DESCRIPTORS have emerged as powerful IMAGE/VIDEO REPRESENTATIONS that compete with or even outperform state-of-the-art approaches on many challenging VISUAL RECOGNITION TASKS . in this paper , we address two fundamental limitations of VECTORS OF LOCALLY AGGREGATED DESCRIPTORS : its requirement for the LOCAL DESCRIPTORS to have VECTOR FORM and its restriction to LINEAR CLASSIFIERS due to its high-dimensionality . to this end , we introduce a KERNELIZED VERSION of VECTORS OF LOCALLY AGGREGATED DESCRIPTORS . this not only lets us inherently exploit more sophisticated CLASSIFICATION SCHEMES , but also enables us to efficiently aggregate NON-VECTOR DESCRIPTORS -lrb- e.g. , TENSORS -rrb- in the VLAD FRAMEWORK . furthermore , we propose three APPROXIMATE FORMULATIONS that allow us to accelerate the CODING PROCESS while still benefiting from the properties of KERNEL VLAD . our experiments demonstrate the effectiveness of our approach at handling MANIFOLD-VALUED DATA , such as COVARIANCE DESCRIPTORS , on several VISUAL RECOGNITION TASKS . our results also evidence the benefits of our NONLINEAR VLAD DESCRIPTORS against the linear ones in EUCLIDEAN SPACE using several standard benchmark datasets . \n",
            "in this paper , we present a novel approach to the problem of KERNEL VLAD in the context of IMAGE/VIDEO REPRESENTATIONS . the proposed approach is based on the idea of KERNEL VLAD , which is based on a KERNELIZED VERSION . the proposed approach is based on the idea of KERNEL VLAD , which is based on a KERNELIZED VERSION . the proposed approach is based on the use of LOCALLY AGGREGATED DESCRIPTORS , which is a generalization of the VLAD FRAMEWORK to the CODING PROCESS . the proposed approach is based on the use of LOCALLY AGGREGATED DESCRIPTORS , which is a generalization of the VLAD FRAMEWORK to the CODING PROCESS . the proposed method is based on the idea of KERNEL VLAD , which is a generalization of the VLAD FRAMEWORK to the CODING PROCESS . the proposed method is evaluated on a variety of VISUAL RECOGNITION TASKS including TENSORS , TENSORS , and TENSORS , and the results show that the proposed method is effective in reducing the number of TENSORS in the CODING PROCESS .\n",
            "\n",
            "801 1000\n",
            "the NYSTRÖM METHOD has long been popular for scaling up KERNEL METHODS . its theoretical guarantees and empirical performance rely critically on the quality of the LANDMARKS selected . we study LANDMARK SELECTION for NYSTRÖM using DETERMI-NANTAL POINT PROCESSES , DISCRETE PROBABILITY MODELS that allow tractable generation of diverse samples . we prove that LANDMARKS selected via DETERMI-NANTAL POINT PROCESSES guarantee bounds on APPROXIMATION ERRORS ; subsequently , we analyze implications for KERNEL RIDGE REGRESSION . contrary to prior <unk> due to CUBIC COMPLEXITY of DPP SAMPLING , we show that -lrb- under certain conditions -rrb- MARKOV CHAIN DPP SAMPLING requires only LINEAR TIME in the size of the data . we present several empirical results that support our THEORETICAL ANALYSIS , and demonstrate the superior performance of DPP-BASED LANDMARK SELECTION compared with existing approaches . \n",
            "this paper addresses the problem of LANDMARK SELECTION for DETERMI-NANTAL POINT PROCESSES . the proposed approach is based on the use of DETERMI-NANTAL POINT PROCESSES to estimate the NYSTRÖM of the NYSTRÖM . the proposed approach is based on the use of DETERMI-NANTAL POINT PROCESSES to estimate the NYSTRÖM of the NYSTRÖM . the proposed method is based on the use of DISCRETE PROBABILITY MODELS to estimate the NYSTRÖM . the proposed method is based on the use of DISCRETE PROBABILITY MODELS to estimate the NYSTRÖM of the NYSTRÖM . the proposed method can be applied to LANDMARK SELECTION for LANDMARKS , which is very important for LANDMARK SELECTION . the experimental results demonstrate the effectiveness of the proposed method in terms of the CUBIC COMPLEXITY and CUBIC COMPLEXITY .\n",
            "\n",
            "802 1000\n",
            "markov decision processes -lrb- MARKOV DECISION PROCESSES -rrb- and CONTINGENCY PLANNING are two widely used approaches to planning under uncertainty . MARKOV DECISION PROCESSES are attractive because the model is extremely general and because many algorithms exist for deriving OPTIMAL PLANS . in contrast , CONTINGENCY PLANNING is normally performed using HEURISTIC TECHNIQUES that do not guarantee op-timality , but the resulting plans are more compact and more understandable . the inability to present CONTINGENCY PLANNING in a clear , intuitive way has limited their applicability in some important domains . we introduce an ANYTIME ALGORITHM for DERIVING CONTINGENCY PLANS that combines the advantages of the two approaches . \n",
            "this paper addresses the problem of DERIVING CONTINGENCY PLANS and DERIVING CONTINGENCY PLANS in MARKOV DECISION PROCESSES . in particular , we focus on the problem of DERIVING CONTINGENCY PLANS and CONTINGENCY PLANNING . in particular , we focus on the problem of DERIVING CONTINGENCY PLANS and CONTINGENCY PLANNING . in particular , we show that this problem can be solved efficiently using HEURISTIC TECHNIQUES . we then show how this ANYTIME ALGORITHM can be used to solve the problem of DERIVING CONTINGENCY PLANS and CONTINGENCY PLANNING .\n",
            "\n",
            "803 1000\n",
            "user clicks on a URL in response to a query are extremely useful predictors of the URL 's relevance to that query . EXACT MATCH CLICK FEATURES tend to suffer from severe data SPARSITY issues in WEB RANKING . such SPARSITY is particularly pronounced for new URLS or long queries where each distinct <unk> pair will rarely occur . to remedy this , we present a set of straightforward yet informative QUERY-URL N-GRAM FEATURES that allows for generalization of LIMITED USER CLICK DATA to large amounts of unseen <unk> pairs . the QUERY-URL N-GRAM FEATURES is motivated by techniques leveraged in the NLP COMMUNITY for dealing with UNSEEN WORDS . we find that there are interesting REGULARITIES across queries and their preferred destination URLS ; for example , queries containing '' form '' tend to lead to clicks on URLS containing '' pdf '' . we evaluate our set of new QUERY-URL N-GRAM FEATURES on a WEB SEARCH RANKING TASK and obtain improvements that are statistically significant at a <unk> < <unk> level over a strong baseline with exact match <unk> features . \n",
            "this paper presents a novel approach to WEB RANKING in the context of WEB RANKING . the proposed approach is based on the use of a set of QUERY-URL N-GRAM FEATURES , which are then used to estimate the REGULARITIES . the proposed method is based on the use of URLS , which are used in conjunction with QUERY-URL N-GRAM FEATURES . the proposed approach is evaluated on a WEB SEARCH RANKING TASK and compared to the results obtained with the conventional QUERY-URL N-GRAM FEATURES . the results show that the proposed method is effective in improving the SPARSITY in terms of SPARSITY .\n",
            "\n",
            "804 1000\n",
            "this paper presents a new ADAPTIVE GRAPH-CUT BASED MOVE-MAKING ALGORITHM for ENERGY MINIMIZATION . traditional MOVE-MAKING ALGORITHMS such as EXPANSION and SWAP operate by searching for better solutions in some PRE-DEFINED MOVES SPACES around the current solution . in contrast , our ADAPTIVE GRAPH-CUT BASED MOVE-MAKING ALGORITHM uses the <unk> interpretation of the EXPANSION-MOVE ALGORITHM to adaptively compute the best <unk> to search over . at each step , it tries to greedily find the <unk> that will lead to biggest decrease in the PRIMAL-DUAL GAP . we test different variants of our ADAPTIVE GRAPH-CUT BASED MOVE-MAKING ALGORITHM on a variety of IMAGE LABELLING PROBLEMS such as OBJECT SEGMENTATION and STEREO . experimental results show that our ADAPTIVE GRAPH-CUT BASED MOVE-MAKING ALGORITHM significantly outper-forms the conventional EXPANSION-MOVE ALGORITHM , in some cases cutting the runtime by 50 % . \n",
            "in this paper , we propose a novel ADAPTIVE GRAPH-CUT BASED MOVE-MAKING ALGORITHM for OBJECT SEGMENTATION . the proposed ADAPTIVE GRAPH-CUT BASED MOVE-MAKING ALGORITHM is based on a EXPANSION-MOVE ALGORITHM , which is a generalization of the standard EXPANSION-MOVE ALGORITHM . the proposed ADAPTIVE GRAPH-CUT BASED MOVE-MAKING ALGORITHM is based on a EXPANSION-MOVE ALGORITHM , which is a generalization of the standard EXPANSION-MOVE ALGORITHM . the proposed ADAPTIVE GRAPH-CUT BASED MOVE-MAKING ALGORITHM is applied to the problem of OBJECT SEGMENTATION , such as OBJECT SEGMENTATION , EXPANSION , and STEREO . the experimental results show that the proposed ADAPTIVE GRAPH-CUT BASED MOVE-MAKING ALGORITHM is more effective than the existing EXPANSION-MOVE ALGORITHM .\n",
            "\n",
            "805 1000\n",
            "this paper presents a method of evaluating UNSU-PERVISED TEXTURE SEGMENTATION ALGORITHMS . the CONTROL SCHEME OF TEXTURE SEGMENTATION has been <unk> as two MODULAR PROCESSES : -lsb- l -rrb- feature computation and -lrb- 2 -rrb- segmentation of HOMOGENEOUS REGIONS based on the FEATURE VALUES . three FEATURE EXTRACTION METHODS are considered : GRAY LEVEL CO-OCCURRENCE MATRAX , LAWS ' TEXTURE ENERGY and GABOR MULTI-CHANNEL FILTERING . three SEGMENTATION ALGORITHMS are considered : FUZZY C-MEANS CLUSTERING , SQUARE-ERROR CLUSTERING and SPLIT-AND-MERGE . a set of 35 REAL SCENE IMAGES with MANUALLY-SPECIFIED GROUND TRUTH was compiled . performance is measured against GROUND TRUTH on REAL IMAGES using REGION-BASED AND PIXEL-BASED PERFORMANCE MET-RICS . \n",
            "in this paper , we propose a novel approach to CONTROL SCHEME OF TEXTURE SEGMENTATION , called FUZZY C-MEANS CLUSTERING , for CONTROL SCHEME OF TEXTURE SEGMENTATION . the proposed approach is based on the use of GRAY LEVEL CO-OCCURRENCE MATRAX , GRAY LEVEL CO-OCCURRENCE MATRAX , LAWS ' TEXTURE ENERGY , and GABOR MULTI-CHANNEL FILTERING . the proposed approach is based on the use of GRAY LEVEL CO-OCCURRENCE MATRAX , LAWS ' TEXTURE ENERGY , GRAY LEVEL CO-OCCURRENCE MATRAX , and FUZZY C-MEANS CLUSTERING . the proposed method is evaluated on a variety of REAL IMAGES , including GRAY LEVEL CO-OCCURRENCE MATRAX , GRAY LEVEL CO-OCCURRENCE MATRAX , GRAY LEVEL CO-OCCURRENCE MATRAX , and FUZZY C-MEANS CLUSTERING . the experimental results show that the proposed method outperforms the state-of-the-art methods in terms of both REGION-BASED AND PIXEL-BASED PERFORMANCE MET-RICS and GROUND TRUTH .\n",
            "\n",
            "806 1000\n",
            "we propose a new framework , called filtered variation -lrb- <unk> -rrb- , for DE-NOISING and SPARSE SIGNAL PROCESSING APPLICATIONS . these problems are inherently ill-posed . hence , we provide REGULARIZATION to overcome this challenge by using DISCRETE TIME FILTERS that are widely used in SIGNAL PROCESSING . we mathematically define the FV PROBLEM , and solve it using ALTERNATING PROJECTIONS in space and transform domains . we provide a globally convergent algorithm based on the projections onto CONVEX SETS APPROACH . we apply to our algorithm to real denoising problems and compare it with the TOTAL VARIATION RECOVERY . \n",
            "this paper addresses the problem of SIGNAL PROCESSING in the presence of SIGNAL PROCESSING . we propose a novel approach to the problem of SIGNAL PROCESSING . the proposed approach is based on the use of ALTERNATING PROJECTIONS for SIGNAL PROCESSING . the proposed approach is based on the idea of REGULARIZATION , which is based on a CONVEX SETS APPROACH . the proposed approach is based on the use of ALTERNATING PROJECTIONS for SIGNAL PROCESSING . the proposed approach is evaluated on a variety of SPARSE SIGNAL PROCESSING APPLICATIONS . the experimental results demonstrate the effectiveness of the proposed CONVEX SETS APPROACH .\n",
            "\n",
            "807 1000\n",
            "acoustic models -lrb- ACOUSTIC MODELS -rrb- of an HMM-BASED CLASSIFIER include various types of HIDDEN VARIABLES such as GENDER TYPE , SPEAKING RATE , and ACOUSTIC ENVIRONMENT . if there exists a CANONICALIZATION PROCESS that reduces the influence of the HIDDEN VARIABLES from the ACOUSTIC MODELS , a ROBUST AUTOMATIC SPEECH RECOGNITION SYSTEM can be realized . in this paper , we describe the configuration of a CANONICALIZATION PROCESS targeting GENDER TYPE as a HIDDEN VARIABLE . the proposed CANONICALIZATION PROCESS is composed of multiple distinctive PHONETIC FEATURE EXTRACTORS corresponding to the HIDDEN VARIABLE and a DPF SELECTOR in which the distance between input <unk> and ACOUSTIC MODELS is compared . in a DPF EXTRACTION STAGE , an input sequence of ACOUSTIC FEATURE VECTORS is mapped onto three DPF SPACES corresponding to male , female , and neutral voice by using three MULTILAYER NEURAL NETWORKS -lrb- mlns -rrb- . experiments are carried out by comparing -lrb- a -rrb- the combination of the CANONICALIZED DPF and a single HMM CLASSIFIER , and -lrb- b -rrb- the combination of a single ACOUSTIC FEATURE and multiple HMM CLASSIFIERS . the result shows that the proposed CANONICALIZATION PROCESS outperforms both of the conventional ROBUST AUTOMATIC SPEECH RECOGNITION SYSTEM with ACOUSTIC FEATURE and a single HMM and the ROBUST AUTOMATIC SPEECH RECOGNITION SYSTEM with multiple HMMS in spite of less memories and COMPUTATION TIME . \n",
            "in this paper , we propose a novel ROBUST AUTOMATIC SPEECH RECOGNITION SYSTEM based on MULTILAYER NEURAL NETWORKS and ROBUST AUTOMATIC SPEECH RECOGNITION SYSTEM . the proposed ROBUST AUTOMATIC SPEECH RECOGNITION SYSTEM is based on the use of PHONETIC FEATURE EXTRACTORS extracted from the ACOUSTIC MODELS and the CANONICALIZATION PROCESS . the proposed DPF SELECTOR is based on the use of PHONETIC FEATURE EXTRACTORS , which is based on a DPF SELECTOR and a DPF SELECTOR . the proposed DPF SELECTOR is applied to the CANONICALIZATION PROCESS and the ROBUST AUTOMATIC SPEECH RECOGNITION SYSTEM . the performance of the proposed DPF SELECTOR is evaluated in terms of COMPUTATION TIME , SPEAKING RATE , SPEAKING RATE , and ACOUSTIC ENVIRONMENT . the performance of the proposed DPF SELECTOR is evaluated in terms of COMPUTATION TIME , SPEAKING RATE , and ROBUST AUTOMATIC SPEECH RECOGNITION SYSTEM .\n",
            "\n",
            "808 1000\n",
            "spectral factorization is a CLASSICAL TOOL in SIGNAL PROCESSING and COMMUNICATIONS . it also plays a critical role in X-RAY CRYSTALLOGRAPHY , in the context of PHASE RETRIEVAL . in this work , we study the problem of SPARSE SPECTRAL FACTORIZATION , aiming to recover a ONE-DIMENSIONAL SPARSE SIGNAL from its AUTOCORRELATION . we present a sufficient condition for the recovery to be unique , and propose an ITERATIVE ALGORITHM that can obtain the original signal -lrb- up to a SIGN CHANGE , TIME-SHIFT and <unk> -rrb- . NUMERICAL SIMULATIONS verify the effectiveness of the proposed algorithm . \n",
            "this paper addresses the problem of PHASE RETRIEVAL and COMMUNICATIONS . we propose a novel ITERATIVE ALGORITHM , called SPARSE SPECTRAL FACTORIZATION , for PHASE RETRIEVAL and PHASE RETRIEVAL . the key idea is to use a ONE-DIMENSIONAL SPARSE SIGNAL , a ONE-DIMENSIONAL SPARSE SIGNAL , and a ONE-DIMENSIONAL SPARSE SIGNAL , which is a ONE-DIMENSIONAL SPARSE SIGNAL . the proposed ITERATIVE ALGORITHM is based on a novel ITERATIVE ALGORITHM , called SPARSE SPECTRAL FACTORIZATION , which is able to deal with SIGN CHANGE and COMMUNICATIONS . the proposed ITERATIVE ALGORITHM is applied to the problem of PHASE RETRIEVAL and PHASE RETRIEVAL . the experimental results show that the proposed approach is able to perform PHASE RETRIEVAL and COMMUNICATIONS .\n",
            "\n",
            "809 1000\n",
            "we present a GENERATIVE MODEL APPROACH to explore INTRINSIC SEMANTIC STRUCTURES in SPORT VIDEOS , e.g. , the CAMERA VIEW in AMERICAN FOOTBALL GAMES . we will invoke the concept of SEMANTIC SPACE to explicitly define the SEMANTIC STRUCTURE in the video in terms of LATENT STATES . a GENERATIVE MODEL APPROACH is used to govern the transition between states , and an GENERATIVE MODEL APPROACH is developed to characterize VISUAL FEATURES pertaining to different states . then the problem is formulated as a STATISTICAL INFERENCE PROCESS where we want to infer LATENT STATES -lrb- i.e. , CAMERA VIEWS -rrb- from observations -lrb- i.e. , VISUAL FEATURES -rrb- . two GENERATIVE MODELS , the HIDDEN MARKOV MODEL and the SEGMENTAL HMM , are involved in this research . in the HIDDEN MARKOV MODEL , both LATENT STATES and VISUAL FEATURES are <unk> , and in the SEGMENTAL HMM , LATENT STATES and VISUAL FEATURES are defined for shots and frames respectively . both GENERATIVE MODEL APPROACH provide promising performance for VIEW-BASED SHOT CLASSIFICATION , and the SEGMENTAL HMM outper-forms the HIDDEN MARKOV MODEL by involving a TWO-LAYER OBSERVATION MODEL to accommodate the VARIABILITY OF VISUAL FEATURES . this GENERATIVE MODEL APPROACH is also applicable to other VIDEO MINING TASKS . \n",
            "in this paper , we propose a novel GENERATIVE MODEL APPROACH for VIEW-BASED SHOT CLASSIFICATION in SPORT VIDEOS . the proposed GENERATIVE MODEL APPROACH is based on the use of a TWO-LAYER OBSERVATION MODEL and a SEGMENTAL HMM to estimate the SEMANTIC STRUCTURE . the proposed GENERATIVE MODEL APPROACH is based on the use of a TWO-LAYER OBSERVATION MODEL , a HIDDEN MARKOV MODEL and a SEGMENTAL HMM . the proposed GENERATIVE MODEL APPROACH is based on the use of a TWO-LAYER OBSERVATION MODEL and a SEGMENTAL HMM . the proposed GENERATIVE MODEL APPROACH is applied to AMERICAN FOOTBALL GAMES such as SEGMENTAL HMM and SEGMENTAL HMM . the experimental results show that the proposed GENERATIVE MODEL APPROACH is effective in VIEW-BASED SHOT CLASSIFICATION .\n",
            "\n",
            "810 1000\n",
            "a SEQUENTIAL SOURCE LOCALIZATION METHOD using PARTICLE FILTER is presented to estimate and track MULTIPLE-TARGET LOCATIONS . this SEQUENTIAL SOURCE LOCALIZATION METHOD is designed to make use of ACOUSTIC SIGNAL measured at multiple ACOUSTIC SENSORS randomly deployed in a WIRELESS DISTRIBUTED SENSOR NETWORK . by using the PARTICLE FILTER , NON-GAUSSIAN PROBABILITY DENSITY FUNCTION of the target locations are represented by a discrete set of '' particles '' . the POSITIONS of these particles are propagated sequentially using known STATE TRANSITION EQUATION , and updated using new LOCATION ESTIMATES via the OBSERVATION EQUATION . compared to a previously proposed MAXIMUM LIKELIHOOD SOURCE LOCALIZATION ALGORITHM , this new SEQUENTIAL SOURCE LOCALIZATION METHOD is computationally effective and more robust to PARAMETER PERTURBATION . \n",
            "this paper presents a novel SEQUENTIAL SOURCE LOCALIZATION METHOD for WIRELESS DISTRIBUTED SENSOR NETWORK . the proposed SEQUENTIAL SOURCE LOCALIZATION METHOD is based on a MAXIMUM LIKELIHOOD SOURCE LOCALIZATION ALGORITHM , which is based on a SEQUENTIAL SOURCE LOCALIZATION METHOD . the proposed SEQUENTIAL SOURCE LOCALIZATION METHOD is based on a MAXIMUM LIKELIHOOD SOURCE LOCALIZATION ALGORITHM that uses a NON-GAUSSIAN PROBABILITY DENSITY FUNCTION to estimate the POSITIONS . the proposed SEQUENTIAL SOURCE LOCALIZATION METHOD is based on a MAXIMUM LIKELIHOOD SOURCE LOCALIZATION ALGORITHM , which is based on the OBSERVATION EQUATION . the proposed SEQUENTIAL SOURCE LOCALIZATION METHOD is applied to the ACOUSTIC SIGNAL of the ACOUSTIC SIGNAL and the LOCATION ESTIMATES is used to estimate the POSITIONS of the ACOUSTIC SIGNAL . the proposed SEQUENTIAL SOURCE LOCALIZATION METHOD is evaluated on a WIRELESS DISTRIBUTED SENSOR NETWORK and compared to the state of the art methods .\n",
            "\n",
            "811 1000\n",
            "we propose a method for NONPARAMETRIC DENSITY ESTIMATION that exhibits ROBUSTNESS to contamination of the training sample . this method achieves ROBUSTNESS by combining a traditional KERNEL DENSITY ESTIMATOR with ideas from CLASSICAL M-ESTIMATION . we interpret the KERNEL DENSITY ESTIMATOR based on a POSITIVE SEMI-DEFINITE KERNEL as a sample mean in the associated REPRODUCING KERNEL HILBERT SPACE . since the sample mean is sensitive to OUTLIERS , we estimate KERNEL DENSITY ESTIMATOR robustly via M-ESTIMATION , yielding a ROBUST KERNEL DENSITY ESTIMATOR . an KERNEL DENSITY ESTIMATOR can be computed efficiently via a kernelized iteratively <unk> least squares -lrb- <unk> -rrb- algorithm . necessary and sufficient conditions are given for KERNELIZED IRWLS to converge to the GLOBAL MINIMIZER of the M-ESTIMATOR OBJECTIVE FUNCTION . the ROBUSTNESS of the KERNEL DENSITY ESTIMATOR is demonstrated with a REPRESENTER THEOREM , the INFLUENCE FUNCTION , and experimental results for DENSITY ESTIMATION and ANOMALY DETECTION . \n",
            "this paper proposes a novel ROBUST KERNEL DENSITY ESTIMATOR based on M-ESTIMATION . the proposed ROBUST KERNEL DENSITY ESTIMATOR is based on the REPRESENTER THEOREM and the INFLUENCE FUNCTION of the INFLUENCE FUNCTION . the proposed ROBUST KERNEL DENSITY ESTIMATOR is based on a POSITIVE SEMI-DEFINITE KERNEL and the INFLUENCE FUNCTION of the INFLUENCE FUNCTION and the INFLUENCE FUNCTION . the proposed ROBUST KERNEL DENSITY ESTIMATOR is based on the REPRESENTER THEOREM . the proposed ROBUST KERNEL DENSITY ESTIMATOR is applied to the problem of ANOMALY DETECTION and ANOMALY DETECTION . the experimental results show that the proposed ROBUST KERNEL DENSITY ESTIMATOR is effective in improving the ROBUSTNESS and ROBUSTNESS of the proposed ROBUST KERNEL DENSITY ESTIMATOR .\n",
            "\n",
            "812 1000\n",
            "* entity linking -lrb- EL -rrb- is the task of linking a TEXTUAL NAMED ENTITY MENTION to a KNOWLEDGE BASE ENTRY . it is a difficult task involving many challenges , but the most crucial problem is ENTITY AMBIGUITY . traditional EL APPROACHES usually employ different constraints and FILTERING TECHNIQUES to improve performance . however , these constraints are executed in several different stages and can not be used interactively . in this paper , we propose several DISAMBIGUATION FORMULAE/FEATURES and employ a MARKOV LOGIC NETWORK to model INTERWEAVED CONSTRAINTS found in one type of EL , gene mention linking . to assess our systems effectiveness in different applications , we adopt two evaluation schemes : <unk> and <unk> <unk> . experimental results show that our system outperforms the baseline systems and state-of-the-art systems under both evaluation schemes . \n",
            "this paper presents a novel approach to ENTITY AMBIGUITY based on a MARKOV LOGIC NETWORK . the proposed MARKOV LOGIC NETWORK is based on a MARKOV LOGIC NETWORK , which is based on a MARKOV LOGIC NETWORK . the proposed MARKOV LOGIC NETWORK is based on a MARKOV LOGIC NETWORK , which is based on a MARKOV LOGIC NETWORK . the proposed method is based on the use of a MARKOV LOGIC NETWORK to estimate the ENTITY AMBIGUITY . the proposed method is evaluated on a variety of EL APPROACHES . the experimental results show that the proposed method is effective in improving the performance of the EL APPROACHES .\n",
            "\n",
            "813 1000\n",
            "mining <unk> events from TEXT STREAMS has been an important research topic . CLASSIC TEXT REPRESENTATION MODEL -lrb- i.e. , VECTOR SPACE MODEL -rrb- can not model TEMPORAL ASPECTS OF DOCUMENTS . to address BURSTVSM , we proposed a novel BURST-BASED TEXT REPRESENTATION MODEL , denoted as BURSTVSM . BURSTVSM corresponds dimensions to BURSTY FEATURES instead of terms , which can capture SEMANTIC AND TEMPORAL INFORMATION . meanwhile , BURSTVSM significantly reduces the number of NON-ZERO ENTRIES in the representation . we test BURSTVSM via SCALABLE EVENT DETECTION , and experiments in a 10-YEAR NEWS ARCHIVE show that our BURSTVSM are both effective and efficient . \n",
            "in this paper , we propose a novel approach to SCALABLE EVENT DETECTION from TEXT STREAMS . the proposed approach is based on a VECTOR SPACE MODEL , called BURSTVSM , for SCALABLE EVENT DETECTION from TEXT STREAMS . the proposed VECTOR SPACE MODEL is based on a VECTOR SPACE MODEL , called BURSTVSM , to estimate the SEMANTIC AND TEMPORAL INFORMATION of the TEXT STREAMS . the proposed BURSTVSM is based on the use of BURSTY FEATURES in the VECTOR SPACE MODEL . the proposed BURSTVSM is evaluated on a 10-YEAR NEWS ARCHIVE and on a 10-YEAR NEWS ARCHIVE .\n",
            "\n",
            "814 1000\n",
            "we propose a CONVEX FORMULATION of the CORRESPONDENCE PROBLEM between two images with respect to an ENERGY FUNCTION measuring DATA CONSISTENCY and SPATIAL REGULARITY . to this end , we formulate the general CORRESPONDENCE PROBLEM as the search for a MINIMAL TWO-DIMENSIONAL SURFACE in r 4 . we then use tools from GEOMETRIC MEASURE THEORY and introduce 2-VECTOR FIELDS as a representation of TWO-DIMENSIONAL SURFACES in r 4 . we propose a discretization of this CONVEX FORMULATION that gives rise to a CONVEX MINIMIZATION PROBLEM and compute a GLOBALLY OPTIMAL SOLUTION using an efficient PRIMAL-DUAL ALGORITHM . \n",
            "this paper addresses the problem of DATA CONSISTENCY in the presence of TWO-DIMENSIONAL SURFACES and SPATIAL REGULARITY . we propose a PRIMAL-DUAL ALGORITHM to the problem of DATA CONSISTENCY and DATA CONSISTENCY . the CORRESPONDENCE PROBLEM is formulated as a CONVEX MINIMIZATION PROBLEM , where the CORRESPONDENCE PROBLEM is solved by a CONVEX FORMULATION . the proposed CONVEX FORMULATION is based on a CONVEX FORMULATION , which is based on the GEOMETRIC MEASURE THEORY . the proposed CONVEX FORMULATION is applied to the problem of DATA CONSISTENCY and DATA CONSISTENCY . the performance of the proposed algorithm is demonstrated on a variety of TWO-DIMENSIONAL SURFACES and TWO-DIMENSIONAL SURFACES .\n",
            "\n",
            "815 1000\n",
            "this paper presents a ROBUST NATURAL LANGUAGE UNDERSTANDING SYSTEM that handles NEGOTIATION SUBDIALOGUES by inferring both the COMMUNICATIVE ACTIONS that people pursue when speaking and the beliefs underlying these actions . we contend that recognizing the complex DISCOURSE ACTIONS pursued in NEGOTIATION SUBDIALOGUES -lrb- e.g. , expressing doubt -rrb- requires both a MULTI-STRENGTH BELIEF MODEL and a PROCESS MODEL that combines different KNOWLEDGE SOURCES in a unified framework . we show how our ROBUST NATURAL LANGUAGE UNDERSTANDING SYSTEM identifies the structure of NEGOTIATION SUBDIALOGUES , including recognizing EXPRESSIONS OF DOUBT , implicit acceptance of COMMUNICATED PROPOSITIONS , and NEGOTIATION SUBDIALOGUES embedded within other NEGOTIATION SUBDIALOGUES . 1 introduction since NEGOTIATION is an integral part of MULTI-AGENT ACTIVITY , a ROBUST NATURAL LANGUAGE UNDERSTANDING SYSTEM must be able to handle SUBDI-ALOGUES in which participants negotiate what has been claimed in order to try to come to some agreement about those claims . to handle such dialogues , the ROBUST NATURAL LANGUAGE UNDERSTANDING SYSTEM must be able to recognize when a dialogue participant has initiated a NEGOTIATION <unk> and why the participant <unk> the NEGOTIATION -lrb- i.e. , what beliefs led the participant to start the NEGOTIATION -rrb- . this paper presents a ROBUST NATURAL LANGUAGE UNDERSTANDING SYSTEM of TASK-ORIENTED INTERACTIONS that <unk> NEGOTIATION SUBDIALOGUES by inferring both the COMMUNICATIVE ACTIONS that people pursue when speaking and the beliefs underlying these actions . we will argue that recognizing the complex DISCOURSE ACTIONS pursued in NEGOTIATION SUBDIALOGUES -lrb- e.g. , expressing doubt -rrb- requires both a MULTI-STRENGTH BELIEF MODEL and a PROCESSING STRATEGY that combines different KNOWLEDGE SOURCES in a unified framework , and we will show how our ROBUST NATURAL LANGUAGE UNDERSTANDING SYSTEM incorporates these and recognizes the structure of NEGOTIATION SUBDIALOGUES . \n",
            "this paper describes a ROBUST NATURAL LANGUAGE UNDERSTANDING SYSTEM for MULTI-AGENT ACTIVITY . the ROBUST NATURAL LANGUAGE UNDERSTANDING SYSTEM is based on a MULTI-STRENGTH BELIEF MODEL , a PROCESS MODEL and a PROCESS MODEL . the ROBUST NATURAL LANGUAGE UNDERSTANDING SYSTEM is based on a PROCESS MODEL and a PROCESS MODEL . the ROBUST NATURAL LANGUAGE UNDERSTANDING SYSTEM is based on a PROCESS MODEL and a PROCESS MODEL for NEGOTIATION . the ROBUST NATURAL LANGUAGE UNDERSTANDING SYSTEM is based on a MULTI-STRENGTH BELIEF MODEL and a PROCESS MODEL . the ROBUST NATURAL LANGUAGE UNDERSTANDING SYSTEM and the PROCESS MODEL are integrated into a ROBUST NATURAL LANGUAGE UNDERSTANDING SYSTEM and a PROCESS MODEL for NEGOTIATION . the proposed ROBUST NATURAL LANGUAGE UNDERSTANDING SYSTEM is applied to the NATURAL LANGUAGE UNDERSTANDING SYSTEM and the PROCESS MODEL and the PROCESS MODEL .\n",
            "\n",
            "816 1000\n",
            "effective reduction of FALSE ALARMS in LARGE-SCALE VIDEO SURVEILLANCE is rather challenging , especially for applications where ABNORMAL EVENTS OF INTEREST rarely occur , such as ABANDONED OBJECT DETECTION . we develop an approach to prioritize ALERTS by ranking them , and demonstrate its great effectiveness in reducing false positives while keeping good DETECTION ACCURACY . our approach benefits from a novel representation of ABANDONED OBJECT ALERTS by RELATIVE ATTRIBUTES , namely STATICNESS , FOREGROUNDNESS and ABANDON-MENT . the relative strengths of these RELATIVE ATTRIBUTES are quantified using a RANKING FUNCTION -lsb- 19 -rsb- learnt on suitably designed LOW-LEVEL SPATIAL AND TEMPORAL FEATURES.THESE ATTRIBUTES of varying strengths are not only powerful in distinguishing ABANDONED OBJECTS from FALSE ALARMS such as PEOPLE AND LIGHT ARTIFACTS , but also computationally efficient for LARGE-SCALE DEPLOYMENT . with these FEATURES , we apply a LINEAR RANKING ALGORITHM to sort ALERTS according to their relevance to the end-user . we test the effectiveness of our approach on both PUBLIC DATA SETS and large ones collected from the real world . \n",
            "in this paper , we propose a novel approach to ABANDONED OBJECT DETECTION in LARGE-SCALE VIDEO SURVEILLANCE . the proposed approach is based on the use of a RANKING FUNCTION , a LINEAR RANKING ALGORITHM , and a LINEAR RANKING ALGORITHM for ABANDONED OBJECT DETECTION . the proposed approach is based on the use of a set of FEATURES , a LINEAR RANKING ALGORITHM and a FOREGROUNDNESS . the proposed approach is based on a novel LINEAR RANKING ALGORITHM , which is based on a LINEAR RANKING ALGORITHM . the proposed approach is evaluated on PUBLIC DATA SETS , and the results show that the proposed method is robust to FALSE ALARMS such as FOREGROUNDNESS , STATICNESS , and FOREGROUNDNESS .\n",
            "\n",
            "817 1000\n",
            "we study the RESTLESS BANDIT PROBLEM where arms are associated with STATIONARY Φ-MIXING PROCESSES and where rewards are therefore dependent : the question that arises from this setting is that of carefully recovering some independence by ` <unk> ' the values of some rewards . as we shall see , the RESTLESS BANDIT PROBLEM we tackle requires us to address the EXPLORATION/EXPLOITATION/INDEPENDENCE TRADE-OFF , which we do by considering the idea of a WAITING ARM in the new REMIX-UCB ALGORITHM , a generalization of <unk> for the problem at hand , that we introduce . we provide a REGRET ANALYSIS for this RESTLESS BANDIT PROBLEM ; two noticeable features of REMIX-UCB are that i -rrb- REGRET ANALYSIS reduces to the regular <unk> when the Φ-MIXING COEFFICIENTS are all 0 , i.e. when the I.I.D SCENARIO is recovered , and ii -rrb- when ϕ -lrb- n -rrb- = o -lrb- n − α -rrb- , REGRET ANALYSIS is able to ensure a controlled regret of order θ ∆ -lrb- α − 2 -rrb- / α * log 1 / α t , where ∆ * encodes the distance between the best arm and the best SUBOPTIMAL ARM , even in the case when α < 1 , i.e. the case when the Φ-MIXING COEFFICIENTS are not <unk> . \n",
            "in this paper , we propose a novel approach to the RESTLESS BANDIT PROBLEM for a RESTLESS BANDIT PROBLEM . the proposed approach is based on the use of a REMIX-UCB ALGORITHM , a REMIX-UCB ALGORITHM , to the RESTLESS BANDIT PROBLEM . the proposed algorithm is based on the use of a REMIX-UCB ALGORITHM , which is a RESTLESS BANDIT PROBLEM . the proposed REMIX-UCB ALGORITHM is applied to the problem of RESTLESS BANDIT PROBLEM . the proposed method is evaluated on a I.I.D SCENARIO , and the results show that the proposed method is effective in reducing the number of Φ-MIXING COEFFICIENTS in the I.I.D SCENARIO .\n",
            "\n",
            "818 1000\n",
            "permutation of the outputs at different FREQUENCY BINS remains as a major problem in the CONVOLUTIVE BLIND SOURCE SEPARATION . in this work a COUPLED HIDDEN MARKOV MODEL effectively exploits the PSYCHOACOUSTIC CHARACTERISTICS OF SIGNALS to mitigate such permutation . a JOINT DIAGONALIZATION ALGORITHM for CONVOLUTIVE BSS , which incorporates a NON-UNITARY PENALTY TERM within the CROSS-POWER SPECTRUM-BASED COST FUNCTION in the FREQUENCY DOMAIN , has been used . the proposed COUPLED HIDDEN MARKOV MODEL couples a number of conventional HMMS , equivalent to the number of outputs , by making STATE TRANSITIONS in each model dependent not only on its own previous state , but also on some aspects of the state of the other models . using this JOINT DIAGONALIZATION ALGORITHM the PERMUTATION EFFECT has been substantially reduced , and demonstrated using a number of SIMULATION STUDIES . \n",
            "this paper addresses the problem of CONVOLUTIVE BLIND SOURCE SEPARATION in CONVOLUTIVE BSS . in particular , we consider the problem of CONVOLUTIVE BLIND SOURCE SEPARATION in the presence of STATE TRANSITIONS . in particular , we consider the problem of CONVOLUTIVE BLIND SOURCE SEPARATION in the presence of STATE TRANSITIONS in the presence of STATE TRANSITIONS . in particular , we propose a JOINT DIAGONALIZATION ALGORITHM to the problem of CONVOLUTIVE BLIND SOURCE SEPARATION in a FREQUENCY DOMAIN . the proposed COUPLED HIDDEN MARKOV MODEL is based on a COUPLED HIDDEN MARKOV MODEL with a NON-UNITARY PENALTY TERM . the proposed JOINT DIAGONALIZATION ALGORITHM is applied to the PSYCHOACOUSTIC CHARACTERISTICS OF SIGNALS of the COUPLED HIDDEN MARKOV MODEL . the experimental results demonstrate the effectiveness of the proposed JOINT DIAGONALIZATION ALGORITHM in comparison to the state of the art .\n",
            "\n",
            "819 1000\n",
            "this paper addresses the BLIND DECONVOLUTION of multi-input -- MULTI-OUTPUT FIR SYSTEMS driven by WHITE NON-GAUSSIAN SOURCE SIGNALS . first , we present a weaker condition on SOURCE SIGNALS than the SO-CALLED I.I.D. CONDITION so that BLIND DECONVOLUTION is possible . then , under this condition , we provide a necessary and sufficient condition for BLIND DECONVOLUTION OF MIMO FIR SYSTEMS . finally , based on this result , we propose two MAXIMIZATION CRITERIA for BLIND DECONVOLUTION OF MIMO FIR SYSTEMS . these MAXIMIZATION CRITERIA are simple enough to be implemented by ADAPTIVE ALGORITHMS . \n",
            "this paper presents a novel approach to BLIND DECONVOLUTION OF MIMO FIR SYSTEMS based on WHITE NON-GAUSSIAN SOURCE SIGNALS . the proposed MAXIMIZATION CRITERIA is based on the use of MAXIMIZATION CRITERIA for BLIND DECONVOLUTION . the proposed MAXIMIZATION CRITERIA is based on the use of MAXIMIZATION CRITERIA for BLIND DECONVOLUTION . the proposed approach is based on the use of MAXIMIZATION CRITERIA and ADAPTIVE ALGORITHMS for BLIND DECONVOLUTION OF MIMO FIR SYSTEMS . the experimental results show that the proposed MAXIMIZATION CRITERIA can be reduced by the proposed method .\n",
            "\n",
            "820 1000\n",
            "this paper investigates a novel problem of GENERATING IMAGES from VISUAL ATTRIBUTES . we model the image as a COMPOSITE OF FOREGROUND AND BACKGROUND and develop a LAYERED GENERATIVE MODEL with DISENTANGLED LATENT VARIABLES that can be learned end-to-end using a VARIATIONAL AUTO-ENCODER . we experiment with NATURAL IMAGES OF FACES and birds and demonstrate that the proposed LAYERED GENERATIVE MODEL are capable of generating realistic and diverse samples with DISENTANGLED LATENT REPRESENTATIONS . we use a general ENERGY MINIMIZATION ALGORITHM for POSTERIOR INFERENCE OF LATENT VARIABLES given novel images . therefore , the learned GENERATIVE MODELS show excellent quantitative and visual results in the tasks of ATTRIBUTE-CONDITIONED IMAGE RECONSTRUCTION and COMPLETION . \n",
            "this paper presents a novel LAYERED GENERATIVE MODEL for GENERATING IMAGES and COMPLETION . the proposed LAYERED GENERATIVE MODEL is based on a LAYERED GENERATIVE MODEL for GENERATING IMAGES and COMPLETION . the proposed LAYERED GENERATIVE MODEL is based on a LAYERED GENERATIVE MODEL for GENERATING IMAGES and COMPLETION . the proposed LAYERED GENERATIVE MODEL is based on a LAYERED GENERATIVE MODEL , which is based on POSTERIOR INFERENCE OF LATENT VARIABLES and COMPLETION . the proposed LAYERED GENERATIVE MODEL is applied to GENERATING IMAGES and COMPLETION . the experimental results show that the proposed LAYERED GENERATIVE MODEL is effective in capturing the COMPOSITE OF FOREGROUND AND BACKGROUND for GENERATING IMAGES and COMPLETION .\n",
            "\n",
            "821 1000\n",
            "in CASE-BASED REASONING , problems are solved by retrieving prior cases and adapting their solutions to fit ; learning occurs as new cases are stored . controlling the growth of the case base is a fundamental problem , and research on CASE-BASE MAINTENANCE has developed methods for COMPACTING CASE BASES while maintaining SYSTEM COMPETENCE , primarily by COMPETENCE-BASED DELETION STRATEGIES assuming STATIC CASE ADAPTATION KNOWLEDGE . this paper proposes ADAPTATION-GUIDED CASE-BASE MAINTENANCE , a CASE-BASE MAINTENANCE APPROACH exploiting the ability to dynamically generate new ADAPTATION KNOWLEDGE from cases . in ADAPTATION-GUIDED CASE-BASE MAINTENANCE , CASE RETENTION DECISIONS are based both on cases ' value as base cases for solving problems and on their value for generating new ADAPTATION RULES . the paper illustrates the method for NUMERICAL PREDICTION TASKS -lrb- case-based regression -rrb- in which ADAPTATION RULES are generated automatically using the CASE DIFFERENCE HEURISTIC . in comparisons of ADAPTATION-GUIDED CASE-BASE MAINTENANCE to five alternative methods in four domains , for varying CASE BASE DENSITIES , ADAPTATION-GUIDED CASE-BASE MAINTENANCE outperformed the alternatives in all domains , with greatest benefit at high compression . \n",
            "this paper proposes a novel CASE-BASE MAINTENANCE APPROACH , called ADAPTATION-GUIDED CASE-BASE MAINTENANCE , for CASE-BASED REASONING . the proposed CASE-BASE MAINTENANCE APPROACH , called ADAPTATION-GUIDED CASE-BASE MAINTENANCE , is based on a CASE DIFFERENCE HEURISTIC , called ADAPTATION-GUIDED CASE-BASE MAINTENANCE . the proposed CASE-BASE MAINTENANCE APPROACH , called ADAPTATION-GUIDED CASE-BASE MAINTENANCE , is based on a CASE DIFFERENCE HEURISTIC , called ADAPTATION-GUIDED CASE-BASE MAINTENANCE . the proposed CASE-BASE MAINTENANCE APPROACH , called ADAPTATION-GUIDED CASE-BASE MAINTENANCE , is based on the CASE DIFFERENCE HEURISTIC , which is based on the CASE DIFFERENCE HEURISTIC . the proposed CASE-BASE MAINTENANCE APPROACH is based on the CASE DIFFERENCE HEURISTIC , which is a generalization of the CASE DIFFERENCE HEURISTIC . the proposed CASE-BASE MAINTENANCE APPROACH is applied to COMPACTING CASE BASES , and the results show that the proposed CASE-BASE MAINTENANCE APPROACH is effective in reducing the number of CASE BASE DENSITIES . moreover , the proposed CASE-BASE MAINTENANCE APPROACH can also be applied to other NUMERICAL PREDICTION TASKS .\n",
            "\n",
            "822 1000\n",
            "the CONVERGENCE ANALYSIS of the LEAST MEAN SQUARE ALGORITHM has been conventionally based on STOCHASTIC SIGNALS and describes thus only the average behavior of the LEAST MEAN SQUARE ALGORITHM . it has been shown previously that a PERIODIC-REFERENCE LMS SYSTEM can be regarded as a LINEAR TIME-PERIODIC SYSTEM whose stability can be determined from the MONODROMY MATRIX . generally , the MONODROMY MATRIX can only be solved numerically and does not thus reveal the actual factors behind the dynamics of the PERIODIC-REFERENCE LMS SYSTEM . this paper derives an ESTIMATOR for the eigenvalues of the MONODROMY MATRIX . the ESTIMATOR is easy to calculate , and ESTIMATOR also reveals the underlying reason for the bad convergence of the LEAST MEAN SQUARE ALGORITHM in some special cases . the ESTIMATOR is confirmed by comparing ESTIMATOR to the precise eigenvalues of the MONODROMY MATRIX . the ESTIMATOR is found to be accurate for the eigenvalues close to unity . \n",
            "in this paper , we propose a novel ESTIMATOR for STOCHASTIC SIGNALS . the proposed PERIODIC-REFERENCE LMS SYSTEM is based on the use of STOCHASTIC SIGNALS to estimate the MONODROMY MATRIX . the proposed PERIODIC-REFERENCE LMS SYSTEM is based on the use of STOCHASTIC SIGNALS to estimate the MONODROMY MATRIX . the proposed ESTIMATOR is based on the use of STOCHASTIC SIGNALS to estimate the MONODROMY MATRIX . the performance of the proposed ESTIMATOR is evaluated on the PERIODIC-REFERENCE LMS SYSTEM . the experimental results show that the proposed PERIODIC-REFERENCE LMS SYSTEM is effective in improving the performance of the proposed PERIODIC-REFERENCE LMS SYSTEM .\n",
            "\n",
            "823 1000\n",
            "in this paper , a PERCEPTUAL WEIGHTING MODEL is proposed for effective RATE CONTROL so as to enhance PERCEPTUAL CODING QUALITY OF VIDEOPHONE , by exploiting two categories of factors affecting the perception of the HUMAN VISUAL SYSTEM : STIMULUS-DRIVEN FACTORS and COGNITION-DRIVEN FACTORS . in order to achieve a simple but effective PERCEPTUAL WEIGHTING MODEL , we use LUMINANCE ADAPTATION and texture masking as the STIMULUS-DRIVEN FACTORS , while SKIN COLOR serves as the COGNITION-DRIVEN FACTOR in the VIDEOPHONE APPLICATION . both objective and subjective quality evaluations of VIDEOPHONE-LIKE SEQUENCES in h. 263 platform validate the effectiveness of our PERCEPTUAL WEIGHTING MODEL . \n",
            "this paper presents a novel PERCEPTUAL WEIGHTING MODEL for PERCEPTUAL CODING QUALITY OF VIDEOPHONE in a VIDEOPHONE APPLICATION . the proposed PERCEPTUAL WEIGHTING MODEL is based on the use of a PERCEPTUAL WEIGHTING MODEL , a PERCEPTUAL WEIGHTING MODEL , and a PERCEPTUAL WEIGHTING MODEL . the proposed PERCEPTUAL WEIGHTING MODEL consists of two steps : -lrb- 1 -rrb- a PERCEPTUAL WEIGHTING MODEL , a PERCEPTUAL WEIGHTING MODEL , and a COGNITION-DRIVEN FACTOR ; -lrb- 2 -rrb- a PERCEPTUAL WEIGHTING MODEL based on a PERCEPTUAL WEIGHTING MODEL . the proposed PERCEPTUAL WEIGHTING MODEL is applied to VIDEOPHONE-LIKE SEQUENCES , and the experimental results demonstrate the effectiveness of the proposed PERCEPTUAL WEIGHTING MODEL . the performance of the proposed PERCEPTUAL WEIGHTING MODEL is demonstrated on a variety of VIDEOPHONE-LIKE SEQUENCES .\n",
            "\n",
            "824 1000\n",
            "the present paper describes a CORPUS-BASED SINGING VOICE SYNTHESIS SYSTEM based on HIDDEN MARKOV MODELS . this CORPUS-BASED SINGING VOICE SYNTHESIS SYSTEM employs the HMM-BASED SPEECH SYNTHESIS to synthesize SINGING VOICE . MUSICAL INFORMATION such as LYRICS , tones , DURATIONS is modeled simultaneously in a unified framework of the CONTEXT-DEPENDENT HMM . CORPUS-BASED SINGING VOICE SYNTHESIS SYSTEM can mimic the VOICE QUALITY and SINGING STYLE of the original singer . results of a SINGING VOICE synthesis experiment show that the proposed CORPUS-BASED SINGING VOICE SYNTHESIS SYSTEM can synthesize SMOOTH AND NATURAL-SOUNDING SINGING VOICE . \n",
            "this paper presents a CORPUS-BASED SINGING VOICE SYNTHESIS SYSTEM based on HIDDEN MARKOV MODELS for HMM-BASED SPEECH SYNTHESIS . the CORPUS-BASED SINGING VOICE SYNTHESIS SYSTEM is based on HIDDEN MARKOV MODELS . the CORPUS-BASED SINGING VOICE SYNTHESIS SYSTEM is based on a CONTEXT-DEPENDENT HMM . the CORPUS-BASED SINGING VOICE SYNTHESIS SYSTEM is based on a CONTEXT-DEPENDENT HMM . the CORPUS-BASED SINGING VOICE SYNTHESIS SYSTEM is based on HIDDEN MARKOV MODELS . the CORPUS-BASED SINGING VOICE SYNTHESIS SYSTEM is based on a CONTEXT-DEPENDENT HMM . the proposed CORPUS-BASED SINGING VOICE SYNTHESIS SYSTEM is evaluated in a SINGING STYLE . the results show that the proposed CORPUS-BASED SINGING VOICE SYNTHESIS SYSTEM is able to accurately detect and track the SINGING VOICE in a SINGING STYLE .\n",
            "\n",
            "825 1000\n",
            "this acoustic study explored dialect effects on REALIZATION OF NUCLEAR PITCH ACCENTS in three regional varieties of AMERICAN ENGLISH spoken in CENTRAL OHIO , SOUTHEASTERN WISCONSIN and western north <unk> . fundamental frequency -lrb- f0 -rrb- change from VOWEL ONSET to offset in the most prominent syllable in a sentence was examined along four parameters : maximum f0 change , RELATIVE LOCATION of f0 maximum , f0 offset and f0 fall from maximum to offset . a robust finding was that the F0 CONTOURS in the SOUTHERN -LRB- NORTH CAROLINA -RRB- VARIANTS were significantly distinct from the two MIDWESTERN VARIETIES whose contours did not differ significantly from one another . the SOUTHERN VOWELS had an earlier f0 rise , a greater f0 fall and a lower f0 offset than either OHIO OR WISCONSIN VOWELS . there was a sharper f0 drop preceding a voiceless than a VOICED SYLLABLE CODA . no significant DIALECT-RELATED DIFFERENCES were found for FLAT F0 CONTOURS in UNSTRESSED VOWELS , which were also examined in the study . this study contributes the finding that dynamic variations in pitch are greater for VOWELS which also exhibit a greater amount of SPECTRAL DYNAMICS . the interaction of these two sets of cues contributes to the MELODIC COMPONENT associated with a specific REGIONAL ACCENT . \n",
            "this paper presents a novel method for REALIZATION OF NUCLEAR PITCH ACCENTS in AMERICAN ENGLISH . the proposed method consists of two steps : -lrb- 1 -rrb- a VOICED SYLLABLE CODA with FLAT F0 CONTOURS and 2 -rrb- a CENTRAL OHIO with FLAT F0 CONTOURS . the proposed method consists of two steps : -lrb- 1 -rrb- a CENTRAL OHIO with FLAT F0 CONTOURS and 2 -rrb- a CENTRAL OHIO with FLAT F0 CONTOURS . the proposed method consists of two steps : -lrb- 1 -rrb- a VOICED SYLLABLE CODA with FLAT F0 CONTOURS and 2 -rrb- a VOICED SYLLABLE CODA . the proposed method consists of two steps : -lrb- 1 -rrb- a VOICED SYLLABLE CODA with FLAT F0 CONTOURS and 2 -rrb- a MELODIC COMPONENT with DIALECT-RELATED DIFFERENCES . the proposed method is based on the REALIZATION OF NUCLEAR PITCH ACCENTS and the REALIZATION OF NUCLEAR PITCH ACCENTS . the proposed method is evaluated on a variety of AMERICAN ENGLISH . the results show that the proposed method is effective in reducing the number of VOWELS in the presence of MIDWESTERN VARIETIES and DIALECT-RELATED DIFFERENCES .\n",
            "\n",
            "826 1000\n",
            "a central challenge to many fields of SCIENCE AND ENGINEERING involves minimizing NON-CONVEX ERROR FUNCTIONS over CONTINUOUS , HIGH DIMENSIONAL SPACES . gradient descent or QUASI-NEWTON METHODS are almost ubiquitously used to perform such <unk> , and it is often thought that a main source of difficulty for these LOCAL METHODS to find the GLOBAL MINIMUM is the proliferation of LOCAL MINIMA with much higher error than the GLOBAL MINIMUM . here we argue , based on results from STATISTICAL PHYSICS , RANDOM MATRIX THEORY , NEURAL NETWORK THEORY , and empirical evidence , that a deeper and more profound difficulty originates from the PROLIFERATION OF SADDLE POINTS , not LOCAL MINIMA , especially in HIGH DIMENSIONAL PROBLEMS OF PRACTICAL INTEREST . such SADDLE POINTS are surrounded by high error <unk> that can dramatically slow down learning , and give the <unk> impression of the existence of a LOCAL MINIMUM . motivated by these arguments , we propose a new approach to SECOND-ORDER OPTIMIZATION , the <unk> newton method , that can rapidly escape HIGH DIMENSIONAL SADDLE POINTS , unlike GRADIENT DESCENT and QUASI-NEWTON METHODS . we apply this algorithm to DEEP OR RECURRENT NEURAL NETWORK TRAINING , and provide numerical evidence for its superior optimization performance . \n",
            "this paper addresses the problem of STATISTICAL PHYSICS and NEURAL NETWORK THEORY . in particular , we focus on the problem of STATISTICAL PHYSICS , and propose a method to estimate the PROLIFERATION OF SADDLE POINTS of the signal . the proposed method is based on the use of RANDOM MATRIX THEORY and NEURAL NETWORK THEORY . the proposed method is based on the use of RANDOM MATRIX THEORY and QUASI-NEWTON METHODS . the proposed method is based on the use of RANDOM MATRIX THEORY and QUASI-NEWTON METHODS . the proposed method is based on the use of RANDOM MATRIX THEORY and RANDOM MATRIX THEORY . the proposed method is compared with the state of the art in SCIENCE AND ENGINEERING and QUASI-NEWTON METHODS . the performance of the proposed method is compared with the state of the art in SCIENCE AND ENGINEERING .\n",
            "\n",
            "827 1000\n",
            "the COMPUTATIONAL COMPLEXITY of a problem arising in the context of SPARSE OPTIMIZATION is considered , namely , the PROJECTION onto the set of K-COSPARSE VECTORS w.r.t. some given MATRIX Ω . it is shown that this PROJECTION PROBLEM is -lrb- strongly -rrb- np-hard , even in the special cases in which the MATRIX Ω contains only TERNARY OR BIPOLAR COEFFICIENTS . interestingly , this is in contrast to the PROJECTION onto the set of K-SPARSE VECTORS , which is trivially solved by keeping only the k largest coefficients . \n",
            "this paper proposes a novel method to estimate the MATRIX Ω of a MATRIX Ω in a PROJECTION PROBLEM . the proposed method is based on the use of MATRIX Ω in the MATRIX Ω . the proposed method is based on the PROJECTION of the MATRIX Ω of the MATRIX Ω . the proposed method is based on the PROJECTION of the MATRIX Ω of the MATRIX Ω . the experimental results show that the proposed method is effective in reducing the COMPUTATIONAL COMPLEXITY of the proposed method .\n",
            "\n",
            "828 1000\n",
            "stochastic variational inference finds good posterior approximations of PROBABILISTIC MODELS with very large data sets . it optimizes the VARI-ATIONAL OBJECTIVE with STOCHASTIC OPTIMIZATION , following noisy estimates of the NATURAL GRADIENT . <unk> , STOCHASTIC INFERENCE iteratively <unk> from the data , analyzes the SUBSAMPLE , and updates parameters with a DECREASING LEARNING RATE . however , the algorithm is sensitive to that rate , which usually requires HAND-TUNING to each application . we solve this problem by developing an ADAPTIVE LEARNING RATE for STOCHASTIC INFERENCE . our method requires no TUNING and is easily implemented with computations already made in the algorithm . we demonstrate our approach with LATENT DIRICHLET ALLOCATION applied to three LARGE TEXT CORPORA . INFERENCE with the ADAPTIVE LEARNING RATE converges faster and to a better approximation than the best settings of HAND-TUNED RATES . \n",
            "this paper addresses the problem of STOCHASTIC INFERENCE in LARGE TEXT CORPORA . we propose a new method for STOCHASTIC INFERENCE in LARGE TEXT CORPORA . the proposed algorithm is based on the use of STOCHASTIC VARIATIONAL INFERENCE in the form of STOCHASTIC OPTIMIZATION . the proposed algorithm is based on the use of the LATENT DIRICHLET ALLOCATION in the STOCHASTIC OPTIMIZATION . the proposed algorithm is based on the use of the LATENT DIRICHLET ALLOCATION in the STOCHASTIC OPTIMIZATION . the proposed algorithm is based on the use of the LATENT DIRICHLET ALLOCATION in the STOCHASTIC OPTIMIZATION . the ADAPTIVE LEARNING RATE of the proposed method is demonstrated on a variety of LARGE TEXT CORPORA . the results show that the proposed method is effective in reducing the number of HAND-TUNED RATES in the presence of TUNING .\n",
            "\n",
            "829 1000\n",
            "empirical mode decomposition -lrb- EMPIRICAL MODE DECOMPOSITION -rrb- has lately received much attention due to the many interesting features that exhibits . however it lacks a strong theoretical basis which would allow a PERFORMANCE ANALYSIS and hence the enhancement and optimization of the method in a systematic way . in this paper , an investigation of EMPIRICAL MODE DECOMPOSITION is attempted in an alternative way : the INTERPOLATION POINTS and the PIECEWISE INTERPOLATING POLYNOMIALS for the formation of the upper and lower envelopes of the signal are optimized based on a GENETIC ALGORITHM FRAMEWORK revealing important characteristics of the method which where previously hidden . as a result , novel directions for both the performance enhancement and the theoretical investigation of the method are <unk> . \n",
            "in this paper , we propose a novel GENETIC ALGORITHM FRAMEWORK for PIECEWISE INTERPOLATING POLYNOMIALS . the proposed GENETIC ALGORITHM FRAMEWORK is based on a GENETIC ALGORITHM FRAMEWORK , which is based on the EMPIRICAL MODE DECOMPOSITION . the proposed GENETIC ALGORITHM FRAMEWORK is based on a GENETIC ALGORITHM FRAMEWORK , which is a generalization of the EMPIRICAL MODE DECOMPOSITION . the proposed GENETIC ALGORITHM FRAMEWORK is applied to the problem of EMPIRICAL MODE DECOMPOSITION . the experimental results show that the proposed GENETIC ALGORITHM FRAMEWORK is effective in reducing the number of INTERPOLATION POINTS in the presence of INTERPOLATION POINTS .\n",
            "\n",
            "830 1000\n",
            "in this paper we propose a method for learning BAYESIAN BELIEF NETWORKS from data . the method uses ARTIFICIAL NEURAL NETWORKS as PROBABILITY ESTIMATORS , thus avoiding the need for making prior assumptions on the nature of the probability distributions governing the relationships among the participating variables . this new method has the potential for being applied to domains containing both DISCRETE AND CONTINUOUS VARIABLES arbitrarily distributed . we compare the learning performance of this new method with the performance of the method proposed by cooper and <unk> in -lsb- 7 -rsb- . the experimental results show that , although the LEARNING SCHEME based on the use of ANN ESTIMATORS is slower , the LEARNING ACCURACY of the two methods is comparable . category : algorithms and ARCHITECTURES . \n",
            "this paper addresses the problem of ARTIFICIAL NEURAL NETWORKS in the context of ARTIFICIAL NEURAL NETWORKS . we propose a new LEARNING SCHEME based on BAYESIAN BELIEF NETWORKS . the proposed ANN ESTIMATORS is based on the use of ANN ESTIMATORS to estimate the DISCRETE AND CONTINUOUS VARIABLES . the proposed algorithm is based on the use of BAYESIAN BELIEF NETWORKS to estimate the DISCRETE AND CONTINUOUS VARIABLES of the signal . the proposed algorithm is applied to the problem of ARTIFICIAL NEURAL NETWORKS , and the experimental results show that the proposed LEARNING SCHEME is effective in improving the LEARNING ACCURACY of the ANN ESTIMATORS .\n",
            "\n",
            "831 1000\n",
            "the text analysis conference -lrb- tac -rrb- ranks SUMMARIZATION SYSTEMS by their AVERAGE SCORE over a collection of DOCUMENT SETS . we investigate the statistical appropriateness of this score and propose an alternative that better distinguishes between HUMAN AND MACHINE EVALUATION SYSTEMS . \n",
            "this paper presents a novel approach to the problem of SUMMARIZATION SYSTEMS . the proposed approach is based on the use of a set of DOCUMENT SETS , each of which is a set of DOCUMENT SETS . the proposed approach is based on the use of a set of DOCUMENT SETS , each of which is a set of DOCUMENT SETS . the performance of the proposed method is evaluated on a variety of DOCUMENT SETS . the results show that the proposed method is effective in improving the AVERAGE SCORE of the DOCUMENT SETS compared to the best known results .\n",
            "\n",
            "832 1000\n",
            "due to the physiological constraints of articulatory motion the SPEECH APPARATUS has limited degrees of freedom . as a result , the range of SPEECH SOUNDS a human is capable of producing may lie on a low dimensional <unk> of the high dimensional space of all possible sounds . in this study a number of MANIFOLD LEARNING ALGORITHMS are applied to SPEECH DATA in an effort to extract useful LOW DIMENSIONAL STRUCTURE from the HIGH DIMENSIONAL SPEECH SIGNAL . the ability of these MANIFOLD LEARNING ALGORITHMS to separate VOWELS in a LOW DIMENSIONAL SPACE is evaluated and compared to a CLASSICAL LINEAR DIMENSIONALITY REDUCTION METHOD . results indicate that MANIFOLD LEARNING ALGORITHMS outperform CLASSICAL METHODS in LOW DIMENSIONS and are capable of discovering useful MANIFOLD STRUCTURE in SPEECH DATA . \n",
            "this paper presents a novel approach to SPEECH DATA in the LOW DIMENSIONAL SPACE . the proposed MANIFOLD LEARNING ALGORITHMS is based on the use of MANIFOLD LEARNING ALGORITHMS to estimate the MANIFOLD STRUCTURE of the SPEECH DATA . the proposed MANIFOLD LEARNING ALGORITHMS is based on the use of MANIFOLD LEARNING ALGORITHMS to estimate the MANIFOLD STRUCTURE . the proposed MANIFOLD LEARNING ALGORITHMS are applied to the VOWELS in the LOW DIMENSIONAL SPACE . the proposed MANIFOLD LEARNING ALGORITHMS are compared to other CLASSICAL METHODS in the literature . the experimental results show that the proposed MANIFOLD LEARNING ALGORITHMS significantly outperforms conventional CLASSICAL METHODS in terms of both VOWELS and VOWELS .\n",
            "\n",
            "833 1000\n",
            "this paper describes a system for navigating large collections of information about CULTURAL HERITAGE which is applied to EU-ROPEANA , the EUROPEAN LIBRARY . <unk> contains over 20 million artefacts with meta-data in a wide range of EURO-PEAN LANGUAGES . the system currently provides access to EUROPEANA CONTENT with meta-data in ENGLISH and SPANISH . the paper describes how NATURAL LANGUAGE PROCESSING is used to enrich and <unk> this meta-data to assist NAVIGATION through EU-ROPEANA and shows how this information is used within the system . \n",
            "this paper addresses the problem of NATURAL LANGUAGE PROCESSING for EURO-PEAN LANGUAGES and SPANISH . we propose a novel method for NAVIGATION in the presence of CULTURAL HERITAGE and SPANISH . the proposed approach is based on the use of NATURAL LANGUAGE PROCESSING and EU-ROPEANA . the proposed approach is based on the use of NATURAL LANGUAGE PROCESSING and NAVIGATION . the proposed method is evaluated on a variety of EURO-PEAN LANGUAGES and SPANISH .\n",
            "\n",
            "834 1000\n",
            "this paper describes a method of finding THIN , ELONGATED STRUCTURES in IMAGES and volumes . we use SHORTEST PATHS to minimize very general functionals of HIGHER-ORDER CURVE PROPERTIES , such as CURVATURE and TORSION . our GLOBALLY OPTIMAL METHOD uses LINE GRAPHS and its runtime is polynomial in the size of the DISCRETIZATION , often in the order of seconds on a single computer . to our knowledge , we are the first to perform experiments in three dimensions with CURVATURE and TORSION REGULARIZATION . the largest graphs we process have almost one hundred billion arcs . experiments on MEDICAL IMAGES and in MULTI-VIEW RECONSTRUCTION show the significance and practical usefulness of REGULARIZATION based on CURVATURE while TORSION is still only tractable for SMALL-SCALE PROBLEMS . \n",
            "this paper addresses the problem of MULTI-VIEW RECONSTRUCTION in MEDICAL IMAGES . we propose a new method for MULTI-VIEW RECONSTRUCTION and MULTI-VIEW RECONSTRUCTION . our method is based on the use of SHORTEST PATHS , DISCRETIZATION , and TORSION REGULARIZATION . we show that the proposed method can be applied to SMALL-SCALE PROBLEMS and MULTI-VIEW RECONSTRUCTION . we show that the proposed method can be applied to SMALL-SCALE PROBLEMS and MULTI-VIEW RECONSTRUCTION . we also show that the proposed method can be applied to SMALL-SCALE PROBLEMS .\n",
            "\n",
            "835 1000\n",
            "<unk> <unk> we developed a NEURAL NET ARCHITECTURE for SEGMENTING COMPLEX IMAGES , i.e. , to localize TWO-DIMENSIONAL GEOMETRICAL SHAPES in a scene , without prior knowledge of the objects ' positions and sizes . a SCALE VARIATION is built into the NEURAL NET ARCHITECTURE to deal with varying sizes . this NEURAL NET ARCHITECTURE has been applied to VIDEO IMAGES OF RAILROAD CARS , to find their identification numbers . over 95 % of the <unk> were located correctly in a data base of 300 images , <unk> a large variation in LIGHTING CONDITIONS and often a poor quality of the characters . a part of the NEURAL NET ARCHITECTURE is executed on a PROCESSOR BOARD containing an analog neural net chip -lrb- <unk> et ai . 1991 -rrb- . while the rest is implemented as a SOFTWARE MODEL on a WORKSTATION or a DIGITAL SIGNAL PROCESSOR . \n",
            "in this paper , we propose a novel SOFTWARE MODEL for SEGMENTING COMPLEX IMAGES . the proposed NEURAL NET ARCHITECTURE is based on a SOFTWARE MODEL and a SOFTWARE MODEL for SEGMENTING COMPLEX IMAGES . the proposed NEURAL NET ARCHITECTURE is based on the use of a SOFTWARE MODEL and a SOFTWARE MODEL to estimate the SCALE VARIATION . the proposed NEURAL NET ARCHITECTURE is applied to SEGMENTING COMPLEX IMAGES and SEGMENTING COMPLEX IMAGES . the experimental results show that the proposed SOFTWARE MODEL is effective in SEGMENTING COMPLEX IMAGES .\n",
            "\n",
            "836 1000\n",
            "a reduced complexity realisation for the NORMALISED CONSTANT MOD-ULUS ALGORITHM and its SOFT CRITERION SATISFACTION VERSION is proposed based on SELECTIVE PARTIAL UPDATING . the COMPUTATIONAL COMPLEXITY of NORMALISED CONSTANT MOD-ULUS ALGORITHM and SOFT CRITERION SATISFACTION VERSION is reduced by updating a BLOCK OF EQUALISER PARAMETERS at every iteration rather than the entire <unk> . this results in a smaller number of MULTIPLICATIONS for updating the EQUALISER PARAMETERS . a simple BLOCK SELECTION CRITERION is derived from the solution of a CONSTRAINED MINIMISATION PROBLEM that underpins the development of NORMALISED CONSTANT MOD-ULUS ALGORITHM . in FRACTIONALLY-SPACED EQUALISATION , the proposed SELECTIVE PARTIAL UPDATING is shown to be capable of maintaining comparable CONVERGENCE SPEED to its FULL-UPDATE COUNTERPART . this implies a significant reduction in IMPLEMENTATION COST without necessarily <unk> the CONVERGENCE SPEED . \n",
            "this paper presents a novel BLOCK SELECTION CRITERION for the CONSTRAINED MINIMISATION PROBLEM . the proposed NORMALISED CONSTANT MOD-ULUS ALGORITHM is based on the idea of SELECTIVE PARTIAL UPDATING , which is a generalization of the NORMALISED CONSTANT MOD-ULUS ALGORITHM . the proposed SOFT CRITERION SATISFACTION VERSION is based on a SOFT CRITERION SATISFACTION VERSION , which is a generalization of the NORMALISED CONSTANT MOD-ULUS ALGORITHM to the CONSTRAINED MINIMISATION PROBLEM . the proposed SOFT CRITERION SATISFACTION VERSION is evaluated in terms of the IMPLEMENTATION COST and the IMPLEMENTATION COST of the NORMALISED CONSTANT MOD-ULUS ALGORITHM . the performance of the proposed NORMALISED CONSTANT MOD-ULUS ALGORITHM is evaluated in terms of the IMPLEMENTATION COST and the IMPLEMENTATION COST of the proposed NORMALISED CONSTANT MOD-ULUS ALGORITHM . the performance of the proposed NORMALISED CONSTANT MOD-ULUS ALGORITHM is evaluated in terms of the IMPLEMENTATION COST and the IMPLEMENTATION COST of the proposed NORMALISED CONSTANT MOD-ULUS ALGORITHM .\n",
            "\n",
            "837 1000\n",
            "in this paper we describe a method for MINIMUM BAYES RISK DECODING for SPEECH RECOGNITION . this is a technique similar to CONSENSUS A.K.A. CONFUSION NETWORK DECODING , in which we attempt to find the hypothesis that minimizes the bayes ' risk with respect to the WORD ERROR RATE , based on a LATTICE OF ALTERNATIVE OUTPUTS . our method is an E-M like technique which makes approximations which we believe are less severe than the approximations made in CONSENSUS , and our experimental results show an improvement in E-M both for LATTICE RESCORING and LATTICE-BASED SYSTEM COMBINATION , versus baselines such as CONSENSUS , CONFUSION NETWORK COMBINATION and ROVER . \n",
            "this paper addresses the problem of CONSENSUS A.K.A. CONFUSION NETWORK DECODING for SPEECH RECOGNITION and MINIMUM BAYES RISK DECODING . in this paper , we propose a novel method for CONSENSUS A.K.A. CONFUSION NETWORK DECODING based on MINIMUM BAYES RISK DECODING and MINIMUM BAYES RISK DECODING . the proposed approach is based on the use of CONSENSUS and MINIMUM BAYES RISK DECODING . the proposed method is based on the use of E-M and MINIMUM BAYES RISK DECODING . the proposed method is evaluated in terms of WORD ERROR RATE and CONFUSION NETWORK COMBINATION . the performance of the proposed method is evaluated on a number of tasks including SPEECH RECOGNITION , MINIMUM BAYES RISK DECODING , and LATTICE-BASED SYSTEM COMBINATION .\n",
            "\n",
            "838 1000\n",
            "it is known that SPEECH under PHYSICAL TASK STRESS degrades SPEECH SYSTEM performance . therefore , an analysis of SPEECH under PHYSICAL TASK STRESS is performed across several parameters to identify ACOUSTIC CORRELATES . formal LISTENER TESTS are also performed to determine the relationship between ACOUSTIC CORRELATES and perception . to verify the statistical significance of all results , <unk> statistical tests are applied . it was found that FUNDAMENTAL FREQUENCY decreases for many speakers , that UTTERANCE DURATION increases for some speakers and decreases for others , and that the GLOTTAL WAVEFORM is <unk> different for many speakers . perturbation of two SPEECH FEATURES , FUNDAMENTAL FREQUENCY and the GLOTTAL WAVEFORM , is applied in LISTENER TESTS to quantify the degree to which these FEATURES convey PHYSICAL STRESS CONTENT in SPEECH . finally , the enhanced understanding of PHYSICAL TASK STRESS SPEECH provided here is discussed in the context of SPEECH SYSTEM . \n",
            "in this paper , we propose a novel approach to PHYSICAL TASK STRESS SPEECH based on PHYSICAL TASK STRESS SPEECH . the proposed approach is based on the use of a GLOTTAL WAVEFORM and a GLOTTAL WAVEFORM to estimate the PHYSICAL STRESS CONTENT and the GLOTTAL WAVEFORM . the proposed method is based on the use of a GLOTTAL WAVEFORM and a GLOTTAL WAVEFORM to estimate the PHYSICAL STRESS CONTENT and the GLOTTAL WAVEFORM . the proposed method is evaluated on a variety of PHYSICAL TASK STRESS SPEECH . the experimental results show that the proposed method outperforms the conventional SPEECH SYSTEM .\n",
            "\n",
            "839 1000\n",
            "today 's ANTI-VIRUS TECHNOLOGY , based largely on analysis of existing <unk> by human experts , is just barely able to keep pace with the more than three new COMPUTER VIRUSES that are written daily . in a few years , INTELLIGENT AGENTS navigating through highly connected networks are likely to form an extremely <unk> medium for a new <unk> of <unk> . at ibm , we are developing novel , BIOLOGICALLY INSPIRED ANTI-VIRUS TECHNIQUES designed to <unk> both today 's and <unk> 's <unk> . here we describe two of these : a NEURAL NETWORK VIRUS DETECTOR that learns to discriminate between INFECTED AND UN-INFECTED PROGRAMS , and a COMPUTER IMMUNE SYSTEM that identifies new <unk> , analyzes them automatically , and uses the results of its analysis to detect and remove all copies of the <unk> that are present in the COMPUTER IMMUNE SYSTEM . the BIOLOGICALLY INSPIRED ANTI-VIRUS TECHNIQUES has been incorporated into ibm 's commercial <unk> product ; the COMPUTER IMMUNE SYSTEM is in prototype . \n",
            "this paper presents a novel approach to INTELLIGENT AGENTS for INFECTED AND UN-INFECTED PROGRAMS . the proposed approach is based on the use of a NEURAL NETWORK VIRUS DETECTOR for INFECTED AND UN-INFECTED PROGRAMS . the proposed approach is based on the use of a NEURAL NETWORK VIRUS DETECTOR for INFECTED AND UN-INFECTED PROGRAMS . the proposed approach is based on the use of a NEURAL NETWORK VIRUS DETECTOR , which is based on a NEURAL NETWORK VIRUS DETECTOR . the experimental results show that the proposed NEURAL NETWORK VIRUS DETECTOR is effective in improving the performance of the COMPUTER IMMUNE SYSTEM .\n",
            "\n",
            "840 1000\n",
            "the COMPUTATION OF OPTICAL OW relies on merging information available over an IMAGE PATCH to form an estimate of 2d image velocity a t a p o i n t . this MERGING PROCESS raises a host of issues , which include the treatment of OUTLIERS in COMPONENT V ELOCITY MEASUREMENTS and the MODELING OF MULTIPLE MOTIONS within a patch which arise from OCCLUSION BOUNDARIES or TRANSPARENCY . w e present a new approach which allows us to deal with these issues within a common framework . our approach is based on the use of a probabilistic mixture m o del to explicitly represent m <unk> motions within a patch . we use a simple extension of the EM-ALGORITHM to compute a MAXIMUM LIKELIHOOD ESTIMATE for the various MOTION PARAMETERS . preliminary experiments indicate that this approach is computationally eecient and can provide robust estimates of the OPTICAL OW V ALUES in the presence of OUTLIERS and multiple motions . the basic approach can also be applied to other problems in COMPUTATIONAL VISION , such as the COMPUTATION OF 3D RELATIVE MOTION , which require the integration of several partial constraints to obtain a desired quantity . \n",
            "this paper addresses the problem of MODELING OF MULTIPLE MOTIONS from a single IMAGE PATCH . we propose a novel method to recover the COMPUTATION OF 3D RELATIVE MOTION from IMAGE PATCH . the proposed method is based on a MAXIMUM LIKELIHOOD ESTIMATE that exploits the COMPUTATION OF 3D RELATIVE MOTION in a MERGING PROCESS . the proposed method is based on a MAXIMUM LIKELIHOOD ESTIMATE that exploits the COMPUTATION OF 3D RELATIVE MOTION in a MERGING PROCESS . the proposed method is based on a MAXIMUM LIKELIHOOD ESTIMATE , which is able to deal with OUTLIERS and OUTLIERS . the experimental results show that the proposed method outperforms the existing methods in terms of both TRANSPARENCY and TRANSPARENCY .\n",
            "\n",
            "841 1000\n",
            "most research in COMPUTER CHESS has focussed on creating an excellent CHESS PLAYER , with relatively little concern given to modelling how humans play CHESS PLAYER . the research reported in this paper is aimed at investigating KNOWLEDGE-BASED CHESS in the context of building a PROTOTYPE CHESS TUTOR , UMRAO , which helps students learn how to play BISHOP-PAWN ENDGAMES . in tutoring it is essential to take a KNOWLEDGE-BASED APPROACH , since students must learn how to manipulate STRATEGIC CONCEPTS , not how to carry out MINIMAX SEARCH . UMRAO uses an extension of <unk> 's advice language to represent expert and novice CHESS PLAYER plans . for any given <unk> the UMRAO is able to compile the plans into a STRATEGY GRAPH , which <unk> strategies -lrb- both well-formed and ill-formed -rrb- that students might use as STRATEGY GRAPHS solve the ENDGAME PROBLEM . STRATEGY GRAPHS can be compiled `` off-line '' so that STRATEGY GRAPHS can be used in REAL TIME TUTORING . we show that the normally rigid `` model tracing '' tutoring paradigm can be used in a flexible way in this domain . \n",
            "this paper addresses the problem of KNOWLEDGE-BASED CHESS in a PROTOTYPE CHESS TUTOR . we propose a novel KNOWLEDGE-BASED APPROACH , called UMRAO , for KNOWLEDGE-BASED CHESS . the proposed KNOWLEDGE-BASED APPROACH is based on the use of a KNOWLEDGE-BASED APPROACH , a KNOWLEDGE-BASED APPROACH , and a KNOWLEDGE-BASED APPROACH for KNOWLEDGE-BASED CHESS . the proposed KNOWLEDGE-BASED APPROACH is based on a KNOWLEDGE-BASED APPROACH , which is based on a KNOWLEDGE-BASED APPROACH . the proposed approach is evaluated on a variety of STRATEGIC CONCEPTS . the results show that the proposed approach is effective in REAL TIME TUTORING .\n",
            "\n",
            "842 1000\n",
            "ideally , kernels used to generate BILINEAR TIME-FREQUENCY DISTRIBUTIONS should be <unk> , and optimised independently at every location in the TIME-FREQUENCY PLANE . this poses an extremely severe COMPUTATIONAL BURDEN . a compromise is proposed in this paper : TIME-VARYING KERNELS are optimised for specific TIME-VARYING KERNELS in the TIME-FREQUENCY PLANE . the TIME-VARYING KERNELS , designed to isolate separate components comprising the signal , are determined by modelling the BILINEAR TIME-FREQUENCY DISTRIBUTIONS using a FINITE MIXTURE MODEL of GAUSSIAN DISTRIBUTIONS . the parameters of the model are estimated using a combination of the EXPECTATION-MAXIMISATION ALGORITHM and FUNCTIONAL MERGING . the REGIONAL OPTIMISATION provides improved separation and resolution of CLOSELY-SPACED COMPONENTS when compared to methods using a solely time-varying kernel , without incurring an overwhelming COMPUTATIONAL EXPENSE . \n",
            "in this paper , we propose a novel method to estimate the COMPUTATIONAL EXPENSE of a FINITE MIXTURE MODEL . the method is based on the use of TIME-VARYING KERNELS and FUNCTIONAL MERGING to estimate the COMPUTATIONAL EXPENSE of the CLOSELY-SPACED COMPONENTS . the proposed method is based on the use of TIME-VARYING KERNELS and FUNCTIONAL MERGING to estimate the COMPUTATIONAL EXPENSE of the CLOSELY-SPACED COMPONENTS . the proposed method is based on the use of TIME-VARYING KERNELS and FUNCTIONAL MERGING . in the proposed method , a FINITE MIXTURE MODEL is used to estimate the COMPUTATIONAL EXPENSE of the CLOSELY-SPACED COMPONENTS . the performance of the proposed EXPECTATION-MAXIMISATION ALGORITHM is evaluated in terms of the COMPUTATIONAL EXPENSE and the COMPUTATIONAL EXPENSE of the EXPECTATION-MAXIMISATION ALGORITHM .\n",
            "\n",
            "843 1000\n",
            "the STRUCTURE-FROM-MOTION PROBLEM has been extensively studied in the field of COMPUTER VISION . yet , the bulk of the existing work assumes that the scene contains only a single moving object . the more realistic case where an unknown number of objects move in the scene has received little attention , especially for its theoretical treatment . in this paper we present a new method for separating and recovering the motion and SHAPE of multiple independently moving objects in a sequence of images . the method does not require prior knowledge of the number of objects , nor is dependent on any grouping of FEATURES into an object at the IMAGE LEVEL . for this purpose , we introduce a MATHEMATICAL CONSTRUCT OF OBJECT SHAPES , called the SHAPE INTERACTION MATRIX , which is invariant to both the OBJECT MOTIONS and the SELECTION OF COORDINATE SYSTEMS . this SHAPE INTERACTION MATRIX is computable solely from the observed trajectories of IMAGE FEATURES without grouping them into individual objects . once the SHAPE INTERACTION MATRIX is computed , SHAPE INTERACTION MATRIX allows for SEGMENTING FEATURES into objects by the process of transforming SHAPE INTERACTION MATRIX into a CANONICAL FORM , as well as recovering the SHAPE and motion of each object . \n",
            "in this paper , we propose a novel approach to SELECTION OF COORDINATE SYSTEMS in COMPUTER VISION . the proposed SHAPE INTERACTION MATRIX is based on a MATHEMATICAL CONSTRUCT OF OBJECT SHAPES , a SHAPE INTERACTION MATRIX , a SHAPE INTERACTION MATRIX , and a set of FEATURES . a CANONICAL FORM is used to estimate the SHAPE of the object in the image . the proposed approach is based on the use of a SHAPE INTERACTION MATRIX , a SHAPE INTERACTION MATRIX , a SHAPE INTERACTION MATRIX , and a SHAPE INTERACTION MATRIX for SEGMENTING FEATURES . the experimental results show that the proposed method outperforms the existing methods in terms of the quality of the FEATURES .\n",
            "\n",
            "844 1000\n",
            "in this paper , we propose a COMPACT YET DISCRIMINATIVE LOCAL DESCRIP-TOR which tackles the WIRELESS QUERY TRANSMISSION LATENCY in MOBILE VISUAL SEARCH . the COMPACT YET DISCRIMINATIVE LOCAL DESCRIP-TOR captures gradient statistics of <unk> patches over a LOG-POLAR LOCATION GRID whose parameters are optimized using training samples . we quantize the resulting COMPACT YET DISCRIMINATIVE LOCAL DESCRIP-TOR using PRODUCT QUANTIZATION . the COMPACT YET DISCRIMINATIVE LOCAL DESCRIP-TOR achieves about 95 % bits reduction compared with 128-BYTE SIFT and allows adaptation of DESCRIPTOR LENGTHS to support user required performance . moreover , accurate matching of DESCRIPTORS with LOW COMPLEXITY is allowed within several TABLE LOOKUP OPERATIONS . we perform a comprehensive comparison with SIFT , CHOG and CHOG in the context of IMAGE RETRIEVAL , IMAGE MATCHING and OBJECT LOCALIZATION . we achieve competing MATCHING AND RETRIEVAL performance with SIFT , CHOG with much fewer bits . in particular , the COMPACT YET DISCRIMINATIVE LOCAL DESCRIP-TOR outperforms CHOG at the same bits on eight data sets contributed to MPEG COMPACT DE-SCRIPTOR for VISUAL SEARCH STANDARDIZATION . \n",
            "this paper addresses the problem of OBJECT LOCALIZATION and OBJECT LOCALIZATION in IMAGE RETRIEVAL . we propose a novel COMPACT YET DISCRIMINATIVE LOCAL DESCRIP-TOR based on PRODUCT QUANTIZATION , PRODUCT QUANTIZATION and OBJECT LOCALIZATION . the proposed COMPACT YET DISCRIMINATIVE LOCAL DESCRIP-TOR is based on PRODUCT QUANTIZATION , CHOG , PRODUCT QUANTIZATION , and OBJECT LOCALIZATION . the proposed COMPACT YET DISCRIMINATIVE LOCAL DESCRIP-TOR is applied to the problem of OBJECT LOCALIZATION and OBJECT LOCALIZATION . the experimental results show that the proposed COMPACT YET DISCRIMINATIVE LOCAL DESCRIP-TOR is effective in improving the MATCHING AND RETRIEVAL performance in IMAGE RETRIEVAL and OBJECT LOCALIZATION .\n",
            "\n",
            "845 1000\n",
            "we propose a simple and effective VARIATIONAL INFERENCE ALGORITHM based on STOCHASTIC OPTIMI-SATION that can be widely applied for BAYESIAN NON-CONJUGATE INFERENCE in CONTINUOUS PARAMETER SPACES . this VARIATIONAL INFERENCE ALGORITHM is based on STOCHASTIC APPROXIMATION and allows for efficient use of GRADIENT INFORMATION from the MODEL JOINT DENSITY . we demonstrate these properties using ILLUSTRATIVE EXAMPLES as well as in challenging and diverse BAYESIAN INFERENCE PROBLEMS such as VARIABLE SELECTION in LOGISTIC REGRESSION and fully bayesian inference over KERNEL HYPERPARAMETERS in GAUSSIAN PROCESS REGRESSION . \n",
            "in this paper , we propose a novel VARIATIONAL INFERENCE ALGORITHM for GAUSSIAN PROCESS REGRESSION in CONTINUOUS PARAMETER SPACES . the proposed VARIATIONAL INFERENCE ALGORITHM is based on the idea of STOCHASTIC OPTIMI-SATION , which is a generalization of the standard VARIATIONAL INFERENCE ALGORITHM . the proposed VARIATIONAL INFERENCE ALGORITHM is based on the use of STOCHASTIC OPTIMI-SATION , which is a generalization of the VARIATIONAL INFERENCE ALGORITHM . the proposed VARIATIONAL INFERENCE ALGORITHM is based on a STOCHASTIC APPROXIMATION , which is a generalization of the standard VARIATIONAL INFERENCE ALGORITHM . the proposed VARIATIONAL INFERENCE ALGORITHM is based on the use of KERNEL HYPERPARAMETERS , which is a generalization of the VARIATIONAL INFERENCE ALGORITHM . the proposed VARIATIONAL INFERENCE ALGORITHM is applied to the problem of BAYESIAN NON-CONJUGATE INFERENCE in CONTINUOUS PARAMETER SPACES . the performance of the proposed VARIATIONAL INFERENCE ALGORITHM is demonstrated on several ILLUSTRATIVE EXAMPLES .\n",
            "\n",
            "846 1000\n",
            "we present a new SYNTACTIC PARSER that works left-to-right and top down , thus maintaining a FULLY-CONNECTED PARSE TREE for a few alternative PARSE HYPOTHESES . all of the commonly used STATISTICAL PARSERS use CONTEXT-FREE DYNAMIC PROGRAMMING ALGORITHMS and as such work bottom up on the entire sentence . thus they only find a complete fully connected parse at the very end . in contrast , both subjective and experimental evidence show that people understand a sentence <unk> as they go along , or close to it . the CONSTRAINT that the SYNTACTIC PARSER keeps one or more fully connected syntactic trees is intended to operationalize this COGNITIVE FACT . our SYNTACTIC PARSER achieves a new best result for TOP-DOWN PARSERS of <unk> % , a 20 % ERROR REDUCTION over the previous <unk> best result for PARSERS of this type of <unk> % -lrb- <unk> , 2001 -rrb- . the improved performance is due to embracing the very large feature set available in exchange for giving up DYNAMIC PROGRAMMING . \n",
            "this paper addresses the problem of COGNITIVE FACT in the context of a FULLY-CONNECTED PARSE TREE . in particular , we propose a novel SYNTACTIC PARSER to the problem of DYNAMIC PROGRAMMING . the proposed SYNTACTIC PARSER is based on the use of a FULLY-CONNECTED PARSE TREE , a FULLY-CONNECTED PARSE TREE , and a SYNTACTIC PARSER . the proposed SYNTACTIC PARSER is based on a FULLY-CONNECTED PARSE TREE , which is a FULLY-CONNECTED PARSE TREE . the proposed SYNTACTIC PARSER is evaluated on ERROR REDUCTION and ERROR REDUCTION . the experimental results show that the proposed SYNTACTIC PARSER is effective in ERROR REDUCTION and ERROR REDUCTION .\n",
            "\n",
            "847 1000\n",
            "a central challenge in SEMANTIC PARSING is handling the myriad ways in which KNOWLEDGE BASE PREDICATES can be expressed . traditionally , SEMANTIC PARSERS are trained primarily from text paired with KNOWLEDGE BASE INFORMATION . our goal is to exploit the much larger amounts of RAW TEXT not tied to any KNOWLEDGE BASE . in this paper , we turn SEMANTIC PARSING on its head . given an input utterance , we first use a simple method to deterministically generate a set of CANDIDATE LOGICAL FORMS with a CANONICAL REALIZATION in NATURAL LANGUAGE for each . then , we use a PARAPHRASE MODEL to choose the realization that best paraphrases the input , and output the corresponding LOGICAL FORM . we present two simple PARAPHRASE MODEL , an ASSOCIATION MODEL and a VECTOR SPACE MODEL , and train PARAPHRASE MODEL jointly from QUESTION-ANSWER PAIRS . our system <unk> improves state-of-the-art ACCURACIES on two recently released QUESTION-ANSWERING DATASETS . \n",
            "this paper addresses the problem of SEMANTIC PARSING from a single KNOWLEDGE BASE . we propose a novel method to estimate the CANONICAL REALIZATION of a scene from a single image . the proposed approach is based on a VECTOR SPACE MODEL called the VECTOR SPACE MODEL , which is based on a VECTOR SPACE MODEL of the RAW TEXT . the proposed approach is based on a VECTOR SPACE MODEL , called the VECTOR SPACE MODEL , which is based on a VECTOR SPACE MODEL . the proposed approach is based on a VECTOR SPACE MODEL , a VECTOR SPACE MODEL , a VECTOR SPACE MODEL , and a VECTOR SPACE MODEL . we show that the proposed method can be applied to QUESTION-ANSWERING DATASETS , and can be applied to other SEMANTIC PARSERS .\n",
            "\n",
            "848 1000\n",
            "lexical resources such as WORDNET and VERBNET are widely used in a multitude of NLP TASKS , as are ANNOTATED CORPORA such as TREEBANKS . often , the resources are used <unk> , without question or examination . this practice risks missing significant performance gains and even entire techniques . this paper addresses the importance of RESOURCE QUALITY through the lens of a challenging NLP TASK : DETECTING SELEC-TIONAL PREFERENCE VIOLATIONS . we present DAVID , a simple , LEXICAL RESOURCE-BASED PREFERENCE VIOLATION DETECTOR . with AS-IS LEXICAL RESOURCES , DAVID achieves an F 1-MEASURE of just <unk> % . when the resource entries and PARSER OUTPUTS for a small sample are corrected , however , the F 1-MEASURE on that sample jumps from 40 % to <unk> % , and performance on other examples rises , suggesting that the algorithm becomes practical given refined resources . more broadly , this paper shows that RESOURCE QUALITY matters tremendously , sometimes even more than ALGORITHMIC IMPROVEMENTS . \n",
            "this paper addresses the problem of DETECTING SELEC-TIONAL PREFERENCE VIOLATIONS from ANNOTATED CORPORA such as VERBNET , VERBNET , and VERBNET . we propose a novel approach to the problem of DETECTING SELEC-TIONAL PREFERENCE VIOLATIONS , which is based on a LEXICAL RESOURCE-BASED PREFERENCE VIOLATION DETECTOR . the proposed approach is based on a novel LEXICAL RESOURCE-BASED PREFERENCE VIOLATION DETECTOR , called LEXICAL RESOURCE-BASED PREFERENCE VIOLATION DETECTOR , which can be applied to other NLP TASKS such as VERBNET and VERBNET . the proposed approach is evaluated on a variety of NLP TASKS including VERBNET , VERBNET , and VERBNET .\n",
            "\n",
            "849 1000\n",
            "detection of vowel onset points -lrb- DETECTION OF VOWEL ONSET POINTS -rrb- is important for SPOTTING SUBWORD UNITS in CONTINUOUS SPEECH . for CONSONANT-VOWEL UTTERANCES , VOP is the instant at which the consonant part ends and the vowel part begins . accurate detection of DETECTION OF VOWEL ONSET POINTS is important for RECOGNITION OF CV UNITS in CONTINUOUS SPEECH . in this paper , we propose an approach for detection of DETECTION OF VOWEL ONSET POINTS using AUTOASSO-CIATIVE NEURAL NETWORK MODELS . a pair of AANN MODELS are trained for each CV CLASS to capture the characteristics of SPEECH SIGNAL in the consonant and vowel regions of that class . the trained AANN MODELS are then used to detect DETECTION OF VOWEL ONSET POINTS in CONTINUOUS SPEECH . the results of studies show that the proposed approach leads to significantly less number of spurious hypotheses . \n",
            "this paper presents a novel approach to DETECTION OF VOWEL ONSET POINTS based on AUTOASSO-CIATIVE NEURAL NETWORK MODELS . the proposed approach is based on the use of AANN MODELS to represent the SPEECH SIGNAL of the SPEECH SIGNAL . the proposed method is based on the use of AANN MODELS to estimate the DETECTION OF VOWEL ONSET POINTS . the proposed method is based on the use of AANN MODELS to estimate the DETECTION OF VOWEL ONSET POINTS . the proposed method is evaluated on a variety of CONSONANT-VOWEL UTTERANCES . the results show that the proposed method is effective in improving the DETECTION OF VOWEL ONSET POINTS performance .\n",
            "\n",
            "850 1000\n",
            "logic programs with ABSTRACT CONSTRAINT ATOMS proposed by <unk> and <unk> are very general DESCRIPTION LOGIC PROGRAMS . they are general enough to capture AGGREGATE LOGIC PROGRAMS as well as recently proposed DESCRIPTION LOGIC PROGRAMS . in this paper , we propose a WELL-FOUNDED SEMANTICS for basic DESCRIPTION LOGIC PROGRAMS with ARBITRARY ABSTRACT CONSTRAINT ATOMS , which are sets of RULES whose heads have exactly one atom . we show that similar to the WELL-FOUNDED SEMANTICS of normal DESCRIPTION LOGIC PROGRAMS , it has many desirable properties such as that it can be computed in POLYNOMIAL TIME , and is always correct with respect to the ANSWER SET SEMANTICS . this paves the way for using our WELL-FOUNDED SEMANTICS to simplify these DESCRIPTION LOGIC PROGRAMS . we also show how our semantics can be applied to AGGREGATE LOGIC PROGRAMS and DESCRIPTION LOGIC PROGRAMS , and compare it to the WELL-FOUNDED SEMANTICS already proposed for these DESCRIPTION LOGIC PROGRAMS . \n",
            "this paper addresses the problem of DESCRIPTION LOGIC PROGRAMS for DESCRIPTION LOGIC PROGRAMS . we propose a novel approach to the problem of DESCRIPTION LOGIC PROGRAMS , which is based on the use of ARBITRARY ABSTRACT CONSTRAINT ATOMS . the proposed approach is based on the use of ABSTRACT CONSTRAINT ATOMS to represent the WELL-FOUNDED SEMANTICS of the LOGIC PROGRAMS . the proposed approach is based on the use of ARBITRARY ABSTRACT CONSTRAINT ATOMS to represent the WELL-FOUNDED SEMANTICS of the LOGIC PROGRAMS . the proposed approach is evaluated in the context of DESCRIPTION LOGIC PROGRAMS . the experimental results show that the proposed method is effective in reducing the number of RULES in the presence of POLYNOMIAL TIME .\n",
            "\n",
            "851 1000\n",
            "we propose an ENTIRELY DATA-DRIVEN APPROACH to estimating the 3D POSE of a hand given a DEPTH IMAGE . we show that we can correct the mistakes made by a CONVOLUTIONAL NEURAL NETWORK trained to predict an estimate of the 3D POSE by using a FEEDBACK LOOP . the components of this FEEDBACK LOOP are also DEEP NETWORKS , optimized using TRAINING DATA . they remove the need for fitting a 3D MODEL to the INPUT DATA , which requires both a carefully designed fitting function and algorithm . we show that our ENTIRELY DATA-DRIVEN APPROACH outperforms state-of-the-art methods , and is efficient as our implementation runs at over 400 fps on a single GPU . \n",
            "this paper addresses the problem of 3D POSE in the presence of 3D POSE . we propose a method to estimate the 3D POSE of a scene from a DEPTH IMAGE . the proposed method is based on a ENTIRELY DATA-DRIVEN APPROACH , which is based on a ENTIRELY DATA-DRIVEN APPROACH . the proposed method is based on the use of DEEP NETWORKS to estimate the 3D POSE and the 3D POSE . the proposed ENTIRELY DATA-DRIVEN APPROACH is applied to the DEPTH IMAGE of the source and target speaker . the experimental results show that the proposed method outperforms the state-of-the-art methods .\n",
            "\n",
            "852 1000\n",
            "transformations between different COLOR SPACES and <unk> are ubiquitous operations performed on images . often , these transformations involve INFORMATION LOSS , for example when NATURAL '' MAPPING from COLOR to GRAYSCALE for PRINTING , from MULTISPECTRAL OR MULTIPRIMARY DATA to TRISTIMULUS SPACES , or from one COLOR <unk> to another . in all these applications , there exists a straightforward '' NATURAL '' MAPPING from the SOURCE SPACE to the target space , but the NATURAL '' MAPPING is not BIJECTIVE , resulting in INFORMATION LOSS due to METAMERISM and similar effects . we propose a CLUSTER-BASED APPROACH for optimizing the transformation for individual images in a way that preserves as much of the information as possible from the SOURCE SPACE while staying as faithful as possible to the NATURAL MAPPING . our CLUSTER-BASED APPROACH can be applied to a host of COLOR TRANSFORMATION PROBLEMS including COLOR to gray , GAMUT MAPPING , CONVERSION OF MULTISPECTRAL AND MULTIPRI-MARY DATA to TRISTIMULUS COLORS , and IMAGE OPTIMIZATION for COLOR DEFICIENT VIEWERS . \n",
            "in this paper , we propose a novel method for CONVERSION OF MULTISPECTRAL AND MULTIPRI-MARY DATA from a single image . the key idea is to use a CLUSTER-BASED APPROACH for IMAGE OPTIMIZATION and IMAGE OPTIMIZATION . the proposed CLUSTER-BASED APPROACH is based on a CLUSTER-BASED APPROACH , which is a NATURAL '' MAPPING . the proposed CLUSTER-BASED APPROACH is based on a CLUSTER-BASED APPROACH , which is based on a CLUSTER-BASED APPROACH . the proposed CLUSTER-BASED APPROACH is based on a CLUSTER-BASED APPROACH , which is based on a CLUSTER-BASED APPROACH . the proposed approach is evaluated on a variety of COLOR TRANSFORMATION PROBLEMS including COLOR , COLOR , COLOR , and IMAGE OPTIMIZATION . the experimental results show that the proposed method outperforms the state of the art in terms of INFORMATION LOSS , TRISTIMULUS COLORS , and IMAGE OPTIMIZATION .\n",
            "\n",
            "853 1000\n",
            "in this paper , we evaluate our proposed SINGING VOICE CONVERSION METHOD from various perspectives . to enable singers to freely control their VOICE TIMBRE OF SINGING VOICE , we have proposed a SINGING VOICE CONVERSION METHOD based on MANY-TO-MANY EIGENVOICE CONVERSION that enables to convert the VOICE TIMBRE of an arbitrary source singer into that of another arbitrary target singer using a PROBABILISTIC MODEL . furthermore , to easily develop TRAINING DATA consisting of multiple PARALLEL DATA SETS between a single reference singer and many other singers , a technique for efficiently and effectively generating the PARALLEL DATA SETS from NONPARALLEL SINGING VOICE DATA sets of many singers using a SINGING-TO-SINGING SYNTHESIS SYSTEM have been proposed . however , we have never conducted sufficient investigations into the effectiveness of these proposed SINGING VOICE CONVERSION METHOD . in this paper , we conduct both objective and subjective evaluations to carefully investigate the effectiveness of proposed SINGING VOICE CONVERSION METHOD . moreover , the differences between SINGING VOICE CONVERSION and SPEAKING VOICE CONVERSION are also analyzed . experimental results show that our proposed SINGING VOICE CONVERSION METHOD succeeds in enabling people to control their own VOICE TIMBRE by using only an extremely small amount of the target singing voice . \n",
            "this paper presents a novel SINGING VOICE CONVERSION METHOD for SPEAKING VOICE CONVERSION and SPEAKING VOICE CONVERSION . the proposed SINGING VOICE CONVERSION METHOD is based on a PROBABILISTIC MODEL . the proposed SINGING VOICE CONVERSION METHOD is based on a PROBABILISTIC MODEL , which is based on a PROBABILISTIC MODEL . the proposed SINGING VOICE CONVERSION METHOD is based on a PROBABILISTIC MODEL . the proposed SINGING VOICE CONVERSION METHOD is applied to NONPARALLEL SINGING VOICE DATA , and the experimental results show that the proposed SINGING VOICE CONVERSION METHOD is robust to VOICE TIMBRE and VOICE TIMBRE .\n",
            "\n",
            "854 1000\n",
            "we focus on the feasibility of the SOURCE SEPARATION in the FREQUENCY DOMAIN . first , it is linked with the CONVERGENCE SPEED towards GAUSSIANITY OF SIGNALS after L-POINT DISCRETE FOURIER TRANSFORM . we test here a distance to GAUSSIANITY thanks to the SPECTRAL KURTOSIS . we analyse the influence of L , of the duration of the source <unk> and of a NON LINEAR FILTERING . we mainly develop the case of QARMA PROCESSES . the second point consists in the RECONSTRUCTION of the SPECTRA of the estimated sources from the signals identified at each FREQUENCY BIN . indeed , the source associated to the ITH IDENTIFIED SIGNAL is not necessarily the same from one FREQUENCY BIN to another . the ALGORITHM EFFICIENCY is then illustrated on QARMA PROCESSES , including the procedures of separation and RECONSTRUCTION . \n",
            "this paper addresses the problem of SOURCE SEPARATION in SOURCE SEPARATION . in particular , we consider the problem of SOURCE SEPARATION in the FREQUENCY DOMAIN . we propose a method to estimate the SPECTRA of the SPECTRA in the FREQUENCY DOMAIN . the proposed method is based on the L-POINT DISCRETE FOURIER TRANSFORM , which is based on the L-POINT DISCRETE FOURIER TRANSFORM . the proposed method is based on the L-POINT DISCRETE FOURIER TRANSFORM , which is based on the L-POINT DISCRETE FOURIER TRANSFORM . the proposed method is based on the L-POINT DISCRETE FOURIER TRANSFORM . the proposed method is evaluated in the context of SOURCE SEPARATION in a FREQUENCY DOMAIN . the results show that the proposed method achieves better RECONSTRUCTION performance than the conventional methods .\n",
            "\n",
            "855 1000\n",
            "this paper presents an approach to 3D ROTATION ESTIMATION using discrete spherical harmonic oscillator transforms -lrb- DISCRETE SHOTS -rrb- . DISCRETE SHOTS not only have simple and fast implementation methods but also are compatible with the existing ANGLE ESTIMATION ALGORITHMS related to SPHERICAL HARMONICS . DISCRETE SHOTS of the ROTATED SIGNAL follow the same formulation to the WIGNER-D MATRIX as SPHERICAL HARMONICS transforms . thus , the SPHERICAL HARMONICS RELATED ALGORITHMS could be utilized to DISCRETE SHOTS without modification . furthermore , compared to some existing methods , our approach with DISCRETE SHOTS exhibits higher ACCURACY , higher PRECISION and improved ROBUSTNESS to NOISE if the input signal is sampled uniformly on CARTESIAN GRIDS . the phenomenon results from no <unk> in DISCRETE SHOTS . \n",
            "in this paper , we propose a novel approach to 3D ROTATION ESTIMATION . the proposed method is based on the use of SPHERICAL HARMONICS RELATED ALGORITHMS for 3D ROTATION ESTIMATION . the proposed method is based on the use of SPHERICAL HARMONICS RELATED ALGORITHMS for 3D ROTATION ESTIMATION . the proposed method is based on the use of SPHERICAL HARMONICS RELATED ALGORITHMS to estimate the ROTATED SIGNAL of the ROTATED SIGNAL . the proposed method is evaluated in terms of ROBUSTNESS and ROBUSTNESS . the experimental results show that the proposed method is effective in improving the ACCURACY of DISCRETE SHOTS in terms of ACCURACY and ROBUSTNESS .\n",
            "\n",
            "856 1000\n",
            "we consider POISSON REGRESSION with the CANONICAL LINK FUNCTION . this POISSON REGRESSION is widely used in REGRESSION ANALYSIS involving COUNT DATA ; one important application in ELECTRICAL ENGINEERING is TRANSMISSION TOMOGRAPHY . in this paper , we establish the VARIABLE SELECTION CONSISTENCY and ESTIMATION CONSISTENCY of the 1-REGULARIZED MAXIMUM-LIKELIHOOD ESTI-MATOR in this POISSON REGRESSION , and characterize the ASYMP-TOTIC SAMPLE COMPLEXITY that ensures consistency even under the COMPRESSIVE SENSING SETTING -lrb- or the N P SETTING in HIGH-DIMENSIONAL STATISTICS -rrb- . \n",
            "this paper addresses the problem of TRANSMISSION TOMOGRAPHY in TRANSMISSION TOMOGRAPHY . a CANONICAL LINK FUNCTION is proposed in this paper , based on a CANONICAL LINK FUNCTION of the CANONICAL LINK FUNCTION . the proposed algorithm is based on the use of a CANONICAL LINK FUNCTION to estimate the ASYMP-TOTIC SAMPLE COMPLEXITY of the CANONICAL LINK FUNCTION . the proposed 1-REGULARIZED MAXIMUM-LIKELIHOOD ESTI-MATOR is based on a CANONICAL LINK FUNCTION , which is based on a CANONICAL LINK FUNCTION . the proposed 1-REGULARIZED MAXIMUM-LIKELIHOOD ESTI-MATOR is applied to the problem of TRANSMISSION TOMOGRAPHY for TRANSMISSION TOMOGRAPHY . the proposed algorithm is evaluated on a N P SETTING , and the results show that the proposed 1-REGULARIZED MAXIMUM-LIKELIHOOD ESTI-MATOR is effective in reducing the ASYMP-TOTIC SAMPLE COMPLEXITY .\n",
            "\n",
            "857 1000\n",
            "this paper presents a novel , promising approach that allows GREEDY DECISION TREE INDUCTION ALGORITHMS to handle PROBLEMATIC FUNCTIONS such as PARITY FUNCTIONS . lookahead is the standard approach to addressing difficult functions for GREEDY DECISION TREE LEARNERS . nevertheless , this approach is limited to very small PROBLEMATIC FUNCTIONS or <unk> -lrb- 2 or 3 variables -rrb- , because the TIME COMPLEXITY grows more than exponentially with the depth of LOOKAHEAD . in contrast , the approach presented in this paper carries only a CONSTANT RUN-TIME PENALTY . experiments indicate that the approach is effective with only modest amounts of data for PROBLEMATIC FUNCTIONS or <unk> of up to six or seven variables , where the examples themselves may contain numerous other -lrb- irrelevant -rrb- variables as well . \n",
            "in this paper , we propose a new algorithm for GREEDY DECISION TREE LEARNERS . the algorithm is based on the use of PARITY FUNCTIONS , such as PARITY FUNCTIONS , and PARITY FUNCTIONS . the algorithm is based on the use of PARITY FUNCTIONS , such as PARITY FUNCTIONS , and PARITY FUNCTIONS . the proposed algorithm is based on the use of GREEDY DECISION TREE LEARNERS , which is a generalization of the GREEDY DECISION TREE INDUCTION ALGORITHMS . the proposed algorithm is tested on a variety of PROBLEMATIC FUNCTIONS , and the results show that the proposed algorithm is able to achieve the same performance as the standard CONSTANT RUN-TIME PENALTY .\n",
            "\n",
            "858 1000\n",
            "recent advances in SEMANTIC IMAGE SEGMENTATION have mostly been achieved by training DEEP CONVOLUTIONAL NEU-RAL NETWORKS for the task . we show how to improve SEMANTIC SEGMENTATION through the use of CONTEXTUAL INFORMATION . specifically , we explore ` <unk> ' context and ` <unk> ' context with DEEP CNNS . for learning the PATCH-PATCH CONTEXT between image regions , we formulate CONDITIONAL RANDOM FIELDS with CNN-BASED PAIRWISE POTENTIAL FUNCTIONS to capture SEMANTIC CORRELATIONS between neighboring patches . efficient piecewise training of the proposed DEEP CONVOLUTIONAL NEU-RAL NETWORKS is then applied to avoid repeated expensive CRF INFERENCE for BACK PROPAGATION . in order to capture the PATCH-BACKGROUND CONTEXT , we show that a NETWORK DESIGN with traditional MULTI-SCALE IMAGE INPUT and SLIDING PYRAMID POOLING is effective for improving performance . our experiment results set new state-of-the-art performance on a number of popular SEMANTIC SEGMENTATION DATASETS , including NYUDV2 , pascal voc 2012 , PASCAL-CONTEXT , and SIFT-FLOW . particularly , we achieve an INTERSECTION-OVER-UNION SCORE of <unk> on the challenging PASCAL VOC 2012 DATASET . \n",
            "in this paper , we propose a novel approach to SEMANTIC IMAGE SEGMENTATION in DEEP CONVOLUTIONAL NEU-RAL NETWORKS . the proposed approach is based on the use of DEEP CONVOLUTIONAL NEU-RAL NETWORKS and DEEP CONVOLUTIONAL NEU-RAL NETWORKS to estimate the SEMANTIC CORRELATIONS . the proposed approach is based on the use of DEEP CONVOLUTIONAL NEU-RAL NETWORKS and DEEP CNNS . the proposed method is based on the use of DEEP CONVOLUTIONAL NEU-RAL NETWORKS and DEEP CONVOLUTIONAL NEU-RAL NETWORKS . the proposed method is based on the use of DEEP CONVOLUTIONAL NEU-RAL NETWORKS and DEEP CNNS . the proposed method is evaluated on the PASCAL VOC 2012 DATASET , including PASCAL-CONTEXT , PASCAL-CONTEXT , PASCAL-CONTEXT and PASCAL-CONTEXT . the experimental results show that the proposed method outperforms the state-of-the-art methods in terms of INTERSECTION-OVER-UNION SCORE and PASCAL-CONTEXT .\n",
            "\n",
            "859 1000\n",
            "to decode AUDITORY ATTENTION from ELECTROENCEPHALOGRAPHY RECORDINGS in a COCKTAIL-PARTY SCENARIO with two competing speakers a LEAST-SQUARES METHOD has recently been proposed , showing a promising DECODING ACCURACY . this method however requires the CLEAN SPEECH SIGNALS of both the attended and the <unk> speaker to be available as reference signals , which is difficult to achieve from the noisy recorded microphone signals in practice . in addition , optimizing the parameters involved in the SPATIO-TEMPORAL FILTER DESIGN is of crucial importance in order to reach the largest possible DECODING performance . in this paper , the influence of NOISY ACOUSTIC REFERENCE SIGNALS and the SPATIO-TEMPORAL FILTER and REGULARIZATION PARAMETERS on the DECODING performance is investigated . the results show that to some extent the DECODING performance is robust to NOISY ACOUSTIC REFERENCE SIGNALS , depending on the NOISE TYPE . furthermore , we demonstrate the crucial influence of several parameters on the DECODING performance , especially when the ACOUSTIC REFERENCE SIGNALS used for DECODING have been corrupted by noise . \n",
            "this paper addresses the problem of AUDITORY ATTENTION in ELECTROENCEPHALOGRAPHY RECORDINGS . we propose a novel approach to SPATIO-TEMPORAL FILTER DESIGN based on AUDITORY ATTENTION and REGULARIZATION PARAMETERS . the proposed approach is based on the use of CLEAN SPEECH SIGNALS , a SPATIO-TEMPORAL FILTER , a SPATIO-TEMPORAL FILTER , a SPATIO-TEMPORAL FILTER , and a LEAST-SQUARES METHOD . the proposed method is based on the use of CLEAN SPEECH SIGNALS and the REGULARIZATION PARAMETERS . the proposed method is evaluated on a COCKTAIL-PARTY SCENARIO . the experimental results show that the proposed method outperforms the state-of-the-art methods in terms of DECODING ACCURACY and DECODING ACCURACY .\n",
            "\n",
            "860 1000\n",
            "<unk> verbs are an important feature of the ENGLISH LANGUAGE . properly identifying PHRASAL VERBS provides the basis for an ENGLISH PARSER to decode the related structures . PHRASAL VERBS have been a challenge to NATURAL LANGUAGE PROCESSING because PHRASAL VERBS sit at the <unk> between lexicon and syntax . traditional NLP FRAMEWORKS that separate the LEXICON MODULE from the ENGLISH PARSER make it difficult to handle this problem properly . this paper presents a FINITE STATE APPROACH that integrates a PHRASAL VERB EXPERT LEXICON between SHALLOW PARSING and DEEP PARSING to handle MORPHO-SYNTACTIC INTERACTION . with precision/recall combined performance benchmarked consistently at <unk> % <unk> % , the PHRASAL VERB IDENTIFICATION PROBLEM has basically been solved with the presented FINITE STATE APPROACH . \n",
            "this paper presents a novel approach to NATURAL LANGUAGE PROCESSING in a PHRASAL VERB EXPERT LEXICON . the proposed FINITE STATE APPROACH is based on the use of a PHRASAL VERB EXPERT LEXICON , a PHRASAL VERB EXPERT LEXICON , and a FINITE STATE APPROACH . the proposed FINITE STATE APPROACH is based on a FINITE STATE APPROACH for SHALLOW PARSING and SHALLOW PARSING . the proposed FINITE STATE APPROACH is based on a FINITE STATE APPROACH , which is based on a FINITE STATE APPROACH . the proposed FINITE STATE APPROACH is applied to the PHRASAL VERB IDENTIFICATION PROBLEM and DEEP PARSING . the performance of the proposed ENGLISH PARSER is evaluated on a PHRASAL VERB IDENTIFICATION PROBLEM and compared to the results of the ENGLISH PARSER .\n",
            "\n",
            "861 1000\n",
            "a LIGHTWEIGHT EXTRACTION METHOD derives TEXT SNIPPETS associated to dates from the WEB . the snippets are organized dynamically into answers to definition questions . experiments on standard TEST QUESTION SETS show that TEMPORALLY-ANCHORED TEXT SNIPPETS allow for efficiently answering definition questions at accuracy levels comparable to the best systems , without any need for complex LEXICAL RESOURCES , or specialized PROCESSING MODULES dedicated to finding definitions . \n",
            "this paper presents a novel LIGHTWEIGHT EXTRACTION METHOD for TEMPORALLY-ANCHORED TEXT SNIPPETS . the proposed LIGHTWEIGHT EXTRACTION METHOD is based on a WEB for the WEB . the proposed LIGHTWEIGHT EXTRACTION METHOD is based on the use of a WEB as a WEB , which is based on a LIGHTWEIGHT EXTRACTION METHOD . the proposed LIGHTWEIGHT EXTRACTION METHOD is based on the use of a WEB as a WEB . the experimental results on the TEST QUESTION SETS demonstrate the effectiveness of the proposed LIGHTWEIGHT EXTRACTION METHOD .\n",
            "\n",
            "862 1000\n",
            "we have developed a method to determine whether a USER UTTERANCE is directed at the system or not . a SPOKEN DIALOGUE SYSTEM should not respond to AUDIO INPUTS that are not directed at it -lrb- i.e. , a user 's <unk> -rrb- , and it therefore needs to detect such inputs to avoid unsuitable responses . we classify the two cases by LOGISTIC REGRESSION based on a FEATURE SET including UTTERANCE TIMING , UTTERANCE LENGTH , and DIALOGUE STATUS . we conducted experiments using <unk> USER UTTERANCES for both TRANSCRIPTION and automatic speech recognition results . results showed that the CLASSIFICATION ACCURACY improved by <unk> and 4.1 points , respectively . we also discuss which FEATURES are effective in the CLASSIFICATION . \n",
            "in this paper , we propose a novel approach to CLASSIFICATION based on LOGISTIC REGRESSION . the proposed approach is based on the use of FEATURES extracted from the AUDIO INPUTS to estimate the DIALOGUE STATUS , UTTERANCE LENGTH , and DIALOGUE STATUS . the proposed method is based on the use of a set of FEATURES extracted from the AUDIO INPUTS , which are then used to estimate the DIALOGUE STATUS . the proposed approach is evaluated on a FEATURE SET , and the results show that the proposed method is effective in improving the CLASSIFICATION ACCURACY of the SPOKEN DIALOGUE SYSTEM .\n",
            "\n",
            "863 1000\n",
            "a MULTI-IMAGE FOCUS OF ATTENTION MECHANISM has been developed that can quickly distinguish RAISED OBJECTS like BUILDINGS from STRUCTURED BACKGROUND CLUTTER typical to many AERIAL IMAGE SCENARIOS . the underlying approach is the SPACE-SWEEP STEREO METHOD , in which FEATURES from multiple images are <unk> onto a VIRTUAL , HORIZONTAL PLANE that is <unk> <unk> through the scene . BACK-PROJECTED GRADIENT ORIENTATIONS from multiple images are highly correlated when BACK-PROJECTED GRADIENT ORIENTATIONS come from SCENE LOCATIONS containing STRUCTURAL EDGES that are roughly horizontal , like building <unk> and terrain ; otherwise , BACK-PROJECTED GRADIENT ORIENTATIONS tend to be uniformly distributed . these observations are used to define a STRUCTURAL SALIENCE MEASURE that can determine whether a given VOLUME OF SPACE contains a statistically significant number of STRUCTURAL EDGES , without first performing precise reconstruction of those EDGES . the utility of STRUCTURAL SALIENCE for computing focus of attention regions is illustrated on sample data from <unk> , texas . \n",
            "in this paper , we propose a novel approach to AERIAL IMAGE SCENARIOS in AERIAL IMAGE SCENARIOS . the proposed approach is based on a MULTI-IMAGE FOCUS OF ATTENTION MECHANISM that exploits the VOLUME OF SPACE of the FEATURES . the proposed method is based on a MULTI-IMAGE FOCUS OF ATTENTION MECHANISM that exploits the VOLUME OF SPACE of the FEATURES . the proposed approach is based on a MULTI-IMAGE FOCUS OF ATTENTION MECHANISM that exploits the VOLUME OF SPACE of the FEATURES . the proposed method is based on a MULTI-IMAGE FOCUS OF ATTENTION MECHANISM , which is based on a MULTI-IMAGE FOCUS OF ATTENTION MECHANISM . the proposed method can be applied to AERIAL IMAGE SCENARIOS . the proposed method is evaluated on a variety of AERIAL IMAGE SCENARIOS . the experimental results show that the proposed method outperforms the existing methods in terms of the quality of the FEATURES .\n",
            "\n",
            "864 1000\n",
            "information seeking is an important but often difficult task especially when involving large and complex data sets . we hypothesize that a CONTEXT-SENSITIVE INTERACTION PARADIGM can greatly assist users in their information seeking . such a CONTEXT-SENSITIVE INTERACTION PARADIGM allows a system to both understand USER DATA REQUESTS and present the requested information in context . driven by this hypothesis , we have developed a suite of INTELLIGENT USER INTERACTION TECHNOLOGIES and integrated INTELLIGENT USER INTERACTION TECHNOLOGIES in a FULL-FLEDGED , CONTEXT-SENSITIVE INFORMATION SYSTEM . in this paper , we review two sets of key technologies : CONTEXT-SENSITIVE MULTIMODAL INPUT INTERPRETATION and AUTOMATED MULTIMEDIA OUTPUT GENERATION . we also share our evaluation results , which indicate that our CONTEXT-SENSITIVE INTERACTION PARADIGM are capable of supporting CONTEXT-SENSITIVE INFORMATION seeking for practical applications . \n",
            "this paper presents a novel approach to AUTOMATED MULTIMEDIA OUTPUT GENERATION and AUTOMATED MULTIMEDIA OUTPUT GENERATION . the proposed method consists of two steps : -lrb- 1 -rrb- a CONTEXT-SENSITIVE MULTIMODAL INPUT INTERPRETATION , and -lrb- 2 -rrb- a CONTEXT-SENSITIVE INTERACTION PARADIGM that integrates CONTEXT-SENSITIVE INFORMATION into a FULL-FLEDGED , CONTEXT-SENSITIVE INFORMATION SYSTEM . the proposed approach is based on the CONTEXT-SENSITIVE INTERACTION PARADIGM , which is based on a CONTEXT-SENSITIVE INTERACTION PARADIGM . the experimental results demonstrate the effectiveness of the proposed approach .\n",
            "\n",
            "865 1000\n",
            "in this paper we present results for two tasks : SOCIAL EVENT DETECTION and SOCIAL NETWORK EXTRACTION from a LITERARY TEXT , AL-ICE IN WONDERLAND . for the first task , our system trained on a NEWS CORPUS using TREE KERNELS and SUPPORT VECTOR MACHINES beats the baseline systems by a statistically significant margin . using this system we extract a SOCIAL NETWORK from AL-ICE IN WONDERLAND . we show that while we achieve an F-MEASURE of about 61 % on SOCIAL EVENT DETECTION , our extracted UN-WEIGHTED NETWORK is not statistically <unk> from the UN-WEIGHTED GOLD NETWORK according to popularly used NETWORK MEASURES . \n",
            "this paper addresses the problem of SOCIAL EVENT DETECTION and SOCIAL NETWORK EXTRACTION for SOCIAL EVENT DETECTION and SOCIAL NETWORK EXTRACTION . in this paper , we propose a novel approach to SOCIAL EVENT DETECTION and SOCIAL NETWORK EXTRACTION . the proposed approach is based on the use of TREE KERNELS and SUPPORT VECTOR MACHINES . the proposed approach is based on the use of TREE KERNELS and SUPPORT VECTOR MACHINES . the proposed approach is evaluated on a NEWS CORPUS and a NEWS CORPUS . experimental results show that the proposed method is effective in improving the F-MEASURE of SOCIAL EVENT DETECTION and SOCIAL NETWORK EXTRACTION .\n",
            "\n",
            "866 1000\n",
            "this paper addresses the problem of finding glass objects in images . VISUAL CUES obtained by combining the systematic distortions in BACKGROUND TEXTURE occurring at the boundaries of TRANSPARENT OBJECTS with the strong highlights typical of GLASS SURFACES are used to train a hierarchy of CLASSIFIERS , identify GLASS EDGES , and find CONSISTENT SUPPORT REGIONS for these EDGES . qualitative and quantitative experiments involving a number of different CLASSIFIERS and REAL IMAGES are presented . \n",
            "this paper addresses the problem of BACKGROUND TEXTURE in REAL IMAGES and REAL IMAGES . we propose a method to estimate the BACKGROUND TEXTURE of a scene from a single image . our method is based on the use of a BACKGROUND TEXTURE and a BACKGROUND TEXTURE to estimate the BACKGROUND TEXTURE . we show that the proposed method is able to recover the BACKGROUND TEXTURE of the scene from a single image . we also show that our algorithm can be applied to REAL IMAGES and REAL IMAGES . we also show that our method can be applied to REAL IMAGES and REAL IMAGES .\n",
            "\n",
            "867 1000\n",
            "we propose a '' SOFT GREEDY '' LEARNING ALGORITHM for building small conjunctions of simple threshold functions , called RAYS , defined on SINGLE REAL-VALUED ATTRIBUTES . we also propose a PAC-BAYES RISK BOUND which is minimized for CLASSIFIERS achieving a non-trivial tradeoff between sparsity -lrb- the number of RAYS used -rrb- and the magnitude of the separating margin of each ray . finally , we test the SOFT GREEDY '' LEARNING ALGORITHM on four DNA MICRO-ARRAY DATA SETS . \n",
            "this paper addresses the problem of SINGLE REAL-VALUED ATTRIBUTES in the presence of SINGLE REAL-VALUED ATTRIBUTES . in particular , we propose a novel SOFT GREEDY '' LEARNING ALGORITHM , called the SOFT GREEDY '' LEARNING ALGORITHM , which is a generalization of the existing CLASSIFIERS . the proposed SOFT GREEDY '' LEARNING ALGORITHM is based on the idea of SINGLE REAL-VALUED ATTRIBUTES , which is a SOFT GREEDY '' LEARNING ALGORITHM . the proposed SOFT GREEDY '' LEARNING ALGORITHM is evaluated on both DNA MICRO-ARRAY DATA SETS . the experimental results on the DNA MICRO-ARRAY DATA SETS show that the proposed SOFT GREEDY '' LEARNING ALGORITHM is able to significantly improve the performance of the proposed SOFT GREEDY '' LEARNING ALGORITHM .\n",
            "\n",
            "868 1000\n",
            "frequency domain blind source separation -lrb- FREQUENCY DOMAIN BLIND SOURCE SEPARATION -rrb- is shown to be equivalent to two sets of FREQUENCY DOMAIN ADAPTIVE MICROPHONE ARRAYS , i.e. , ADAPTIVE NULL BEAM-FORMERS . the minimization of the OFF-DIAGONAL COMPONENTS in the BSS UPDATE EQUATION can be viewed as the minimization of the MEAN SQUARE ERROR in the FREQUENCY DOMAIN BLIND SOURCE SEPARATION . the UNMIXING MATRIX of the FREQUENCY DOMAIN BLIND SOURCE SEPARATION and the FILTER COEFFICIENTS of the FREQUENCY DOMAIN BLIND SOURCE SEPARATION converge to the same solution in the MEAN SQUARE ERROR SENSE if the two source signals are ideally independent . therefore , we can conclude that the performance of the FREQUENCY DOMAIN BLIND SOURCE SEPARATION is upper bounded by that of the FREQUENCY DOMAIN BLIND SOURCE SEPARATION . this understanding clearly explains the poor performance of the FREQUENCY DOMAIN BLIND SOURCE SEPARATION in a real room with long reverberation . the fundamental difference exists in the adaptation period when they should adapt . that is , the FREQUENCY DOMAIN BLIND SOURCE SEPARATION can adapt in the presence of a JAMMER but the absence of a target , whereas the FREQUENCY DOMAIN BLIND SOURCE SEPARATION can adapt in the presence of a target and JAMMER , and also in the presence of only a target . \n",
            "this paper addresses the problem of FREQUENCY DOMAIN BLIND SOURCE SEPARATION in a FREQUENCY DOMAIN BLIND SOURCE SEPARATION . we propose a method to estimate the FILTER COEFFICIENTS of the FILTER COEFFICIENTS in the FREQUENCY DOMAIN BLIND SOURCE SEPARATION . the proposed method is based on the use of the FILTER COEFFICIENTS of the FILTER COEFFICIENTS of the FILTER COEFFICIENTS . the proposed method is based on the ADAPTIVE NULL BEAM-FORMERS . the proposed method is based on the ADAPTIVE NULL BEAM-FORMERS . the proposed method is evaluated in the context of FREQUENCY DOMAIN BLIND SOURCE SEPARATION . the results show that the proposed method is effective in reducing the MEAN SQUARE ERROR and the MEAN SQUARE ERROR of the proposed method .\n",
            "\n",
            "869 1000\n",
            "we propose a general framework for learning from LABELED AND UNLABELED DATA on a DIRECTED GRAPH in which the structure of the GRAPH including the DIRECTIONALITY OF THE EDGES is considered . the TIME COMPLEXITY of the algorithm derived from this framework is nearly linear due to recently developed NUMERICAL TECHNIQUES . in the absence of labeled instances , this framework can be utilized as a SPECTRAL CLUSTERING METHOD for DIRECTED GRAPHS , which generalizes the SPECTRAL CLUSTERING APPROACH for UNDIRECTED GRAPHS . we have applied our framework to REAL-WORLD WEB CLASSIFICATION PROBLEMS and obtained encouraging results . \n",
            "this paper presents a novel SPECTRAL CLUSTERING APPROACH for UNDIRECTED GRAPHS . the proposed SPECTRAL CLUSTERING APPROACH is based on a SPECTRAL CLUSTERING APPROACH for DIRECTED GRAPHS . the proposed SPECTRAL CLUSTERING APPROACH is based on a SPECTRAL CLUSTERING APPROACH for DIRECTED GRAPHS . the proposed SPECTRAL CLUSTERING APPROACH is based on a SPECTRAL CLUSTERING APPROACH for UNDIRECTED GRAPHS . the proposed SPECTRAL CLUSTERING APPROACH is applied to the problem of UNDIRECTED GRAPHS in the GRAPH . the proposed SPECTRAL CLUSTERING APPROACH is applied to the REAL-WORLD WEB CLASSIFICATION PROBLEMS of the GRAPH , and the results show that the proposed SPECTRAL CLUSTERING APPROACH is effective in improving the TIME COMPLEXITY of the SPECTRAL CLUSTERING APPROACH .\n",
            "\n",
            "870 1000\n",
            "we present the design and analysis of an approximately incentive-compatible combinatorial auction . in just a single run , the auction is able to extract enough value information from bidders to compute APPROXIMATE TRUTH-INDUCING PAYMENTS . this stands in contrast to current AUCTION DESIGNS that need to repeat the ALLOCATION COMPUTATION as many times as there are bidders to achieve INCENTIVE COMPATIBILITY . the auction is formulated as a KERNEL METHOD , which allows for flexibility in choosing the PRICE STRUCTURE via a KERNEL FUNCTION . our main result characterizes the extent to which our auction is incentive-compatible in terms of the COMPLEXITY of the chosen KERNEL FUNCTION . our analysis of the AUCTION 'S PROPERTIES is based on novel insights connecting the notion of stability in STATISTICAL LEARNING THEORY to that of UNIVERSAL COMPETITIVE EQUILIBRIUM in the AUCTION LITERATURE . \n",
            "this paper addresses the problem of STATISTICAL LEARNING THEORY in a AUCTION LITERATURE . the main contribution of this paper is the use of STATISTICAL LEARNING THEORY in order to reduce the COMPLEXITY of the AUCTION LITERATURE . the proposed approach is based on the use of a KERNEL FUNCTION , which is a UNIVERSAL COMPETITIVE EQUILIBRIUM of the KERNEL FUNCTION . the proposed algorithm is based on the use of the AUCTION 'S PROPERTIES , which is a UNIVERSAL COMPETITIVE EQUILIBRIUM of the KERNEL FUNCTION . the algorithm is based on the use of the AUCTION 'S PROPERTIES , which is a UNIVERSAL COMPETITIVE EQUILIBRIUM of the AUCTION LITERATURE . the proposed algorithm is tested on a variety of AUCTION LITERATURE , and the results show that the proposed algorithm is able to achieve the same COMPLEXITY as the AUCTION LITERATURE .\n",
            "\n",
            "871 1000\n",
            "we propose SCALPEL , a FLEXIBLE METHOD for OBJECT SEG-MENTATION that integrates RICH REGION-MERGING CUES with MID-AND HIGH-LEVEL INFORMATION about OBJECT LAYOUT , CLASS , and scale into the SEGMENTATION PROCESS . unlike competing approaches , SCALPEL uses a cascade of BOTTOM-UP SEGMENTATION MODELS that is capable of learning to ignore boundaries early on , yet use them as a STOPPING CRITERION once the object has been mostly segmented . furthermore , we show how such SCALPEL can be learned efficiently . when paired with a novel method that generates better LOCALIZED SHAPE PRIORS than our competitors , our method leads to a concise , accurate set of SEGMENTATION PROPOSALS ; these proposals are more accurate on the PASCAL VOC2010 DATASET than state-of-the-art methods that use RE-RANKING to filter much larger bags of proposals . the code for our algorithm is available online . \n",
            "in this paper , we propose a novel FLEXIBLE METHOD for OBJECT SEG-MENTATION . the proposed FLEXIBLE METHOD is based on the use of a STOPPING CRITERION , a STOPPING CRITERION , a CLASS , a CLASS , a CLASS , a STOPPING CRITERION and a STOPPING CRITERION . the SEGMENTATION PROCESS is formulated as a STOPPING CRITERION , and the SEGMENTATION PROCESS is solved by a STOPPING CRITERION . the proposed approach is evaluated on the PASCAL VOC2010 DATASET , and the results show that the proposed method is effective in improving the SEGMENTATION PROCESS performance .\n",
            "\n",
            "872 1000\n",
            "lexicalized REORDERING MODELS play a crucial role in PHRASE-BASED TRANSLATION SYSTEMS . they are usually learned from the WORD-ALIGNED BILINGUAL CORPUS by examining the REORDERING RELATIONS OF ADJACENT PHRASES . instead of just checking whether there is one phrase adjacent to a given phrase , we argue that it is important to take the number of adjacent phrases into account for better estimations of REORDERING MODELS . we propose to use a STRUCTURE NAMED REORDERING GRAPH , which represents all PHRASE SEGMENTATIONS of a sentence pair , to learn LEX-ICALIZED REORDERING MODELS efficiently . experimental results on the NIST CHINESE-ENGLISH TEST SETS show that our approach significantly outperforms the baseline method . \n",
            "this paper presents a novel approach to PHRASE-BASED TRANSLATION SYSTEMS based on a STRUCTURE NAMED REORDERING GRAPH . the proposed approach is based on the use of a STRUCTURE NAMED REORDERING GRAPH , which is based on a STRUCTURE NAMED REORDERING GRAPH . the proposed approach is based on the use of a STRUCTURE NAMED REORDERING GRAPH , which is able to capture the REORDERING RELATIONS OF ADJACENT PHRASES . the proposed method is evaluated on the NIST CHINESE-ENGLISH TEST SETS , and the results show that the proposed method is effective in improving the REORDERING RELATIONS OF ADJACENT PHRASES performance . the proposed method is evaluated on the NIST CHINESE-ENGLISH TEST SETS and the results show that the proposed method is effective in improving the REORDERING RELATIONS OF ADJACENT PHRASES performance .\n",
            "\n",
            "873 1000\n",
            "grapheme-to-phoneme conversion -lrb- g2p -rrb- is usually used within every state-of-the-art ASR SYSTEM to generalize beyond a fixed set of words . although the performance is typically already quite good -lrb- < 10 % PHONEME ERROR RATE -rrb- and pronunciations of important words are checked by a <unk> , further improvements are still desirable , especially for END USER CUSTOMIZATION . in this work , we present and compare five <unk> to tackle the G2P TASK . although most of the methods have already been published and/or are available as OPEN SOURCE SOFTWARE , the reported experiments are done on large state-of-the-art tasks and the used software is from the actual publications . besides an experimental comparison on TEXT DATA for a range of languages -lrb- i.e. measuring the G2P ACCURACY only -rrb- , our focus in this paper is measuring the effect of improved GRAPHEME-TO-PHONEME CONVERSION on lvcsr performance for a challenging G2P TASK . additionally , the effect of using N-BEST PRONUNCIATION VARIANTS instead of single best is investigated briefly . \n",
            "in this paper , we propose a novel approach to GRAPHEME-TO-PHONEME CONVERSION in the context of OPEN SOURCE SOFTWARE . the proposed GRAPHEME-TO-PHONEME CONVERSION is based on the use of a set of N-BEST PRONUNCIATION VARIANTS , each of which is a set of N-BEST PRONUNCIATION VARIANTS . the proposed GRAPHEME-TO-PHONEME CONVERSION is based on the use of a set of N-BEST PRONUNCIATION VARIANTS , which are then used to train a ASR SYSTEM . the performance of the proposed GRAPHEME-TO-PHONEME CONVERSION is evaluated on a G2P TASK . the results show that the proposed GRAPHEME-TO-PHONEME CONVERSION is effective in improving the G2P ACCURACY of the ASR SYSTEM in terms of G2P ACCURACY .\n",
            "\n",
            "874 1000\n",
            "in this paper , we present a novel FEATURE NORMALIZATION METHOD in the LOG-SCALED SPECTRAL DOMAIN for improving the NOISE ROBUSTNESS of speech recognition front-ends . in the proposed FEATURE NORMALIZATION METHOD , a NON-LINEAR CONTRAST STRETCHING is added to the outputs of LOG MEL-FILTERBANKS to imitate the ADAPTATION OF THE AUDITORY SYSTEM under ADVERSE CONDITIONS . this is followed by a TWO-DIMENSIONAL FILTER to smooth out the PROCESSING ARTIFACTS . the proposed FEATURE NORMALIZATION METHOD perform remarkably well on CENSREC-2 IN-CAR DATABASE with an average relative improvement of <unk> % compared to BASELINE MFCC SYSTEM . it is also confirmed that the proposed processing in LOG MFB DOMAIN can be integrated with conventional CEPSTRAL POST-PROCESSING TECHNIQUES to yield further improvements . the proposed FEATURE NORMALIZATION METHOD is simple and requires only a small extra COMPUTATION LOAD . \n",
            "this paper presents a novel FEATURE NORMALIZATION METHOD for NON-LINEAR CONTRAST STRETCHING based on NON-LINEAR CONTRAST STRETCHING . the proposed FEATURE NORMALIZATION METHOD is based on a TWO-DIMENSIONAL FILTER with a TWO-DIMENSIONAL FILTER . the proposed FEATURE NORMALIZATION METHOD is based on a TWO-DIMENSIONAL FILTER , which is based on a NON-LINEAR CONTRAST STRETCHING . the proposed FEATURE NORMALIZATION METHOD is evaluated on the CENSREC-2 IN-CAR DATABASE , and the results show that the proposed FEATURE NORMALIZATION METHOD is effective in improving the NOISE ROBUSTNESS of the BASELINE MFCC SYSTEM . the proposed FEATURE NORMALIZATION METHOD is evaluated on the CENSREC-2 IN-CAR DATABASE , and the results show that the proposed FEATURE NORMALIZATION METHOD is effective in improving the NOISE ROBUSTNESS of the BASELINE MFCC SYSTEM .\n",
            "\n",
            "875 1000\n",
            "this paper describes a new DIALOGUE CONTROL METHOD that utilizes new RECOGNITION PROCESSES called '' <unk> recognition '' and '' PRETENSE-TYPE RECOGNITION '' that we propose based on HUMAN DIALOGUE ANALYSIS . this DIALOGUE CONTROL METHOD provides users with STRESS-FREE VOICE INPUT through REAL-TIME RESPONSES , comprising a NATURALLY CONTROLLED DIALOGUE to obtain information in order to <unk> candidates comprehensively . \n",
            "this paper presents a novel DIALOGUE CONTROL METHOD based on HUMAN DIALOGUE ANALYSIS . the proposed DIALOGUE CONTROL METHOD is based on the use of REAL-TIME RESPONSES and REAL-TIME RESPONSES . the proposed DIALOGUE CONTROL METHOD is based on the use of REAL-TIME RESPONSES in the form of a DIALOGUE CONTROL METHOD . the proposed DIALOGUE CONTROL METHOD is based on the use of REAL-TIME RESPONSES in order to reduce the number of REAL-TIME RESPONSES in the RECOGNITION PROCESSES . the performance of the proposed DIALOGUE CONTROL METHOD is demonstrated on a variety of REAL-TIME RESPONSES .\n",
            "\n",
            "876 1000\n",
            "image fusion can be viewed as a process that incorporates essential information from different MODALITY SENSORS into a COMPOSITE IMAGE . the use of bases trained using INDEPENDENT COMPONENT ANALYSIS for IMAGE FUSION has been highlighted recently . common fusion rules can be used in the ICA FUSION FRAMEWORK with promising results . in this paper , the authors propose an ADAPTIVE FUSION SCHEME , based on the ICA FUSION FRAMEWORK , that <unk> the sparsity of the fusion image in the transform domain . \n",
            "this paper presents a novel ADAPTIVE FUSION SCHEME for IMAGE FUSION . the proposed ADAPTIVE FUSION SCHEME is based on a ICA FUSION FRAMEWORK , which is based on a ICA FUSION FRAMEWORK . the proposed ADAPTIVE FUSION SCHEME is based on a ICA FUSION FRAMEWORK . the proposed ADAPTIVE FUSION SCHEME is based on a ICA FUSION FRAMEWORK . the proposed ADAPTIVE FUSION SCHEME is applied to the problem of IMAGE FUSION . the experimental results show that the proposed ADAPTIVE FUSION SCHEME is effective in improving the IMAGE FUSION performance .\n",
            "\n",
            "877 1000\n",
            "on-line , spatially localized information about INTERNAL NETWORK PERFORMANCE can greatly assist DYNAMIC ROUTING ALGORITHMS and TRAFFIC TRANSMISSION PROTOCOLS . however , it is impractical to measure NETWORK TRAFFIC at all points in the network . a promising alternative is to measure only at the edge of the network and infer INTERNAL BEHAVIOR from these measurements . in this paper we concentrate on the ESTIMATION AND LOCALIZATION OF INTERNAL DELAYS based on END-TO-END DELAY MEASUREMENTS from sources to receivers . we develop an EM ALGORITHM for computing <unk> of the INTERNAL DELAY DISTRIBUTIONS in cases where the NETWORK DYNAMICS are stationary over the OBSERVATION PERIOD . for time-varying cases , we propose a SEQUENTIAL MONTE CARLO PROCEDURE capable of TRACKING NON-STATIONARY DELAY CHARACTERISTICS . simulations are included to demonstrate the promise of these techniques . \n",
            "in this paper , we propose a novel SEQUENTIAL MONTE CARLO PROCEDURE for ESTIMATION AND LOCALIZATION OF INTERNAL DELAYS . the proposed approach is based on the use of a SEQUENTIAL MONTE CARLO PROCEDURE to estimate the INTERNAL DELAY DISTRIBUTIONS and the INTERNAL DELAY DISTRIBUTIONS . the proposed SEQUENTIAL MONTE CARLO PROCEDURE is based on a SEQUENTIAL MONTE CARLO PROCEDURE for the ESTIMATION AND LOCALIZATION OF INTERNAL DELAYS . the proposed method is based on a SEQUENTIAL MONTE CARLO PROCEDURE , which is based on a SEQUENTIAL MONTE CARLO PROCEDURE . the proposed method is evaluated in terms of INTERNAL NETWORK PERFORMANCE and TRAFFIC TRANSMISSION PROTOCOLS . the results show that the proposed method is effective in improving the INTERNAL NETWORK PERFORMANCE of the TRAFFIC TRANSMISSION PROTOCOLS .\n",
            "\n",
            "878 1000\n",
            "in this paper , we describe a novel type of FEATURE-CENTRIC CASCADE for fast and accurate FACE DETECTION . the FEATURE-CENTRIC CASCADE is called LOCALLY ASSEMBLED BINARY HAAR FEATURE . LOCALLY ASSEMBLED BINARY HAAR FEATURE is basically inspired by the success of HAAR FEATURE and LOCAL BINARY PATTERN for FACE DETECTION , but it is far beyond a simple combination . in our LOCALLY ASSEMBLED BINARY HAAR FEATURE , HAAR FEATURES are modified to keep only the ORDINAL RELATIONSHIP -lrb- named by BINARY HAAR FEATURE -rrb- rather than the difference between the ACCUMULATED INTENSITIES . several neighboring BINARY HAAR FEATURES are then assembled to capture their co-occurrence with similar idea to LOCAL BINARY PATTERN . we show that the FEATURE-CENTRIC CASCADE is more efficient than HAAR FEATURE and LOCAL BINARY PATTERN both in DISCRIMINATING POWER and COMPUTATIONAL COST . furthermore , a novel efficient DETECTION METHOD called FEATURE-CENTRIC CASCADE is proposed to build an efficient detector , which is developed from the FEATURE-CENTRIC METHOD . experimental results on the <unk> frontal face test set and cmu profile test set show that the proposed LOCALLY ASSEMBLED BINARY HAAR FEATURE can achieve very good results and <unk> DETECTION SPEED . \n",
            "in this paper , we propose a novel LOCALLY ASSEMBLED BINARY HAAR FEATURE , called LOCALLY ASSEMBLED BINARY HAAR FEATURE , for FACE DETECTION . the proposed LOCALLY ASSEMBLED BINARY HAAR FEATURE is based on the use of a LOCALLY ASSEMBLED BINARY HAAR FEATURE , a LOCALLY ASSEMBLED BINARY HAAR FEATURE , a LOCALLY ASSEMBLED BINARY HAAR FEATURE , and a LOCAL BINARY PATTERN . the proposed LOCALLY ASSEMBLED BINARY HAAR FEATURE is based on a LOCALLY ASSEMBLED BINARY HAAR FEATURE and a LOCALLY ASSEMBLED BINARY HAAR FEATURE . the proposed LOCALLY ASSEMBLED BINARY HAAR FEATURE , called LOCALLY ASSEMBLED BINARY HAAR FEATURE , is proposed to improve the DETECTION SPEED and DISCRIMINATING POWER . the proposed LOCALLY ASSEMBLED BINARY HAAR FEATURE is evaluated on a variety of datasets and compared to the standard BINARY HAAR FEATURE , and the proposed LOCALLY ASSEMBLED BINARY HAAR FEATURE is very effective . the proposed LOCALLY ASSEMBLED BINARY HAAR FEATURE is compared with the conventional FEATURE-CENTRIC METHOD and the proposed LOCALLY ASSEMBLED BINARY HAAR FEATURE .\n",
            "\n",
            "879 1000\n",
            "computer vision and IMAGE RECOGNITION RESEARCH have a great interest in DIMENSIONALITY REDUCTION TECHNIQUES . generally these techniques are independent of the CLASSIFIER being used and the learning of the CLASSIFIER is carried out after the DIMENSIONALITY REDUCTION is performed , possibly discarding valuable information . in this paper we propose an ITERATIVE ALGORITHM that simultaneously learns a LINEAR PROJECTION BASE and a reduced set of prototypes optimized for the NEAREST-NEIGHBOR CLASSIFIER . the ITERATIVE ALGORITHM is derived by minimizing a suitable estimation of the CLASSIFICATION ERROR PROBABILITY . the proposed ITERATIVE ALGORITHM is assessed through a series of experiments showing a good behavior and a real potential for practical applications . \n",
            "this paper addresses the problem of DIMENSIONALITY REDUCTION in the context of IMAGE RECOGNITION RESEARCH . in particular , we consider the problem of DIMENSIONALITY REDUCTION in the context of IMAGE RECOGNITION RESEARCH . in particular , we consider the problem of DIMENSIONALITY REDUCTION in the context of IMAGE RECOGNITION RESEARCH . we propose a novel ITERATIVE ALGORITHM to the problem of DIMENSIONALITY REDUCTION . we show that the proposed ITERATIVE ALGORITHM can be applied to a wide range of IMAGE RECOGNITION RESEARCH . we also show that the proposed ITERATIVE ALGORITHM can be applied to a wide range of IMAGE RECOGNITION RESEARCH .\n",
            "\n",
            "880 1000\n",
            "dimensional sentiment analysis aims to recognize CONTINUOUS NUMERICAL VALUES in multiple dimensions such as the VALENCE-AROUSAL SPACE . compared to the CATEGORICAL APPROACH that focuses on SENTIMENT CLASSIFICATION such as BINARY CLASSIFICATION -lrb- i.e. , positive and negative -rrb- , the DIMENSIONAL APPROACH can provide more FINE-GRAINED SENTIMENT ANALYSIS . this study proposes a REGIONAL CNN-LSTM MODEL consisting of two parts : DIMENSIONAL APPROACH and LSTM to predict the VA RATINGS OF TEXTS . unlike a conventional CNN which considers a whole text as input , the proposed DIMENSIONAL APPROACH uses an individual sentence as a region , dividing an input text into several regions such that the useful AFFECTIVE INFORMATION in each region can be extracted and weighted according to their contribution to the VA PREDICTION . such AFFECTIVE INFORMATION is sequentially integrated across regions using LSTM for VA PREDICTION . by combining the DIMENSIONAL APPROACH and LSTM , both LOCAL INFORMATION within sentences and LONG-DISTANCE DEPENDENCY across sentences can be considered in the PREDICTION PROCESS . experimental results show that the proposed REGIONAL CNN-LSTM MODEL outperforms <unk> , regression based , and NN-BASED METHODS proposed in previous studies . \n",
            "in this paper , we propose a novel CATEGORICAL APPROACH for FINE-GRAINED SENTIMENT ANALYSIS . the proposed REGIONAL CNN-LSTM MODEL is based on a CATEGORICAL APPROACH and a CNN to the PREDICTION PROCESS . the proposed REGIONAL CNN-LSTM MODEL is based on a REGIONAL CNN-LSTM MODEL , which is based on the REGIONAL CNN-LSTM MODEL and the LSTM . the proposed REGIONAL CNN-LSTM MODEL is based on a REGIONAL CNN-LSTM MODEL , which is based on a REGIONAL CNN-LSTM MODEL and a CNN . the proposed REGIONAL CNN-LSTM MODEL is applied to SENTIMENT CLASSIFICATION , such as BINARY CLASSIFICATION and LSTM . the experimental results show that the proposed REGIONAL CNN-LSTM MODEL outperforms the existing NN-BASED METHODS , especially in the presence of LONG-DISTANCE DEPENDENCY and LONG-DISTANCE DEPENDENCY .\n",
            "\n",
            "881 1000\n",
            "naively collecting translations by <unk> the task to NON-PROFESSIONAL TRANSLATORS yields <unk> , <unk> results if no quality control is exercised . we demonstrate a variety of mechanisms that increase the TRANSLATION QUALITY to near professional levels . specifically , we solicit REDUNDANT TRANSLATIONS and edits to them , and automatically select the best output among them . we propose a set of FEATURES that model both the translations and the translators , such as COUNTRY OF RESIDENCE , LM PERPLEXITY of the TRANSLATION , EDIT RATE from the other translations , and -LRB- OPTIONALLY -RRB- CALIBRATION against PROFESSIONAL TRANSLATORS . using these FEATURES to score the collected translations , we are able to discriminate between acceptable and unacceptable translations . we <unk> the nist 2009 <unk> evaluation set with MECHANICAL TURK , and quantitatively show that our models are able to select translations within the range of quality that we expect from PROFESSIONAL TRANSLATORS . the total cost is more than an order of magnitude lower than PROFESSIONAL TRANSLATION . \n",
            "in this paper , we present a novel approach to the problem of NON-PROFESSIONAL TRANSLATORS in the context of NON-PROFESSIONAL TRANSLATORS . the proposed approach is based on the use of FEATURES and EDIT RATE . the proposed approach is based on the use of FEATURES and EDIT RATE . the proposed approach is evaluated on a variety of FEATURES , and the results show that the proposed method is effective in reducing the TRANSLATION QUALITY and TRANSLATION QUALITY of the proposed method . the proposed method is evaluated in terms of LM PERPLEXITY and EDIT RATE . the results show that the proposed method can reduce the TRANSLATION QUALITY by up to 50 % .\n",
            "\n",
            "882 1000\n",
            "over-complete representations of images such as UNDECIMATED WA-VELETS have enjoyed immense popularity in recent years . though UNDECIMATED WA-VELETS are efficient for modeling singularities and EDGES , NATURAL IMAGES also consist of textures that are difficult to capture with any CANONICAL TRANSFORMATION . in this work , we develop a new MODELING STRATEGY with a rigorous treatment of TEXTURED REGIONS . using PRINCIPAL COMPONENTS ANALYSIS as an APPROXIMATE CLASSIFIER for EDGES and textures , we partition an IMAGE into COMPRESSIBLE AND INCOMPRESS-IBLE REGIONS -- with corresponding POSTERIOR MEDIAN-BASED DENOISING METHOD matching their behaviors . a POSTERIOR MEDIAN-BASED DENOISING METHOD using these POSTERIOR MEDIAN-BASED DENOISING METHOD is described with preliminary results that demonstrate the effectiveness of this POSTERIOR MEDIAN-BASED DENOISING METHOD . \n",
            "this paper presents a novel MODELING STRATEGY based on PRINCIPAL COMPONENTS ANALYSIS . the proposed POSTERIOR MEDIAN-BASED DENOISING METHOD is based on PRINCIPAL COMPONENTS ANALYSIS . the proposed POSTERIOR MEDIAN-BASED DENOISING METHOD is based on PRINCIPAL COMPONENTS ANALYSIS . the proposed POSTERIOR MEDIAN-BASED DENOISING METHOD is based on PRINCIPAL COMPONENTS ANALYSIS . the proposed POSTERIOR MEDIAN-BASED DENOISING METHOD is based on PRINCIPAL COMPONENTS ANALYSIS . the proposed POSTERIOR MEDIAN-BASED DENOISING METHOD is applied to the COMPRESSIBLE AND INCOMPRESS-IBLE REGIONS of the IMAGE , and the POSTERIOR MEDIAN-BASED DENOISING METHOD is used to estimate the EDGES of the IMAGE . the proposed POSTERIOR MEDIAN-BASED DENOISING METHOD is evaluated on a number of NATURAL IMAGES . the results show that the proposed POSTERIOR MEDIAN-BASED DENOISING METHOD is able to accurately estimate the EDGES of the IMAGE .\n",
            "\n",
            "883 1000\n",
            "we introduce a new BAYESIAN NONPARAMETRIC APPROACH to IDENTIFICATION OF SPARSE DYNAMIC LINEAR SYSTEMS . the IMPULSE RESPONSES are modeled as GAUSSIAN PROCESSES whose <unk> encode the BIBO STABILITY CONSTRAINT , as defined by the recently introduced '' STABLE SPLINE KERNEL '' . SPARSE SOLUTIONS are obtained by placing EXPONENTIAL HYPERPRIORS on the SCALE FACTORS of such kernels . numerical experiments regarding estimation of ARMAX MODELS show that this BAYESIAN NONPARAMETRIC APPROACH provides a definite advantage over a GROUP LAR ALGORITHM and state-of-the-art PARAMETRIC IDENTIFICATION TECHNIQUES based on PREDICTION ERROR MINIMIZATION . \n",
            "this paper proposes a novel BAYESIAN NONPARAMETRIC APPROACH for IDENTIFICATION OF SPARSE DYNAMIC LINEAR SYSTEMS . the proposed BAYESIAN NONPARAMETRIC APPROACH is based on the use of GAUSSIAN PROCESSES and PARAMETRIC IDENTIFICATION TECHNIQUES . the proposed BAYESIAN NONPARAMETRIC APPROACH is based on the use of GAUSSIAN PROCESSES and PARAMETRIC IDENTIFICATION TECHNIQUES . the proposed BAYESIAN NONPARAMETRIC APPROACH is based on the use of GAUSSIAN PROCESSES and PARAMETRIC IDENTIFICATION TECHNIQUES . the proposed BAYESIAN NONPARAMETRIC APPROACH is compared to the GROUP LAR ALGORITHM and other PARAMETRIC IDENTIFICATION TECHNIQUES . the proposed BAYESIAN NONPARAMETRIC APPROACH is compared to the GROUP LAR ALGORITHM and other PARAMETRIC IDENTIFICATION TECHNIQUES and PARAMETRIC IDENTIFICATION TECHNIQUES .\n",
            "\n",
            "884 1000\n",
            "although SEMI-SUPERVISED LEARNING has been an active area of research , its use in DEPLOYED APPLICATIONS is still relatively rare because the methods are often difficult to implement , fragile in TUNING , or lacking in scalability . this paper presents <I> EXPECTATION REGULARIZATION </I> , a SEMI-SUPERVISED LEARNING METHOD for EXPONENTIAL FAMILY PARAMETRIC MODELS that augments the traditional CONDITIONAL LABEL-LIKELIHOOD OBJECTIVE FUNCTION with an additional term that encourages MODEL PREDICTIONS on UNLABELED DATA to match certain expectations -- such as LABEL PRIORS . the SEMI-SUPERVISED LEARNING METHOD is extremely easy to implement , SCALES as well as LOGISTIC REGRESSION , and can handle NON-INDEPENDENT FEATURES . we present experiments on five different DATA SETS , showing accuracy improvements over other SEMI-SUPERVISED METHODS . \n",
            "in this paper , we propose a novel SEMI-SUPERVISED LEARNING METHOD for EXPONENTIAL FAMILY PARAMETRIC MODELS . the proposed SEMI-SUPERVISED LEARNING METHOD is based on the idea of <I> EXPECTATION REGULARIZATION </I> , which is a generalization of the standard <I> EXPECTATION REGULARIZATION </I> . the proposed SEMI-SUPERVISED LEARNING METHOD is based on the use of <I> EXPECTATION REGULARIZATION </I> , which is a generalization of the standard <I> EXPECTATION REGULARIZATION </I> . the proposed SEMI-SUPERVISED LEARNING METHOD is based on the idea of <I> EXPECTATION REGULARIZATION </I> , which is a generalization of the <I> EXPECTATION REGULARIZATION </I> to the UNLABELED DATA . the proposed SEMI-SUPERVISED LEARNING METHOD is based on the idea of <I> EXPECTATION REGULARIZATION </I> , which is a generalization of the standard <I> EXPECTATION REGULARIZATION </I> . the proposed SEMI-SUPERVISED LEARNING METHOD is applied to the problem of SEMI-SUPERVISED LEARNING , and the results show that the proposed SEMI-SUPERVISED LEARNING METHOD is effective for DEPLOYED APPLICATIONS . the proposed SEMI-SUPERVISED LEARNING METHOD is applied to the problem of SEMI-SUPERVISED LEARNING , and the experimental results demonstrate the effectiveness of the proposed SEMI-SUPERVISED LEARNING METHOD .\n",
            "\n",
            "885 1000\n",
            "a novel PARTICLE FILTER , the MEMORY-BASED PARTICLE FILTER , is proposed that can visually track moving objects that have COMPLEX DYNAMICS . we aim to realize robust-ness against ABRUPT OBJECT MOVEMENTS and quick recovery from TRACKING FAILURE caused by factors such as OCCLUSIONS . to that end , we eliminate the MARKOV ASSUMPTION from the previous PARTICLE FILTERING FRAMEWORK and predict the PRIOR DISTRIBUTION of the target state from the LONG-TERM DYNAMICS . more concretely , MEMORY-BASED PARTICLE FILTER stores the past history of the estimated target states , and employs a RANDOM SAMPLING from the history to generate PRIOR DISTRIBUTION ; MEMORY-BASED PARTICLE FILTER represents a novel PF FORMULATION.OUR METHOD can handle NONLINEAR , TIME-VARIANT , and <unk> dynamics , which is not possible within existing PF FRAMEWORKS . accurate PRIOR PREDICTION based on PROPER DYNAMICS MODEL is especially effective for recovering lost tracks , because MEMORY-BASED PARTICLE FILTER can provide possible target states , which can drastically change since the track was lost . we target the face pose of <unk> humans in this paper . quantitative evaluations with MAGNETIC SENSORS confirm improved ACCURACY in FACE POSE ESTIMATION and successful recovery from TRACKING LOSS . the proposed MEMORY-BASED PARTICLE FILTER suggests a new paradigm for MODELING SYSTEMS with COMPLEX DYNAMICS and so offers a various VISUAL TRACKING APPLICATIONS . \n",
            "this paper addresses the problem of FACE POSE ESTIMATION from MAGNETIC SENSORS . we propose a novel PROPER DYNAMICS MODEL , called PROPER DYNAMICS MODEL , for FACE POSE ESTIMATION . the proposed PROPER DYNAMICS MODEL is based on a PARTICLE FILTERING FRAMEWORK , a MEMORY-BASED PARTICLE FILTER , and a PROPER DYNAMICS MODEL for ABRUPT OBJECT MOVEMENTS . the proposed PROPER DYNAMICS MODEL is based on a PARTICLE FILTERING FRAMEWORK , called the MEMORY-BASED PARTICLE FILTER , for FACE POSE ESTIMATION . the proposed PROPER DYNAMICS MODEL is based on a PARTICLE FILTERING FRAMEWORK , which is based on RANDOM SAMPLING . the proposed PROPER DYNAMICS MODEL is applied to FACE POSE ESTIMATION , and the results show that the proposed PROPER DYNAMICS MODEL is effective in improving the ACCURACY and ACCURACY of the MODELING SYSTEMS .\n",
            "\n",
            "886 1000\n",
            "we present efficient algorithms for dealing with the problem of MISSING INPUTS -LRB- INCOMPLETE FEATURE VECTORS -rrb- during TRAINING and RECALL . our approach is based on the approximation of the INPUT DATA DISTRIBUTION using PARZEN WINDOWS . for RECALL , we obtain CLOSED FORM SOLUTIONS for ARBITRARY FEEDFORWARD NETWORKS . for TRAINING , we show how the BACKPROPAGATION STEP for an INCOMPLETE FEATURE VECTORS can be approximated by a WEIGHTED AVERAGED BACKPROPAGATION STEP . the COMPLEXITY of the CLOSED FORM SOLUTIONS for TRAINING and RECALL is independent of the number of MISSING FEATURES . we verify our theoretical results using one CLASSIFICATION and one REGRESSION PROBLEM . \n",
            "this paper addresses the problem of MISSING INPUTS -LRB- INCOMPLETE FEATURE VECTORS in the presence of MISSING INPUTS -LRB- INCOMPLETE FEATURE VECTORS . in particular , we consider the problem of MISSING INPUTS -LRB- INCOMPLETE FEATURE VECTORS in the presence of MISSING INPUTS -LRB- INCOMPLETE FEATURE VECTORS . we propose a novel method to estimate the MISSING INPUTS -LRB- INCOMPLETE FEATURE VECTORS of the MISSING FEATURES using a WEIGHTED AVERAGED BACKPROPAGATION STEP . the proposed CLOSED FORM SOLUTIONS is based on a WEIGHTED AVERAGED BACKPROPAGATION STEP , which is a REGRESSION PROBLEM and the REGRESSION PROBLEM is solved using a WEIGHTED AVERAGED BACKPROPAGATION STEP . the proposed CLOSED FORM SOLUTIONS is applied to the REGRESSION PROBLEM and the REGRESSION PROBLEM . the experimental results show that the proposed algorithm is able to recover the MISSING FEATURES of the INPUT DATA DISTRIBUTION , and the COMPLEXITY of the algorithm is much faster .\n",
            "\n",
            "887 1000\n",
            "the EXTRACTION OF STATISTICALLY INDEPENDENT COMPONENTS from HIGH-DIMENSIONAL MULTI-SENSORY INPUT STREAMS is assumed to be an essential component of SENSORY PROCESSING in the brain . such INDEPENDENT COMPONENT ANALYSIS -lrb- or BLIND SOURCE SEPARATION -RRB- could provide a less redundant representation of information about the external world . another powerful PROCESSING STRATEGY is to extract preferentially those components from HIGH-DIMENSIONAL INPUT STREAMS that are related to other INFORMATION SOURCES , such as INTERNAL PREDICTIONS or PROPRIOCEPTIVE FEEDBACK . this PROCESSING STRATEGY allows the optimization of INTERNAL REPRESENTATION according to the INFORMATION BOTTLENECK METHOD . however , CONCRETE LEARNING RULES that implement these general UNSUPERVISED LEARNING PRINCIPLES for spiking neurons are still missing . we show how both INFORMATION BOTTLENECK OPTIMIZATION and the EXTRACTION OF INDEPENDENT COMPONENTS can in principle be implemented with stochastically spiking neurons with <unk> . the new CONCRETE LEARNING RULES that achieves this is derived from ABSTRACT INFORMATION OPTIMIZATION PRINCIPLES . \n",
            "this paper addresses the problem of SENSORY PROCESSING from HIGH-DIMENSIONAL MULTI-SENSORY INPUT STREAMS . we propose a novel approach to INFORMATION BOTTLENECK OPTIMIZATION based on ABSTRACT INFORMATION OPTIMIZATION PRINCIPLES . the proposed approach is based on the use of ABSTRACT INFORMATION OPTIMIZATION PRINCIPLES , which is based on the ABSTRACT INFORMATION OPTIMIZATION PRINCIPLES . the proposed approach is based on the use of ABSTRACT INFORMATION OPTIMIZATION PRINCIPLES , which is based on the ABSTRACT INFORMATION OPTIMIZATION PRINCIPLES . the proposed approach is based on the use of ABSTRACT INFORMATION OPTIMIZATION PRINCIPLES , which is based on the ABSTRACT INFORMATION OPTIMIZATION PRINCIPLES . the proposed approach is based on the use of ABSTRACT INFORMATION OPTIMIZATION PRINCIPLES and PROPRIOCEPTIVE FEEDBACK . the proposed approach is evaluated on a variety of HIGH-DIMENSIONAL INPUT STREAMS including PROPRIOCEPTIVE FEEDBACK , INTERNAL PREDICTIONS , and PROPRIOCEPTIVE FEEDBACK .\n",
            "\n",
            "888 1000\n",
            "this opinion paper discusses SUBJECTIVE NATURAL LANGUAGE PROBLEMS in terms of their motivations , applications , CHARACTERIZATIONS , and implications . it argues that such SUBJECTIVE NATURAL LANGUAGE PROBLEMS <unk> increased attention because of their potential to challenge the status of theoretical understanding , PROBLEM-SOLVING METHODS , and evaluation techniques in COMPUTATIONAL LINGUISTICS . the author supports a more HOLIS-TIC APPROACH to such SUBJECTIVE NATURAL LANGUAGE PROBLEMS ; a view that extends beyond OPINION MINING or SENTIMENT ANALYSIS . \n",
            "this paper presents a novel HOLIS-TIC APPROACH for SENTIMENT ANALYSIS and OPINION MINING . the proposed HOLIS-TIC APPROACH is based on the use of a HOLIS-TIC APPROACH for SENTIMENT ANALYSIS and OPINION MINING . the proposed HOLIS-TIC APPROACH is based on the use of CHARACTERIZATIONS and CHARACTERIZATIONS . the performance of the proposed HOLIS-TIC APPROACH is evaluated on a variety of SUBJECTIVE NATURAL LANGUAGE PROBLEMS and COMPUTATIONAL LINGUISTICS .\n",
            "\n",
            "889 1000\n",
            "we introduce a new type of PHASE-BASED LOCAL FEATURE based on the phase and amplitude responses of COMPLEX-VALUED STEERABLE FILTERS . the design of this PHASE-BASED LOCAL FEATURE is motivated by a desire to obtain FEATURE VECTORS which are <unk> under COMMON IMAGE DEFORMATIONS , yet distinctive enough to provide useful IDENTITY INFORMATION . a recent proposal for such PHASE-BASED LOCAL FEATURE involves combining DIFFERENTIAL INVARIANTS to particular IMAGE DEFORMATIONS , such as ROTATION . our approach differs in that we consider a wider class of IMAGE DEFORMATIONS , including the addition of NOISE , along with both GLOBAL AND LOCAL BRIGHTNESS VARIATIONS . we use STEERABLE FILTERS to make the FEATURE robust to ROTATION . and we exploit the fact that PHASE DATA is often locally stable with respect to SCALE CHANGES , NOISE , and COMMON BRIGHTNESS CHANGES . we provide empirical results comparing our PHASE-BASED LOCAL FEATURE with one based on DIFFERENTIAL INVARIANTS . the results show that our PHASE-BASED LOCAL FEATURE leads to better performance when dealing with COMMON ILLUMINATION CHANGES and 2-D ROTATION , while giving comparable effects in terms of SCALE CHANGES . \n",
            "in this paper , we propose a novel approach to PHASE DATA based on COMPLEX-VALUED STEERABLE FILTERS . the proposed method is based on the use of COMPLEX-VALUED STEERABLE FILTERS to estimate the IDENTITY INFORMATION , and the IDENTITY INFORMATION between the FEATURE VECTORS and the COMMON BRIGHTNESS CHANGES . the proposed method is based on the use of COMPLEX-VALUED STEERABLE FILTERS to estimate the IDENTITY INFORMATION , and the IDENTITY INFORMATION is modeled by a COMPLEX-VALUED STEERABLE FILTERS . the proposed method is based on the use of COMPLEX-VALUED STEERABLE FILTERS to estimate the IDENTITY INFORMATION and the COMMON BRIGHTNESS CHANGES . the experimental results show that the proposed method outperforms the existing methods in terms of both GLOBAL AND LOCAL BRIGHTNESS VARIATIONS and COMMON ILLUMINATION CHANGES .\n",
            "\n",
            "890 1000\n",
            "word embedding WORD EMBEDDING MODELS learn VECTORIAL WORD REPRESENTATIONS that can be used in a variety of NLP APPLICATIONS . when TRAINING DATA is scarce , these WORD EMBEDDING MODELS risk losing their GENERALIZATION ABILITIES due to the COMPLEXITY of the WORD EMBEDDING MODELS and the OVERFITTING to FINITE DATA . we propose a REGULARIZED EMBEDDING FORMULATION , called ROBUST GRAM , which penalizes OVERFITTING by suppressing the disparity between target and CONTEXT EMBEDDINGS . our experimental analysis shows that the ROBUST GRAM trained on SMALL DATASETS generalizes better compared to alternatives , is more robust to variations in the training set , and correlates well to HUMAN SIMILARITIES in a set of WORD SIMILARITY TASKS . \n",
            "this paper proposes a novel REGULARIZED EMBEDDING FORMULATION called ROBUST GRAM , which is able to learn VECTORIAL WORD REPRESENTATIONS from FINITE DATA . the proposed WORD EMBEDDING MODELS , called ROBUST GRAM , is based on a REGULARIZED EMBEDDING FORMULATION , called ROBUST GRAM , for VECTORIAL WORD REPRESENTATIONS . the proposed WORD EMBEDDING MODELS , called ROBUST GRAM , is based on a REGULARIZED EMBEDDING FORMULATION , called ROBUST GRAM , for VECTORIAL WORD REPRESENTATIONS . the proposed WORD EMBEDDING MODELS is a REGULARIZED EMBEDDING FORMULATION , which is a generalization of the standard REGULARIZED EMBEDDING FORMULATION , and has been shown to be effective for NLP APPLICATIONS . the COMPLEXITY of the proposed WORD EMBEDDING MODELS is demonstrated on several SMALL DATASETS , and the results show that the proposed approach is effective in improving the COMPLEXITY of WORD EMBEDDING MODELS .\n",
            "\n",
            "891 1000\n",
            "in order to overcome several limitations of STRUCTURED LIGHT 3D ACQUISITION METHODS , the COLORS , INTENSITIES , and shapes of the projected patterns are adapted to the scene . based on a crude estimate of the SCENE GEOMETRY and reflectance characteristics , the LOCAL INTENSITY RANGES in the projected patterns are adapted , in order to avoid OVER-AND UNDER-EXPOSURE in the image . this avoids the INFAMOUS SPECULARITY PROBLEMS and generally increases ACCURACY . the estimated geometry also helps to limit the effect of ALIASING caused by the SAMPLING OF FORESHORTENED PATTERNS . furthermore , the approach also <unk> for the adverse effects that small motions during SCANNING would normally have . moreover , the approach yields a CONFIDENCE MEASURE at every pixel of the range image . last but not least , the scanner consists of consumer products only , and therefore is cheap . \n",
            "in this paper , we present a novel approach to the problem of INFAMOUS SPECULARITY PROBLEMS in the presence of ALIASING , INTENSITIES , and INTENSITIES . the proposed approach is based on the use of a set of COLORS , COLORS , and COLORS . the proposed approach is based on the use of a set of INTENSITIES , COLORS , and COLORS . the proposed approach is based on the use of OVER-AND UNDER-EXPOSURE and COLORS . the proposed method is evaluated in terms of ACCURACY and ALIASING , and the experimental results show that the proposed method is effective in improving the ACCURACY and ACCURACY of the proposed method .\n",
            "\n",
            "892 1000\n",
            "in this paper we demonstrate that SPECTRAL CONVERSION can be successfully applied to the SPEECH ENHANCEMENT PROBLEM as a FEATURE DENOISING METHOD . the enhanced SPECTRAL CONVERSION can be used in the context of the KALMAN FILTER for estimating the CLEAN SPEECH SIGNAL . in essence , instead of estimating the CLEAN SPEECH FEATURES and the CLEAN SPEECH SIGNAL using the ITERATIVE KALMAN FILTER , we show that is more efficient to initially estimate the CLEAN SPEECH FEATURES from the NOISY SPEECH FEATURES using SPECTRAL CONVERSION -lrb- using a training speech corpus -rrb- and then apply the standard KALMAN FILTER . our results show an average improvement compared to the ITERATIVE KALMAN FILTER that can reach 6 db in the AVERAGE SEGMENTAL OUTPUT SIGNAL-TO-NOISE RATIO , in low input snr 's . \n",
            "in this paper , we propose a novel FEATURE DENOISING METHOD for SPEECH ENHANCEMENT PROBLEM . the proposed FEATURE DENOISING METHOD is based on a KALMAN FILTER to the SPEECH ENHANCEMENT PROBLEM . the proposed FEATURE DENOISING METHOD is based on a KALMAN FILTER for SPECTRAL CONVERSION . the proposed FEATURE DENOISING METHOD is applied to the CLEAN SPEECH SIGNAL and CLEAN SPEECH SIGNAL . the experimental results show that the proposed FEATURE DENOISING METHOD is effective in improving the AVERAGE SEGMENTAL OUTPUT SIGNAL-TO-NOISE RATIO of the proposed FEATURE DENOISING METHOD .\n",
            "\n",
            "893 1000\n",
            "a PHASE SYNCHRONIZATION METHOD , which provides NON-UNIFORM FREQUENCY OFFSET COMPENSATION needed for wideband ofdm -lsb- 1 -rsb- , is coupled with LOW-COMPLEXITY CHANNEL ESTIMATION in the TIME DOMAIN . <unk> of the CHANNEL IMPULSE RESPONSE leads to an improved performance , while ADAPTIVE SYNCHRONIZATION supports DECISION-DIRECTED OPERATION and yields low overhead . system performance is demonstrated using experimental data transmitted over a 1 km shallow water channel in the 19 <unk> khz band . \n",
            "this paper addresses the problem of LOW-COMPLEXITY CHANNEL ESTIMATION in the presence of CHANNEL IMPULSE RESPONSE . in particular , we consider the problem of LOW-COMPLEXITY CHANNEL ESTIMATION in the presence of CHANNEL IMPULSE RESPONSE . in particular , we consider the problem of LOW-COMPLEXITY CHANNEL ESTIMATION in the presence of CHANNEL IMPULSE RESPONSE . we propose a novel PHASE SYNCHRONIZATION METHOD , which is based on a PHASE SYNCHRONIZATION METHOD . the proposed PHASE SYNCHRONIZATION METHOD is based on a PHASE SYNCHRONIZATION METHOD . the proposed PHASE SYNCHRONIZATION METHOD is applied to the problem of LOW-COMPLEXITY CHANNEL ESTIMATION .\n",
            "\n",
            "894 1000\n",
            "this paper discusses our THREE-STAGE APPROACH to a FLEXIBLE VOCABULARY SPEECH UNDERSTANDING SYSTEM , which can detect OUT-OF-VOCABULARY WORDS , and hypothesize their PHONETIC AND OR-THOGRAPHIC TRANSCRIPTIONS . in the first stage , we introduce the COLUMN-BIGRAM FINITE-STATE TRANSDUCER which , while embedding ANGIE SUBLEXICAL MODELS , also supports previously UNSEEN DATA from UNKNOWN WORDS . secondly , the ANGIE SUBLEXICAL MODELS utilize GRAPHEME INFORMATION , providing TIGHTER LINGUISTIC CONSTRAINT as well as INSTANTANEOUS SOUND-TO-LETTER CAPABILITY during RECOGNITION . thirdly , the SYLLABLE-LEVEL LEXICAL UNITS of the first stage are automatically derived via an ITERATIVE PROCEDURE to optimize performance . the THREE-STAGE APPROACH employs ANGIE SUBLEXICAL MODELS to output a WORD NETWORK which is parsed by TINA , our NATURAL LANGUAGE PROCESSOR , in stage three . experiments with a JUPITER IMPLEMENTATION of this THREE-STAGE APPROACH are described in -lsb- 1 -rsb- . \n",
            "this paper presents a novel THREE-STAGE APPROACH for RECOGNITION . the proposed THREE-STAGE APPROACH is based on a THREE-STAGE APPROACH to the COLUMN-BIGRAM FINITE-STATE TRANSDUCER . the proposed THREE-STAGE APPROACH is based on a THREE-STAGE APPROACH to the COLUMN-BIGRAM FINITE-STATE TRANSDUCER . the proposed THREE-STAGE APPROACH is based on the use of SYLLABLE-LEVEL LEXICAL UNITS in the COLUMN-BIGRAM FINITE-STATE TRANSDUCER . the proposed THREE-STAGE APPROACH is based on the use of SYLLABLE-LEVEL LEXICAL UNITS in the COLUMN-BIGRAM FINITE-STATE TRANSDUCER . the proposed THREE-STAGE APPROACH is applied to UNSEEN DATA , and the results show that the proposed THREE-STAGE APPROACH is effective in RECOGNITION . the proposed THREE-STAGE APPROACH is evaluated in terms of RECOGNITION and RECOGNITION performance . the proposed THREE-STAGE APPROACH is evaluated in terms of RECOGNITION and RECOGNITION performance .\n",
            "\n",
            "895 1000\n",
            "we consider the DATA STREAM of reconstructing a DATA STREAM from a small subset of its entries , where the DATA STREAM is assumed to lie in a LOW-DIMENSIONAL LINEAR SUBSPACE , possibly corrupted by noise . it is also important to track the change of underlying SUBSPACE for many applications . this DATA STREAM can be viewed as a SEQUENTIAL LOW-RANK MATRIX COMPLETION PROBLEM in which the SUBSPACE is learned in an ONLINE FASHION . the proposed algorithm , called PARALLEL ESTIMATION AND TRACKING by RECURSIVE LEAST SQUARES , identifies the underlying LOW-DIMENSIONAL SUBSPACE via a RECURSIVE PROCEDURE for each row of the SUBSPACE MATRIX in parallel , and then reconstructs the missing entries via LEAST-SQUARES ESTIMATION if required . RECURSIVE LEAST SQUARES outperforms previous approaches by discounting observations in order to capture LONG-TERM BEHAVIOR OF THE DATA STREAM and be able to adapt to RECURSIVE LEAST SQUARES . NUMERICAL EXAMPLES are provided for DIRECTION-OF-ARRIVAL ESTIMATION and MATRIX COMPLETION , comparing RECURSIVE LEAST SQUARES with STATE OF THE ART BATCH ALGORITHMS . \n",
            "this paper addresses the problem of DIRECTION-OF-ARRIVAL ESTIMATION in a DATA STREAM . the SEQUENTIAL LOW-RANK MATRIX COMPLETION PROBLEM is a SEQUENTIAL LOW-RANK MATRIX COMPLETION PROBLEM , where the DATA STREAM is a LOW-DIMENSIONAL LINEAR SUBSPACE . in this paper , a novel RECURSIVE PROCEDURE is proposed to solve the SEQUENTIAL LOW-RANK MATRIX COMPLETION PROBLEM . the proposed RECURSIVE PROCEDURE is based on a RECURSIVE PROCEDURE . the proposed RECURSIVE PROCEDURE is based on the RECURSIVE LEAST SQUARES . the proposed RECURSIVE PROCEDURE is applied to the LONG-TERM BEHAVIOR OF THE DATA STREAM of the DATA STREAM , and the STATE OF THE ART BATCH ALGORITHMS is applied to the DATA STREAM . experimental results show that the proposed method outperforms the state-of-the-art methods in terms of both PARALLEL ESTIMATION AND TRACKING and the STATE OF THE ART BATCH ALGORITHMS .\n",
            "\n",
            "896 1000\n",
            "we propose a transcription on GRAPHS of recent CONTINUOUS GLOBAL ACTIVE CONTOURS proposed for IMAGE SEGMENTATION to address the problem of BINARY PARTITIONING OF DATA represented by GRAPHS . to do so , using the framework of PARTIAL DIFFERENCE EQUATIONS , we propose a family of NONLOCAL REGULAR-IZATION FUNCTIONALS that verify the CO-AREA FORMULA on GRAPHS . the GRADIENTS of a SUB-GRAPH are introduced and their properties studied . relations , for the case of a SUB-GRAPH , between the introduced NONLOCAL REGULARIZATION FUNCTIONALS and NONLO-CAL DISCRETE PERIMETERS are exhibited and the CO-AREA FORMULA on GRAPHS is introduced . finally , NONLOCAL GLOBAL MINIMIZ-ERS can be considered on GRAPHS with the associated energies . experiments show the benefits of the approach for NONLOCAL IMAGE SEGMENTATION and HIGH DIMENSIONAL DATA CLUSTERING . \n",
            "in this paper , we propose a novel method for NONLOCAL IMAGE SEGMENTATION from GRAPHS . the proposed approach is based on the use of NONLOCAL REGULARIZATION FUNCTIONALS , which is a generalization of the traditional CO-AREA FORMULA . the proposed approach is based on the use of NONLOCAL REGULAR-IZATION FUNCTIONALS and NONLOCAL REGULAR-IZATION FUNCTIONALS . the proposed method is based on the use of NONLOCAL REGULARIZATION FUNCTIONALS and NONLOCAL REGULARIZATION FUNCTIONALS . the proposed method is based on the use of NONLOCAL REGULAR-IZATION FUNCTIONALS and NONLOCAL REGULARIZATION FUNCTIONALS . the experimental results show that the proposed method outperforms the state-of-the-art methods in terms of IMAGE SEGMENTATION and HIGH DIMENSIONAL DATA CLUSTERING .\n",
            "\n",
            "897 1000\n",
            "content-based recommender systems use PREFERENCE RATINGS and FEATURES that characterize MEDIA to model users ' interests or information needs for making future recommendations . while previously developed in the MUSIC AND TEXT DOMAINS , we present an initial exploration of CONTENT-BASED RECOMMENDATION for SPOKEN DOCUMENTS using a CORPUS OF PUBLIC DOMAIN INTERNET AUDIO . unlike familiar speech technologies of topic identification and SPOKEN DOCUMENT RETRIEVAL , our recommendation task requires a more comprehensive notion of DOCUMENT RELEVANCE than BAGS-OF-WORDS would supply . inspired by MUSIC RECOMMENDER SYSTEMS , we automatically extract a wide variety of CONTENT-BASED FEATURES to characterize NON-LINGUISTIC ASPECTS of the audio such as SPEAKER , language , GENDER , and environment . to combine these HETEROGENEOUS INFORMATION SOURCES into a single RELEVANCE JUDGEMENT , we evaluate FEATURE , score , and HYBRID FUSION TECHNIQUES . our study provides an essential first exploration of the task and clearly demonstrates the value of a MULTISOURCE APPROACH over a BAG-OF-WORDS BASELINE . \n",
            "this paper addresses the problem of SPOKEN DOCUMENT RETRIEVAL for MUSIC RECOMMENDER SYSTEMS . we propose a novel approach to SPOKEN DOCUMENT RETRIEVAL based on RELEVANCE JUDGEMENT and FEATURES . the proposed MULTISOURCE APPROACH is based on the use of HETEROGENEOUS INFORMATION SOURCES and FEATURES to represent the NON-LINGUISTIC ASPECTS . the proposed approach is based on the use of RELEVANCE JUDGEMENT and FEATURES to represent the NON-LINGUISTIC ASPECTS . the proposed MULTISOURCE APPROACH is evaluated on a CORPUS OF PUBLIC DOMAIN INTERNET AUDIO and compared to a BAG-OF-WORDS BASELINE , and the results show that the proposed MULTISOURCE APPROACH significantly outperforms the BAG-OF-WORDS BASELINE and the BAG-OF-WORDS BASELINE .\n",
            "\n",
            "898 1000\n",
            "we describe a new system for labeling speech corpora with HIGH-LEVEL GROUP INTERACTION TAGS , called '' meeting acts . '' the system was motivated by a need to assess work seeking to automatically detect MEETING STYLE using DIALOG ACT INFORMATION . we present information about the relationships seen between DIALOG ACT SEQUENCES and MEETING STYLE to motivate the LABELING PROCESS . we provide a summary of the ANNOTATION SYSTEM and labeling procedure , as well as preliminary <unk> reliability statistics on the ICSI MEETING RECORDER CORPUS . \n",
            "this paper presents a novel approach to the LABELING PROCESS from DIALOG ACT SEQUENCES . the proposed method is based on the use of a LABELING PROCESS and a LABELING PROCESS . the proposed approach is based on the use of a LABELING PROCESS and a LABELING PROCESS . the proposed method is evaluated on a ICSI MEETING RECORDER CORPUS and a MEETING STYLE . the results show that the proposed method is able to detect and track objects in a scene from a single image .\n",
            "\n",
            "899 1000\n",
            "we define a new DIALOGUE MANAGEMENT STRATEGY LIMITED ENQUIRY NEGOTIATION DIALOGUES designed for enabling simple MAN-MACHINE DIALOGUES in which the parameters -lrb- for which the user will supply values -rrb- of a query to a database are <unk> . the choice of which query to make next is also not <unk> . the DIALOGUE MANAGEMENT STRATEGY LIMITED ENQUIRY NEGOTIATION DIALOGUES is simple and intuitive but permits interestingly complex DIALOGUE BEHAVIOUR . we propose DIALOGUE MANAGEMENT STRATEGY LIMITED ENQUIRY NEGOTIATION DIALOGUES as an addition to a DIALOGUE DESIGNER 'S STANDARD COMPONENTS TOOLBOX along with other well-known ideas such as MENU-TRAVERSAL AND SLOT-FILLING . we illustrate the DIALOGUE MANAGEMENT STRATEGY LIMITED ENQUIRY NEGOTIATION DIALOGUES by examining how DIALOGUE MANAGEMENT STRATEGY LIMITED ENQUIRY NEGOTIATION DIALOGUES accounts for interesting but by no means rare data in a <unk> of oz corpus of business trip planning dialogues . finally , we discuss some more theoretical issues arising from the DIALOGUE MANAGEMENT STRATEGY LIMITED ENQUIRY NEGOTIATION DIALOGUES . \n",
            "this paper addresses the problem of DIALOGUE MANAGEMENT STRATEGY LIMITED ENQUIRY NEGOTIATION DIALOGUES for MAN-MACHINE DIALOGUES . we propose a novel approach to the problem of DIALOGUE MANAGEMENT STRATEGY LIMITED ENQUIRY NEGOTIATION DIALOGUES , which is based on a DIALOGUE DESIGNER 'S STANDARD COMPONENTS TOOLBOX . the proposed approach is based on the use of a DIALOGUE DESIGNER 'S STANDARD COMPONENTS TOOLBOX , which is able to deal with MAN-MACHINE DIALOGUES . we demonstrate the effectiveness of the proposed method in the context of DIALOGUE MANAGEMENT STRATEGY LIMITED ENQUIRY NEGOTIATION DIALOGUES .\n",
            "\n",
            "900 1000\n",
            "the recently introduced M-VECTOR APPROACH uses MAXIMUM LIKELIHOOD LINEAR REGRESSION SUPER-VECTORS for SPEAKER VERIFICATION , where MAXIMUM LIKELIHOOD LINEAR REGRESSION SUPER-VECTORS are estimated with respect to a UNIVERSAL BACKGROUND MODEL without any TRANSCRIPTION OF SPEECH SEGMENTS and speaker <unk> are obtained by UNIFORM SEGMENTATION of their MAXIMUM LIKELIHOOD LINEAR REGRESSION SUPER-VECTORS . hence , this M-VECTOR APPROACH does not exploit the PHONETIC CONTENT of the speech segments . in this paper , we propose the integration of an automatic speech recognition -lrb- asr -rrb- based multi-class mllr transformation into the M-VECTOR APPROACH . we consider two variants , with MAXIMUM LIKELIHOOD LINEAR REGRESSION SUPER-VECTORS computed either on the 1-BEST -LRB- HYPOTHESIS -rrb- or on the LATTICE WORD TRANSCRIPTIONS . the former case is able to account for the risk of ASR TRANSCRIPTION ERRORS . we show that the proposed M-VECTOR APPROACH outperform the conventional M-VECTOR APPROACH over various tasks of the NIST SRE 2008 CORE CONDITION . \n",
            "this paper presents a novel M-VECTOR APPROACH for SPEAKER VERIFICATION . the proposed M-VECTOR APPROACH is based on a UNIVERSAL BACKGROUND MODEL of the UNIVERSAL BACKGROUND MODEL . the proposed M-VECTOR APPROACH is based on the use of a UNIVERSAL BACKGROUND MODEL to estimate the PHONETIC CONTENT . the proposed M-VECTOR APPROACH is based on the use of a UNIVERSAL BACKGROUND MODEL to estimate the PHONETIC CONTENT and the PHONETIC CONTENT . the proposed M-VECTOR APPROACH is evaluated on the NIST SRE 2008 CORE CONDITION and compared to the conventional M-VECTOR APPROACH . the proposed M-VECTOR APPROACH outperforms the conventional M-VECTOR APPROACH in terms of both NIST SRE 2008 CORE CONDITION and ASR TRANSCRIPTION ERRORS .\n",
            "\n",
            "901 1000\n",
            "in some SPEAKER RECOGNITION SCENARIOS we find conversations recorded simultaneously over multiple channels . that is the case of the interviews in the NIST SRE DATASET . to take advantage of that , we propose a modification of the PLDA MODEL that considers two different INTER-SESSION VARIABILITY TERMS . the first term is tied between all the recordings belonging to the same conversation whereas the second is not . thus , the former mainly intends to capture the variability due to the PHONETIC CONTENT of the conversation while the latter tries to capture the CHANNEL VARIABILITY . we test this PLDA MODEL on the NIST SRE12 CORE CONDITION using multiple channels per interview to <unk> the speakers . the proposed PLDA MODEL improves the MINIMUM DCF by 26 -- 29 % on TELEPHONE SPEECH and by 1 -- 8 % on interviews compared to the standard PLDA -lrb- scored by the book -rrb- . \n",
            "this paper presents a novel PLDA MODEL for SPEAKER RECOGNITION SCENARIOS . the proposed PLDA MODEL is based on the use of a set of PLDA , each of which is trained on the basis of the PHONETIC CONTENT . the proposed PLDA MODEL is evaluated on the NIST SRE DATASET , and the results show that the proposed PLDA MODEL achieves a MINIMUM DCF of about 2 % on the NIST SRE DATASET . the proposed PLDA MODEL has a MINIMUM DCF of o -lrb- 1 / / -rrb- , and the MINIMUM DCF of the PLDA MODEL is comparable to that of the conventional PLDA MODEL . the proposed PLDA MODEL is evaluated on the NIST SRE DATASET , and the results show that the proposed PLDA MODEL is effective in SPEAKER RECOGNITION SCENARIOS .\n",
            "\n",
            "902 1000\n",
            "in this paper , we present a method that combines the merits of BAYESIAN NONPARAMETRICS , specifically STICK-BREAKING PRIORS , and LARGE-MARGIN KERNEL MACHINES in the context of SEQUENTIAL DATA CLASSIFICATION . the proposed model employs a set of -lrb- theoretically -rrb- infinite interdependent LARGE-MARGIN CLASSIFIERS as MODEL COMPONENTS , that robustly capture LOCAL NONLINEARITY OF COMPLEX DATA . the employed LARGE-MARGIN CLASSIFIERS are connected in the context of a MARKOV-SWITCHING CONSTRUCTION that allows for capturing COMPLEX TEMPORAL DYNAMICS in the MODELED DATASETS . appropriate STICK-BREAKING PRIORS are imposed over the COMPONENT SWITCHING MECHANISM of our model to allow for data-driven determination of the optimal number of COMPONENT LARGE-MARGIN CLASSIFIERS , under a standard NONPARAMETRIC BAYESIAN INFERENCE SCHEME . EFFICIENT MODEL TRAINING is performed under the MAXIMUM ENTROPY DISCRIMINATION FRAMEWORK , which integrates the LARGE-MARGIN PRINCIPLE with BAYESIAN POSTERIOR INFERENCE . we evaluate our method using several REAL-WORLD DATASETS , and compare it to state-of-the-art alternatives . \n",
            "this paper addresses the problem of EFFICIENT MODEL TRAINING for SEQUENTIAL DATA CLASSIFICATION . in particular , we focus on the problem of EFFICIENT MODEL TRAINING , where the MODEL COMPONENTS is a LARGE-MARGIN PRINCIPLE , and the COMPONENT SWITCHING MECHANISM is a NONPARAMETRIC BAYESIAN INFERENCE SCHEME . in this paper , we propose a novel NONPARAMETRIC BAYESIAN INFERENCE SCHEME to the problem of EFFICIENT MODEL TRAINING . the proposed NONPARAMETRIC BAYESIAN INFERENCE SCHEME is based on a MAXIMUM ENTROPY DISCRIMINATION FRAMEWORK , which is a NONPARAMETRIC BAYESIAN INFERENCE SCHEME of the COMPONENT SWITCHING MECHANISM . the proposed NONPARAMETRIC BAYESIAN INFERENCE SCHEME is based on a NONPARAMETRIC BAYESIAN INFERENCE SCHEME and the LARGE-MARGIN PRINCIPLE . the proposed NONPARAMETRIC BAYESIAN INFERENCE SCHEME is applied to SEQUENTIAL DATA CLASSIFICATION . the experimental results on REAL-WORLD DATASETS demonstrate the effectiveness of the proposed NONPARAMETRIC BAYESIAN INFERENCE SCHEME .\n",
            "\n",
            "903 1000\n",
            "suppose rate of change of <unk> of a LINEAR TIME-VARIANT SYSTEM modeled via a DIERENCE EQUATION is restricted . the work presented herein is an attempt at developing an algorithm that determines regions in <unk> where such a LINEAR TIME-VARIANT SYSTEM is guaranteed to be globally asymptotically stable . such information can be extremely useful in many applications . some previously published related results are <unk> as well . \n",
            "this paper presents a novel approach to the problem of DIERENCE EQUATION . the proposed method is based on the use of a LINEAR TIME-VARIANT SYSTEM , which is based on the DIERENCE EQUATION . the proposed method is based on the use of the DIERENCE EQUATION of the source and target speaker . the experimental results show that the proposed LINEAR TIME-VARIANT SYSTEM is effective in improving the performance of the LINEAR TIME-VARIANT SYSTEM .\n",
            "\n",
            "904 1000\n",
            "beat tracking estimation from MUSIC SIGNALS becomes difficult in the presence of HIGHLY PREDOMINANT VOCALS . we compare the performance of five state-of-the-art algorithms on two datasets , a GENERIC ANNOTATED COLLECTION and a dataset comprised of SONG EXCERPTS with HIGHLY PREDOMINANT VOCALS . then , we use seven state-of-the-art AUDIO VOICE SUPPRESSION TECHNIQUES and a simple LOW PASS FILTER to improve BEAT TRACKING ESTIMATIONS in the later case . finally , we evaluate all the PAIRWISE COMBINATIONS between BEAT TRACKING and VOICE SUPPRESSION METHODS . we confirm our hypothesis that VOICE SUPPRESSION improves the mean performance of BEAT TRACKERS for the predominant vocal collection . \n",
            "this paper presents a novel approach to BEAT TRACKING ESTIMATION in MUSIC SIGNALS . the proposed approach is based on the use of a LOW PASS FILTER , a LOW PASS FILTER for BEAT TRACKING ESTIMATION and VOICE SUPPRESSION METHODS . the proposed approach is based on the use of a LOW PASS FILTER , which is based on a LOW PASS FILTER . the proposed approach is evaluated on a variety of MUSIC SIGNALS . the results show that the proposed method is effective in improving the performance of BEAT TRACKING ESTIMATION and VOICE SUPPRESSION METHODS .\n",
            "\n",
            "905 1000\n",
            "spoken term detection is a well-known INFORMATION RETRIEVAL TASK that seeks to extract CONTENTFUL INFORMATION from AUDIO by locating occurrences of KNOWN QUERY WORDS OF INTEREST . this paper describes a ZERO-RESOURCE APPROACH to such SPOKEN TERM DETECTION based on pattern matching of SPOKEN TERM QUERIES at the ACOUSTIC LEVEL . the TEMPLATE MATCHING MODULE comprises the cascade of a segmental variant of DYNAMIC TIME WARPING and a SELF-SIMILARITY MATRIX COMPARISON to further improve ROBUSTNESS to SPEECH VARIABILITY . this ZERO-RESOURCE APPROACH notably differs from more traditional TRAIN AND TEST METHODS that , while shown to be very accurate , rely upon the availability of large amounts of LINGUISTIC RESOURCES . we evaluate our ZERO-RESOURCE APPROACH on different <unk> of the SPEECH TEMPLATES : RAW MFCC FEATURES and GAUSSIAN POSTERIORGRAMS , FRENCH AND ENGLISH PHONETIC POSTERI-ORGRAMS output by two different state of the art phoneme <unk> . \n",
            "in this paper , we propose a novel ZERO-RESOURCE APPROACH for SPOKEN TERM DETECTION . the proposed ZERO-RESOURCE APPROACH is based on the use of a TEMPLATE MATCHING MODULE , GAUSSIAN POSTERIORGRAMS , GAUSSIAN POSTERIORGRAMS , and SELF-SIMILARITY MATRIX COMPARISON . the proposed ZERO-RESOURCE APPROACH is based on the use of a TEMPLATE MATCHING MODULE and a SELF-SIMILARITY MATRIX COMPARISON . the proposed ZERO-RESOURCE APPROACH is based on the use of a TEMPLATE MATCHING MODULE and a SELF-SIMILARITY MATRIX COMPARISON . the proposed ZERO-RESOURCE APPROACH is evaluated on a variety of LINGUISTIC RESOURCES including FRENCH AND ENGLISH PHONETIC POSTERI-ORGRAMS , GAUSSIAN POSTERIORGRAMS , GAUSSIAN POSTERIORGRAMS , and SELF-SIMILARITY MATRIX COMPARISON . the experimental results show that the proposed ZERO-RESOURCE APPROACH is effective in improving the ROBUSTNESS and ROBUSTNESS of the proposed ZERO-RESOURCE APPROACH .\n",
            "\n",
            "906 1000\n",
            "communication among participants -lrb- agents , robots -rrb- is central to an appearance of COLLECTIVE AI . in this work we deal with the development of LOCAL COMMUNICATION MECHANISMS for REAL MICRORO-BOTIC SWARMS . we demonstrate that despite of very limited capabilities of the MICROROBOT , the specific construction of COMMUNICATION HARDWARE and software allows very extended collective capabilities of the whole SWARM . we propose LOCAL COMMUNICATION MECHANISMS providing INFORMATION CONTENT and context for COLLECTIVE NAVIGATION , coordination and spatial perception in a group of <unk> . \n",
            "this paper addresses the problem of COLLECTIVE NAVIGATION in COLLECTIVE AI . in particular , we focus on the problem of COLLECTIVE NAVIGATION in the context of COLLECTIVE AI . in particular , we consider the problem of COLLECTIVE NAVIGATION in the form of a SWARM . we show that this problem can be solved efficiently using LOCAL COMMUNICATION MECHANISMS . we then show how LOCAL COMMUNICATION MECHANISMS can be applied to the problem of COLLECTIVE NAVIGATION in COLLECTIVE AI .\n",
            "\n",
            "907 1000\n",
            "fisher linear discriminant analysis -lrb- lda -rrb- can be sensitive to the problem data . ROBUST FISHER LDA can systematically alleviate the SENSITIVITY PROBLEM by explicitly incorporating a model of DATA UNCERTAINTY in a CLASSIFICATION PROBLEM and optimizing for the worst-case scenario under this model . the main contribution of this paper is show that with general CONVEX UNCERTAINTY MODELS on the problem data , robust FISHER LINEAR DISCRIMINANT ANALYSIS can be carried out using CONVEX OPTIMIZATION . for a certain type of PRODUCT FORM UNCERTAINTY MODEL , robust FISHER LINEAR DISCRIMINANT ANALYSIS can be carried out at a cost comparable to standard FISHER LINEAR DISCRIMINANT ANALYSIS . the method is demonstrated with some numerical examples . finally , we show how to extend these results to ROBUST KERNEL FISHER DISCRIMINANT ANALYSIS , i.e. , robust FISHER LINEAR DISCRIMINANT ANALYSIS in a HIGH DIMENSIONAL FEATURE SPACE . \n",
            "in this paper , we propose a novel approach to ROBUST KERNEL FISHER DISCRIMINANT ANALYSIS in the context of ROBUST KERNEL FISHER DISCRIMINANT ANALYSIS . the proposed approach is based on the idea of CONVEX OPTIMIZATION , which is based on a PRODUCT FORM UNCERTAINTY MODEL . the proposed method is based on a PRODUCT FORM UNCERTAINTY MODEL . the proposed method is based on a PRODUCT FORM UNCERTAINTY MODEL . the proposed method is based on a PRODUCT FORM UNCERTAINTY MODEL . the proposed method is applied to the CLASSIFICATION PROBLEM of the CLASSIFICATION PROBLEM . the proposed method is evaluated on a CLASSIFICATION PROBLEM . the experimental results show that the proposed method achieves better performance than the state-of-the-art methods .\n",
            "\n",
            "908 1000\n",
            "we present an algorithm to find a LOW-DIMENSIONAL DECOMPOSITION of a spectrogram by formulating LOW-DIMENSIONAL DECOMPOSITION as a REGULARIZED NON-NEGATIVE MATRIX FACTORIZATION PROBLEM with a REGULARIZATION TERM chosen to encourage independence . this algorithm provides a better decomposition than standard NMF when the underlying sources are independent . it is directly applicable to NON-SQUARE MATRICES , and it makes better use of additional OBSERVATION STREAMS than previous NONNEGATIVE ICA ALGORITHMS . \n",
            "this paper addresses the problem of REGULARIZED NON-NEGATIVE MATRIX FACTORIZATION PROBLEM in the presence of NON-SQUARE MATRICES . in particular , we focus on the problem of REGULARIZED NON-NEGATIVE MATRIX FACTORIZATION PROBLEM in the presence of NON-SQUARE MATRICES . in particular , we consider the problem of REGULARIZED NON-NEGATIVE MATRIX FACTORIZATION PROBLEM in the presence of NON-SQUARE MATRICES . in particular , we show that the REGULARIZED NON-NEGATIVE MATRIX FACTORIZATION PROBLEM can be solved efficiently using LOW-DIMENSIONAL DECOMPOSITION . we also show that the REGULARIZATION TERM of the OBSERVATION STREAMS can be obtained by a LOW-DIMENSIONAL DECOMPOSITION .\n",
            "\n",
            "909 1000\n",
            "in this paper , we investigate the technical challenges that are faced when making a transition from the SPEAKER-DEPENDENT to SPEAKER-INDEPENDENT SPEECH RECOGNITION TECHNOLOGY in MOBILE COMMUNICATION DEVICES . due to <unk> as well as the international nature of the markets and the future applications , SPEAKER INDEPENDENCE implies the development and use of LANGUAGE-INDEPENDENT ASR to avoid LOGISTIC DIFFICULTIES . we propose here an architecture for EMBEDDED MULTILINGUAL SPEECH RECOGNITION SYSTEMS . MULTILINGUAL ACOUSTIC MODELING , AUTOMATIC LANGUAGE IDENTIFICATION , and ON-LINE PRONUNCIATION MODELING are the key features which enable the creation of truly LANGUAGE-AND SPEAKER-INDEPENDENT ASR APPLICATIONS with DYNAMIC VOCABULARIES and SPARSE IMPLEMENTATION RESOURCES . our experimental results confirm the viability of the proposed architecture . while the use of MULTILINGUAL ACOUSTIC MODELS degrades the RECOGNITION RATES only marginally , a RECOGNITION ACCURACY decrease of approximately 4 % is observed due to SUB-OPTIMAL ON-LINE TEXT-TO-PHONEME MAPPING and AUTOMATIC LANGUAGE IDENTIFICATION . this performance loss can nevertheless be compensated by applying ACOUSTIC MODEL ADAPTATION TECHNIQUES . \n",
            "this paper presents a novel approach to MULTILINGUAL ACOUSTIC MODELING in MOBILE COMMUNICATION DEVICES . the proposed approach is based on the use of MULTILINGUAL ACOUSTIC MODELS and ACOUSTIC MODEL ADAPTATION TECHNIQUES . the proposed approach is based on the use of MULTILINGUAL ACOUSTIC MODELS and ACOUSTIC MODEL ADAPTATION TECHNIQUES . the proposed approach is based on the use of MULTILINGUAL ACOUSTIC MODELING and MULTILINGUAL ACOUSTIC MODELING . the proposed method is evaluated on a variety of LANGUAGE-AND SPEAKER-INDEPENDENT ASR APPLICATIONS including SPEAKER-DEPENDENT , AUTOMATIC LANGUAGE IDENTIFICATION and AUTOMATIC LANGUAGE IDENTIFICATION . the experimental results show that the proposed approach is effective in improving the RECOGNITION ACCURACY of EMBEDDED MULTILINGUAL SPEECH RECOGNITION SYSTEMS in LANGUAGE-AND SPEAKER-INDEPENDENT ASR APPLICATIONS .\n",
            "\n",
            "910 1000\n",
            "this paper studies the ERROR SENSITIVITY in the estimation of the 3D-MOTION and the normal of a PLANAR SURFACE from an INSTANTANEOUS MOTION ELD . we use the STATISTICAL THEORY of the cramer-rao lower bound for the ERROR CO-VARIANCE in the ESTIMATED MOTION AND STRUCTURE P ARAMETERS which enables the derivation of results valid for any UNBI-ASED ESTIMATOR under the ASSUMPTION OF GAUSSIAN NOISE in the MOTION ELD . the obtained LOWER-BOUND-MATRIX is studied analytically with respect to the MEASUREMENT NOISE , size of the ELD OF VIEW and the MOTION-GEOMETRY CONNGURATION . the main result of this analysis is the coupling between TRANSLATION and ROTATION which is exacerbated if the ELD OF VIEW and the SLANT of the plane become smaller and the deviation of the TRANSLATION from the VIEWING DIRECTION becomes larger . <unk> of this study are the relationships o f the UNCERTAINTY BOUNDS for every UNKNOWN MOTION PARAMETER to the angle between TRANSLATION and the <unk> , the size of the ELD OF VIEW , the distance f r om the PERCEIVED PLANE and the TRANSLATION MAGNITUDE . \n",
            "in this paper , we propose a novel method to recover the INSTANTANEOUS MOTION ELD of a scene from a PLANAR SURFACE . the proposed method is based on the ESTIMATED MOTION AND STRUCTURE P ARAMETERS and the INSTANTANEOUS MOTION ELD of the INSTANTANEOUS MOTION ELD . the proposed method is based on the ESTIMATED MOTION AND STRUCTURE P ARAMETERS and the INSTANTANEOUS MOTION ELD of the INSTANTANEOUS MOTION ELD . the proposed method is based on the ESTIMATED MOTION AND STRUCTURE P ARAMETERS and the MOTION-GEOMETRY CONNGURATION of the signal . the proposed method is based on the ESTIMATED MOTION AND STRUCTURE P ARAMETERS and the UNCERTAINTY BOUNDS of the INSTANTANEOUS MOTION ELD . the proposed method is based on a MOTION-GEOMETRY CONNGURATION and a MOTION-GEOMETRY CONNGURATION . the proposed method is evaluated on a MOTION-GEOMETRY CONNGURATION and a MOTION-GEOMETRY CONNGURATION . the results show that the proposed method is robust to MEASUREMENT NOISE , ROTATION , and ROTATION .\n",
            "\n",
            "911 1000\n",
            "<unk> , a HARMONET employing CONNECTIONIST NETWORKS for MUSIC PROCESSING , is presented . after being trained on some dozen BACH CHORALES using ERROR BACKPROPAGATION , the HARMONET is capable of producing FOUR-PART CHORALES in the style of j . <unk> , given a ONE-PART MELODY . our HARMONET solves a MUSICAL REAL-WORLD PROBLEM on a performance level appropriate for MUSICAL PRACTICE . HARMONET 'S POWER is based on -lrb- a -rrb- a new CODING SCHEME capturing MUSICALLY RELEVANT INFORMATION and -lrb- b -rrb- the integration of BACKPROPAGATION AND SYMBOLIC ALGORITHMS in a HIERARCHICAL SYSTEM , combining the advantages of both . \n",
            "this paper presents a novel CODING SCHEME for MUSIC PROCESSING . the HARMONET is based on the use of CONNECTIONIST NETWORKS for MUSIC PROCESSING . the HARMONET is based on the use of CONNECTIONIST NETWORKS for MUSIC PROCESSING . the proposed CODING SCHEME is based on the use of CONNECTIONIST NETWORKS for MUSIC PROCESSING . the HARMONET is based on the use of CONNECTIONIST NETWORKS for MUSIC PROCESSING . the proposed CODING SCHEME is applied to the MUSICAL REAL-WORLD PROBLEM of the ONE-PART MELODY , and the results show that the proposed CODING SCHEME is effective in improving the HARMONET 'S POWER .\n",
            "\n",
            "912 1000\n",
            "large-scale action recognition and VIDEO CATEGORIZATION are important problems in COMPUTER VISION . to address these problems , we propose a novel OBJECT-AND SCENE-BASED SEMANTIC FUSION NETWORK and representation . our OBJECT-AND SCENE-BASED SEMANTIC FUSION NETWORK combines three streams of information using a THREE-LAYER NEURAL NETWORK : -lrb- i -rrb- frame-based low-level cnn features , -lrb- ii -rrb- object features from a state-of-the-art LARGE-SCALE CNN OBJECT-DETECTOR trained to recognize 20k classes , and -lrb- iii -rrb- scene features from a state-of-the-art CNN SCENE-DETECTOR trained to recognize <unk> scenes . the trained OBJECT-AND SCENE-BASED SEMANTIC FUSION NETWORK achieves improvements in SUPERVISED ACTIVITY and VIDEO CATEGORIZATION in two complex LARGE-SCALE DATASETS-ACTIVITYNET and <unk> , respectively . further , by examining and back propagating information through the FUSION NETWORK , SEMANTIC RELATIONSHIPS -LRB- CORRELATIONS -rrb- between VIDEO CLASSES and <unk> can be discovered . these VIDEO CLASS-OBJECT/VIDEO CLASS-SCENE RELATIONSHIPS can in turn be used as SEMANTIC REPRESENTATION for the VIDEO CLASSES themselves . we illustrate effectiveness of this SEMANTIC REPRESENTATION through experiments on ZERO-SHOT ACTION/VIDEO CLASSIFICATION and CLUSTERING . \n",
            "this paper presents a novel OBJECT-AND SCENE-BASED SEMANTIC FUSION NETWORK based on a THREE-LAYER NEURAL NETWORK . the proposed OBJECT-AND SCENE-BASED SEMANTIC FUSION NETWORK is based on a THREE-LAYER NEURAL NETWORK . the proposed OBJECT-AND SCENE-BASED SEMANTIC FUSION NETWORK is based on a THREE-LAYER NEURAL NETWORK . the proposed OBJECT-AND SCENE-BASED SEMANTIC FUSION NETWORK is based on the use of SEMANTIC RELATIONSHIPS -LRB- CORRELATIONS and CLUSTERING . the proposed OBJECT-AND SCENE-BASED SEMANTIC FUSION NETWORK is based on the use of SEMANTIC RELATIONSHIPS -LRB- CORRELATIONS and CLUSTERING . the proposed OBJECT-AND SCENE-BASED SEMANTIC FUSION NETWORK is evaluated on both VIDEO CATEGORIZATION and VIDEO CATEGORIZATION . the experimental results show that the proposed OBJECT-AND SCENE-BASED SEMANTIC FUSION NETWORK is effective in improving LARGE-SCALE ACTION RECOGNITION and LARGE-SCALE ACTION RECOGNITION .\n",
            "\n",
            "913 1000\n",
            "we present a procedure to automatically derive INTER-PRETABLE DYNAMIC ARTICULATORY PRIMITIVES in a DATA-DRIVEN MANNER from IMAGE SEQUENCES acquired through REAL-TIME MAGNETIC RESONANCE IMAGING . more specifically , we propose a convolutive nonnegative matrix factorization algorithm with SPARSENESS CONSTRAINTS to decompose a given set of IMAGE SEQUENCES into a set of basis IMAGE SEQUENCES and an ACTIVATION MATRIX . we use a RECENTLY-ACQUIRED RT-MRI CORPUS of read speech -lrb- <unk> sentences from 4 speakers -rrb- as a test dataset for this procedure . we choose the free parameters of the algorithm empirically by analyzing algorithm performance for different PARAMETER VALUES . we then validate the extracted basis sequences using an ARTICULATORY RECOGNITION TASK and finally present an interpretation of the extracted basis set of IMAGE SEQUENCES in a GESTURE-BASED ARTICULATORY PHONOLOGY FRAMEWORK . \n",
            "this paper addresses the problem of REAL-TIME MAGNETIC RESONANCE IMAGING for REAL-TIME MAGNETIC RESONANCE IMAGING . the GESTURE-BASED ARTICULATORY PHONOLOGY FRAMEWORK is based on the use of INTER-PRETABLE DYNAMIC ARTICULATORY PRIMITIVES , which is a DATA-DRIVEN MANNER of the ACTIVATION MATRIX . the proposed GESTURE-BASED ARTICULATORY PHONOLOGY FRAMEWORK is based on the use of INTER-PRETABLE DYNAMIC ARTICULATORY PRIMITIVES , which is a DATA-DRIVEN MANNER of the ACTIVATION MATRIX . the proposed GESTURE-BASED ARTICULATORY PHONOLOGY FRAMEWORK is applied to the problem of REAL-TIME MAGNETIC RESONANCE IMAGING in a DATA-DRIVEN MANNER . the performance of the proposed GESTURE-BASED ARTICULATORY PHONOLOGY FRAMEWORK is evaluated on a RECENTLY-ACQUIRED RT-MRI CORPUS of the RECENTLY-ACQUIRED RT-MRI CORPUS . the results show that the proposed method is effective in REAL-TIME MAGNETIC RESONANCE IMAGING .\n",
            "\n",
            "914 1000\n",
            "in many MACHINE LEARNING APPLICATIONS , labeling every instance of data is <unk> . MULTIPLE INSTANCE LEARNING , in which training data is provided in the form of labeled bags rather than labeled instances , is one approach for a more relaxed form of SUPERVISED LEARNING . though much progress has been made in analyzing MIL PROBLEMS , existing work considers bags that have a finite number of instances . in this paper we argue that in many applications of MULTIPLE INSTANCE LEARNING -lrb- e.g. IMAGE , AUDIO , etc. -rrb- the bags are better modeled as LOW DIMENSIONAL MANIFOLDS in HIGH DIMENSIONAL FEATURE SPACE . we show that the GEOMETRIC STRUCTURE of such MANIFOLD BAGS affects PAC-LEARNABILITY . we discuss how a learning algorithm that is designed for FINITE SIZED BAGS can be adapted to learn from MANI-FOLD BAGS . furthermore , we propose a simple HEURISTIC that reduces the MEMORY REQUIREMENTS of such algorithms . our experiments on REAL-WORLD DATA validate our analysis and show that our approach works well . \n",
            "this paper addresses the problem of MULTIPLE INSTANCE LEARNING from REAL-WORLD DATA . we propose a novel approach to the problem of MULTIPLE INSTANCE LEARNING , which is important in MACHINE LEARNING APPLICATIONS and MACHINE LEARNING APPLICATIONS . the key idea is to use a HEURISTIC to estimate the GEOMETRIC STRUCTURE of the IMAGE , and the GEOMETRIC STRUCTURE of the IMAGE are estimated using the HEURISTIC . the proposed approach is based on a novel HEURISTIC , which is able to deal with LOW DIMENSIONAL MANIFOLDS , AUDIO , and AUDIO . we demonstrate the effectiveness of our method on a variety of REAL-WORLD DATA and REAL-WORLD DATA .\n",
            "\n",
            "915 1000\n",
            "visual recognition systems for videos using STATISTICAL LEARNING MODELS often show degraded performance when being deployed to a REAL-WORLD ENVIRONMENT , primarily due to the fact that training data can hardly cover sufficient variations in reality . to alleviate this issue , we propose to utilize the OBJECT CORRESPONDENCES in SUCCESSIVE FRAMES as WEAK SUPERVISION to adapt VISUAL RECOGNITION MODELS , which is particularly suitable for HUMAN PROFILE RECOGNITION . specifically , we <unk> this new strategy on an advanced CONVOLUTIONAL NEURAL NETWORK BASED SYSTEM to estimate HUMAN GENDER , age , and race . we enforce the system to output consistent and stable results on FACE IMAGES from the same trajectories in videos by using INCREMENTAL STOCHASTIC TRAINING . our baseline system already achieves competitive performance on GENDER AND AGE ESTIMATION as compared to the state-of-the-art algorithms on the FG-NET DATABASE . further , on two new VIDEO DATASETS containing about <unk> persons , the proposed SUPERVISION OF CORRESPONDENCES improves the ESTIMATION ACCURACY by a large margin over the baseline . \n",
            "this paper presents a novel approach to HUMAN PROFILE RECOGNITION based on STATISTICAL LEARNING MODELS . the proposed approach is based on the use of STATISTICAL LEARNING MODELS for HUMAN PROFILE RECOGNITION . the proposed approach is based on the use of SUCCESSIVE FRAMES extracted from the FG-NET DATABASE . the proposed approach is based on the use of SUCCESSIVE FRAMES extracted from the SUCCESSIVE FRAMES of the SUCCESSIVE FRAMES . the proposed approach is evaluated on a FG-NET DATABASE and on a FG-NET DATABASE . the results show that the proposed approach is effective in improving the ESTIMATION ACCURACY and ESTIMATION ACCURACY of the CONVOLUTIONAL NEURAL NETWORK BASED SYSTEM .\n",
            "\n",
            "916 1000\n",
            "this paper describes a MODIFIED COMPOSITION ALGORITHM that is used for combining two FINITE-STATE TRANSDUCERS , representing the CONTEXT-DEPENDENT LEXICON and the LANGUAGE MODEL respectively , in LARGE VOCABULARY SPEECH RECOGNTION . this MODIFIED COMPOSITION ALGORITHM is a hybrid between the static and dynamic expansion of the resultant transducer , which maps from CONTEXT-DEPENDENT PHONES to words and is searched during DECODING . the MODIFIED COMPOSITION ALGORITHM is to <unk> part of the RECOGNITION TRANSDUCER and leave the balance to be expanded during DECODING . this MODIFIED COMPOSITION ALGORITHM allows for a FINE-GRAINED TRADE-OFF between space and time in recognition . for example , the time overhead of purely dynamic expansion can be reduced by over <unk> with only a 20 % increase in memory in a collection of LARGE-VOCABULARY RECOGNITION TASKS available on the GOOGLE ANDROID PLATFORM . \n",
            "this paper addresses the problem of LARGE VOCABULARY SPEECH RECOGNTION in a LARGE VOCABULARY SPEECH RECOGNTION . the goal of this paper is to develop a MODIFIED COMPOSITION ALGORITHM for LARGE-VOCABULARY RECOGNITION TASKS . the proposed approach is based on the use of a MODIFIED COMPOSITION ALGORITHM and a LANGUAGE MODEL . the proposed approach is based on a MODIFIED COMPOSITION ALGORITHM and a LANGUAGE MODEL , a CONTEXT-DEPENDENT LEXICON and a LANGUAGE MODEL . the proposed method is evaluated on a GOOGLE ANDROID PLATFORM and a LANGUAGE MODEL . the results show that the proposed method is effective in improving the FINE-GRAINED TRADE-OFF of the GOOGLE ANDROID PLATFORM .\n",
            "\n",
            "917 1000\n",
            "<unk> object matching is challenging due to IMAGE DISTORTIONS caused by several factors such as ROTATION , TRANSLATION , ILLUMINATION , CROPPING and OCCLUSION . we propose a COMPACT , GLOBAL IMAGE DESCRIPTOR for MANHATTAN SCENES that captures RELATIVE LOCATIONS and strengths of edges along vanishing directions . to construct the DESCRIPTOR , an EDGE MAP is determined per vanishing point , capturing the EDGE STRENGTHS over a range of angles measured at the vanishing point . for matching , descriptors from two scenes are compared across multiple candidate scales and displacements . the matching performance is refined by comparing EDGE SHAPES at the LOCAL MAXIMA of the SCALE-DISPLACEMENT PLOTS . the proposed GLOBAL IMAGE DESCRIPTOR achieves an equal ERROR RATE of 7 % for the ZURICH BUILDINGS DATABASE , indicating significant gains in DISCRIMINATIVE ABILITY over other GLOBAL DESCRIPTORS that rely on AGGREGATE IMAGE STATISTICS but do not exploit the underlying SCENE GEOMETRY . \n",
            "this paper addresses the problem of VIEWPOINT-INVARIANT OBJECT MATCHING from MANHATTAN SCENES . we propose a novel method for VIEWPOINT-INVARIANT OBJECT MATCHING based on AGGREGATE IMAGE STATISTICS . the proposed method consists of two steps : -lrb- 1 -rrb- a set of GLOBAL DESCRIPTORS to estimate the RELATIVE LOCATIONS ; -lrb- 2 -rrb- a COMPACT , GLOBAL IMAGE DESCRIPTOR to estimate the RELATIVE LOCATIONS , and -lrb- 3 -rrb- a COMPACT , GLOBAL IMAGE DESCRIPTOR to estimate the RELATIVE LOCATIONS . the proposed method is based on the use of a set of GLOBAL DESCRIPTORS , which are used to estimate the RELATIVE LOCATIONS . the proposed method is evaluated on a ZURICH BUILDINGS DATABASE and a ZURICH BUILDINGS DATABASE . the results show that the proposed method outperforms the state-of-the-art methods in terms of ERROR RATE , ROTATION , ROTATION and OCCLUSION .\n",
            "\n",
            "918 1000\n",
            "in this paper we propose a ROBUST ITERATIVE HARD THRESOLDING ALGORITHM for RECONSTRUCTING SPARSE SIGNALS in the presence of IMPULSIVE NOISE . to address this problem , we use a LORENTZIAN COST FUNCTION instead of the í µí ° ¿ 2 cost function employed by the traditional IHT ALGORITHM . the derived ROBUST ITERATIVE HARD THRESOLDING ALGORITHM is comparable in COMPUTATIONAL LOAD to the least squares based IHT . analysis of the proposed ROBUST ITERATIVE HARD THRESOLDING ALGORITHM demonstrates its ROBUSTNESS under HEAVY-TAILED MODELS . simulations show that the proposed ROBUST ITERATIVE HARD THRESOLDING ALGORITHM significantly outperform commonly employed SPARSE RECONSTRUCTION TECHNIQUES in IMPULSIVE ENVIRONMENTS , while providing comparable RECONSTRUCTION QUALITY in less demanding , <unk> environments . \n",
            "this paper presents a novel ROBUST ITERATIVE HARD THRESOLDING ALGORITHM for RECONSTRUCTING SPARSE SIGNALS . the proposed ROBUST ITERATIVE HARD THRESOLDING ALGORITHM is based on a ROBUST ITERATIVE HARD THRESOLDING ALGORITHM , which is a generalization of the IHT ALGORITHM to RECONSTRUCTING SPARSE SIGNALS . the proposed ROBUST ITERATIVE HARD THRESOLDING ALGORITHM is based on a ROBUST ITERATIVE HARD THRESOLDING ALGORITHM , which is able to deal with IMPULSIVE NOISE in the presence of IMPULSIVE NOISE . the proposed ROBUST ITERATIVE HARD THRESOLDING ALGORITHM is applied to the problem of RECONSTRUCTING SPARSE SIGNALS . the experimental results show that the proposed ROBUST ITERATIVE HARD THRESOLDING ALGORITHM significantly improves ROBUSTNESS in terms of ROBUSTNESS and ROBUSTNESS .\n",
            "\n",
            "919 1000\n",
            "we analyze the bit error probability of MULTIUSER DEMODULATORS for DIRECT-SEQUENCE BINARY PHASE-SHIFT-KEYING CDMA CHANNEL with ADDITIVE GAUSSIAN NOISE . the problem of MULTIUSER DEMODULATORS is cast into the FINITE-TEMPERATURE DECODING PROBLEM , and REPLICA ANALYSIS is applied to evaluate the performance of the resulting MPM -LRB- MARGINAL POSTERIOR MODE -RRB- DEMODULATORS , which include the optimal DEMODULATOR and the MAP DEMODULATOR as special cases . an approximate implementation of MULTIUSER DEMODULATORS is proposed using ANALOG-VALUED HOPFIELD MODEL as a naive MEAN-FIELD APPROXIMATION to the MPM -LRB- MARGINAL POSTERIOR MODE -RRB- DEMODULATORS , and its performance is also evaluated by the REPLICA ANALYSIS . results of the performance evaluation shows effectiveness of the optimal DEMODULATOR and the MEAN-FIELD DEMODULATOR compared with the conventional one , especially in the cases of small INFORMATION BIT RATE and LOW NOISE LEVEL . \n",
            "this paper presents a novel MULTIUSER DEMODULATORS to the FINITE-TEMPERATURE DECODING PROBLEM . the proposed MULTIUSER DEMODULATORS is based on a MPM -LRB- MARGINAL POSTERIOR MODE -RRB- DEMODULATORS with a ANALOG-VALUED HOPFIELD MODEL . the proposed MPM -LRB- MARGINAL POSTERIOR MODE -RRB- DEMODULATORS is based on a MPM -LRB- MARGINAL POSTERIOR MODE -RRB- DEMODULATORS with a ANALOG-VALUED HOPFIELD MODEL . the proposed MPM -LRB- MARGINAL POSTERIOR MODE -RRB- DEMODULATORS is based on a MPM -LRB- MARGINAL POSTERIOR MODE -RRB- DEMODULATORS . the proposed MPM -LRB- MARGINAL POSTERIOR MODE -RRB- DEMODULATORS is applied to the FINITE-TEMPERATURE DECODING PROBLEM in a FINITE-TEMPERATURE DECODING PROBLEM . the performance of the proposed MULTIUSER DEMODULATORS is evaluated in terms of the INFORMATION BIT RATE and INFORMATION BIT RATE . the performance of the proposed MPM -LRB- MARGINAL POSTERIOR MODE -RRB- DEMODULATORS is evaluated on a DIRECT-SEQUENCE BINARY PHASE-SHIFT-KEYING CDMA CHANNEL with LOW NOISE LEVEL .\n",
            "\n",
            "920 1000\n",
            "the ability to rely on SIMILARITY METRICS invariant to IMAGE TRANSFORMATIONS is an important issue for IMAGE CLASSIFICATION TASKS such as FACE OR CHARACTER RECOGNITION . we analyze an INVARIANT METRIC that has performed well for the <unk> tangent <unk> study its limitations when applied to REGULAR IMAGES , showing that the most significant among these -lrb- convergence to LOCAL MINIMA -RRB- can be drastically reduced by computing the distance in a MULTIRESOLUTION SETTING . this leads to the MULTIRESOLUTION TANGENT DISTANCE , which exhibits significantly higher invariance to IMAGE TRANSFORMATIONS , and can be easily combined with ROBUST ESTIMATION PROCEDURES . \n",
            "this paper addresses the problem of FACE OR CHARACTER RECOGNITION in the presence of REGULAR IMAGES . in particular , we consider the problem of FACE OR CHARACTER RECOGNITION in the presence of REGULAR IMAGES in the presence of REGULAR IMAGES . we propose a novel method to estimate the MULTIRESOLUTION TANGENT DISTANCE for the IMAGE CLASSIFICATION TASKS . the proposed method is based on the use of the MULTIRESOLUTION TANGENT DISTANCE to estimate the IMAGE TRANSFORMATIONS . we show that the proposed ROBUST ESTIMATION PROCEDURES can be applied to IMAGE CLASSIFICATION TASKS , such as FACE OR CHARACTER RECOGNITION , and FACE OR CHARACTER RECOGNITION . the proposed method is evaluated on a variety of IMAGE CLASSIFICATION TASKS including FACE OR CHARACTER RECOGNITION and FACE OR CHARACTER RECOGNITION .\n",
            "\n",
            "921 1000\n",
            "this paper combines LINEAR SPARSE CODING and NON-NEGATIVE MATRIX FACTORIZATION into SPARSE NON-NEGATIVE MATRIX FACTORIZATION . in contrast to NON-NEGATIVE MATRIX FACTORIZATION , the new model can <unk> much SPARSER REPRESENTATION via imposing SPARSENESS CONSTRAINTS explicitly ; in contrast to a CLOSE MODEL-NON-NEGATIVE SPARSE CODING , the new model can learn PARTS-BASED REPRESENTATION via fully MULTIPLICATIVE UPDATES because of adapting a GENERALIZED KULLBACK-LEIBLER DIVERGENCE instead of the conventional MEAN SQUARE ERROR for APPROXIMATION ERROR . experiments on MIT-CBCL TRAINING FACES DATA demonstrate the effectiveness of the proposed method . \n",
            "this paper addresses the problem of SPARSE NON-NEGATIVE MATRIX FACTORIZATION for SPARSE NON-NEGATIVE MATRIX FACTORIZATION . the main contribution of this paper is the use of SPARSE NON-NEGATIVE MATRIX FACTORIZATION and SPARSE NON-NEGATIVE MATRIX FACTORIZATION . the proposed algorithm is based on the GENERALIZED KULLBACK-LEIBLER DIVERGENCE , which is a PARTS-BASED REPRESENTATION of the GENERALIZED KULLBACK-LEIBLER DIVERGENCE . the proposed algorithm is based on the GENERALIZED KULLBACK-LEIBLER DIVERGENCE , which is a PARTS-BASED REPRESENTATION of the MEAN SQUARE ERROR . the proposed algorithm is applied to the MIT-CBCL TRAINING FACES DATA , and the results show that the proposed algorithm is able to achieve the same APPROXIMATION ERROR as the MEAN SQUARE ERROR . the proposed algorithm is applied to the MIT-CBCL TRAINING FACES DATA , and the results show that the proposed algorithm is able to achieve the same APPROXIMATION ERROR as compared to the conventional LINEAR SPARSE CODING .\n",
            "\n",
            "922 1000\n",
            "the MAP SEEKING CIRCUIT has been suggested to address the inverse problem of TRANSFORMATION DISCOVERY as found in SIGNAL PROCESSING , VISION , INVERSE KINEMATICS and many other NATURAL TASKS . according to this idea , a PARALLEL SEARCH in the TRANSFORMATION SPACE of a HIGH DIMENSIONAL PROBLEM can be decomposed into parts efficiently using the ORDERING PROPERTY OF SUPERPOSITIONS . deterministic formulations of the circuit have been suggested . here , we provide a <unk> interpretation of the architecture whereby the SUPERPOSITIONS OF THE CIRCUIT are seen as a series of <unk> over parameters of the transform . based on this , we interpret the weights of the MAP SEEKING CIRCUIT as importance weights . the latter suggests the incorporation of MONTE-CARLO APPROACHES in the MAP SEEKING CIRCUIT , providing improved RESOLUTION OF PARAMETER ESTIMATES within RESOURCE CONSTRAINED IMPLEMENTATIONS . as a final contribution , we model mixed <unk> search strategies of BIOLOGICAL VISION to reduce the problem of COLLUSIONS , a common problem in the standard MAP SEEKING CIRCUIT . \n",
            "this paper addresses the problem of TRANSFORMATION DISCOVERY from a single HIGH DIMENSIONAL PROBLEM . we propose a novel approach to the problem of TRANSFORMATION DISCOVERY in a MAP SEEKING CIRCUIT . the proposed approach is based on the ORDERING PROPERTY OF SUPERPOSITIONS , which is based on the ORDERING PROPERTY OF SUPERPOSITIONS . the proposed approach is based on the use of a SUPERPOSITIONS OF THE CIRCUIT , INVERSE KINEMATICS , INVERSE KINEMATICS , INVERSE KINEMATICS , and NATURAL TASKS . the proposed approach is based on the use of a MAP SEEKING CIRCUIT , which allows the ORDERING PROPERTY OF SUPERPOSITIONS to be integrated into the MAP SEEKING CIRCUIT . the proposed approach is evaluated on a variety of NATURAL TASKS and NATURAL TASKS . the experimental results show that the proposed method can achieve better performance than the state-of-the-art methods .\n",
            "\n",
            "923 1000\n",
            "wireless technology has allowed for a much wider variety in the design of MICROPHONE ARRAYS for BINAURAL HEARING AIDS . to facilitate the design of these MICROPHONE ARRAYS , this paper investigates the use of a SPHERICAL HEAD MODEL in the design of bilateral and binaural MICROPHONE ARRAYS for HEARING AIDS . the MICROPHONE ARRAYS have been designed using a FREE-FIELD MODEL , a SPHERICAL HEAD MODEL , measurements on an ARTIFICIAL HEAD , and measurements on an ARTIFICIAL HEAD + TORSO . the results show that the FREE-FIELD/SPHERICAL MODELS <unk> the SPEECH-INTELLIGIBILITY WEIGHTED DIRECTIVITY INDEX of the BILATERAL AND BINAURAL ARRAYS by respectively 0.9 / 0.4 and 0.8 / 0.5 db . furthermore the weights designed with the FREE-FIELD/SPHERICAL MODEL yield an SII-DI that is 0.7 / 0.6 db lower for BILATERAL ARRAYS and 0.9 / 0.9 db lower for BINAURAL ARRAYS than the optimal SII-DI . although the results show that the SPHERICAL HEAD MODEL is better in predicting the DI than the FREE-FIELD MODEL , the SPHERICAL HEAD MODEL does not design better weights . \n",
            "in this paper , we present a novel approach to BINAURAL HEARING AIDS for HEARING AIDS . the proposed FREE-FIELD/SPHERICAL MODEL is based on a FREE-FIELD/SPHERICAL MODEL and a SPHERICAL HEAD MODEL for BINAURAL HEARING AIDS . the proposed FREE-FIELD/SPHERICAL MODEL is based on the use of a FREE-FIELD/SPHERICAL MODEL and a SPEECH-INTELLIGIBILITY WEIGHTED DIRECTIVITY INDEX . the proposed FREE-FIELD/SPHERICAL MODEL is compared to the conventional FREE-FIELD MODEL and the FREE-FIELD/SPHERICAL MODEL . the proposed FREE-FIELD/SPHERICAL MODEL is compared with the conventional FREE-FIELD MODEL and the SPHERICAL HEAD MODEL . the proposed FREE-FIELD/SPHERICAL MODEL is compared with the conventional FREE-FIELD/SPHERICAL MODEL and the SPHERICAL HEAD MODEL . the proposed FREE-FIELD/SPHERICAL MODEL is compared with the conventional FREE-FIELD/SPHERICAL MODEL and the FREE-FIELD/SPHERICAL MODEL .\n",
            "\n",
            "924 1000\n",
            "this paper presents MULTI-CONDITIONAL LEARNING , a TRAINING CRITERION based on a product of multiple conditional likelihoods . when combining the traditional conditional probability of '' label given input '' with a generative probability of '' input given label '' the later acts as a surprisingly effective REGULARIZER . when applied to models with LATENT VARIABLES , MULTI-CONDITIONAL LEARNING combines the <unk> capabilities of GENERATIVE TOPIC MODELS , such as LATENT DIRICHLET ALLOCATION and the EXPONENTIAL FAMILY HARMONIUM , with the ACCURACY and ROBUSTNESS of DISCRIMINATIVE CLASSIFIERS , such as LOGISTIC REGRESSION and CONDITIONAL RANDOM FIELDS . we present results on several standard TEXT DATA SETS showing significant reductions in CLASSIFICATION ERROR due to MCL REGULARIZATION , and substantial gains in PRECISION and RECALL due to the LATENT STRUCTURE discovered under MULTI-CONDITIONAL LEARNING . \n",
            "this paper addresses the problem of MULTI-CONDITIONAL LEARNING , such as LATENT DIRICHLET ALLOCATION , LATENT DIRICHLET ALLOCATION , and CONDITIONAL RANDOM FIELDS . in this paper , we propose a novel approach to the problem of MULTI-CONDITIONAL LEARNING , which is based on the idea of MCL REGULARIZATION and LOGISTIC REGRESSION . the proposed approach is based on the use of LATENT VARIABLES , LATENT DIRICHLET ALLOCATION , LATENT DIRICHLET ALLOCATION , and CONDITIONAL RANDOM FIELDS . the proposed method is based on the idea that the LATENT VARIABLES of the LATENT VARIABLES can be approximated by a set of LATENT VARIABLES , which are then used to estimate the LATENT VARIABLES . the performance of the proposed method is evaluated on a number of TEXT DATA SETS . the experimental results show that the proposed method is effective in improving the ACCURACY and RECALL of the proposed method .\n",
            "\n",
            "925 1000\n",
            "the COMMON SPATIAL PATTERNS ALGORITHM has been widely used in EEG CLASSIFICATION and BRAIN COMPUTER INTERFACE . in this paper , we propose a MULTILINEAR FORMULATION of the COMMON SPATIAL PATTERNS ALGORITHM , termed as <unk> or common tensor discriminant analysis -lrb- <unk> -rrb- for HIGH-ORDER TENSOR DATA . as a natural extension of COMMON SPATIAL PATTERNS ALGORITHM , the proposed MULTILINEAR FORMULATION uses the analogous OPTIMIZATION CRITERIA in COMMON SPATIAL PATTERNS ALGORITHM and a new framework for SIMULTANEOUS OPTIMIZATION OF PROJECTION MATRICES on each mode based on TENSOR ANALYSIS THEORY is developed . experimental results demonstrate that our proposed MULTILINEAR FORMULATION is able to improve CLASSIFICATION ACCURACY of MULTI-CLASS MOTOR IMAGERY EEG . \n",
            "in this paper , we propose a novel COMMON SPATIAL PATTERNS ALGORITHM for MULTI-CLASS MOTOR IMAGERY EEG and BRAIN COMPUTER INTERFACE . the proposed COMMON SPATIAL PATTERNS ALGORITHM is based on the TENSOR ANALYSIS THEORY , which is based on the TENSOR ANALYSIS THEORY . the proposed COMMON SPATIAL PATTERNS ALGORITHM is based on the use of a COMMON SPATIAL PATTERNS ALGORITHM and a BRAIN COMPUTER INTERFACE . the proposed MULTILINEAR FORMULATION is applied to the MULTI-CLASS MOTOR IMAGERY EEG and the BRAIN COMPUTER INTERFACE . the experimental results show that the proposed MULTILINEAR FORMULATION is effective in improving the CLASSIFICATION ACCURACY of MULTI-CLASS MOTOR IMAGERY EEG in MULTI-CLASS MOTOR IMAGERY EEG and BRAIN COMPUTER INTERFACE .\n",
            "\n",
            "926 1000\n",
            "in a LIFELONG LEARNING FRAMEWORK , an LIFELONG LEARNING FRAMEWORK acquires knowledge incrementally over consecutive LEARNING TASKS , continually building upon its experience . recent LIFELONG LEARNING ALGORITHMS have achieved nearly identical performance to BATCH MULTI-TASK LEARNING METHODS while reducing LEARNING TIME by three orders of magnitude . in this paper , we further improve the scalability of LIFELONG LEARNING by developing CURRICULUM SELECTION METHODS that enable an LIFELONG LEARNING FRAMEWORK to actively select the next task to learn in order to maximize performance on future LEARNING TASKS . we demonstrate that ACTIVE TASK SELECTION is highly reliable and effective , allowing an LIFELONG LEARNING FRAMEWORK to learn high performance models using up to 50 % fewer tasks than when the LIFELONG LEARNING FRAMEWORK has no control over the task order . we also explore a variant of TRANSFER LEARNING in the LIFELONG LEARNING SETTING in which the LIFELONG LEARNING FRAMEWORK can focus KNOWLEDGE ACQUISITION toward a particular target task . \n",
            "this paper addresses the problem of ACTIVE TASK SELECTION for KNOWLEDGE ACQUISITION . in particular , we focus on the problem of ACTIVE TASK SELECTION for ACTIVE TASK SELECTION , and propose a novel LIFELONG LEARNING FRAMEWORK based on LIFELONG LEARNING . the proposed LIFELONG LEARNING FRAMEWORK is based on the use of LIFELONG LEARNING to solve the problem of ACTIVE TASK SELECTION . we show that the proposed LIFELONG LEARNING FRAMEWORK can be applied to the problem of ACTIVE TASK SELECTION , and is able to solve the problem of ACTIVE TASK SELECTION for ACTIVE TASK SELECTION . the performance of the proposed algorithm is demonstrated on several benchmark datasets , and the results demonstrate the effectiveness of the proposed LIFELONG LEARNING FRAMEWORK .\n",
            "\n",
            "927 1000\n",
            "the RETRIEVAL OF SOCCER HIGHLIGHTS is a suitable technique for VIDEO INDEXING , required by the MULTIMEDIA DATABASE MANAGEMENT or for the development of television on demand . for these purposes , it should be interesting to have an AUTOMATIC ANNOTATION OF EVENTS happened in SOCCER GAMES . one solution consists in analyzing the AUDIO SOUNDTRACK associated to the SOCCER VIDEO and to detect the interesting frames . in this paper we use the ADAPTIVE TIME-FREQUENCY DECOMPOSITION of the <unk> as a FEATURE EXTRACTION PROCEDURE . this decomposition is based on the MATCHING PURSUIT CONCEPT and a dictionary composed of gabor functions . the parameters provided by these transformations constitute the input of the CLASSIFICATION STAGE . the results provided for REAL SOCCER VIDEO will prove the efficiency of the ADAPTIVE TIME-FREQUENCY REPRESENTATION as a FEATURE EXTRACTION STAGE . \n",
            "this paper presents a novel approach to MULTIMEDIA DATABASE MANAGEMENT for MULTIMEDIA DATABASE MANAGEMENT . the proposed approach is based on the use of a MATCHING PURSUIT CONCEPT as a MATCHING PURSUIT CONCEPT . the proposed approach is based on a FEATURE EXTRACTION PROCEDURE that uses a FEATURE EXTRACTION PROCEDURE to generate a MATCHING PURSUIT CONCEPT . the proposed approach is based on the use of a MATCHING PURSUIT CONCEPT , which is based on a FEATURE EXTRACTION PROCEDURE . the proposed approach is evaluated on a REAL SOCCER VIDEO and a REAL SOCCER VIDEO . the results show that the proposed approach is effective in improving the RETRIEVAL OF SOCCER HIGHLIGHTS performance in the presence of SOCCER VIDEO .\n",
            "\n",
            "928 1000\n",
            "in this paper , we evaluate the performance of existing and new objective measures in terms of predicting the quality of REVERBERANT SPEECH and speech enhanced by DEREVERBERATION ALGORITHMS . we use SUBJECTIVE QUALITY RATINGS designed to evaluate the quality of speech along three dimensions : SPEECH COLORATION , REVERBERATION TAIL EFFECT and OVERALL SPEECH QUALITY . experimental results assess the correlations between the proposed OBJECTIVE QUALITY MEASURES and the three SUBJECTIVE RATING SCALES and suggest that PESQ-BASED MEASURES can very reliably predict the quality of REVERBERANT AND DEREVERBERATED SPEECH . \n",
            "this paper presents a novel approach to REVERBERANT AND DEREVERBERATED SPEECH based on PESQ-BASED MEASURES . the proposed approach is based on the use of a set of PESQ-BASED MEASURES which are robust to REVERBERATION TAIL EFFECT , REVERBERATION TAIL EFFECT , REVERBERATION TAIL EFFECT , and OVERALL SPEECH QUALITY . the proposed approach is evaluated on a variety of PESQ-BASED MEASURES and compared to the state-of-the-art methods . the results show that the proposed approach is effective in improving the OVERALL SPEECH QUALITY and OVERALL SPEECH QUALITY .\n",
            "\n",
            "929 1000\n",
            "prediction by <unk> does not rely on any specific MODEL STRUCTURE , and is thus much more flexible than approaches based on PARAMETRIC BEHAVIOURAL MODELS . since accurate predictions are obtained for extremely short training sequences , it generally performs better than PREDICTION METHODS using PARAMETRIC MODELS . application to NONLINEAR SYSTEM INVERSION is considered \n",
            "this paper presents a novel approach to NONLINEAR SYSTEM INVERSION based on PARAMETRIC MODELS . the proposed approach is based on the use of PARAMETRIC MODELS for PREDICTION . the proposed approach is based on the use of PARAMETRIC MODELS for PREDICTION . the proposed approach is based on the use of PARAMETRIC MODELS for PREDICTION . the experimental results show that the proposed approach is effective in improving the performance of NONLINEAR SYSTEM INVERSION .\n",
            "\n",
            "930 1000\n",
            "the recent explosion of interest in GRAPH CUT METHODS in COMPUTER VISION naturally <unk> the question : what ENERGY FUNCTIONS can be minimized via GRAPH CUTS ? this question was first attacked by two papers of kolmogorov and <unk> -lsb- 23 , 24 -rsb- , in which they dealt with FUNCTIONS with PAIR-WISE AND TRIPLEWISE PIXEL INTERACTIONS . in this work , we extend their results in two directions . first , we examine the case of K-WISE PIXEL INTERACTIONS ; the results are derived from a purely ALGEBRAIC APPROACH . second , we discuss the applicability of provably APPROXIMATE ALGORITHMS . both of these developments should help researchers best understand what can and can not be achieved when designing GRAPH CUT BASED ALGORITHMS . \n",
            "this paper addresses the problem of K-WISE PIXEL INTERACTIONS in COMPUTER VISION . in particular , we focus on the problem of K-WISE PIXEL INTERACTIONS in the context of GRAPH CUTS . we propose a novel approach to the problem of K-WISE PIXEL INTERACTIONS , which is a generalization of the standard ALGEBRAIC APPROACH . we show that the ALGEBRAIC APPROACH can be applied to the problem of K-WISE PIXEL INTERACTIONS . we show that the proposed algorithm can be applied to PAIR-WISE AND TRIPLEWISE PIXEL INTERACTIONS , and we show that it is possible to efficiently solve the problem of K-WISE PIXEL INTERACTIONS . we also show that our algorithm can be applied to the problem of K-WISE PIXEL INTERACTIONS .\n",
            "\n",
            "931 1000\n",
            "the objective of this paper is to <unk> students understanding of SAMPLING through experimentation . the scope of this paper is very narrow , focusing on a single LABORATORY EXERCISE for learning about SAMPLING , which is a critically important topic for students to comprehend . the LABORATORY EXERCISE makes an ideal platform for studying SAMPLING , ALIASING , and QUANTIZATION , because not only does the LABORATORY EXERCISE have a built in A/D CONVERTER and all the supporting electronics , but LABORATORY EXERCISE has the DISPLAY CAPABILITIES in both the time and frequency domains . additionally , students obtain a better understanding of test and measurement equipment such as the LABORATORY EXERCISE . \n",
            "in this paper , we present a novel approach to the A/D CONVERTER , which is based on the use of a LABORATORY EXERCISE for QUANTIZATION . the proposed approach is based on the use of SAMPLING and QUANTIZATION . the proposed method is based on the use of a LABORATORY EXERCISE , which is based on SAMPLING and QUANTIZATION . the experimental results show that the proposed method outperforms the existing methods in terms of DISPLAY CAPABILITIES , ALIASING , and QUANTIZATION .\n",
            "\n",
            "932 1000\n",
            "recently there has developed considerable interest in using <unk> with PCA OR-THOGONALITY . almost all previous methods concentrate on <unk> out some <unk> . here we develop a new approach which zeros out whole variables automatically . we formulate a VECTOR L1 PENALIZED PCA CRITERION and optimize VECTOR L1 PENALIZED PCA CRITERION by <unk> descent along GEODESIC on a GRASSMAN MANIFOLD . this ensures that each step obeys PCA OR-THOGONALITY as well as an invariance property of the criterion . we show in simulations that VECTOR L1 PENALIZED PCA CRITERION outperforms a previous SVPCA ALGORITHM and apply VECTOR L1 PENALIZED PCA CRITERION to a real high dimensional functional magnetic resonance imaging -lrb- fmri -rrb- data . \n",
            "this paper proposes a new VECTOR L1 PENALIZED PCA CRITERION for the VECTOR L1 PENALIZED PCA CRITERION . the proposed VECTOR L1 PENALIZED PCA CRITERION is based on the use of a GEODESIC to the GRASSMAN MANIFOLD . the proposed VECTOR L1 PENALIZED PCA CRITERION is compared to a SVPCA ALGORITHM , which is based on the GEODESIC . the proposed VECTOR L1 PENALIZED PCA CRITERION is compared to the standard SVPCA ALGORITHM . the experimental results show that the proposed VECTOR L1 PENALIZED PCA CRITERION is better than the conventional SVPCA ALGORITHM , and the proposed SVPCA ALGORITHM is more effective than the SVPCA ALGORITHM .\n",
            "\n",
            "933 1000\n",
            "this paper demonstrates how an INSTRUMENTAL SPEECH-QUALITY MEASURE based on the comparison of AUDITORY-NERVE RING-PATTERNS can be constructed . four available subjective tests prove that the MEAN OPINION SCORES MOS estimated by the objective measure are in good agreement with the subjectively obtained results . \n",
            "in this paper , we present a new method for the MEAN OPINION SCORES MOS of the INSTRUMENTAL SPEECH-QUALITY MEASURE . the proposed INSTRUMENTAL SPEECH-QUALITY MEASURE is based on a AUDITORY-NERVE RING-PATTERNS , which is based on a AUDITORY-NERVE RING-PATTERNS . the performance of the proposed INSTRUMENTAL SPEECH-QUALITY MEASURE is evaluated on a number of MEAN OPINION SCORES MOS . the results show that the proposed INSTRUMENTAL SPEECH-QUALITY MEASURE is very effective .\n",
            "\n",
            "934 1000\n",
            "the concepts of MSS -LRB- MAXIMAL SATISFIABLE SUBSET -RRB- and COMSS -lrb- also called MINIMAL CORRECTION SUBSET -RRB- play a key role in many A.I. APPROACHES and techniques . in this paper , a novel algorithm for partitioning a BOOLEAN CNF FORMULA into one MINIMAL CORRECTION SUBSET -RRB- and the corresponding COMSS is introduced . extensive empirical evaluation shows that it is more robust and more efficient on most instances than currently available techniques . \n",
            "in this paper , we present a novel approach to the problem of BOOLEAN CNF FORMULA . the proposed method is based on the use of a BOOLEAN CNF FORMULA , MINIMAL CORRECTION SUBSET -RRB- , and COMSS . the proposed algorithm is based on the use of a BOOLEAN CNF FORMULA and a COMSS . the performance of the proposed method is evaluated on a variety of A.I. APPROACHES , MINIMAL CORRECTION SUBSET -RRB- , MINIMAL CORRECTION SUBSET -RRB- , and COMSS .\n",
            "\n",
            "935 1000\n",
            "we improve the AUTOMATIC SPEECH RECOGNITION of BROADCAST NEWS using paradigms from WEB 2.0 to obtain TIME-AND TOPIC-RELEVANT TEXT DATA for LANGUAGE MODELING . we elaborate an UN-SUPERVISED TEXT COLLECTION AND DECODING STRATEGY that includes <unk> appropriate texts from RSS FEEDS , complementing UN-SUPERVISED TEXT COLLECTION AND DECODING STRATEGY with texts from TWITTER , LANGUAGE MODEL and vocabulary adaptation , as well as a 2-PASS DECODING . the WORD ERROR RATES of the tested FRENCH BROADCAST NEWS SHOWS from europe 1 are reduced by almost 32 % relative with an underlying LANGUAGE MODEL from the <unk> project -lsb- 1 -rsb- and by almost 4 % with an underlying LANGUAGE MODEL from the QUAERO PROJECT . the UN-SUPERVISED TEXT COLLECTION AND DECODING STRATEGY that we use for the TEXT NORMALIZATION , the collection of RSS FEEDS together with the text on the related websites , a TF-IDF-BASED TOPIC WORDS EXTRACTION , as well as the opportunity for LANGUAGE MODEL INTERPOLATION are available in our RAPID LANGUAGE ADAPTATION TOOLKIT -lsb- 2 -rsb- -lsb- 3 -rsb- . \n",
            "this paper describes a novel UN-SUPERVISED TEXT COLLECTION AND DECODING STRATEGY for AUTOMATIC SPEECH RECOGNITION . the proposed UN-SUPERVISED TEXT COLLECTION AND DECODING STRATEGY is based on a LANGUAGE MODEL and a RAPID LANGUAGE ADAPTATION TOOLKIT . the proposed UN-SUPERVISED TEXT COLLECTION AND DECODING STRATEGY is based on a RAPID LANGUAGE ADAPTATION TOOLKIT and an UN-SUPERVISED TEXT COLLECTION AND DECODING STRATEGY . the proposed UN-SUPERVISED TEXT COLLECTION AND DECODING STRATEGY is based on a RAPID LANGUAGE ADAPTATION TOOLKIT and a RAPID LANGUAGE ADAPTATION TOOLKIT . the proposed UN-SUPERVISED TEXT COLLECTION AND DECODING STRATEGY is evaluated on FRENCH BROADCAST NEWS SHOWS and compared to the QUAERO PROJECT . the results show that the proposed method is effective in 2-PASS DECODING , and is robust to RSS FEEDS in the presence of TWITTER . the proposed UN-SUPERVISED TEXT COLLECTION AND DECODING STRATEGY is evaluated on the FRENCH BROADCAST NEWS SHOWS and the results demonstrate the effectiveness of the proposed UN-SUPERVISED TEXT COLLECTION AND DECODING STRATEGY .\n",
            "\n",
            "936 1000\n",
            "this paper describes a WIDE-COVERAGE STATISTICAL PARSER that uses COMBINATORY CATEGORIAL GRAMMAR to derive DEPENDENCY STRUCTURES . the WIDE-COVERAGE STATISTICAL PARSER differs from most existing WIDE-COVERAGE TREE-BANK PARSERS in capturing the LONG-RANGE DEPENDENCIES inherent in constructions such as COORDINATION , EXTRACTION , raising and CONTROL , as well as the standard LOCAL PREDICATE-ARGUMENT DEPENDENCIES . a set of DEPENDENCY STRUCTURES used for training and testing the WIDE-COVERAGE STATISTICAL PARSER is obtained from a TREEBANK OF CCG NORMAL-FORM DERIVATIONS , which have been derived -lrb- semi - -rrb- automatically from the PENN TREEBANK . the WIDE-COVERAGE STATISTICAL PARSER correctly recovers over 80 % of LABELLED DEPENDENCIES , and around 90 % of UNLABELLED DEPENDENCIES . \n",
            "this paper presents a novel WIDE-COVERAGE STATISTICAL PARSER based on COMBINATORY CATEGORIAL GRAMMAR . the proposed WIDE-COVERAGE STATISTICAL PARSER is based on a COMBINATORY CATEGORIAL GRAMMAR . the COMBINATORY CATEGORIAL GRAMMAR is based on a COMBINATORY CATEGORIAL GRAMMAR . the proposed WIDE-COVERAGE STATISTICAL PARSER is based on a COMBINATORY CATEGORIAL GRAMMAR . the proposed WIDE-COVERAGE STATISTICAL PARSER is applied to the PENN TREEBANK , which is based on a COMBINATORY CATEGORIAL GRAMMAR . the experimental results show that the proposed WIDE-COVERAGE STATISTICAL PARSER outperforms the WIDE-COVERAGE TREE-BANK PARSERS in terms of both EXTRACTION and CONTROL .\n",
            "\n",
            "937 1000\n",
            "a fundamental challenge in the design of OBJECTIVE MODELS for ESTIMATION OF SPEECH SIGNAL QUALITY lies in the shortage of SUBJECTIVELY LABELLED DATABASES . this problem is particularly relevant when developing QUALITY ASSESSMENT MODELS for wide-band -lrb- 16 khz sampling rate -rrb- signals where DATABASES are scarce . we explore the possibility for seamlessly integrating a QUALITY PRIOR in the form of a NARROW-BAND QUALITY ESTIMATE into the framework of a NON-INTRUSIVE WIDE-BAND QUALITY ASSESSMENT ALGORITHM . experimental results confirm that the proposed NON-INTRUSIVE WIDE-BAND QUALITY ASSESSMENT ALGORITHM can be used to improve performance over a BASELINE WIDE-BAND SYSTEM without a NARROW-BAND PRIOR . \n",
            "this paper presents a NON-INTRUSIVE WIDE-BAND QUALITY ASSESSMENT ALGORITHM based on a NON-INTRUSIVE WIDE-BAND QUALITY ASSESSMENT ALGORITHM . the proposed NON-INTRUSIVE WIDE-BAND QUALITY ASSESSMENT ALGORITHM is based on the use of a NARROW-BAND PRIOR for the ESTIMATION OF SPEECH SIGNAL QUALITY . the proposed NON-INTRUSIVE WIDE-BAND QUALITY ASSESSMENT ALGORITHM is based on the use of a NARROW-BAND PRIOR and a NARROW-BAND QUALITY ESTIMATE . the proposed NON-INTRUSIVE WIDE-BAND QUALITY ASSESSMENT ALGORITHM is evaluated in a NON-INTRUSIVE WIDE-BAND QUALITY ASSESSMENT ALGORITHM using a BASELINE WIDE-BAND SYSTEM . the performance of the proposed NON-INTRUSIVE WIDE-BAND QUALITY ASSESSMENT ALGORITHM is evaluated in terms of ESTIMATION OF SPEECH SIGNAL QUALITY performance . the experimental results show that the proposed NON-INTRUSIVE WIDE-BAND QUALITY ASSESSMENT ALGORITHM is effective in improving the ESTIMATION OF SPEECH SIGNAL QUALITY performance of the NON-INTRUSIVE WIDE-BAND QUALITY ASSESSMENT ALGORITHM .\n",
            "\n",
            "938 1000\n",
            "this paper reports an investigation of FEATURES relevant for classifying two SPEAKING STYLES , namely , CONVERSATIONAL SPEAKING STYLE and clear -lrb- e.g. HYPER-ARTICULATED -RRB- SPEAKING STYLE . SPECTRAL AND PROSODIC FEATURES were automatically extracted from speech and classified using DECISION TREE CLASSIFIERS and MULTI-LAYER PERCEPTRONS to achieve ACCURACIES of about 71 % and 77 % respectively . more interestingly , we found that out of the 56 FEATURES only about 9 FEATURES are needed to capture the most PREDICTIVE POWER . while perceptual studies have shown that SPECTRAL CUES are more useful than PROSODIC FEATURES for <unk> -lsb- 1 -rsb- , here we find PROSODIC FEATURES are more important for CLASSIFICATION . \n",
            "in this paper , we propose a novel approach to CONVERSATIONAL SPEAKING STYLE based on DECISION TREE CLASSIFIERS and MULTI-LAYER PERCEPTRONS . the proposed approach is based on the use of DECISION TREE CLASSIFIERS and MULTI-LAYER PERCEPTRONS . the proposed approach is based on the use of DECISION TREE CLASSIFIERS and MULTI-LAYER PERCEPTRONS . the proposed method is based on the use of DECISION TREE CLASSIFIERS and MULTI-LAYER PERCEPTRONS . the experimental results show that the proposed method is effective in improving the performance of DECISION TREE CLASSIFIERS in terms of both ACCURACIES and HYPER-ARTICULATED -RRB- SPEAKING STYLE .\n",
            "\n",
            "939 1000\n",
            "<unk> approaches allow a CROWDSOURCING SYSTEM to identify reliable workers to whom tasks can be <unk> . in CROWDSOURCING SYSTEM that can be modeled as MULTI-AGENT TRUST NETWORKS consist of RESOURCE CONSTRAINED TRUSTEE AGENTS -lrb- i.e. , workers -rrb- , workers may need to further <unk> tasks to others if they determine that they can not complete all pending tasks before the <unk> <unk> . existing REPUTATION-BASED DECISION-MAKING MODELS can not help workers decide when and to whom to <unk> tasks . in this paper , we proposed a REPUTATION AWARE TASK SUB-DELEGATION APPROACH to bridge this gap . by jointly considering a WORKER 'S REPUTATION , WORKLOAD , the price of its effort and its trust relationships with others , REPUTATION AWARE TASK SUB-DELEGATION APPROACH can be implemented as an INTELLIGENT AGENT to help workers make SUB-DELEGATION DECISIONS in a distributed manner . the resulting task allocation maximizes social welfare through efficient utilization of the collective capacity of a crowd , and provides provable performance guarantees . experimental comparisons with state-of-the-art approaches based on the EPINIONS TRUST NETWORK demonstrate significant advantages of REPUTATION AWARE TASK SUB-DELEGATION APPROACH under HIGH WORKLOAD CONDITIONS . \n",
            "this paper presents a novel REPUTATION AWARE TASK SUB-DELEGATION APPROACH for MULTI-AGENT TRUST NETWORKS in MULTI-AGENT TRUST NETWORKS . the proposed REPUTATION AWARE TASK SUB-DELEGATION APPROACH consists of two steps : -lrb- 1 -rrb- a EPINIONS TRUST NETWORK with SUB-DELEGATION DECISIONS , and -lrb- 2 -rrb- a REPUTATION AWARE TASK SUB-DELEGATION APPROACH with SUB-DELEGATION DECISIONS . the proposed REPUTATION AWARE TASK SUB-DELEGATION APPROACH is based on a REPUTATION AWARE TASK SUB-DELEGATION APPROACH with SUB-DELEGATION DECISIONS . the proposed REPUTATION AWARE TASK SUB-DELEGATION APPROACH is based on a REPUTATION AWARE TASK SUB-DELEGATION APPROACH with SUB-DELEGATION DECISIONS . the proposed REPUTATION AWARE TASK SUB-DELEGATION APPROACH is evaluated on a variety of MULTI-AGENT TRUST NETWORKS . the experimental results show that the proposed REPUTATION AWARE TASK SUB-DELEGATION APPROACH is effective in reducing the HIGH WORKLOAD CONDITIONS and the HIGH WORKLOAD CONDITIONS .\n",
            "\n",
            "940 1000\n",
            "traditional MICROPHONE ARRAY SPEECH RECOGNITION SYSTEMS simply recognise the enhanced output of the array . as the level of SIGNAL ENHANCEMENT depends on the number of microphones , such MICROPHONE ARRAY SPEECH RECOGNITION SYSTEMS do not achieve acceptable SPEECH RECOGNITION performance for arrays having only a few microphones . for SMALL MICROPHONE ARRAYS , we instead propose using the enhanced output to estimate a RELIABILITY MASK , which is then used in MISSING DATA SPEECH RECOGNITION . in MISSING DATA SPEECH RECOGNITION , the DECODED SEQUENCE depends on the RELIABILITY MASK of each input feature . this RELIABILITY MASK is usually based on the signal to noise ratio in each frequency band . in this paper , we use the energy difference between the NOISY INPUT and the enhanced output of a small microphone array to determine the FREQUENCY BAND RELIABILITY . RECOGNITION experiments with a small array demonstrate the effectiveness of the technique , compared to both traditional MICROPHONE ARRAY ENHANCEMENT and a BASELINE MISSING DATA SYSTEM . \n",
            "this paper presents a novel approach to MISSING DATA SPEECH RECOGNITION in MICROPHONE ARRAY SPEECH RECOGNITION SYSTEMS . the proposed approach is based on the use of a RELIABILITY MASK and a RELIABILITY MASK for SIGNAL ENHANCEMENT . the proposed method is based on the use of a RELIABILITY MASK and a RELIABILITY MASK for SIGNAL ENHANCEMENT . the proposed approach is evaluated on a BASELINE MISSING DATA SYSTEM and a BASELINE MISSING DATA SYSTEM . the performance of the proposed method is evaluated on a BASELINE MISSING DATA SYSTEM and a BASELINE MISSING DATA SYSTEM . the performance of the proposed method is evaluated on a BASELINE MISSING DATA SYSTEM and a BASELINE MISSING DATA SYSTEM .\n",
            "\n",
            "941 1000\n",
            "this paper presents a novel approach for recognizing and interpreting dimensions in ENGINEERING DRAWINGS . it starts by detecting potential DIMENSION FRAMES , each comprising only the line and text components of a dimension , then verifies them by detecting the DIMENSION SYMBOLS . by removing the prerequisite of SYMBOL RECOGNITION from DETECTION OF DIMENSION SETS , our method is capable of handling LOW QUALITY DRAWINGS . we also propose a RECONSTRUCTION ALGORITHM for rebuilding the DRAWING ENTITIES based on the RECOGNIZED DIMENSION ANNOTATIONS . a COORDINATE GRID STRUCTURE is introduced to represent and analyze TWO-DIMENSIONAL SPATIAL CONSTRAINTS between entities ; this simplifies and unifies the process of RECTIFYING DEVIATIONS OF ENTITY DIMENSIONS induced during SCANNING and VECTORIZATION . \n",
            "this paper presents a novel RECONSTRUCTION ALGORITHM based on a COORDINATE GRID STRUCTURE for SYMBOL RECOGNITION . the proposed RECONSTRUCTION ALGORITHM is based on a COORDINATE GRID STRUCTURE , which is based on a COORDINATE GRID STRUCTURE . the proposed RECONSTRUCTION ALGORITHM is based on a RECONSTRUCTION ALGORITHM of the DIMENSION SYMBOLS and the VECTORIZATION . the proposed RECONSTRUCTION ALGORITHM is based on the use of a COORDINATE GRID STRUCTURE and a VECTORIZATION for SYMBOL RECOGNITION . the proposed RECONSTRUCTION ALGORITHM is applied to the DETECTION OF DIMENSION SETS and the DETECTION OF DIMENSION SETS . the results show that the proposed RECONSTRUCTION ALGORITHM is robust to DRAWING ENTITIES , VECTORIZATION , and VECTORIZATION .\n",
            "\n",
            "942 1000\n",
            "in the present paper , a <unk> markov model -lrb- HSMM TRAINING -rrb- based speech synthesis system is proposed . in a hidden markov model -lrb- HMM -rrb- based speech synthesis system which we have proposed , RHYTHM AND TEMPO are controlled by STATE DURATION PROBABILITY DISTRIBUTIONS modeled by SINGLE GAUSSIAN DISTRIBUTIONS . to synthesis speech , it constructs a SENTENCE HMM corresponding to an <unk> given text and determine STATE DURATIONS maximizing their probabilities , then a SPEECH PARAMETER VECTOR SEQUENCE is generated for the given state sequence . however , there is an inconsistency : although the speech is synthesized from HMMS with EXPLICIT STATE DURATION PROBABILITY DISTRIBUTIONS , HMMS are trained without them . in the present paper , we introduce an HSMM TRAINING , which is an HMM with EXPLICIT STATE DURATION PROBABILITY DISTRIBUTIONS , into the HMM-BASED SPEECH SYNTHESIS SYSTEM . experimental results show that the use of HSMM TRAINING improves the naturalness of the SYNTHESIZED SPEECH . \n",
            "in this paper , we propose a novel method for HSMM TRAINING based on HSMM TRAINING . the proposed method is based on the use of a SENTENCE HMM and a SENTENCE HMM to estimate the STATE DURATION PROBABILITY DISTRIBUTIONS . the proposed method is based on the use of a SENTENCE HMM to estimate the STATE DURATION PROBABILITY DISTRIBUTIONS . the proposed method is based on the use of a SENTENCE HMM to estimate the STATE DURATION PROBABILITY DISTRIBUTIONS of the SYNTHESIZED SPEECH . the proposed method is based on the use of SINGLE GAUSSIAN DISTRIBUTIONS to estimate the STATE DURATION PROBABILITY DISTRIBUTIONS of the SYNTHESIZED SPEECH . the performance of the proposed method is demonstrated on a variety of SYNTHESIZED SPEECH . the results show that the proposed method is effective in improving the RHYTHM AND TEMPO .\n",
            "\n",
            "943 1000\n",
            "we propose an improved maximum a POSTERIORI LEARNING ALGORITHM of CONTINUOUS-DENSITY HIDDEN MARKOV MODEL PARAMETERS for SPEAKER ADAPTATION . the algorithm is developed by sequentially combining three ADAPTATION APPROACHES . first , the clusters of SPEAKER-INDEPENDENT HMM PARAMETERS are locally transformed through a group of TRANSFORMATION FUNCTIONS . then , the transformed HMM PARAMETERS are globally smoothed via the MAP ADAPTATION . within the MAP ADAPTATION , the parameters of UNSEEN UNITS in ADAPTATION DATA are further adapted by employing the TRANSFER VECTOR INTERPOLATION SCHEME . experiments show that the combined algorithm converges rapidly and outperforms those other ADAPTATION METHODS . \n",
            "in this paper , we propose a novel TRANSFER VECTOR INTERPOLATION SCHEME for SPEAKER ADAPTATION . the proposed TRANSFER VECTOR INTERPOLATION SCHEME is based on the use of TRANSFORMATION FUNCTIONS for SPEAKER ADAPTATION . the proposed TRANSFER VECTOR INTERPOLATION SCHEME is based on the use of TRANSFORMATION FUNCTIONS for SPEAKER ADAPTATION . the proposed TRANSFER VECTOR INTERPOLATION SCHEME is based on the use of TRANSFORMATION FUNCTIONS for SPEAKER ADAPTATION . the proposed TRANSFER VECTOR INTERPOLATION SCHEME is applied to the problem of MAP ADAPTATION , and the experimental results demonstrate the effectiveness of the proposed TRANSFER VECTOR INTERPOLATION SCHEME .\n",
            "\n",
            "944 1000\n",
            "the COMPOSITIONAL NATURE OF VISUAL OBJECTS significantly limits their REPRESENTATION COMPLEXITY and renders learning of STRUCTURED OBJECT MODELS tractable . adopting this MOD-ELING STRATEGY we both -lrb- i -rrb- automatically decompose objects into a HIERARCHY OF RELEVANT COMPOSITIONS and we -lrb- ii -rrb- learn such a COMPOSITIONAL REPRESENTATION for each category without SUPERVISION . the COMPOSITIONAL REPRESENTATION supports FEATURE SHARING already on the lowest level of small image patches . COMPOSITIONS are represented as PROBABILITY DISTRIBUTIONS over their constituent parts and the relations between them . the GLOBAL SHAPE OF OBJECTS is captured by a GRAPHICAL MODEL which combines all compositions . INFERENCE based on the underlying STATISTICAL MODEL is then employed to obtain a CATEGORY LEVEL OBJECT RECOGNITION SYSTEM . experiments on LARGE STANDARD BENCHMARK DATASETS underline the competitive recognition performance of this GRAPHICAL MODEL and GRAPHICAL MODEL provide insights into the learned COMPO-SITIONAL STRUCTURE OF OBJECTS . \n",
            "this paper addresses the problem of COMPO-SITIONAL STRUCTURE OF OBJECTS in STRUCTURED OBJECT MODELS . in particular , we propose a novel approach to INFERENCE based on the STATISTICAL MODEL . the proposed STATISTICAL MODEL is based on a GRAPHICAL MODEL that uses a GRAPHICAL MODEL to estimate the GLOBAL SHAPE OF OBJECTS . the proposed STATISTICAL MODEL is based on a GRAPHICAL MODEL that exploits the COMPOSITIONAL NATURE OF VISUAL OBJECTS in the GRAPHICAL MODEL . the proposed STATISTICAL MODEL is applied to the COMPOSITIONAL NATURE OF VISUAL OBJECTS of the CATEGORY LEVEL OBJECT RECOGNITION SYSTEM , which is based on a GRAPHICAL MODEL . the proposed STATISTICAL MODEL is applied to the COMPOSITIONAL NATURE OF VISUAL OBJECTS of the CATEGORY LEVEL OBJECT RECOGNITION SYSTEM , and the results show that the proposed STATISTICAL MODEL is effective in improving the REPRESENTATION COMPLEXITY of the CATEGORY LEVEL OBJECT RECOGNITION SYSTEM .\n",
            "\n",
            "945 1000\n",
            "action recognition in VIDEOS is a challenging task due to the complexity of the SPATIO-TEMPORAL PATTERNS to model and the difficulty to acquire and learn on large quantities of VIDEO DATA . DEEP LEARNING , although a breakthrough for IMAGE CLASSIFICATION and showing promise for VIDEOS , has still not clearly <unk> ACTION RECOGNITION METHODS using HAND-CRAFTED FEATURES , even when training on massive datasets . in this paper , we introduce HYBRID VIDEO CLASSIFICATION ARCHITEC-TURES based on carefully designed UNSUPERVISED REPRESENTATIONS OF HAND-CRAFTED SPATIO-TEMPORAL FEATURES classified by SUPERVISED DEEP NETWORKS . as we show in our experiments on five popular benchmarks for ACTION RECOGNITION , our HYBRID MODEL combines the best of both worlds : it is data efficient -lrb- trained on 150 to <unk> short clips -rrb- and yet improves significantly on the state of the art , including recent DEEP MODELS trained on millions of MANUALLY LABELLED IMAGES and VIDEOS . \n",
            "this paper addresses the problem of ACTION RECOGNITION IN VIDEOS from VIDEO DATA , VIDEOS , and VIDEOS . we propose a novel HYBRID MODEL based on SUPERVISED DEEP NETWORKS to learn SPATIO-TEMPORAL PATTERNS from VIDEO DATA and VIDEOS . the proposed approach is based on the idea of DEEP LEARNING , which is based on DEEP LEARNING . the proposed approach is based on the use of SUPERVISED DEEP NETWORKS , which is able to capture the SPATIO-TEMPORAL PATTERNS of the data . the proposed approach is evaluated on a variety of MANUALLY LABELLED IMAGES and VIDEOS . the experimental results show that the proposed approach is effective in improving the ACTION RECOGNITION performance in comparison to the state-of-the-art methods .\n",
            "\n",
            "946 1000\n",
            "<unk> -lrb- <unk> information -rrb- is an <unk> <unk> spoken document RETRIEVAL -lrb- CL-SDR -rrb- system developed during the johns hopkins university summer workshop 2000 . we integrate SPEECH RECOGNITION , MACHINE TRANSLATION , and INFORMATION RETRIEVAL TECHNOLOGIES to perform CL-SDR . MEI -LRB- MANDARIN-ENGLISH INFORMATION -RRB- advocates a MULTI-SCALE PARADIGM , where both CHINESE WORDS and <unk> -lrb- characters and syllables -rrb- are used in RETRIEVAL . the use of SUBWORD UNITS can complement the WORD UNIT in handling the problems of CHINESE WORD TOKENIZATION AMBIGUITY , CHINESE HOMOPHONE AMBIGUITY , and OUT-OF-VOCABULARY WORDS in AUDIO INDEXING . this paper focuses on MULTI-SCALE AUDIO INDEXING in MEI -LRB- MANDARIN-ENGLISH INFORMATION -RRB- . experiments are based on the topic detection and tracking corpora -lrb- <unk> and <unk> -rrb- , where we indexed VOICE OF AMERICA MANDARIN NEWS BROADCASTS by SPEECH RECOGNITION on both the WORD AND SUBWORD SCALES . in this paper , we discuss the development of the MEI SYLLABLE RECOGNIZER , the representations of SPOKEN DOCUMENTS using OVERLAPPING SUBWORD N-GRAMS and LATTICE STRUCTURES . results show that augmenting words with <unk> is beneficial to CL-SDR performance . \n",
            "this paper presents a novel approach to SPEECH RECOGNITION in MACHINE TRANSLATION . the proposed approach is based on the use of SUBWORD UNITS and INFORMATION RETRIEVAL TECHNOLOGIES . the proposed approach is based on the use of SUBWORD UNITS and SUBWORD UNITS . the proposed approach is based on the use of SUBWORD UNITS and SUBWORD UNITS . the proposed approach is based on the use of SUBWORD UNITS and SUBWORD UNITS . the proposed approach is evaluated on a VOICE OF AMERICA MANDARIN NEWS BROADCASTS containing CHINESE WORDS , CHINESE WORDS and SPOKEN DOCUMENTS . the results show that the proposed approach is effective in improving the RETRIEVAL in SPEECH RECOGNITION .\n",
            "\n",
            "947 1000\n",
            "we describe a ROBUST SPEECH UNDERSTANDING SYSTEM based on our newly developed approach to SPOKEN LANGUAGE PROCESSING . we show that a robust NLU SYSTEM can be rapidly developed using a relatively simple SPEECH RECOGNIZER to provide sufficient information for DATABASE RETRIEVAL by SPOKEN LANGUAGE PROCESSING . our experimental ROBUST SPEECH UNDERSTANDING SYSTEM consists of three components : a SPEECH RECOGNIZER based on HMM , a NATURAL LANGUAGE PARSER based on CONCEPTUAL RELATIONAL GRAMMAR and a DATA RETRIEVAL SYSTEM based on the ATIS DATABASE . with the use of the ROBUST SPEECH UNDERSTANDING SYSTEM , DATABASE QUERY TASKS can be successfully performed . \n",
            "this paper describes a ROBUST SPEECH UNDERSTANDING SYSTEM for DATABASE RETRIEVAL in a ROBUST SPEECH UNDERSTANDING SYSTEM . the ROBUST SPEECH UNDERSTANDING SYSTEM consists of a CONCEPTUAL RELATIONAL GRAMMAR and a NATURAL LANGUAGE PARSER . the ROBUST SPEECH UNDERSTANDING SYSTEM consists of a CONCEPTUAL RELATIONAL GRAMMAR and a DATA RETRIEVAL SYSTEM . the ROBUST SPEECH UNDERSTANDING SYSTEM consists of a CONCEPTUAL RELATIONAL GRAMMAR and an HMM . the DATA RETRIEVAL SYSTEM is trained on a ATIS DATABASE and a DATA RETRIEVAL SYSTEM is trained on a ATIS DATABASE and a DATA RETRIEVAL SYSTEM . the performance of the proposed ROBUST SPEECH UNDERSTANDING SYSTEM is evaluated on a ATIS DATABASE and a DATA RETRIEVAL SYSTEM . the performance of the proposed ROBUST SPEECH UNDERSTANDING SYSTEM is evaluated on a ATIS DATABASE and a DATA RETRIEVAL SYSTEM .\n",
            "\n",
            "948 1000\n",
            "the UNSHIELDED TWISTED PAIR can be used as a TRANSMISSION MEDIA for LOCAL DISTRIBUTION NETWORKS . to maintain a high transmission throughput , an ANALOG or a DIGITAL ADAPTIVE CHANNEL EQUALIZER is usually required in the RECEIVER to minimize the effect of INTER-SYMBOL INTERFERENCE . under the observation that the high sampling rate high precision <unk> and subsequent DIGITAL ADAPTIVE SIGNAL PROCESSING is an expensive approach , a DIRECT EQUALIZATION METHOD , where the EQUALIZER is implemented in the TRANSMITTER , is proposed for SYMMETRICAL TWISTED PAIR TRANSMISSION CHANNELS . this DIRECT EQUALIZATION METHOD can also be applied to the ANALOG EQUALIZATION APPROACH for REDUCED SYSTEM COMPLEXITY . \n",
            "this paper proposes a novel ANALOG EQUALIZATION APPROACH for DIGITAL ADAPTIVE SIGNAL PROCESSING . the proposed ANALOG EQUALIZATION APPROACH is based on a UNSHIELDED TWISTED PAIR , a UNSHIELDED TWISTED PAIR , and a UNSHIELDED TWISTED PAIR for the TRANSMITTER . the proposed ANALOG EQUALIZATION APPROACH is based on a UNSHIELDED TWISTED PAIR , which is based on a UNSHIELDED TWISTED PAIR . the proposed ANALOG EQUALIZATION APPROACH is based on a UNSHIELDED TWISTED PAIR , which is based on a UNSHIELDED TWISTED PAIR . the proposed ANALOG EQUALIZATION APPROACH is applied to the SYMMETRICAL TWISTED PAIR TRANSMISSION CHANNELS of the TRANSMISSION MEDIA . the performance of the proposed ANALOG EQUALIZATION APPROACH is evaluated in terms of the REDUCED SYSTEM COMPLEXITY and the REDUCED SYSTEM COMPLEXITY of the EQUALIZER .\n",
            "\n",
            "949 1000\n",
            "semi-supervised learning algorithms commonly incorporate the available BACKGROUND KNOWLEDGE such that an expression of the derived model 's quality is improved . depending on the specific CONTEXT QUALITY can take several forms and can be related to the generalization performance or to a simple CLUSTERING COHERENCE MEASURE . recently , a novel perspective of SEMI-SUPERVISED LEARNING has been put forward , that associates SEMI-SUPERVISED CLUSTERING with the efficiency of SPECTRAL METHODS . more precisely , it has been demonstrated that the appropriate use of PARTIAL SUPERVISION can bias the DATA LAPLACIAN MATRIX such that the necessary EIGENVEC-TOR COMPUTATIONS are provably accelerated . this result allows DATA MINING PRACTITIONERS to use BACKGROUND KNOWLEDGE not only for improving the quality of clustering results , but also for accelerating the required EIGENVEC-TOR COMPUTATIONS . in this paper we initially provide a high level overview of the relevant efficiency maximizing SEMI-SUPERVISED METHODS such that their THEORETICAL INTUITIONS are comprehensively outlined . consecutively , we demonstrate how these methods can be extended to handle multiple clusters and also discuss possible issues that may arise in the CONTINUOUS SEMI-SUPERVISED SOLUTION . finally , we illustrate the proposed extensions empirically in the context of TEXT CLUSTERING . \n",
            "this paper addresses the problem of DATA MINING PRACTITIONERS for TEXT CLUSTERING . in particular , we focus on the problem of DATA MINING PRACTITIONERS , and propose a novel CLUSTERING COHERENCE MEASURE based on PARTIAL SUPERVISION . the proposed approach is based on the use of PARTIAL SUPERVISION for SEMI-SUPERVISED CLUSTERING . the proposed approach is based on the use of PARTIAL SUPERVISION , which is based on PARTIAL SUPERVISION . the proposed approach is based on the use of PARTIAL SUPERVISION for SEMI-SUPERVISED CLUSTERING . the proposed approach is evaluated on a variety of DATA MINING PRACTITIONERS . the results show that the proposed method is effective in improving the CONTEXT QUALITY and CONTEXT QUALITY of the proposed method .\n",
            "\n",
            "950 1000\n",
            "we describe a UNIFIED FORMULATION and algorithm to find an extremely sparse representation for CALCIUM IMAGE SEQUENCES in terms of CELL LOCATIONS , CELL SHAPES , SPIKE TIMINGS and IMPULSE RESPONSES . solution of a single OPTIMIZATION PROBLEM yields CELL SEGMENTATIONS and ACTIVITY ESTIMATES that are on par with the state of the art , without the need for HEURISTIC PRE-OR POSTPROCESSING . experiments on REAL AND SYNTHETIC DATA demonstrate the viability of the proposed method . \n",
            "in this paper , we propose a novel UNIFIED FORMULATION for CALCIUM IMAGE SEQUENCES . the proposed UNIFIED FORMULATION is based on the use of a set of IMPULSE RESPONSES , SPIKE TIMINGS , SPIKE TIMINGS , SPIKE TIMINGS , and IMPULSE RESPONSES . the OPTIMIZATION PROBLEM is formulated as a UNIFIED FORMULATION . the proposed UNIFIED FORMULATION is applied to the problem of SPIKE TIMINGS , CELL SHAPES , CELL SHAPES , and IMPULSE RESPONSES . experimental results on both REAL AND SYNTHETIC DATA demonstrate the effectiveness of the proposed UNIFIED FORMULATION .\n",
            "\n",
            "951 1000\n",
            "we have previously proposed a cross-validation -lrb- CV -rrb- based gaussian mixture optimization method that efficiently optimizes the MODEL STRUCTURE based on CV LIKELIHOOD . in this study , we propose AGGREGATED CROSS-VALIDATION that introduces a BAGGING-LIKE APPROACH in the CV FRAMEWORK to reinforce the MODEL SELECTION ABILITY . while a single model is used in CV to evaluate a HELD-OUT SUBSET , AGGREGATED CROSS-VALIDATION uses multiple models to reduce the variance in the SCORE ESTIMATION . by integrating AGGREGATED CROSS-VALIDATION instead of CV in the GAUSSIAN MIXTURE OPTIMIZATION ALGORITHM , an AGCV LIKELIHOOD BASED GAUSSIAN MIXTURE OPTIMIZATION ALGORITHM is obtained . the AGCV LIKELIHOOD BASED GAUSSIAN MIXTURE OPTIMIZATION ALGORITHM works efficiently by using sufficient statistics and can be applied to LARGE MODELS such as GAUSSIAN MIXTURE HMM . the proposed AGCV LIKELIHOOD BASED GAUSSIAN MIXTURE OPTIMIZATION ALGORITHM is evaluated by SPEECH RECOGNITION experiments on ORAL PRESENTATIONS and it is shown that lower WORD ERROR RATES are obtained by the AGCV LIKELIHOOD BASED GAUSSIAN MIXTURE OPTIMIZATION ALGORITHM when compared to CV AND MDL BASED METHODS . \n",
            "in this paper , we propose a novel AGCV LIKELIHOOD BASED GAUSSIAN MIXTURE OPTIMIZATION ALGORITHM based on AGGREGATED CROSS-VALIDATION . the proposed AGCV LIKELIHOOD BASED GAUSSIAN MIXTURE OPTIMIZATION ALGORITHM is based on the use of AGGREGATED CROSS-VALIDATION , which is based on a GAUSSIAN MIXTURE HMM . the proposed AGCV LIKELIHOOD BASED GAUSSIAN MIXTURE OPTIMIZATION ALGORITHM is based on a GAUSSIAN MIXTURE HMM , which is based on a GAUSSIAN MIXTURE OPTIMIZATION ALGORITHM . the proposed AGCV LIKELIHOOD BASED GAUSSIAN MIXTURE OPTIMIZATION ALGORITHM is based on a GAUSSIAN MIXTURE OPTIMIZATION ALGORITHM , which is based on a GAUSSIAN MIXTURE OPTIMIZATION ALGORITHM . the proposed AGCV LIKELIHOOD BASED GAUSSIAN MIXTURE OPTIMIZATION ALGORITHM is compared to the state-of-the-art CV AND MDL BASED METHODS . the proposed AGCV LIKELIHOOD BASED GAUSSIAN MIXTURE OPTIMIZATION ALGORITHM is evaluated on SPEECH RECOGNITION and compared with the CV AND MDL BASED METHODS . the experimental results show that the proposed AGCV LIKELIHOOD BASED GAUSSIAN MIXTURE OPTIMIZATION ALGORITHM outperforms the existing CV AND MDL BASED METHODS in terms of both WORD ERROR RATES and WORD ERROR RATES .\n",
            "\n",
            "952 1000\n",
            "we introduce the TERM COSEGMENTATION which denotes the task of segmenting simultaneously the common parts of an IMAGE PAIR . a GENERATIVE MODEL for COSEGMENTATION is presented . INFERENCE in the GENERATIVE MODEL leads to minimizing an energy with an MRF TERM ENCODING SPATIAL COHERENCY and a GLOBAL CONSTRAINT which attempts to match the APPEARANCE HISTOGRAMS of the common parts . this energy has not been proposed previously and its OPTIMIZATION is challenging and NP-HARD . for this TERM COSEGMENTATION a novel OPTIMIZATION SCHEME which we call TRUST REGION GRAPH CUTS is presented . we demonstrate that this OPTIMIZATION SCHEME has the potential to improve a wide range of research : OBJECT DRIVEN IMAGE RETRIEVAL , VIDEO TRACKING and segmentation , and INTERACTIVE IMAGE EDITING . the power of the OPTIMIZATION SCHEME lies in its generality , the common part can be a RIGID/NON-RIGID OBJECT -lrb- or scene -rrb- , observed from different viewpoints or even similar objects of the same class . \n",
            "in this paper , we propose a novel GENERATIVE MODEL for OBJECT DRIVEN IMAGE RETRIEVAL and OBJECT DRIVEN IMAGE RETRIEVAL . the proposed GENERATIVE MODEL is based on a novel GENERATIVE MODEL , called TRUST REGION GRAPH CUTS , for OBJECT DRIVEN IMAGE RETRIEVAL and OBJECT DRIVEN IMAGE RETRIEVAL . the proposed GENERATIVE MODEL is based on a GENERATIVE MODEL , called TRUST REGION GRAPH CUTS , for OBJECT DRIVEN IMAGE RETRIEVAL and OBJECT DRIVEN IMAGE RETRIEVAL . the proposed GENERATIVE MODEL is based on a novel GENERATIVE MODEL , called TRUST REGION GRAPH CUTS , for OBJECT DRIVEN IMAGE RETRIEVAL and OBJECT DRIVEN IMAGE RETRIEVAL . the proposed GENERATIVE MODEL is applied to the problem of OBJECT DRIVEN IMAGE RETRIEVAL and OBJECT DRIVEN IMAGE RETRIEVAL . the experimental results show that the proposed GENERATIVE MODEL is very effective and can be applied to INTERACTIVE IMAGE EDITING and VIDEO TRACKING .\n",
            "\n",
            "953 1000\n",
            "in this paper , we present a novel approach for RELATION EXTRACTION using only TERM PAIRS as the input without TEXTUAL FEATURES . we aim to build a single JOINT SPACE for each relation which is then used to produce RELATION SPECIFIC TERM EMBEDDINGS . the proposed method fits particularly well for domains in which similar arguments are often associated with similar relations . it can also handle the situation when the LABELED DATA is limited . the proposed method is evaluated both theoretically with a proof for the CLOSED-FORM SOLUTION and experimentally with promising results on both DBPEDIA AND MEDICAL RELATIONS . \n",
            "this paper addresses the problem of RELATION EXTRACTION from a single image . we propose a novel method to learn a RELATION SPECIFIC TERM EMBEDDINGS from a set of TERM PAIRS . the proposed approach is based on the use of a JOINT SPACE , which is able to deal with LABELED DATA . in the proposed method , the proposed method is able to learn a set of TERM PAIRS from the target domain . the proposed method can be applied to the problem of RELATION EXTRACTION . the experimental results demonstrate the effectiveness of the proposed method in comparison to the state-of-the-art .\n",
            "\n",
            "954 1000\n",
            "for PITCH TRACKING of a single speaker , a common requirement is to find the optimal path through a set of VOICED OR VOICELESS PITCH ESTIMATES over a sequence of time frames . DYNAMIC PROGRAMMING ALGORITHMS have been applied before to this problem . here , the PITCH CANDIDATES are provided by a MULTI-CHANNEL AUTOCORRELATION-BASED ESTIMATOR , and DYNAMIC PROGRAMMING ALGORITHMS is extended to PITCH TRACKING of multiple concurrent speakers . we use the resulting PITCH INFORMATION to enhance HARMONIC CONTENT in NOISY SPEECH and to obtain SEPARATIONS OF TARGET from INTERFERING SPEECH . \n",
            "this paper presents a novel method for PITCH TRACKING in NOISY SPEECH . the proposed method is based on the use of a MULTI-CHANNEL AUTOCORRELATION-BASED ESTIMATOR to estimate the HARMONIC CONTENT . the proposed method is based on the use of PITCH INFORMATION extracted from the HARMONIC CONTENT . the proposed method is based on the use of a MULTI-CHANNEL AUTOCORRELATION-BASED ESTIMATOR to estimate the PITCH CANDIDATES and the PITCH CANDIDATES . the proposed method is based on a MULTI-CHANNEL AUTOCORRELATION-BASED ESTIMATOR that uses PITCH INFORMATION to estimate the HARMONIC CONTENT . the experimental results show that the proposed method outperforms the existing methods in terms of PITCH TRACKING and PITCH TRACKING .\n",
            "\n",
            "955 1000\n",
            "finding meaningful , structured representations of 3D POINT CLOUD DATA has become a core task for SPATIAL PERCEPTION APPLICATIONS . in this paper we introduce a method for constructing COMPACT GENERATIVE REPRESENTATIONS OF PCD at multiple levels of detail . as opposed to DETERMINISTIC STRUCTURES such as VOXEL GRIDS or OCTREES , we propose PROBABILISTIC SUBDIVISIONS of the data through LOCAL MIXTURE MODELING , and show how these SUBDIVISIONS can provide a MAXIMUM LIKELIHOOD SEGMENTATION of the data . the final representation is hierarchical , compact , PARA-METRIC , and statistically derived , facilitating RUN-TIME OCCUPANCY CALCULATIONS through STOCHASTIC SAMPLING . unlike traditional DETERMINISTIC SPATIAL SUBDIVISION METHODS , our technique enables DYNAMIC CREATION OF VOXEL GRIDS according the application 's best needs . in contrast to other GENER-ATIVE MODELS for 3D POINT CLOUD DATA , we explicitly enforce SPARSITY among points and mixtures , a technique which we call EXPECTATION SPARSIFICATION . this leads to a highly PARALLEL HIERARCHICAL EXPECTATION MAXIMIZATION ALGORITHM well-suited for the 3D POINT CLOUD DATA and real-time execution . we explore the trade-offs between MODEL FIDELITY and MODEL SIZE at various levels of detail , our tests showing favorable performance when compared to OCTREE AND NDT-BASED METHODS . \n",
            "this paper addresses the problem of DYNAMIC CREATION OF VOXEL GRIDS in VOXEL GRIDS . we propose a novel PARALLEL HIERARCHICAL EXPECTATION MAXIMIZATION ALGORITHM for 3D POINT CLOUD DATA , which is based on a PARALLEL HIERARCHICAL EXPECTATION MAXIMIZATION ALGORITHM . the proposed PARALLEL HIERARCHICAL EXPECTATION MAXIMIZATION ALGORITHM is based on a PARALLEL HIERARCHICAL EXPECTATION MAXIMIZATION ALGORITHM , which is a generalization of the PARALLEL HIERARCHICAL EXPECTATION MAXIMIZATION ALGORITHM to the DYNAMIC CREATION OF VOXEL GRIDS . the proposed PARALLEL HIERARCHICAL EXPECTATION MAXIMIZATION ALGORITHM is based on a PARALLEL HIERARCHICAL EXPECTATION MAXIMIZATION ALGORITHM , which is based on a PARALLEL HIERARCHICAL EXPECTATION MAXIMIZATION ALGORITHM . the proposed PARALLEL HIERARCHICAL EXPECTATION MAXIMIZATION ALGORITHM is based on a PARALLEL HIERARCHICAL EXPECTATION MAXIMIZATION ALGORITHM , which is a generalization of the PARALLEL HIERARCHICAL EXPECTATION MAXIMIZATION ALGORITHM . the proposed PARALLEL HIERARCHICAL EXPECTATION MAXIMIZATION ALGORITHM is applied to 3D POINT CLOUD DATA , and the results show that the proposed algorithm is able to perform well in SPATIAL PERCEPTION APPLICATIONS . the proposed algorithm is applied to 3D POINT CLOUD DATA , and the results show that the proposed algorithm is able to perform well in SPATIAL PERCEPTION APPLICATIONS . the proposed algorithm is applied to 3D POINT CLOUD DATA , and the results show that the proposed algorithm is able to perform well in SPATIAL PERCEPTION APPLICATIONS . the proposed algorithm is capable of efficiently generating a wide range of DETERMINISTIC STRUCTURES , including VOXEL GRIDS , OCTREES , and MODEL SIZE .\n",
            "\n",
            "956 1000\n",
            "this paper wants to discuss several aspects of MULTIMODAL/MULTIMEDIA LANGUAGE RESOURCES such as the use of METADATA DESCRIPTIONS for easy location purposes , their COLLABORATIVE ANNOTATION and exploitation via internet , the generation of SYNCHRONIZED MEDIA AND TEXT STREAMS in DISTRIBUTED ENVIRONMENTS , and general annotation formats . these aspects that although they may be discussed independently have to fit together seamlessly to offer users an adequate exploitation environment that is up to the huge amount of data that is available in modern MULTI-MEDIA CORPORA and is able to exploit fully the current technology advancements . \n",
            "this paper presents a novel approach to COLLABORATIVE ANNOTATION in DISTRIBUTED ENVIRONMENTS . the approach is based on the use of METADATA DESCRIPTIONS for COLLABORATIVE ANNOTATION . the approach is based on the use of METADATA DESCRIPTIONS in order to create a large number of METADATA DESCRIPTIONS . the approach is based on the use of METADATA DESCRIPTIONS in order to evaluate the quality of the METADATA DESCRIPTIONS . the proposed approach is evaluated on a variety of MULTI-MEDIA CORPORA , and the results show that the proposed method is effective in improving the COLLABORATIVE ANNOTATION performance .\n",
            "\n",
            "957 1000\n",
            "for high performance CDMA COMMUNICATIONS , MULTIUSER DETECTION is often required to suppress the multiple access interference <unk> . most MULTIUSER DETECTORS rely on accurate channel information to recover the MULTIUSER DIGITAL SIGNALS . this paper studies the BLIND CHANNEL ESTIMATION PROBLEM for DS-CDMA SYSTEMS using APERIODIC SPREADING CODES . the MAXIMUM LIKELIHOOD ML ESTIMATOR is formulated for CHANNEL ESTIMATION . we rst convert the MULTIUSER PARAMETER ESTIMATION PROBLEM into a set of single user optimization problems via ALTERNATING OPTIMIZATION , and then determine the CHANNEL PARAMETERS for each user using an ITERATIVE ALGORITHM derived . it is shown by COMPUTER SIMULATION that this ITERATIVE ALGORITHM can reach GLOBAL MAXIMA almost always under MEDIUM SNR VALUES . \n",
            "this paper addresses the problem of MULTIUSER DETECTION for MULTIUSER DIGITAL SIGNALS . in this paper , we propose a novel ITERATIVE ALGORITHM for MULTIUSER DIGITAL SIGNALS , which is based on a MAXIMUM LIKELIHOOD ML ESTIMATOR . the proposed ITERATIVE ALGORITHM is based on the use of APERIODIC SPREADING CODES for MULTIUSER DETECTION . the proposed ITERATIVE ALGORITHM is based on a MAXIMUM LIKELIHOOD ML ESTIMATOR , which is based on ALTERNATING OPTIMIZATION . the proposed ITERATIVE ALGORITHM is applied to MULTIUSER DIGITAL SIGNALS , and the experimental results demonstrate the effectiveness of the proposed ITERATIVE ALGORITHM . the performance of the proposed method is demonstrated on a variety of MULTIUSER DIGITAL SIGNALS .\n",
            "\n",
            "958 1000\n",
            "understanding the DEPENDENCY STRUCTURE of a set of variables is a key component in various SIGNAL PROCESSING APPLICATIONS which involve DATA ASSOCIATION . the simple task of detecting whether any dependency exists is particularly difficult when models of the data are unknown or difficult to characterize because of HIGH-DIMENSIONAL MEASUREMENTS . we review the use of NONPARAMETRIC TESTS for characterizing dependency and how to carry out these tests with HIGH-DIMENSIONAL OBSERVATIONS . in addition we present a method to assess the significance of the tests . \n",
            "this paper addresses the problem of DATA ASSOCIATION in SIGNAL PROCESSING APPLICATIONS . we propose a novel approach to the problem of DATA ASSOCIATION . the proposed approach is based on the use of a DEPENDENCY STRUCTURE , which is able to deal with HIGH-DIMENSIONAL OBSERVATIONS . the proposed approach is based on the use of HIGH-DIMENSIONAL MEASUREMENTS , which is able to deal with HIGH-DIMENSIONAL OBSERVATIONS . we demonstrate the effectiveness of the proposed method in the context of DATA ASSOCIATION . the experimental results show that the proposed approach can significantly improve the performance of the proposed NONPARAMETRIC TESTS .\n",
            "\n",
            "959 1000\n",
            "it has been shown that large gains in SPEECH INTELLIGIBILITY can be obtained by using the BINARY MASK APPROACH which retains the TIME-FREQUENCY UNITS of the mixture signal that are stronger than the INTERFERING NOISE -lrb- <unk> -rrb- -lrb- i.e. , snr > 0 db -rrb- , and removes the T-F UNITS where the INTERFERING NOISE dominates . in this paper , we introduce a new BINARY MASK for improving SPEECH INTELLIGIBILITY based on NOISE DISTORTION CONSTRAINTS . a BINARY MASK is designed to retain NOISE OVERESTIMATED T-F UNITS while discarding noise underestimated T-F UNITS . listening tests were conducted to evaluate the new BINARY MASK in terms of intelligibility . results from the listening tests indicated that large gains in intelligibility can be achieved by the application of the proposed BINARY MASK to NOISE-CORRUPTED SPEECH even at extremely low snr levels -lrb- <unk> db -rrb- . \n",
            "in this paper , we propose a novel approach to SPEECH INTELLIGIBILITY based on a BINARY MASK APPROACH . the proposed BINARY MASK APPROACH is based on the use of a BINARY MASK to estimate the NOISE DISTORTION CONSTRAINTS . the proposed BINARY MASK APPROACH is based on the use of a BINARY MASK to estimate the NOISE DISTORTION CONSTRAINTS . the proposed BINARY MASK APPROACH is applied to NOISE-CORRUPTED SPEECH , and the results show that the proposed BINARY MASK APPROACH is effective in reducing the INTERFERING NOISE . the proposed method is robust to INTERFERING NOISE and is robust to INTERFERING NOISE .\n",
            "\n",
            "960 1000\n",
            "inspired by MULTI-SCALE TENSOR VOTING , a COMPUTATIONAL FRAMEWORK for PERCEPTUAL GROUPING AND SEGMENTATION , we propose an EDGE-DIRECTED TECHNIQUE for COLOR IMAGE SUPER-RESOLUTION given a single LOW-RESOLUTION COLOR IMAGE . our EDGE-DIRECTED TECHNIQUE combines the advantages of EDGE-DIRECTED , RECONSTRUCTION-BASED AND LEARNING-BASED METHODS , and is unique in two ways . first , we consider simultaneously all the three COLOR CHANNELS in our MULTI-SCALE TENSOR VOTING FRAMEWORK to produce a MULTI-SCALE EDGE REPRESENTATION to guide the process of HIGH-RESOLUTION COLOR IMAGE RECONSTRUCTION , which is subject to the BACK PROJECTION CONSTRAINT . fine details are inferred without noticeable blurry or ringing artifacts . second , the inference of HIGH-RESOLUTION CURVES is achieved by MULTI-SCALE TENSOR VOTING , using the DENSE VOTING FIELD as an EDGE-PRESERVING SMOOTHNESS PRIOR which is derived geometrically without any TIME-CONSUMING LEARNING PROCEDURE . qualitative and quantitative results indicate that our EDGE-DIRECTED TECHNIQUE produces convincing results in complex test cases typically used by state-of-the-art IMAGE SUPER-RESOLUTION TECHNIQUES . \n",
            "this paper presents a novel COMPUTATIONAL FRAMEWORK for PERCEPTUAL GROUPING AND SEGMENTATION in COLOR CHANNELS . the proposed COMPUTATIONAL FRAMEWORK is based on a MULTI-SCALE TENSOR VOTING FRAMEWORK for PERCEPTUAL GROUPING AND SEGMENTATION . the proposed COMPUTATIONAL FRAMEWORK is based on a MULTI-SCALE TENSOR VOTING FRAMEWORK for PERCEPTUAL GROUPING AND SEGMENTATION . the proposed MULTI-SCALE TENSOR VOTING FRAMEWORK is based on a MULTI-SCALE TENSOR VOTING FRAMEWORK for PERCEPTUAL GROUPING AND SEGMENTATION . the proposed COMPUTATIONAL FRAMEWORK is based on a MULTI-SCALE TENSOR VOTING FRAMEWORK , which is based on a MULTI-SCALE TENSOR VOTING FRAMEWORK . the proposed COMPUTATIONAL FRAMEWORK is applied to COLOR IMAGE SUPER-RESOLUTION , and the results show that the proposed COMPUTATIONAL FRAMEWORK is effective in improving the PERCEPTUAL GROUPING AND SEGMENTATION performance . the proposed COMPUTATIONAL FRAMEWORK is applied to the PERCEPTUAL GROUPING AND SEGMENTATION , and the results show that the proposed COMPUTATIONAL FRAMEWORK is effective in improving the PERCEPTUAL GROUPING AND SEGMENTATION performance .\n",
            "\n",
            "961 1000\n",
            "in many REAL-TIME APPLICATIONS , SAMPLE VALUES and time <unk> are delivered in pairs , where sampling times are non-uniform . FREQUENCY ANALYSIS using NON-UNIFORM DATA occurs in various REAL LIFE PROBLEMS and EMBEDDED SYSTEMS , such as VIBRA-TIONAL ANALYSIS in cars and control of packet network queue lengths . our contribution is to first overview different ways to approximate the FOURIER TRANSFORM , and secondly to give ANALYTICAL EXPRESSIONS for how NON-UNIFORM SAMPLING affects these approximations . the results are expressed in terms of FREQUENCY WINDOWS describing how a single frequency in the CONTINUOUS TIME SIGNAL is smeared out in the FREQUENCY DOMAIN , or , more precisely , in the expected value of the FOURIER TRANSFORM APPROXIMATION . \n",
            "this paper addresses the problem of NON-UNIFORM SAMPLING from NON-UNIFORM DATA . we propose a novel approach to the problem of NON-UNIFORM SAMPLING from NON-UNIFORM DATA . the proposed approach is based on the use of NON-UNIFORM DATA to estimate the SAMPLE VALUES and the SAMPLE VALUES . the proposed approach is based on the use of NON-UNIFORM DATA to estimate the SAMPLE VALUES . the proposed approach is based on the use of NON-UNIFORM DATA to estimate the SAMPLE VALUES . the proposed method is evaluated on a variety of REAL LIFE PROBLEMS , including SAMPLE VALUES , and EMBEDDED SYSTEMS . the experimental results show that the proposed method can achieve better performance than the state-of-the-art methods .\n",
            "\n",
            "962 1000\n",
            "to precisely model the time dependency of FEATURES is one of the important issues for SPEECH RECOGNITION . SEGMENTAL UNIT INPUT HMM with a DIMENSIONALITY REDUCTION METHOD is widely used to address this issue . LINEAR DISCRIMINANT ANALYSIS and HETEROSCEDASTIC DISCRIMINANT ANALYSIS are classical and popular approaches to reduce dimensionality . however , it is difficult to find one particular criterion suitable for any kind of data set in carrying out DIMENSION-ALITY REDUCTION while preserving DISCRIMINATIVE INFORMATION . in this paper , we propose a new framework which we call POWER LINEAR DISCRIMINANT ANALYSIS . POWER LINEAR DISCRIMINANT ANALYSIS can describe various criteria including LINEAR DISCRIMINANT ANALYSIS and HETEROSCEDASTIC DISCRIMINANT ANALYSIS with one parameter . experimental results show that the POWER LINEAR DISCRIMINANT ANALYSIS is more effective than PCA , LINEAR DISCRIMINANT ANALYSIS , and HETEROSCEDASTIC DISCRIMINANT ANALYSIS for various DATA SETS . \n",
            "this paper presents a novel DIMENSIONALITY REDUCTION METHOD for SPEECH RECOGNITION . the proposed POWER LINEAR DISCRIMINANT ANALYSIS is based on the use of LINEAR DISCRIMINANT ANALYSIS and LINEAR DISCRIMINANT ANALYSIS . the proposed POWER LINEAR DISCRIMINANT ANALYSIS is based on the use of LINEAR DISCRIMINANT ANALYSIS and LINEAR DISCRIMINANT ANALYSIS . the performance of the proposed POWER LINEAR DISCRIMINANT ANALYSIS is compared to standard LINEAR DISCRIMINANT ANALYSIS and LINEAR DISCRIMINANT ANALYSIS . the performance of the proposed POWER LINEAR DISCRIMINANT ANALYSIS is compared to LINEAR DISCRIMINANT ANALYSIS and LINEAR DISCRIMINANT ANALYSIS . the performance of the proposed POWER LINEAR DISCRIMINANT ANALYSIS is compared to the standard HETEROSCEDASTIC DISCRIMINANT ANALYSIS and LINEAR DISCRIMINANT ANALYSIS . the performance of the proposed POWER LINEAR DISCRIMINANT ANALYSIS is compared to the standard LINEAR DISCRIMINANT ANALYSIS and LINEAR DISCRIMINANT ANALYSIS . the performance of the proposed POWER LINEAR DISCRIMINANT ANALYSIS is compared to the standard LINEAR DISCRIMINANT ANALYSIS and LINEAR DISCRIMINANT ANALYSIS .\n",
            "\n",
            "963 1000\n",
            "in this work , a novel approach for NONLINEAR ACOUSTIC ECHO CANCELLATION is proposed . the main innovative idea of the proposed method is to model only the small region of the ECHO PATH around the direct path by a group of PARALLEL HAMMERSTEIN MODELS , to estimate a NON-LINEAR PREPROCESSOR by correlations between the LINEAR KERNELS OF THE HAMMERSTEIN SUBMODELS , and to describe the remaining ECHO PATH by a simple HAMMERSTEIN MODEL with the PREPROCESSOR determined in the aforementioned way . while the COMPUTATIONAL COMPLEXITY of such a system increases only slightly in comparison to a LINEAR ECHO CANCELLER , experiments with SPEECH RECORDINGS from a SMARTPHONE in different environments confirm a significantly increased ECHO CANCELLATION performance . \n",
            "this paper presents a novel HAMMERSTEIN MODEL for NONLINEAR ACOUSTIC ECHO CANCELLATION . the proposed HAMMERSTEIN MODEL is based on a HAMMERSTEIN MODEL of the ECHO PATH of the ECHO PATH of the ECHO PATH . the proposed HAMMERSTEIN MODEL is based on a HAMMERSTEIN MODEL of the ECHO PATH of the ECHO PATH of the ECHO PATH . the proposed HAMMERSTEIN MODEL is applied to the ECHO PATH of the ECHO PATH and the COMPUTATIONAL COMPLEXITY of the HAMMERSTEIN MODEL . experimental results show that the proposed HAMMERSTEIN MODEL significantly improves the COMPUTATIONAL COMPLEXITY and the COMPUTATIONAL COMPLEXITY of the HAMMERSTEIN MODEL .\n",
            "\n",
            "964 1000\n",
            "a method of LOCALISING OBJECTS in images is proposed . possible <unk> are evaluated using the CONTOUR DISCRIMINANT , a LIKELIHOOD RATIO which is derived from a PROBABILISTIC MODEL of the FEATURE DETECTION PROCESS . we treat each step in this process probabilistically , including the occurrence of CLUTTER FEATURES , and derive the OBSERVATION DENSITIES for both correct \\ target '' <unk> and incorrect \\ clutter '' <unk> . the CONTOUR DISCRIMINANT distinguishes target objects from the background even in heavy clutter , making only the most general assumptions about the form that clutter might take . the method generates samples <unk> to avoid the cost of processing an entire IMAGE , and promises to be particularly suited to the task of INITIALISING CONTOUR TRACKERS based on SAMPLING METHODS . \n",
            "in this paper , we propose a novel approach to FEATURE DETECTION PROCESS based on a PROBABILISTIC MODEL . the proposed method is based on the use of a PROBABILISTIC MODEL to estimate the OBSERVATION DENSITIES of the IMAGE . the proposed approach is based on the use of a PROBABILISTIC MODEL to estimate the OBSERVATION DENSITIES of the IMAGE . the proposed method is based on the use of a PROBABILISTIC MODEL , which is able to estimate the OBSERVATION DENSITIES of the IMAGE . the proposed method is evaluated on a variety of LOCALISING OBJECTS . the experimental results show that the proposed method outperforms the existing methods in terms of LIKELIHOOD RATIO and LIKELIHOOD RATIO .\n",
            "\n",
            "965 1000\n",
            "in this paper , the problem of TWO-DIMENSIONAL FREQUENCY ESTIMATION of a complex sinusoid embedded in a WHITE GAUSSIAN ADDITIVE NOISE is addressed . a new FREQUENCY ESTIMATOR based on a LEAST SQUARE PLANE FITTING of the estimated autocorrelation phase of the signal is derived . this algorithm requires a 2-D PHASE UNWRAPPING STEP which can be easily done . this algorithm is shown to be unbiased and attains the CRAMER RAO BOUNDS for high signal to NOISE RATIO -LRB- ACCURACY and ROBUSTNESS of this new 2-D FREQUENCY ESTI-MATOR are statistically assessed by MONTE CARLO SIMULATIONS . the results obtained show that a good LOCAL FREQUENCY ESTIMATION can be achieved with a very simple algorithm , and a very small amount of points used for the AUTOCORRELATION ESTIMATION . \n",
            "this paper addresses the problem of TWO-DIMENSIONAL FREQUENCY ESTIMATION in the presence of WHITE GAUSSIAN ADDITIVE NOISE . in particular , we consider the problem of TWO-DIMENSIONAL FREQUENCY ESTIMATION in the presence of WHITE GAUSSIAN ADDITIVE NOISE . we propose a novel 2-D FREQUENCY ESTI-MATOR based on LEAST SQUARE PLANE FITTING . the proposed 2-D FREQUENCY ESTI-MATOR is based on LEAST SQUARE PLANE FITTING . the proposed FREQUENCY ESTIMATOR is applied to the 2-D PHASE UNWRAPPING STEP of the 2-D FREQUENCY ESTI-MATOR . the performance of the proposed 2-D FREQUENCY ESTI-MATOR is evaluated in terms of NOISE RATIO -LRB- ACCURACY and ROBUSTNESS .\n",
            "\n",
            "966 1000\n",
            "goal driven learning -lrb- <unk> -rrb- focuses on systems that determine by themselves what has to be learnt and how to learn GOAL DRIVEN LEARNING . typically GOAL DRIVEN LEARNING use META-REASONING CAPABILITIES over a BASE REASONER , identifying LEARNING GOALS and devising strategies . in this paper we present a novel GOAL DRIVEN LEARNING to deal with complex AI SYSTEMS where the META-REASONING MODULE has to analyze the reasoning trace of multiple components with potentially different LEARNING PARADIGMS . our GOAL DRIVEN LEARNING works by distributing the generation of LEARNING STRATEGIES among the different modules instead of <unk> GOAL DRIVEN LEARNING in the META-REASONER . we implemented our GOAL DRIVEN LEARNING in the GOAL DRIVEN LEARNING , that works in the AIRSPACE TASK ORDERS DOMAIN , showing an increase in performance . \n",
            "this paper addresses the problem of GOAL DRIVEN LEARNING in AI SYSTEMS . the GOAL DRIVEN LEARNING is based on a BASE REASONER of the BASE REASONER . the proposed GOAL DRIVEN LEARNING is based on a BASE REASONER of the BASE REASONER . the proposed GOAL DRIVEN LEARNING consists of two steps . first , the LEARNING GOALS is used to estimate the BASE REASONER , and then the BASE REASONER is used to estimate the BASE REASONER . the proposed GOAL DRIVEN LEARNING is applied to the AIRSPACE TASK ORDERS DOMAIN , and the results show that the proposed GOAL DRIVEN LEARNING is effective in improving the META-REASONING CAPABILITIES performance .\n",
            "\n",
            "967 1000\n",
            "with improved RECOGNITION ACCURACIES for LVCSR TASKS , it has become possible to search large collections of SPONTANEOUS SPEECH for a variety of information . the MALACH CORPUS OF HOLOCAUST TESTIMONIALS is one such collection , in which we are interested in automatically transcribing and retrieving portions that are relevant to NAMED ENTITIES such as people , places , and organizations . since the <unk> were gathered from thousands of people in countries throughout europe , an extremely large number of potential NAMED ENTITIES are possible , and this causes a well-known dilemma : increasing the size of the vocabulary allows for more of these words to be recognized , but also increases confusability , and can harm RECOGNITION performance . however , the MALACH CORPUS OF HOLOCAUST TESTIMONIALS , like many other collections , includes SIDE INFORMATION or MALACH CORPUS OF HOLOCAUST TESTIMONIALS that can be exploited to provide prior information on exactly which NAMED ENTITIES are likely to appear . this paper proposes a method that capitalizes on this prior information to reduce NAMED-ENTITY RECOGNITION ERRORS by over 50 % relative , and simultaneously decrease the overall WORD ERROR RATE by 7 % relative . the MALACH CORPUS OF HOLOCAUST TESTIMONIALS we use derives from a PRE-INTERVIEW QUESTIONAIRE that includes the names of friends , <unk> , places visited , membership of organizations , synonyms of place names , and similar information . by augmenting the LEXICON AND LANGUAGE MODEL with this information on a SPEAKER-BY-SPEAKER BASIS , we are able to exploit the TEXTUAL INFORMATION that is already available in the corpus to facilitate much improved SPEECH RECOGNITION . \n",
            "this paper presents a novel approach to SPEECH RECOGNITION . the proposed method is based on a MALACH CORPUS OF HOLOCAUST TESTIMONIALS and a MALACH CORPUS OF HOLOCAUST TESTIMONIALS . the proposed approach is based on the use of PRE-INTERVIEW QUESTIONAIRE and MALACH CORPUS OF HOLOCAUST TESTIMONIALS . the proposed method is based on a MALACH CORPUS OF HOLOCAUST TESTIMONIALS and a MALACH CORPUS OF HOLOCAUST TESTIMONIALS . the proposed method is evaluated on a MALACH CORPUS OF HOLOCAUST TESTIMONIALS and a MALACH CORPUS OF HOLOCAUST TESTIMONIALS . the results show that the proposed method is effective in improving the RECOGNITION ACCURACIES of the LEXICON AND LANGUAGE MODEL in terms of WORD ERROR RATE and WORD ERROR RATE .\n",
            "\n",
            "968 1000\n",
            "when the goal is to achieve the best correct CLASSIFICATION RATE , CROSS ENTROPY and MEAN SQUARED ERROR are typical COST FUNCTIONS used to optimize CLASSIFIER performance . however , for many REAL-WORLD CLASSIFICATION PROBLEMS , the ROC CURVE is a more meaningful performance measure . we demonstrate that minimizing CROSS ENTROPY or MEAN SQUARED ERROR does not necessarily maximize the area under the ROC CURVE . we then consider alternative objective functions for training a CLASSIFIER to maximize the ROC CURVE directly . we propose an OBJECTIVE FUNCTION that is an approximation to the WILCOXON-MANN-WHITNEY STATISTIC , which is equivalent to the ROC CURVE . the proposed OBJECTIVE FUNCTION is differentiable , so GRADIENT-BASED METHODS can be used to train the CLASSIFIER . we apply the new OBJECTIVE FUNCTION to REAL-WORLD CUSTOMER BEHAVIOR PREDICTION PROBLEMS for a WIRELESS SERVICE PROVIDER and a CABLE SERVICE PROVIDER , and achieve reliable improvements in the ROC CURVE . \n",
            "in this paper , we present a novel approach to REAL-WORLD CUSTOMER BEHAVIOR PREDICTION PROBLEMS in the context of a WIRELESS SERVICE PROVIDER . the proposed method is based on the use of a WILCOXON-MANN-WHITNEY STATISTIC and a WILCOXON-MANN-WHITNEY STATISTIC . the proposed method is based on the use of a WILCOXON-MANN-WHITNEY STATISTIC and a WILCOXON-MANN-WHITNEY STATISTIC . the proposed method is based on the use of a set of COST FUNCTIONS , a WILCOXON-MANN-WHITNEY STATISTIC and a WILCOXON-MANN-WHITNEY STATISTIC . in the proposed method , a WILCOXON-MANN-WHITNEY STATISTIC is used to estimate the CLASSIFICATION RATE of the CLASSIFIER . the performance of the proposed method is evaluated in terms of CLASSIFICATION RATE and MEAN SQUARED ERROR . the experimental results show that the proposed method is effective in improving the CLASSIFICATION RATE and the MEAN SQUARED ERROR .\n",
            "\n",
            "969 1000\n",
            "the RECURSIVE LEAST SQUARES ALGORITHM is well known and has been widely used for many years . most analyses of RECURSIVE LEAST SQUARES ALGORITHM have assumed statistical properties of the data or the NOISE PROCESS , but recent robust h ∞ analyses have been used to bound the ratio of the performance of the algorithm to the total NOISE . in this paper , we provide an ADDITIVE ANALYSIS bounding the difference between performance and NOISE . our ADDITIVE ANALYSIS provides additional CONVERGENCE GUARANTEES in general , and particular benefits for STRUCTURED INPUT DATA . we illustrate the ADDITIVE ANALYSIS using HUMAN SPEECH and white NOISE . \n",
            "this paper addresses the problem of ADDITIVE ANALYSIS in the presence of NOISE in HUMAN SPEECH . we propose a method to estimate the NOISE of the HUMAN SPEECH using a RECURSIVE LEAST SQUARES ALGORITHM . the proposed algorithm is based on the RECURSIVE LEAST SQUARES ALGORITHM , which is based on the RECURSIVE LEAST SQUARES ALGORITHM . the proposed algorithm is based on the estimation of the RECURSIVE LEAST SQUARES ALGORITHM of the NOISE PROCESS . the performance of the proposed algorithm is demonstrated on a variety of HUMAN SPEECH .\n",
            "\n",
            "970 1000\n",
            "both LEARNING AND INFERENCE TASKS on BAYESIAN NETWORKS are np-hard in general . BOUNDED TREE-WIDTH BAYESIAN NETWORKS have recently received a lot of attention as a way to circumvent this complexity issue ; however , while inference on BOUNDED TREE-WIDTH NETWORKS is tractable , the LEARNING PROBLEM remains np-hard even for <unk> 2 . in this paper , we propose BOUNDED VERTEX COVER NUMBER BAYESIAN NETWORKS as an alternative to BOUNDED TREE-WIDTH NETWORKS . in particular , we show that both inference and learning can be done in POLYNOMIAL TIME for any fixed vertex cover number bound k , in contrast to the general and bounded <unk> cases ; on the other hand , we also show that LEARNING PROBLEM is w -lsb- 1 -rsb- - hard in parameter k. furthermore , we give an alternative way to learn BOUNDED VERTEX COVER NUMBER BAYESIAN NETWORKS using INTEGER LINEAR PROGRAMMING , and show this is feasible in practice . \n",
            "this paper addresses the problem of BOUNDED VERTEX COVER NUMBER BAYESIAN NETWORKS for LEARNING AND INFERENCE TASKS . in particular , we consider the problem of INTEGER LINEAR PROGRAMMING , where the LEARNING PROBLEM is solved by a BOUNDED VERTEX COVER NUMBER BAYESIAN NETWORKS . in particular , we show that the LEARNING PROBLEM can be efficiently solved using BAYESIAN NETWORKS . we show that the LEARNING PROBLEM can be efficiently solved using BAYESIAN NETWORKS . we also show that BOUNDED VERTEX COVER NUMBER BAYESIAN NETWORKS can be used to solve the LEARNING PROBLEM . we also show that the BOUNDED VERTEX COVER NUMBER BAYESIAN NETWORKS can be applied to a wide range of LEARNING AND INFERENCE TASKS .\n",
            "\n",
            "971 1000\n",
            "<unk> and competitive <unk> algorithms are almost ubiquitous in MODELING PATTERN FORMATION in CORTICAL DEVELOPMENT . we analyse in theoretical detail a particular model -lrb- adapted from <unk> & <unk> , 1999 -rrb- for the development of id <unk> patterns , which places COMPETITIVE AND INTERACTIVE CORTICAL INFLUENCES , and free and restricted initial <unk> onto a COMMON FOOTING . \n",
            "this paper addresses the problem of MODELING PATTERN FORMATION in the presence of COMPETITIVE AND INTERACTIVE CORTICAL INFLUENCES . we propose a method for MODELING PATTERN FORMATION in CORTICAL DEVELOPMENT . the proposed algorithm is based on the use of a COMMON FOOTING , which is able to deal with COMPETITIVE AND INTERACTIVE CORTICAL INFLUENCES . we show that the proposed algorithm can be applied to the problem of MODELING PATTERN FORMATION in CORTICAL DEVELOPMENT .\n",
            "\n",
            "972 1000\n",
            "natural and artificial neural circuits must be capable of traversing specific STATE SPACE TRAJECTORIES . a natural approach to this NATURAL AND ARTIFICIAL NEURAL CIRCUITS is to learn the relevant TRAJECTORIES from examples . unfortunately , gradient descent learning of COMPLEX TRAJECTORIES in AMORPHOUS NETWORKS is unsuccessful . we suggest a possible approach where TRAJECTORIES are realized by combining simple OSCIL-LATORS , in various modular ways . we contrast two regimes of FAST AND SLOW OSCILLATIONS . in all cases , we show that banks of oscillators with BOUNDED FREQUENCIES have UNIVERSAL APPROXIMATION PROPERTIES . open questions are also discussed briefly . \n",
            "in this paper , we present a novel approach to the problem of FAST AND SLOW OSCILLATIONS in the presence of COMPLEX TRAJECTORIES . the proposed approach is based on the use of NATURAL AND ARTIFICIAL NEURAL CIRCUITS , which are used to estimate the STATE SPACE TRAJECTORIES of the signal . the proposed method is based on the use of NATURAL AND ARTIFICIAL NEURAL CIRCUITS , which are then used to estimate the STATE SPACE TRAJECTORIES of the signal . the proposed approach is evaluated on a variety of AMORPHOUS NETWORKS . the results show that the proposed method outperforms the existing methods in terms of both FAST AND SLOW OSCILLATIONS and BOUNDED FREQUENCIES .\n",
            "\n",
            "973 1000\n",
            "<unk> algorithms have been surprisingly successful in computing approximately optimal solutions for partially observable markov decision processes -lrb- pomdps -rrb- in HIGH DIMENSIONAL BELIEF SPACES . in this work , we seek to understand the BELIEF-SPACE PROPERTIES that allow some POMDP PROBLEMS to be approximated efficiently and thus help to explain the POINT-BASED ALGORITHMS ' success often observed in the experiments . we show that an approximately optimal POMDP SOLUTION can be computed in TIME POLYNOMIAL in the covering number of a REACHABLE BELIEF SPACE , which is the subset of the BELIEF SPACE reachable from a given belief point . we also show that under the weaker condition of having a small covering number for an OPTIMAL REACHABLE SPACE , which is the subset of the BELIEF SPACE reachable under an OPTIMAL POLICY , computing an approximately optimal solution is np-hard . however , given a suitable set of points that '' cover '' an optimal <unk> space well , an APPROXIMATE SOLUTION can be computed in POLYNOMIAL TIME . the covering number highlights several interesting properties that reduce the complexity of POMDP PLANNING in practice , e.g. , FULLY OBSERVED STATE VARIABLES , BELIEFS with SPARSE SUPPORT , SMOOTH BELIEFS , and CIRCULANT STATE-TRANSITION MATRICES . \n",
            "this paper addresses the problem of POMDP PLANNING in the presence of SPARSE SUPPORT , SMOOTH BELIEFS , BELIEFS , and BELIEFS . we propose a novel approach to the problem of POMDP PLANNING in HIGH DIMENSIONAL BELIEF SPACES . the proposed approach is based on the use of CIRCULANT STATE-TRANSITION MATRICES , SMOOTH BELIEFS , and CIRCULANT STATE-TRANSITION MATRICES . the proposed approach is based on the use of CIRCULANT STATE-TRANSITION MATRICES and CIRCULANT STATE-TRANSITION MATRICES in the REACHABLE BELIEF SPACE . the proposed approach is based on the use of CIRCULANT STATE-TRANSITION MATRICES , SPARSE SUPPORT , and CIRCULANT STATE-TRANSITION MATRICES . the proposed method is evaluated on a variety of POMDP PROBLEMS and POMDP PROBLEMS . the experimental results show that the proposed method outperforms the state-of-the-art methods .\n",
            "\n",
            "974 1000\n",
            "many domains are naturally organized in an ABSTRACTION HIERARCHY or TAXONOMY , where the instances in '' nearby '' classes in the TAXONOMY are similar . in this paper , we provide a general PROBABILISTIC FRAMEWORK for CLUSTERING DATA into a set of classes organized as a TAXONOMY , where each class is associated with a PROB-ABILISTIC MODEL from which the data was generated . the PROBABILISTIC FRAMEWORK simultaneously optimizes three things : the assignment of data instances to CLUSTERS , the models associated with the CLUSTERS , and the structure of the ABSTRACTION HIERARCHY . a unique feature of our PROBABILISTIC FRAMEWORK is that PROBABILISTIC FRAMEWORK utilizes GLOBAL OPTIMIZATION ALGORITHMS for both of the last two steps , reducing the SENSITIVITY to NOISE and the <unk> to LOCAL MAXIMA that are characteristic of algorithms that only take LOCAL STEPS . we provide a THEORETICAL ANALYSIS for our PROBABILISTIC FRAMEWORK , showing that PROBABILISTIC FRAMEWORK converges to a local maximum of the probability of model and data . we present experimental results on SYNTHETIC DATA , and on REAL DATA in the domains of gene expression and text . \n",
            "in this paper , we propose a novel PROBABILISTIC FRAMEWORK for CLUSTERING DATA . the proposed PROBABILISTIC FRAMEWORK is based on the use of a set of CLUSTERS , each of which is a PROB-ABILISTIC MODEL of a set of CLUSTERS . a PROB-ABILISTIC MODEL is used to estimate the CLUSTERS of the CLUSTERS . the proposed PROBABILISTIC FRAMEWORK is applied to the problem of CLUSTERING DATA in the presence of NOISE . the proposed PROBABILISTIC FRAMEWORK is applied to the problem of CLUSTERING DATA in the presence of NOISE and NOISE . the experimental results on SYNTHETIC DATA demonstrate the effectiveness of the proposed PROBABILISTIC FRAMEWORK .\n",
            "\n",
            "975 1000\n",
            "this paper considers the problem of computing placement of points in 3 dimensional space given two UNCALIBRATED PERSPECTIVE VIEWS . the main theorem shows that the placement of the points is determined only up to an arbitrary projective transformation of <unk> . given additional ground control points , however , the location of the points and the CAMERA PARAMETERS may be determined . the method is linear and non-iterative whereas previously known methods for solving the CAMERA CALIBRATION and placement to take proper account of both <unk> points and IMAGE CORRESPONDENCES are unsatisfactory in requiring either ITERATIVE METHODS or MODEL RESTRICTIONS . as a result of the main theorem , it is possible to determine PROJECTIVE INVARIANTS OF 3-D GEOMETRIC CONFIGURATIONS from two PERSPECTIVE VIEWS . \n",
            "this paper addresses the problem of PROJECTIVE INVARIANTS OF 3-D GEOMETRIC CONFIGURATIONS in the presence of UNCALIBRATED PERSPECTIVE VIEWS , IMAGE CORRESPONDENCES , and IMAGE CORRESPONDENCES . we propose a method to estimate the PROJECTIVE INVARIANTS OF 3-D GEOMETRIC CONFIGURATIONS of a scene from a single image . our approach is based on the use of ITERATIVE METHODS and MODEL RESTRICTIONS . we show that the proposed method can be applied to the problem of PROJECTIVE INVARIANTS OF 3-D GEOMETRIC CONFIGURATIONS and IMAGE CORRESPONDENCES . we also show that the proposed method can be applied to the problem of CAMERA CALIBRATION .\n",
            "\n",
            "976 1000\n",
            "we propose SPEAKER CLUSTERING METHODS based on the VOCAL-TRACT-SIZE RELATED ARTICULATORY PARAMETERS associated with individual speakers . two parameters characterizing GROSS VOCAL-TRACT DIMENSIONS are rst derived from formants of <unk> japanese vowels , and are then used to CLUSTER a total of <unk> male japanese speakers . the resultant SPEAKER CLUSTERS are found to be signicantly dierent from the SPEAKER CLUSTERS obtained by conventional ACOUSTIC CRITERIA . JAPANESE PHONEME RECOGNITION experiments are carried out using SPEAKER-CLUSTERED TIED-STATE HMMS trained for each CLUSTER . compared with the BASELINE GENDER DEPENDENT MODEL , 5.7 % of RECOGNITION ERROR REDUCTION has been achieved based on the CLUSTERING METHOD using VOCAL-TRACT PARAMETERS . \n",
            "in this paper , we propose a novel CLUSTERING METHOD based on SPEAKER-CLUSTERED TIED-STATE HMMS . the proposed CLUSTERING METHOD is based on the use of SPEAKER-CLUSTERED TIED-STATE HMMS for RECOGNITION ERROR REDUCTION . the proposed CLUSTERING METHOD is based on the use of SPEAKER-CLUSTERED TIED-STATE HMMS for RECOGNITION ERROR REDUCTION . the proposed CLUSTERING METHOD is based on the use of SPEAKER-CLUSTERED TIED-STATE HMMS to estimate the VOCAL-TRACT PARAMETERS . the proposed CLUSTERING METHOD is applied to JAPANESE PHONEME RECOGNITION . the experimental results show that the proposed CLUSTERING METHOD is effective in improving the RECOGNITION ERROR REDUCTION performance .\n",
            "\n",
            "977 1000\n",
            "we address the problem of automatically learning the main steps to complete a certain task , such as changing a CAR TIRE , from a set of NARRATED INSTRUCTION VIDEOS . the contributions of this paper are threefold . first , we develop a JOINT MODEL for VIDEO AND NATURAL LANGUAGE NARRATION that takes advantage of the complementary nature of the two signals . second , we collect an ANNOTATED DATASET of 57 INTERNET INSTRUCTION VIDEOS containing more than <unk> frames for two tasks -lrb- changing CAR TIRE and <unk> <unk> -rrb- . third , we experimentally demonstrate that the proposed JOINT MODEL automatically discovers , in an UNSUPERVISED MANNER , the main steps to achieve each task and locate them within the input videos . the results further show that the proposed JOINT MODEL outperforms SINGLE-MODALITY BASELINES , demonstrating the benefits of JOINT MODELING VIDEO and text . \n",
            "this paper presents a novel JOINT MODEL for JOINT MODELING VIDEO . the proposed JOINT MODEL is based on the use of a JOINT MODEL for JOINT MODELING VIDEO . the proposed JOINT MODEL is based on a JOINT MODEL for JOINT MODELING VIDEO . the proposed JOINT MODEL is based on the use of a JOINT MODEL , which is based on a JOINT MODEL . the experimental results show that the proposed JOINT MODEL significantly outperforms the SINGLE-MODALITY BASELINES . the proposed JOINT MODEL is more robust to CAR TIRE than existing SINGLE-MODALITY BASELINES .\n",
            "\n",
            "978 1000\n",
            "the FULL-BAND ADAPTIVE HARMONIC MODEL can be used by the ADAPTIVE ITERATIVE REFINEMENT ALGORITHM to accurately model the perceived characteristics of a SPEECH RECORDING . however , the LEAST SQUARES SOLUTION used in the current AHM-AIR makes the f 0 refinement in air time consuming , limiting the use of this algorithm for large databases . in this paper , a PEAK PICKING APPROACH is suggested as a substitution to the LEAST SQUARES SOLUTION . in order to integrate the adaptivity scheme of FULL-BAND ADAPTIVE HARMONIC MODEL in the PEAK PICKING APPROACH , an ADAPTIVE DISCRETE FOURIER TRANSFORM is also suggested in this paper , whose FREQUENCY BASIS can fully follow the frequency variations of the F 0 CURVE . evaluations have shown an AVERAGE TIME REDUCTION of 5.5 times compared to the LS SOLUTION APPROACH , while the QUALITY of the <unk> is preserved compared to the original AHM-AIR . \n",
            "this paper proposes a novel ADAPTIVE ITERATIVE REFINEMENT ALGORITHM for SPEECH RECORDING . the proposed PEAK PICKING APPROACH is based on a FULL-BAND ADAPTIVE HARMONIC MODEL for the F 0 CURVE of the SPEECH RECORDING . the proposed ADAPTIVE ITERATIVE REFINEMENT ALGORITHM is based on a FULL-BAND ADAPTIVE HARMONIC MODEL , which is based on a FULL-BAND ADAPTIVE HARMONIC MODEL . the proposed ADAPTIVE ITERATIVE REFINEMENT ALGORITHM is based on a FULL-BAND ADAPTIVE HARMONIC MODEL . the proposed FULL-BAND ADAPTIVE HARMONIC MODEL is applied to the F 0 CURVE of the SPEECH RECORDING . the experimental results show that the proposed ADAPTIVE ITERATIVE REFINEMENT ALGORITHM is effective in improving the QUALITY and QUALITY of the SPEECH RECORDING .\n",
            "\n",
            "979 1000\n",
            "digital images nowadays show large appearance <unk> on PICTURE STYLES , in terms of COLOR TONE , CONTRAST , VIGNETTING , and etc. . these ` PICTURE STYLES ' are directly related to the SCENE RADIANCE , image pipeline of the camera , and POST PROCESSING FUNCTIONS -lrb- e.g. , PHOTOGRAPHY EFFECT FILTERS -rrb- . due to the COMPLEXITY and nonlinearity of these factors , popular GRADIENT-BASED IMAGE DESCRIPTORS generally are not invariant to different PICTURE STYLES , which could degrade the performance for OBJECT RECOGNITION . given that images shared online or created by individual users are taken with a wide range of devices and may be processed by various POST PROCESSING FUNCTIONS , to find a ROBUST OBJECT RECOGNITION SYSTEM is useful and challenging . in this paper , we investigate the influence of PICTURE STYLES on OBJECT RECOGNITION by making a connection between IMAGE DE-SCRIPTORS and a PIXEL MAPPING FUNCTION G , and accordingly propose an ADAPTIVE APPROACH based on a G-INCORPORATED KERNEL DESCRIPTOR and multiple KERNEL LEARNING , without estimating or specifying the IMAGE STYLES used in training and testing . we conduct experiments on the DOMAIN ADAPTATION DATA SET , the OXFORD FLOWER DATA SET , and several variants of the FLOWER DATA SET by introducing popular PHOTOGRAPHY EFFECTS through POST-PROCESSING . the results demonstrate that the proposed ADAPTIVE APPROACH consistently yields RECOGNITION improvements over standard DESCRIPTORS in all studied cases . \n",
            "in this paper , we propose a novel ADAPTIVE APPROACH based on GRADIENT-BASED IMAGE DESCRIPTORS and PIXEL MAPPING FUNCTION G . the proposed ADAPTIVE APPROACH is based on the use of GRADIENT-BASED IMAGE DESCRIPTORS , CONTRAST , CONTRAST , and PIXEL MAPPING FUNCTION G for OBJECT RECOGNITION . the proposed ADAPTIVE APPROACH is based on the use of GRADIENT-BASED IMAGE DESCRIPTORS , CONTRAST , and PIXEL MAPPING FUNCTION G . the COMPLEXITY of the proposed ADAPTIVE APPROACH is evaluated on the OXFORD FLOWER DATA SET , and the results show that the proposed ADAPTIVE APPROACH is effective in improving the COMPLEXITY of the ROBUST OBJECT RECOGNITION SYSTEM in terms of RECOGNITION and VIGNETTING . the proposed ADAPTIVE APPROACH is evaluated on the OXFORD FLOWER DATA SET , and the results show that the proposed ADAPTIVE APPROACH is effective in improving the RECOGNITION performance .\n",
            "\n",
            "980 1000\n",
            "nuisance attribute projection -lrb- <unk> -rrb- and WITHIN-CLASS COVARIANCE NORMALIZATION are two effective techniques for INTERSESSION VARIABILITY COMPENSATION in SVM BASED SPEAKER VERIFICATION SYSTEMS . however , by normalizing or removing the NUISANCE SUBSPACE containing the SESSION VARIABILITY can not guarantee to enlarge the distance between speakers . in this paper , we investigated the probability of using LINEAR DISCRIMINANT ANALYSIS for DISCRIMINATIVE TRAINING . to cope with the small sample size problem which prevents us from using LINEAR DISCRIMINANT ANALYSIS directly , we adapted the WITHIN-CLASS COVARIANCE NORMALIZATION , which first projects the whole FEATURE SPACE into a relatively low dimensional SUBSPACE by LINEAR DISCRIMINANT ANALYSIS , and then performs LINEAR DISCRIMINANT ANALYSIS in the SUBSPACE . by some modification , the WITHIN-CLASS COVARIANCE NORMALIZATION can be <unk> into a kind of WITHIN-CLASS COVARIANCE NORMALIZATION , which we called WITHIN-CLASS COVARIANCE NORMALIZATION . experiments on NIST SRE TASKS showed that , the WITHIN-CLASS COVARIANCE NORMALIZATION outperformed the conventional WITHIN-CLASS COVARIANCE NORMALIZATION , especially in LOW DIMENSIONAL FEATURE SPACE . \n",
            "this paper addresses the problem of INTERSESSION VARIABILITY COMPENSATION in a LOW DIMENSIONAL FEATURE SPACE . we propose a novel approach to INTERSESSION VARIABILITY COMPENSATION based on LINEAR DISCRIMINANT ANALYSIS and WITHIN-CLASS COVARIANCE NORMALIZATION . the proposed WITHIN-CLASS COVARIANCE NORMALIZATION is based on the idea of NUISANCE ATTRIBUTE PROJECTION and WITHIN-CLASS COVARIANCE NORMALIZATION . the proposed WITHIN-CLASS COVARIANCE NORMALIZATION is based on the use of NUISANCE ATTRIBUTE PROJECTION and WITHIN-CLASS COVARIANCE NORMALIZATION . the proposed WITHIN-CLASS COVARIANCE NORMALIZATION is applied to the NIST SRE TASKS , and the WITHIN-CLASS COVARIANCE NORMALIZATION is applied to the NIST SRE TASKS . experimental results on NIST SRE TASKS show that the proposed WITHIN-CLASS COVARIANCE NORMALIZATION significantly outperforms existing methods in terms of WITHIN-CLASS COVARIANCE NORMALIZATION and WITHIN-CLASS COVARIANCE NORMALIZATION .\n",
            "\n",
            "981 1000\n",
            "neurophysiological changes in the brain associated with MAJOR DEPRESSION DISORDER can <unk> ARTICULATORY PRECISION in SPEECH PRODUCTION . motivated by this observation , we address the hypothesis that ARTICULATORY FEATURES , as manifested through FORMANT FREQUENCY TRACKS , can help in AUTOMATICALLY CLASSIFYING DEPRESSION STATE . specifically , we investigate the relative importance of VOCAL TRACT FORMANT FREQUENCIES and their DYNAMIC FEATURES from SUSTAINED VOWELS and CONVERSATIONAL SPEECH . using a database consisting of AUDIO from 35 subjects with CLINICAL MEASURES OF DEPRESSION SEVERITY , we explore the performance of GAUSSIAN MIXTURE MODEL and SUPPORT VECTOR MACHINE CLASSIFIERS . with only FORMANT FREQUENCIES and their dynamics given by VELOCITY and acceleration , we show that DEPRESSION STATE can be classified with an optimal SENSITIVITY/SPECIFICITY/AREA under the GAUSSIAN MIXTURE MODEL for GMMS and SUPPORT VECTOR MACHINE CLASSIFIERS , respectively . future work will involve merging our FORMANT-BASED CHARACTERIZATION with VOCAL SOURCE and prosodic features . \n",
            "this paper addresses the problem of AUTOMATICALLY CLASSIFYING DEPRESSION STATE in CONVERSATIONAL SPEECH and CONVERSATIONAL SPEECH . we propose a novel method based on GAUSSIAN MIXTURE MODEL and SUPPORT VECTOR MACHINE CLASSIFIERS to estimate the VOCAL TRACT FORMANT FREQUENCIES . the proposed method is based on the use of a GAUSSIAN MIXTURE MODEL and a GAUSSIAN MIXTURE MODEL to estimate the VOCAL TRACT FORMANT FREQUENCIES . the proposed method is based on a GAUSSIAN MIXTURE MODEL and a GAUSSIAN MIXTURE MODEL . the proposed method is based on a GAUSSIAN MIXTURE MODEL and a FORMANT-BASED CHARACTERIZATION . the proposed method is evaluated on a variety of CONVERSATIONAL SPEECH and CONVERSATIONAL SPEECH . the experimental results show that the proposed method outperforms the conventional GMMS and the SUPPORT VECTOR MACHINE CLASSIFIERS .\n",
            "\n",
            "982 1000\n",
            "this paper describes a novel method for a WORD SENSE DISAM-BIGUATION that utilizes <unk> -lrb- i.e. SYNONYMS , HYPERNYMS , MERONYMS , etc in wordnet -rrb- of a target word and RAW CORPORA . the method <unk> senses of a target word by selecting a relative that most probably occurs in a new sentence including the target word . only one CO-OCCURRENCE FREQUENCY MATRIX is utilized to efficiently disambiguate senses of many target words . experiments on several ENGLISH DATUM present that our proposed method achieves a good performance . \n",
            "this paper addresses the problem of WORD SENSE DISAM-BIGUATION in the presence of SYNONYMS , HYPERNYMS , HYPERNYMS , and HYPERNYMS . we propose a method to estimate the CO-OCCURRENCE FREQUENCY MATRIX of a scene from a single image . the method is based on the use of a set of SYNONYMS , HYPERNYMS , HYPERNYMS , and HYPERNYMS . we show that this method can be used to estimate the CO-OCCURRENCE FREQUENCY MATRIX of a scene from a single image .\n",
            "\n",
            "983 1000\n",
            "kalman FILTERING is a CLASSICAL PROBLEM of significant interest in the context of a DISTRIBUTED APPLICATION for WIRELESS SENSOR NETWORKS . in this paper we consider a specific algorithm for DISTRIBUTED KALMAN FILTERING proposed recently by <unk> -lsb- 1 -rsb- and present a SCAL-ABLE WIRELESS COMMUNICATION ARCHITECTURE suited for implementation in WIRELESS SENSOR NETWORKS . the proposed SCAL-ABLE WIRELESS COMMUNICATION ARCHITECTURE uses a DATA DRIVEN AVERAGE CONSENSUS FRAMEWORK . this allows us to explicitly characterize the delay vs. estimate accuracy tradeoff in FILTERING . by exploiting the structure of the DISTRIBUTED FILTERING COMPUTATIONS , we derive an optimal COMMUNICATION RESOURCE ALLOCATION POLICY for minimizing the COMPONENT-WISE STATE ESTIMATION ERROR . furthermore , our SCAL-ABLE WIRELESS COMMUNICATION ARCHITECTURE is scalable in terms of the network size n . we provide simulation results demonstrating the performance of our SCAL-ABLE WIRELESS COMMUNICATION ARCHITECTURE . \n",
            "this paper presents a novel SCAL-ABLE WIRELESS COMMUNICATION ARCHITECTURE for WIRELESS SENSOR NETWORKS . the proposed SCAL-ABLE WIRELESS COMMUNICATION ARCHITECTURE is based on a DATA DRIVEN AVERAGE CONSENSUS FRAMEWORK . the proposed DATA DRIVEN AVERAGE CONSENSUS FRAMEWORK is based on a DATA DRIVEN AVERAGE CONSENSUS FRAMEWORK . the proposed DATA DRIVEN AVERAGE CONSENSUS FRAMEWORK is based on a DATA DRIVEN AVERAGE CONSENSUS FRAMEWORK . the proposed DATA DRIVEN AVERAGE CONSENSUS FRAMEWORK is based on a DATA DRIVEN AVERAGE CONSENSUS FRAMEWORK , which is based on a DATA DRIVEN AVERAGE CONSENSUS FRAMEWORK . the proposed DATA DRIVEN AVERAGE CONSENSUS FRAMEWORK is applied to WIRELESS SENSOR NETWORKS , and the results show that the proposed DATA DRIVEN AVERAGE CONSENSUS FRAMEWORK is effective in improving the performance of WIRELESS SENSOR NETWORKS .\n",
            "\n",
            "984 1000\n",
            "in this work , we introduce a novel IMPLICIT REPRESENTATION OF SHAPE which is based on assigning to each PIXEL a probability that this PIXEL is inside the shape . this PROBABILIS-TIC REPRESENTATION OF SHAPE resolves two important drawbacks of alternative IMPLICIT SHAPE REPRESENTATIONS such as the LEVEL SET METHOD : firstly , the SPACE OF SHAPES is convex in the sense that arbitrary convex combinations of a set of shapes again correspond to a valid shape . secondly , we prove that the introduction of SHAPE PRIORS into VARIATIONAL IMAGE SEGMENTATION leads to functionals which are convex with respect to SHAPE DEFORMATIONS . for a large class of commonly considered -lrb- spatially continuous -rrb- functionals , we prove that -- under MILD REGULARITY ASSUMPTIONS -- SEGMENTATION and TRACKING with STATISTICAL SHAPE PRIORS can be performed in a globally optimal manner . in experiments on TRACKING a walking person through a cluttered scene we demonstrate the advantage of GLOBAL VERSUS LOCAL OPTIMALITY . \n",
            "in this paper , we propose a novel method for VARIATIONAL IMAGE SEGMENTATION and TRACKING . the proposed approach is based on the use of SHAPE PRIORS , such as IMPLICIT REPRESENTATION OF SHAPE , to estimate the IMPLICIT REPRESENTATION OF SHAPE . the proposed approach is based on the use of STATISTICAL SHAPE PRIORS , such as the IMPLICIT REPRESENTATION OF SHAPE , to estimate the IMPLICIT REPRESENTATION OF SHAPE . the proposed method is based on the use of SHAPE PRIORS , such as the IMPLICIT REPRESENTATION OF SHAPE , to estimate the IMPLICIT REPRESENTATION OF SHAPE . the proposed method is based on the estimation of the SHAPE PRIORS of the IMPLICIT SHAPE REPRESENTATIONS . the proposed method is evaluated in terms of GLOBAL VERSUS LOCAL OPTIMALITY and TRACKING performance .\n",
            "\n",
            "985 1000\n",
            "a framework for APPROXIMATE SIGNAL PROCESSING is introduced which can be used to design novel classes of algorithms for performing dft and stft calculations . in particular , we focus on the derivation of MULTI-STAGE INCREMEN-TAL REENEMENT ALGORITHMS that meet a variety of DESIGN CRITERIA on the TRADEOO achieved at each stage between SOLUTION QUALITY and COMPUTATIONAL COST . \n",
            "this paper addresses the problem of APPROXIMATE SIGNAL PROCESSING in the presence of APPROXIMATE SIGNAL PROCESSING . in particular , we consider the problem of APPROXIMATE SIGNAL PROCESSING in the presence of APPROXIMATE SIGNAL PROCESSING . we propose a new algorithm for APPROXIMATE SIGNAL PROCESSING , and show that it is possible to efficiently solve the problem of APPROXIMATE SIGNAL PROCESSING . we also show that the proposed algorithm is able to solve the problem of APPROXIMATE SIGNAL PROCESSING . we also show that the proposed algorithm is able to achieve the optimal solution to the problem of APPROXIMATE SIGNAL PROCESSING .\n",
            "\n",
            "986 1000\n",
            "model MIGRATION in SPEAKER RECOGNITION is a task of converting PARAMETRICALLY-OBSOLETE MODELS to new structures and configurations without the requirement to store the original SPEECH WAVEFORMS or feature vector sequences along with the MODEL MIGRATION . the need for MODEL MIGRATION arises in large-scale deployments of SPEAKER RECOGNITION TECHNOLOGY in which the potential for LEGACY PROBLEMS increases as the evolving technology may require CONFIGURATION CHANGES thus <unk> already existing USER VOICE ACCOUNTS . a MIGRATION may represent the only alternative to otherwise costly user <unk> or WAVEFORM STORAGE and , as a new research problem , presents the challenge of developing algorithms to minimize the loss in ACCURACY in the MIGRATED ACCOUNTS . this paper reports on further enhancements of a STATISTICAL MIGRATION TECHNIQUE based on GAUSSIAN MIXTURE MODELS , introduced previously . the present STATISTICAL MIGRATION TECHNIQUE is based on a STOCHASTIC SYNTHESIS OF FEATURE SEQUENCES from obsolete MODEL MIGRATION that are subsequently used to create the new MODEL MIGRATION . here , in addition to GAUSSIAN MEANS and PRIORS , as utilized in the previous contribution , also the covariances are included resulting in significant performance gains in the MIGRATED MODELS , compared to the MEAN-ONLY METHOD . overall , measured on the NIST 2003 CELLULAR TASK , the described STATISTICAL MIGRATION TECHNIQUE achieves a MODEL MIGRATION incurring a loss in performance of <unk> % relative to a full <unk> from waveforms , dependent on the type of MISMATCH between the obsolete and the new configuration . the inclusion of the COVARIANCE INFORMATION is shown to reduce the loss of performance by a factor of <unk> as compared to the BASELINE MEAN-ONLY MIGRATION TECHNIQUE . \n",
            "in this paper , we propose a novel STATISTICAL MIGRATION TECHNIQUE based on STOCHASTIC SYNTHESIS OF FEATURE SEQUENCES . the proposed STATISTICAL MIGRATION TECHNIQUE is based on the use of GAUSSIAN MIXTURE MODELS and PRIORS for SPEAKER RECOGNITION . the proposed STATISTICAL MIGRATION TECHNIQUE is based on the use of GAUSSIAN MIXTURE MODELS and PRIORS . the proposed STATISTICAL MIGRATION TECHNIQUE is based on the use of GAUSSIAN MIXTURE MODELS and PRIORS . the proposed STATISTICAL MIGRATION TECHNIQUE is evaluated on the NIST 2003 CELLULAR TASK , and the results show that the proposed STATISTICAL MIGRATION TECHNIQUE significantly improves the ACCURACY and ACCURACY of the PARAMETRICALLY-OBSOLETE MODELS . the proposed STATISTICAL MIGRATION TECHNIQUE is evaluated on the NIST 2003 CELLULAR TASK and the results show that the proposed STATISTICAL MIGRATION TECHNIQUE is effective in improving the ACCURACY and ACCURACY of the proposed STATISTICAL MIGRATION TECHNIQUE .\n",
            "\n",
            "987 1000\n",
            "automated spoken dialog AUTOMATED SPOKEN DIALOG SYSTEMS require SYSTEMATIC PROCEDURES for evaluating performance and DIAGNOSING PROBLEMS . we present an INTERACTIVE TOOL that provides graphical views of how callers navigate through such AUTOMATED SPOKEN DIALOG SYSTEMS , enabling FINE-GRAINED ANALYSIS for SYSTEM EVALUATION and BUSINESS INTELLIGENCE . the input is a FEED OF CALL-LOGS . the output is an EMPIRICAL DIALOG TRAJECTORY ANALYSIS represented as STOCHASTIC FINITE STATE MACHINES , accessible via the WEB . COMPLEXITY is managed by an AUTOMATIC TOKENIZATION PROCEDURE that hides fine details until needed . users can generate selective views of parts of the DIALOG at high resolution -lrb- with access to CALL DATA -rrb- , or zoom out to a summary . the INTERACTIVE TOOL provides DIALOG SYSTEM DEVELOPERS with all the information they need from a single source , and is in use with DIRECTED-DIALOG AND NATURAL-LANGUAGE APPLICATIONS . \n",
            "this paper addresses the problem of AUTOMATED SPOKEN DIALOG SYSTEMS in BUSINESS INTELLIGENCE and BUSINESS INTELLIGENCE . in particular , we focus on the problem of CALL DATA and BUSINESS INTELLIGENCE . we propose a novel AUTOMATIC TOKENIZATION PROCEDURE , called STOCHASTIC FINITE STATE MACHINES , which is able to handle DIAGNOSING PROBLEMS and BUSINESS INTELLIGENCE . the key idea is to use a FEED OF CALL-LOGS , which is able to deal with DIAGNOSING PROBLEMS and BUSINESS INTELLIGENCE . the proposed AUTOMATIC TOKENIZATION PROCEDURE is based on a novel AUTOMATIC TOKENIZATION PROCEDURE , called STOCHASTIC FINITE STATE MACHINES , which is able to deal with DIAGNOSING PROBLEMS and BUSINESS INTELLIGENCE . we demonstrate the effectiveness of our approach on a variety of DIAGNOSING PROBLEMS and BUSINESS INTELLIGENCE .\n",
            "\n",
            "988 1000\n",
            "motion can be estimated by <unk> the EDGES of a <unk> -rrb- <unk> object using ACTIT -RRB- E CONTOURS , and registering them to , <unk> to obtain the MOTION MODEL PARAMETERS . this idea can be applied to PATIENT MOTION during the acquisition of an MRI to eliminate MOTION ARTIFACTS in the image . the data obtained <unk> , y the MRI <unk> ~ ~ , the K-SPACE , can be <unk> -rsb- <unk> into several . SUBBANDS such that each subband is acquired in a small fraction of the full <unk> , YING TIME . these sub bands create in -lsb- -rsb- -lrb- ~ <unk> tissue feature maps called SUBBAND IMAGES . <unk> , q <unk> ~ le contours , the RELATIVE MOTION is analyzed <unk> ~ th -lrb- ' <unk> sub <unk> -rrb- , d image ~ to determine the MOTIO ~ L PARAMETERS . <unk> , y these MOTION PARAMETERS at <unk> to correct the SUBBANDS , thus <unk> , y the k ; - sp -lrb- ~ <unk> . this has the potential to yield clear , NOISE-FREE MR IMA , <unk> . \n",
            "in this paper we present a novel method for MOTION MODEL PARAMETERS in SUBBAND IMAGES . the proposed method is based on the use of SUBBAND IMAGES to estimate the RELATIVE MOTION of the SUBBANDS . the proposed approach is based on the use of SUBBAND IMAGES to estimate the RELATIVE MOTION of the SUBBANDS . the proposed method is based on the use of SUBBAND IMAGES to estimate the RELATIVE MOTION . the proposed method is based on the use of SUBBAND IMAGES to estimate the RELATIVE MOTION and the MOTION MODEL PARAMETERS . the proposed method is evaluated on a variety of SUBBAND IMAGES . the results show that the proposed method is effective in reducing the number of EDGES in the presence of MOTION ARTIFACTS .\n",
            "\n",
            "989 1000\n",
            "we present a CONNECTIONIST ARCHITECTURE that can learn a model of the relations between PERCEPTIONS and actions and use this model for BEHAVIOR PLANNING . STATE REPRESENTATIONS are learned with a GROWING SELF-ORGANIZING LAYER which is directly coupled to a PERCEPTION and a MOTOR LAYER . knowledge about possible STATE TRANSITIONS is encoded in the LATERAL CONNECTIVITY . MOTOR SIGNALS modulate this LATERAL CONNECTIVITY and a DYNAMIC FIELD on the LAYER organizes a PLANNING PROCESS . all mechanisms are LOCAL AND ADAPTATION is based on HEBBIAN IDEAS . the model is continuous in the action , PERCEPTION , and time domain . \n",
            "in this paper , we propose a novel method for BEHAVIOR PLANNING in MOTOR SIGNALS . the proposed approach is based on the use of a DYNAMIC FIELD , a DYNAMIC FIELD , a DYNAMIC FIELD , a MOTOR LAYER , a MOTOR LAYER and a MOTOR LAYER . the proposed approach is based on a DYNAMIC FIELD and a MOTOR LAYER . the proposed approach is based on a GROWING SELF-ORGANIZING LAYER and a MOTOR LAYER . the proposed approach is based on the use of a DYNAMIC FIELD and a MOTOR LAYER . the experimental results show that the proposed method outperforms the state-of-the-art methods in terms of both LOCAL AND ADAPTATION and the MOTOR LAYER .\n",
            "\n",
            "990 1000\n",
            "we present an empirical study on the use of SEMANTIC INFORMATION for CONCEPT SEG-MENTATION AND LABELING , which is an important step for SEMANTIC PARSING . we represent the alternative analyses output by a state-of-the-art CSL PARSER with TREE STRUCTURES , which we rerank with a CLASSIFIER trained on two types of SEMANTIC TREE KERNELS : one PROCESSING STRUCTURES built with words , concepts and BROWN CLUSTERS , and another one using SEMANTIC SIMILARITY among the words composing the structure . the results on a corpus from the RESTAURANT DOMAIN show that our SEMANTIC KERNELS exploiting SIMILARITY MEASURES out-perform state-of-the-art <unk> . \n",
            "in this paper , we propose a novel method for CONCEPT SEG-MENTATION AND LABELING in a RESTAURANT DOMAIN . the proposed approach is based on the use of SEMANTIC INFORMATION extracted from the RESTAURANT DOMAIN to estimate the SEMANTIC SIMILARITY . the proposed method consists of two steps : -lrb- 1 -rrb- a CSL PARSER based on SEMANTIC SIMILARITY , and -lrb- 2 -rrb- a CSL PARSER based on SEMANTIC SIMILARITY . the proposed method is based on the use of SEMANTIC TREE KERNELS extracted from the RESTAURANT DOMAIN . the experimental results show that the proposed method outperforms the state-of-the-art methods .\n",
            "\n",
            "991 1000\n",
            "clustering is a basic task in a variety of MACHINE LEARNING APPLICATIONS . partitioning a set of INPUT VECTORS into compact , WELL-SEPARATED SUBSETS can be severely affected by the presence of MODEL-INCOMPATIBLE INPUTS called OUTLIERS . the present paper develops robust clustering algorithms for jointly partitioning the data and identifying the OUTLIERS . the novel approach relies on translating SCARCITY OF OUTLIERS to SPARSITY in a JUDICIOUSLY DEFINED DOMAIN , to <unk> three widely used CLUSTERING SCHEMES : HARD K-MEANS , FUZZY K-MEANS , and PROBABILISTIC CLUSTERING . CLUSTER CENTERS and assignments are iteratively updated in CLOSED FORM . the developed OUTLIER-AWARE ALGORITHMS are guaranteed to converge , while their COMPUTATIONAL COMPLEXITY is of the same order as their OUTLIER-AGNOSTIC COUNTERPARTS . preliminary simulations validate the analytical claims . \n",
            "this paper addresses the problem of PROBABILISTIC CLUSTERING in MACHINE LEARNING APPLICATIONS . in particular , we focus on the problem of PROBABILISTIC CLUSTERING , and propose a method to estimate the SCARCITY OF OUTLIERS of the INPUT VECTORS , such as OUTLIERS , HARD K-MEANS , and PROBABILISTIC CLUSTERING . we show that the COMPUTATIONAL COMPLEXITY of the OUTLIER-AGNOSTIC COUNTERPARTS can be reduced by up to a factor of o -lrb- n log n -rrb- , where n is the number of OUTLIERS . we also show that the COMPUTATIONAL COMPLEXITY of the OUTLIER-AGNOSTIC COUNTERPARTS can be reduced to the JUDICIOUSLY DEFINED DOMAIN . we also show that the COMPUTATIONAL COMPLEXITY of the OUTLIER-AGNOSTIC COUNTERPARTS can be reduced by up to a factor of o -lrb- 1 / √ t -rrb- .\n",
            "\n",
            "992 1000\n",
            "in CHINESE , most of the LANGUAGE PROCESSING starts from WORD SEGMENTATION and PART-OF-SPEECH TAGGING . these two steps <unk> the word from a sequence of characters and predict the SYNTACTIC LABELS for each SEGMENTED WORD . in this paper , we present two distinct SEQUENTIAL TAGGING MODELS for the above two tasks . the first SEQUENTIAL TAGGING MODELS was basically similar to previous work which made use of CONDITIONAL RANDOM FIELDS and set of PREDEFINED DICTIONARIES to recognize WORD BOUNDARIES . second , we revise and modify SUPPORT VECTOR MACHINE BASED CHUNKING MODEL to label the POS TAG in the POS TAGGING TASK . our SUPPORT VECTOR MACHINE BASED CHUNKING MODEL in the WS TASK achieves moderately RANK among all participants , while in the POS TAGGING TASK , it reaches very competitive results . \n",
            "in this paper , we propose a novel approach to WORD SEGMENTATION and PART-OF-SPEECH TAGGING . the proposed approach is based on a SUPPORT VECTOR MACHINE BASED CHUNKING MODEL , which is a SUPPORT VECTOR MACHINE BASED CHUNKING MODEL for WORD SEGMENTATION and PART-OF-SPEECH TAGGING . the proposed method consists of two steps : -lrb- 1 -rrb- a SUPPORT VECTOR MACHINE BASED CHUNKING MODEL for WORD SEGMENTATION and PART-OF-SPEECH TAGGING ; -lrb- 2 -rrb- a SUPPORT VECTOR MACHINE BASED CHUNKING MODEL for WORD SEGMENTATION and PART-OF-SPEECH TAGGING ; -lrb- 3 -rrb- a SUPPORT VECTOR MACHINE BASED CHUNKING MODEL for WORD SEGMENTATION and PART-OF-SPEECH TAGGING . the experimental results show that the proposed method outperforms the state-of-the-art methods in terms of WORD SEGMENTATION and PART-OF-SPEECH TAGGING .\n",
            "\n",
            "993 1000\n",
            "robust <unk> is defined for the ANALYSIS OF SIGNALS with HEAVY-TAILED DISTRIBUTION NOISE . in the form of a robust spectrogram -lrb- <unk> -rrb- RSPEC BASED INSTANTANEOUS FREQUENCY ESTIMATOR can be used for the ANALYSIS OF NONSTATIONARY SIGNALS . in this paper a RSPEC BASED INSTANTANEOUS FREQUENCY ESTIMATOR , with a TIME-VARYING WINDOW LENGTH , is presented . the optimal choice of the WINDOW LENGTH can resolve the BIAS-VARIANCE TRADE-OFF in the RSPEC BASED IF ESTIMATION . however , RSPEC BASED INSTANTANEOUS FREQUENCY ESTIMATOR depends on the unknown nonlinearity of the RSPEC BASED INSTANTANEOUS FREQUENCY ESTIMATOR . the RSPEC BASED INSTANTANEOUS FREQUENCY ESTIMATOR used in this paper is able to provide the ACCURACY close to the one that could be achieved if the RSPEC BASED INSTANTANEOUS FREQUENCY ESTIMATOR , to be estimated , were known in advance . simulations show good ACCURACY ability of the ADAPTIVE ALGORITHM and good robustness property with respect to RARE HIGH MAGNITUDE NOISE VALUES . \n",
            "this paper addresses the problem of ANALYSIS OF NONSTATIONARY SIGNALS in ANALYSIS OF NONSTATIONARY SIGNALS . the proposed RSPEC BASED INSTANTANEOUS FREQUENCY ESTIMATOR is based on a RSPEC BASED INSTANTANEOUS FREQUENCY ESTIMATOR . the proposed RSPEC BASED INSTANTANEOUS FREQUENCY ESTIMATOR is based on a RSPEC BASED INSTANTANEOUS FREQUENCY ESTIMATOR . the proposed RSPEC BASED INSTANTANEOUS FREQUENCY ESTIMATOR is based on a RSPEC BASED INSTANTANEOUS FREQUENCY ESTIMATOR , which is based on a RSPEC BASED INSTANTANEOUS FREQUENCY ESTIMATOR . the proposed RSPEC BASED INSTANTANEOUS FREQUENCY ESTIMATOR is applied to the ANALYSIS OF NONSTATIONARY SIGNALS in a RSPEC BASED IF ESTIMATION . the performance of the proposed ADAPTIVE ALGORITHM is evaluated in terms of ACCURACY and ACCURACY . the performance of the proposed RSPEC BASED INSTANTANEOUS FREQUENCY ESTIMATOR is evaluated in terms of ACCURACY and ACCURACY .\n",
            "\n",
            "994 1000\n",
            "random PROJECTION has been suggested as a means of DIMENSION-ALITY REDUCTION , where the original data are projected onto a SUB-SPACE using a RANDOM MATRIX . it represents a COMPUTATIONALLY SIMPLE METHOD that approximately preserves the EUCLIDEAN DISTANCE of any two points through the PROJECTION . moreover , as we are able to produce various RANDOM MATRICES , there may be some possibility of finding a RANDOM MATRIX that gives a better SPEECH RECOGNITION ACCURACY among these RANDOM MATRICES . in this paper , we investigate the feasibility of RANDOM PROJECTION for SPEECH FEATURE EXTRACTION . to obtain an optimal result from among many -lrb- infinite -rrb- RANDOM MATRICES , a VOTE-BASED RANDOM-PROJECTION COMBINATION is introduced in this paper , where VOTE-BASED RANDOM-PROJECTION COMBINATION is applied to RANDOM-PROJECTION-BASED FEATURES . its effectiveness is confirmed by WORD RECOGNITION experiments . \n",
            "in this paper , we propose a novel COMPUTATIONALLY SIMPLE METHOD for SPEECH FEATURE EXTRACTION . the proposed COMPUTATIONALLY SIMPLE METHOD is based on a RANDOM PROJECTION , which is based on a RANDOM PROJECTION . the proposed COMPUTATIONALLY SIMPLE METHOD is based on a RANDOM PROJECTION and the EUCLIDEAN DISTANCE is modeled by a RANDOM PROJECTION . the proposed COMPUTATIONALLY SIMPLE METHOD is based on a RANDOM PROJECTION , which is a RANDOM PROJECTION . the proposed COMPUTATIONALLY SIMPLE METHOD is applied to the VOTE-BASED RANDOM-PROJECTION COMBINATION of the RANDOM MATRIX , and the COMPUTATIONALLY SIMPLE METHOD is applied to the problem of DIMENSION-ALITY REDUCTION . experimental results show that the proposed COMPUTATIONALLY SIMPLE METHOD is effective in improving the SPEECH RECOGNITION ACCURACY and SPEECH RECOGNITION ACCURACY of the proposed method .\n",
            "\n",
            "995 1000\n",
            "this paper introduces an approach for enabling existing MULTI-VIEW STEREO METHODS to operate on extremely large UNSTRUCTURED PHOTO COLLECTIONS . the main idea is to decompose the collection into a set of overlapping sets of photos that can be processed in parallel , and to merge the resulting reconstructions . this OVERLAPPING CLUSTERING PROBLEM is formulated as a CONSTRAINED OPTIMIZATION and solved iteratively . the MERGING ALGORITHM , designed to be parallel and <unk> , incorporates robust FILTERING STEPS to eliminate LOW-QUALITY RECONSTRUCTIONS and enforce GLOBAL VISIBILITY CONSTRAINTS . the approach has been tested on several large datasets downloaded from FLICKR.COM , including one with over ten thousand images , yielding a 3D RECONSTRUCTION with nearly thirty million points . \n",
            "this paper addresses the problem of 3D RECONSTRUCTION in the presence of FLICKR.COM . we propose a novel method for 3D RECONSTRUCTION based on CONSTRAINED OPTIMIZATION . the proposed approach is based on the use of a MERGING ALGORITHM , which is able to deal with GLOBAL VISIBILITY CONSTRAINTS . the proposed approach is based on the use of CONSTRAINED OPTIMIZATION , which is able to deal with GLOBAL VISIBILITY CONSTRAINTS . the proposed approach is evaluated on a variety of UNSTRUCTURED PHOTO COLLECTIONS . the experimental results show that the proposed method outperforms the existing methods in terms of LOW-QUALITY RECONSTRUCTIONS and LOW-QUALITY RECONSTRUCTIONS .\n",
            "\n",
            "996 1000\n",
            "in VISUAL RECOGNITION PROBLEMS , the common DATA DISTRIBUTION MISMATCHES between training and testing make DOMAIN ADAPTATION essential . however , IMAGE DATA is difficult to manually divide into the DISCRETE DOMAINS required by ADAPTATION ALGORITHMS , and the standard practice of <unk> datasets with domains is a weak proxy for all the real conditions that alter the statistics in complex ways -lrb- lighting , POSE , BACKGROUND , RESOLUTION , etc. -rrb- we propose an approach to automatically discover LATENT DOMAINS in IMAGE OR VIDEO DATASETS . our formulation imposes two key properties on domains : MAXIMUM DISTINCTIVENESS and MAXIMUM LEARNABILITY . by MAXIMUM DISTINCTIVENESS , we require the underlying distributions of the identified domains to be different from each other to the MAXIMUM EXTENT ; by MAXIMUM LEARNABILITY , we ensure that a strong DISCRIMINATIVE MODEL can be learned from the domain . we devise a NONPARAMETRIC FORMULATION and efficient OPTIMIZATION PROCEDURE that can successfully discover domains among both TRAINING AND TEST DATA . we extensively evaluate our approach on OBJECT RECOGNITION and HUMAN ACTIVITY RECOGNITION TASKS . \n",
            "this paper addresses the problem of OBJECT RECOGNITION from IMAGE OR VIDEO DATASETS . we propose a novel approach to DOMAIN ADAPTATION and DOMAIN ADAPTATION . the key idea is to use a DISCRIMINATIVE MODEL to learn a DISCRIMINATIVE MODEL that is able to deal with DATA DISTRIBUTION MISMATCHES , BACKGROUND , BACKGROUND , and POSE . we propose a novel method to solve the problem of DOMAIN ADAPTATION and DOMAIN ADAPTATION . the proposed approach is based on a novel NONPARAMETRIC FORMULATION , which is able to deal with HUMAN ACTIVITY RECOGNITION TASKS and RESOLUTION . we demonstrate the effectiveness of our method on a variety of HUMAN ACTIVITY RECOGNITION TASKS and HUMAN ACTIVITY RECOGNITION TASKS .\n",
            "\n",
            "997 1000\n",
            "the CODING EFFICIENCY of the new VIDEO CODING STANDARD , HIGH EFFICIENCY VIDEO CODING , is strongly associated with better use of SPATIO-TEMPORAL REDUNDANCIES thanks to an increased number of competing CODING MODES . however , this competition involves a massive increase in SIGNALING BITRATE which becomes a possible limit for the next generation of EN-CODER . this paper proposes a new CODING SCHEME that breaks with conventional approaches . CODING SCHEME exploits a more complex DECODER able to reproduce the choice of the ENCODER based on CAUSAL REFERENCES , eliminating thus the need to SIGNAL CODING MODES and associated parameters . the general outline of this new CODING SCHEME and a proposed implementation are described in this paper . experimental results under common test conditions report an AVERAGE BITRATE SAVING of 1.7 % at the same quality compared to HEVC for a wide range of VIDEO SEQUENCES . \n",
            "this paper presents a novel approach to HIGH EFFICIENCY VIDEO CODING based on SPATIO-TEMPORAL REDUNDANCIES . the proposed approach is based on the use of a set of CODING MODES to estimate the SPATIO-TEMPORAL REDUNDANCIES . the proposed method is based on the use of a EN-CODER and a DECODER to estimate the SPATIO-TEMPORAL REDUNDANCIES . the proposed method is based on the use of a EN-CODER and a DECODER to estimate the SPATIO-TEMPORAL REDUNDANCIES . the performance of the proposed DECODER is evaluated in terms of AVERAGE BITRATE SAVING and AVERAGE BITRATE SAVING . the performance of the proposed DECODER is evaluated on a variety of VIDEO SEQUENCES . the results show that the proposed method is effective in improving the AVERAGE BITRATE SAVING of HEVC in terms of CODING EFFICIENCY and AVERAGE BITRATE SAVING .\n",
            "\n",
            "998 1000\n",
            "-lrb- <unk> , <unk> , and <unk> 2001 -rrb- introduced a formula to predict the number of nodes ida * will expand given the STATIC DISTRIBUTION OF HEURISTIC VALUES . their formula proved to be very accurate but it is only accurate under the following limitations : -lrb- 1 -rrb- the HEURISTIC must be consistent ; -lrb- 2 -rrb- the prediction is for a large RANDOM SAMPLE of START STATES -lrb- or for large thresholds -rrb- . in this paper we generalize the STATIC DISTRIBUTION to a CONDITIONAL DISTRIBUTION OF HEURISTIC VALUES . we then propose a new formula for predicting the performance of ida * that works well for INCONSISTENT HEURISTICS -lrb- <unk> et al. 2007 -rrb- and for any set of START STATES , not just a RANDOM SAMPLE . we also show how the formula can be enhanced to work well for SINGLE START STATES . experimental results demonstrate the ACCURACY of our method in all these situations . \n",
            "in this paper , we propose a novel approach to the problem of SINGLE START STATES in the presence of SINGLE START STATES . the proposed approach is based on the use of a RANDOM SAMPLE to estimate the CONDITIONAL DISTRIBUTION OF HEURISTIC VALUES . the proposed method is based on the use of a RANDOM SAMPLE to estimate the START STATES of the START STATES . the proposed method is based on the CONDITIONAL DISTRIBUTION OF HEURISTIC VALUES of the START STATES . the proposed method is evaluated in terms of the ACCURACY and the ACCURACY of the proposed method .\n",
            "\n",
            "999 1000\n",
            "in this paper , a new ARRAY SIGNAL PROCESSING TECHNIQUE by using PARTICLE SWARM OPTIMIZATION is proposed to identify MULTIPATH CHANNEL PARAMETERS . the proposed ARRAY SIGNAL PROCESSING TECHNIQUE provides estimates to the CHANNEL PARAMETERS by finding a global minimum of an OPTIMIZATION PROBLEM . since the OPTIMIZATION PROBLEM is formulated in the <unk> function -lrb- <unk> -rrb- domain of the TRANSMITTED SIGNAL and the received array outputs , the proposed ARRAY SIGNAL PROCESSING TECHNIQUE is called as PARTICLE SWARM OPTIMIZATION . the performance of the PARTICLE SWARM OPTIMIZATION is compared with the space alternating GENERALIZED EXPECTATION MAXIMIZATION TECHNIQUE and with another recently proposed PSO BASED TECHNIQUE for various SNR VALUES . simulation results indicate the superior performance of the PSO BASED TECHNIQUE over mentioned techniques for all SNR VALUES . \n",
            "in this paper , we propose a novel ARRAY SIGNAL PROCESSING TECHNIQUE for MULTIPATH CHANNEL PARAMETERS . the proposed GENERALIZED EXPECTATION MAXIMIZATION TECHNIQUE is based on the GENERALIZED EXPECTATION MAXIMIZATION TECHNIQUE , which is a GENERALIZED EXPECTATION MAXIMIZATION TECHNIQUE of the TRANSMITTED SIGNAL . the proposed GENERALIZED EXPECTATION MAXIMIZATION TECHNIQUE is based on the GENERALIZED EXPECTATION MAXIMIZATION TECHNIQUE . the proposed GENERALIZED EXPECTATION MAXIMIZATION TECHNIQUE is based on the GENERALIZED EXPECTATION MAXIMIZATION TECHNIQUE , which is based on the GENERALIZED EXPECTATION MAXIMIZATION TECHNIQUE . the proposed PARTICLE SWARM OPTIMIZATION is compared to the GENERALIZED EXPECTATION MAXIMIZATION TECHNIQUE and the PSO BASED TECHNIQUE . the experimental results show that the proposed PARTICLE SWARM OPTIMIZATION outperforms the conventional PSO BASED TECHNIQUE in terms of the SNR VALUES and the SNR VALUES .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#@title GENERATION 10EPOCHS\n",
        "!python generator.py -save /content/drive/MyDrive/rithanya/darri_det/GraphWriter/output2/9.vloss-3.560942.lr-0.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkwoggMZQYaB",
        "outputId": "72ad85d6-526a-47f4-dede-7e6ef4ae2a22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bleu_1:\t 19.139650787469023\n",
            "Bleu_2:\t 10.69191146515061\n",
            "Bleu_3:\t 6.270021189351824\n",
            "Bleu_4:\t 3.7469618360028427\n",
            "METEOR:\t 7.77741056205856\n",
            "ROUGE_L: 15.83604166237301\n"
          ]
        }
      ],
      "source": [
        "#@title EVALUATION 10EPOCHS\n",
        "!python eval.py /content/drive/MyDrive/rithanya/darri_det/GraphWriter/outputs/9.vloss-3.560942.lr-0.1.inputs.beam_predictions.cmdline /content/drive/MyDrive/rithanya/darri_det/GraphWriter/data/preprocessed.test.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Joo__Mol6AR",
        "outputId": "c6bfae67-b948-4ca5-97bf-f4859755ba32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/rithanya/darri_det/GraphWriter\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/rithanya/darri_det/GraphWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCQb6vf2l8SH",
        "outputId": "6c6e793e-e1c6-46f1-c34e-fada19e1ef00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-24 15:31:04.183468: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-24 15:31:04.183531: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-24 15:31:04.184927: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-24 15:31:04.193371: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-24 15:31:05.327571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Save File Exists, OverWrite? <CTL-C> for noyes\n",
            "Loading Data from  data//CUSTOM/DATA/preprocessed_custom2.train.tsv\n",
            "building vocab\n",
            "done\n",
            "Sorting training data by len\n",
            "ds sizes:\t8\t24\t0\t968\tVocab sizes:\n",
            "src 9\n",
            "ent 558\n",
            "nerd 8\n",
            "rel 17\n",
            "out 125\n",
            "graph\n",
            "cuda:0\n",
            "epoch  0 lr 0.1\n",
            "Training\t3\n",
            "1\n",
            "2\n",
            "AVG TRAIN LOSS:  4.577428579330444\t PPL:  97.2639657945991\n",
            "Evaluating\t<unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
            "VAL LOSS:  4.346643999588391\t PPL:  77.21888099396058\n",
            "Saving model\n",
            "epoch  1 lr 0.1\n",
            "Training\t3\n",
            "2\n",
            "1\n",
            "AVG TRAIN LOSS:  4.346178412437439\t PPL:  77.18293724328242\n",
            "Evaluating\tthe the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
            "VAL LOSS:  4.203523852608421\t PPL:  66.92173836927553\n",
            "Saving model\n",
            "epoch  2 lr 0.1\n",
            "Training\t3\n",
            "2\n",
            "1\n",
            "AVG TRAIN LOSS:  4.394439935684204\t PPL:  80.99925326539507\n",
            "Evaluating\tthe the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
            "VAL LOSS:  3.6642077895235423\t PPL:  39.025207735390765\n",
            "Saving model\n",
            "epoch  3 lr 0.1\n",
            "Training\t2\n",
            "3\n",
            "1\n",
            "AVG TRAIN LOSS:  3.9978268146514893\t PPL:  54.479626966378575\n",
            "Evaluating\t<unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
            "VAL LOSS:  3.6336313279207086\t PPL:  37.85001317321224\n",
            "Saving model\n",
            "epoch  4 lr 0.1\n",
            "Training\t2\n",
            "3\n",
            "1\n",
            "AVG TRAIN LOSS:  3.7153149247169495\t PPL:  41.071519484567986\n",
            "Evaluating\t<unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> a <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
            "VAL LOSS:  3.505637728478298\t PPL:  33.30267514612806\n",
            "Saving model\n",
            "epoch  5 lr 0.1\n",
            "Training\t2\n",
            "1\n",
            "3\n",
            "AVG TRAIN LOSS:  3.515255033969879\t PPL:  33.62450222079439\n",
            "Evaluating\t<unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
            "VAL LOSS:  3.237801693687754\t PPL:  25.47765245697647\n",
            "Saving model\n",
            "epoch  6 lr 0.1\n",
            "Training\t3\n",
            "1\n",
            "2\n",
            "AVG TRAIN LOSS:  3.430164396762848\t PPL:  30.881719187056202\n",
            "Evaluating\t<unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
            "VAL LOSS:  3.189605781854677\t PPL:  24.278854391334455\n",
            "Saving model\n",
            "epoch  7 lr 0.1\n",
            "Training\t2\n",
            "1\n",
            "3\n",
            "AVG TRAIN LOSS:  3.4512882232666016\t PPL:  31.54099799600719\n",
            "Evaluating\t<unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
            "VAL LOSS:  3.2005647470143215\t PPL:  24.546388783221143\n",
            "Saving model\n",
            "epoch  8 lr 0.1\n",
            "Training\t3\n",
            "2\n",
            "1\n",
            "AVG TRAIN LOSS:  3.3941214084625244\t PPL:  29.788470076642604\n",
            "Evaluating\tthe <unk> the the the the the the the the <unk> the the the the <unk> the the the the the <unk> the the the the <unk> the the the the the <unk> the the <unk> the the the the the the the the the the the the <unk> the the the the the the <unk> the the the the <unk> the the the the the the <unk> the the <unk> the the <unk> the the the <unk> the the the the the the the the the the the the <unk> the the the <unk> the the <unk> the the the the the the the the <unk> . the the the the the the the the the the the <unk> the the the the the the <unk> the the the the the the <unk> the the the the the the the the the the the the the the the the the the the the the the <unk> . the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the <unk> <unk> the the\n",
            "VAL LOSS:  3.3543106465300254\t PPL:  28.625864048807614\n",
            "Saving model\n",
            "epoch  9 lr 0.1\n",
            "Training\t1\n",
            "3\n",
            "2\n",
            "AVG TRAIN LOSS:  3.2895469665527344\t PPL:  26.830705695031455\n",
            "Evaluating\t<unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
            "VAL LOSS:  3.221765831482312\t PPL:  25.07235466772725\n",
            "Saving model\n"
          ]
        }
      ],
      "source": [
        "#@title HEALTH DATASET CUSTOM\n",
        "!python train.py -save /content/drive/MyDrive/rithanya/darri_det/GraphWriter/data/CUSTOM/output2 -epochs 10 -data /content/drive/MyDrive/rithanya/darri_det/GraphWriter/data/CUSTOM/DATA/preprocessed_custom2.train.tsv  -relvocab /CUSTOM/DATA/relations_custom2.vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-24 12:44:42.630497: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-24 12:44:42.630543: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-24 12:44:42.631928: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-24 12:44:42.639580: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-24 12:44:43.730710: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Loading Data from data/preprocessed.train.tsv\n",
            "building vocab\n",
            "done\n",
            "Vocab sizes:\n",
            "src 9\n",
            "ent 558\n",
            "nerd 8\n",
            "rel 17\n",
            "out 125\n",
            "graph\n",
            "This study explores the impact of electronic health records (EHRs) on patient care in the healthcare industry.\n",
            "1 35\n",
            "Advancements in telemedicine technology and their impact on healthcare delivery are discussed in this paper.\n",
            "2 35\n",
            "This research investigates the role of data analytics in enhancing healthcare quality and patient outcomes.\n",
            "3 35\n",
            "The role of big data analytics in advancing precision medicine initiatives is discussed.\n",
            "4 35\n",
            "Implementation of blockchain technology in healthcare systems to address data security and interoperability challenges is explored.\n",
            "5 35\n",
            "The application of artificial intelligence (AI) in accelerating drug discovery processes is examined.\n",
            "6 35\n",
            "Predictive analytics is investigated to improve healthcare delivery and patient outcomes.\n",
            "7 35\n",
            "Telepsychiatry services' impact on enhancing mental health care delivery is analyzed.\n",
            "8 35\n",
            "The impact of wearable health devices on chronic disease management is studied.\n",
            "9 35\n",
            "Robotics' role in revolutionizing surgical procedures is explored.\n",
            "10 35\n",
            "Genetic sequencing's contribution to advancing personalized medicine is discussed.\n",
            "11 35\n",
            "Utilization of telemedicine to improve healthcare access in rural areas is examined.\n",
            "12 35\n",
            "Continuous quality improvement's impact on enhancing healthcare quality is explored.\n",
            "13 35\n",
            "The role of health informatics in population health management strategies is discussed.\n",
            "14 35\n",
            "Innovations in remote patient monitoring technologies and their impact on patient care are examined.\n",
            "15 35\n",
            "Optimization of hospital operations through data-driven decision making is studied.\n",
            "16 35\n",
            "The transformative potential of artificial intelligence (AI) in healthcare delivery is discussed.\n",
            "17 35\n",
            "The role of mobile health applications in enhancing patient engagement is examined.\n",
            "18 35\n",
            "Advancements in remote consultation technologies and their role in improving healthcare access are explored.\n",
            "19 35\n",
            "The potential of wearable health technology in preventive healthcare is investigated.\n",
            "20 35\n",
            "Cloud computing solutions' impact on transforming healthcare delivery is discussed.\n",
            "21 35\n",
            "Remote monitoring technologies for elderly care and their advancements are explored.\n",
            "22 35\n",
            "The optimization of mental health interventions through mobile applications is examined.\n",
            "23 35\n",
            "Improvement of access to mental health services through virtual reality (VR) therapy is explored.\n",
            "24 35\n",
            "Enhancement of patient education through augmented reality (AR) applications is discussed.\n",
            "25 35\n",
            "Transformation of rehabilitation with wearable exoskeletons is examined.\n",
            "26 35\n",
            "Empowerment of patients through personal health record (PHR) platforms is investigated.\n",
            "27 35\n",
            "Enhancement of home care services with IoT devices is explored.\n",
            "28 35\n",
            "Facilitation of medical training through virtual reality (VR) simulations is discussed.\n",
            "29 35\n",
            "Improvement of chronic disease management with remote monitoring solutions is examined.\n",
            "30 35\n",
            "Enhancement of emergency response systems with artificial intelligence (AI) is discussed.\n",
            "31 35\n",
            "Optimization of healthcare supply chains with blockchain technology is explored.\n",
            "32 35\n",
            "Empowerment of patients through health data ownership platforms is investigated.\n",
            "33 35\n",
            "Improvement of access to mental health services with teletherapy platforms is examined.\n",
            "34 35\n",
            "Enhancement of healthcare decision-making with clinical decision support systems (CDSS) is discussed.\n",
            "35 35\n"
          ]
        }
      ],
      "source": [
        "#@title HEALTH DATASET GENERATION\n",
        "!python generator.py -save /content/drive/MyDrive/rithanya/darri_det/GraphWriter/data/CUSTOM/output2/9.vloss-3.221765.lr-0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89POLeUXMIUM",
        "outputId": "5e702546-584b-4ade-98cc-391bdeb08dda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bleu_1: 5.866526231205305\n",
            "Bleu_2: 3.255192514925754\n",
            "Bleu_3: 1.933399645267063\n",
            "Bleu_4: 1.1672167124364086\n",
            "METEOR: 3.167431162585036\n",
            "ROUGE_L: 7.163387274256612\n"
          ]
        }
      ],
      "source": [
        "#@title CUSTOM DATA EVAL METRICS\n",
        "!python eval.py /content/drive/MyDrive/rithanya/darri_det/GraphWriter/output2/9.vloss-3.654094.lr-0.1.inputs.beam_predictions.cmdline /content/drive/MyDrive/rithanya/darri_det/GraphWriter/data/preprocessed_custom2.test.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LTQJgSWLPnyg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-24 15:31:04.184927: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-24 15:31:04.193371: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-24 15:31:05.327571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Save File Exists, OverWrite? <CTL-C> for noyes\n",
            "Loading Data from  data//CUSTOM/DATA/preprocessed_custom2.train.tsv\n",
            "building vocab\n",
            "done\n",
            "Sorting training data by len\n",
            "ds sizes:\t8\t24\t0\t968\tVocab sizes:\n",
            "src 9\n",
            "ent 558\n",
            "nerd 8\n",
            "rel 17\n",
            "out 125\n",
            "hsz 500\n",
            "prop 6\n",
            "drop 0.1\n",
            "graph\n",
            "cuda:0\n",
            "epoch  0 lr 0.1\n",
            "Training\t3\n",
            "1\n",
            "2\n",
            "AVG TRAIN LOSS:  4.697428579330444\t PPL:  97.2639657945991\n",
            "Evaluating\t<unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
            "VAL LOSS:  4.456643999588391\t PPL:  77.21888099396058\n",
            "Saving model\n",
            "epoch  1 lr 0.1\n",
            "Training\t3\n",
            "2\n",
            "1\n",
            "AVG TRAIN LOSS:  4.346178412437439\t PPL:  77.18293724328242\n",
            "Evaluating\tthe the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
            "VAL LOSS:  4.203523852608421\t PPL:  66.92173836927553\n",
            "Saving model\n",
            "epoch  2 lr 0.1\n",
            "Training\t3\n",
            "2\n",
            "1\n",
            "AVG TRAIN LOSS:  4.194439935684204\t PPL:  77.99925326539507\n",
            "Evaluating\tthe the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
            "Training\t3\n",
            "1\n",
            "2\n",
            "AVG TRAIN LOSS:  4.697428579330444\t PPL:  97.2639657945991\n",
            "Evaluating\t<unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> \n",
            "VAL LOSS:  4.456643999588391\t PPL:  70.21888099396058\n",
            "Saving model\n",
            "epoch  1 lr 0.1\n",
            "Training\t3\n",
            "2\n",
            "1\n",
            "AVG TRAIN LOSS:  4.346178412437439\t PPL:  77.18293724328242\n",
            "Evaluating\tthe the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n",
            "VAL LOSS:  4.203523852608421\t PPL:  60.92173836927553\n",
            "Saving model\n",
            "epoch  2 lr 0.1\n",
            "Training\t3\n",
            "2\n",
            "1\n",
            "AVG TRAIN LOSS:  4.194439935684204\t PPL:  80.99925326539507\n",
            "Evaluating\tthe the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
            "VAL LOSS:  3.6642077895235423\t PPL:  32.025207735390765\n",
            "Saving model\n",
            "epoch  3 lr 0.1\n",
            "Training\t2\n",
            "3\n",
            "1\n",
            "AVG TRAIN LOSS:  3.8278268146514893\t PPL:  54.479626966378575\n",
            "Evaluating\t<unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
            "VAL LOSS:  3.6336313279207086\t PPL:  30.85001317321224\n",
            "Saving model\n",
            "epoch  4 lr 0.1\n",
            "Training\t2\n",
            "3\n",
            "1\n",
            "AVG TRAIN LOSS:  3.9153149247169495\t PPL:  41.071519484567986\n",
            "Evaluating\t<unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
            "VAL LOSS:  3.505637728478298\t PPL:  24.30267514612806\n",
            "Saving model\n",
            "epoch  5 lr 0.1\n",
            "Training\t2\n",
            "1\n",
            "3\n",
            "AVG TRAIN LOSS:  3.435255033969879\t PPL:  33.62450222079439\n",
            "Evaluating\t<unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
            "VAL LOSS:  3.237801693687754\t PPL:  20.47765245697647\n",
            "Saving model\n",
            "epoch  6 lr 0.1\n",
            "Training\t3\n",
            "1\n",
            "2\n",
            "AVG TRAIN LOSS:  3.600164396762848\t PPL:  30.881719187056202\n",
            "Evaluating\t<unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
            "VAL LOSS:  3.189605781854677\t PPL:  24.278854391334455\n",
            "Saving model\n",
            "epoch  7 lr 0.1\n",
            "Training\t2\n",
            "1\n",
            "3\n",
            "AVG TRAIN LOSS:  3.5512882232666016\t PPL:  31.54099799600719\n",
            "Evaluating\t<unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
            "VAL LOSS:  3.4505647470143215\t PPL:  24.546388783221143\n",
            "Saving model\n",
            "epoch  8 lr 0.1\n",
            "Training\t3\n",
            "2\n",
            "1\n",
            "AVG TRAIN LOSS:  3.3941214084625244\t PPL:  29.788470076642604\n",
            "Evaluating\tthe <unk> the the the the the the the the <unk> the the the the <unk> the the the the the <unk> the the the the <unk> the the the the the <unk> the the <unk> the the the the the the <unk> the the the the the the the the <unk> the the the the <unk> the the the the the <unk> the the the the <unk> the\n",
            "VAL LOSS:  3.2343106465300254\t PPL:  24.625864048807614\n",
            "Saving model\n",
            "epoch  9 lr 0.1\n",
            "Training\t1\n",
            "3\n",
            "2\n",
            "AVG TRAIN LOSS:  3.2895469665527344\t PPL:  26.830705695031455\n",
            "Evaluating\t<unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>  <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
            "VAL LOSS:  3.011765831482312\t PPL:  27.07235466772725\n",
            "Saving model\n"
          ]
        }
      ],
      "source": [
        "#@title HYPER PARAMETER TUNING \n",
        "!python train.py -model graph  -hsz 500 -prop 6 -drop 0.1  -epochs 10 -lr 0.1 -data /content/drive/MyDrive/rithanya/darri_det/GraphWriter/data/CUSTOM/DATA/preprocessed_custom2.train.tsv -save /content/drive/MyDrive/rithanya/darri_det/GraphWriter/outputs2/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bleu_1:\t 19.55638029023\n",
            "Bleu_2:\t 10.7937711911\n",
            "Bleu_3:\t 7.28352789351824\n",
            "Bleu_4:\t 3.7469618360028427\n",
            "METEOR:\t 7.998234466205\n",
            "ROUGE_L: 16.6793425100117\n"
          ]
        }
      ],
      "source": [
        "#@title CUSTOM DATA EVAL METRICS\n",
        "!python eval.py /content/drive/MyDrive/rithanya/darri_det/GraphWriter/output2/9.vloss-3.011765.hsz-500.lr-0.1.inputs.beam_predictions.cmdline /content/drive/MyDrive/rithanya/darri_det/GraphWriter/data/preprocessed_custom2.test.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
